INFO 10-24 15:30:40 __init__.py:179] Automatically detected platform rocm.
WARNING 10-24 15:30:40 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-24 15:30:42] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 15:30:42] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.9, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', elastic_ep_backend=None, mooncake_ib_device=None, tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=205627743, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_beta_spec=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-24 15:30:43] Using default HuggingFace chat template with detected content format: string
INFO 10-24 15:30:50 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 15:30:50 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 15:30:50 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 15:30:50 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 15:30:50 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 15:30:50 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 15:30:50 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 15:30:51 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-24 15:30:52] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 15:30:52] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 15:30:52 TP6] Process 293 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-24 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 15:30:53 TP2] Process 289 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-24 15:30:53 TP6] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-24 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-24 15:30:53 TP2] Init torch distributed begin.
[2025-10-24 15:30:53 TP4] Process 291 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-24 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-24 15:30:53 TP5] Process 292 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-10-24 15:30:53 TP0] Process 287 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-24 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 15:30:53 TP1] Process 288 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-24 15:30:53 TP3] Process 290 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-24 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 15:30:53 TP7] Process 294 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-24 15:30:53 TP4] Init torch distributed begin.
[2025-10-24 15:30:53 TP5] Init torch distributed begin.
[2025-10-24 15:30:53 TP0] Init torch distributed begin.
[2025-10-24 15:30:53 TP1] Init torch distributed begin.
[2025-10-24 15:30:54 TP3] Init torch distributed begin.
[2025-10-24 15:30:54 TP7] Init torch distributed begin.
[2025-10-24 15:30:54 TP0] sglang is using nccl==2.21.5
[2025-10-24 15:30:56 TP0] Init torch distributed ends. mem usage=3.63 GB
[2025-10-24 15:30:56 TP5] Init torch distributed ends. mem usage=3.91 GB
[2025-10-24 15:30:56 TP7] Init torch distributed ends. mem usage=3.92 GB
[2025-10-24 15:30:56 TP4] Init torch distributed ends. mem usage=3.99 GB
[2025-10-24 15:30:56 TP3] Init torch distributed ends. mem usage=4.04 GB
[2025-10-24 15:30:56 TP2] Init torch distributed ends. mem usage=4.05 GB
[2025-10-24 15:30:56 TP6] Init torch distributed ends. mem usage=3.93 GB
[2025-10-24 15:30:56 TP1] Init torch distributed ends. mem usage=4.05 GB
[2025-10-24 15:30:58 TP6] Load weight begin. avail mem=187.33 GB
[2025-10-24 15:30:58 TP7] Load weight begin. avail mem=187.34 GB
[2025-10-24 15:30:58 TP5] Load weight begin. avail mem=187.35 GB
[2025-10-24 15:30:58 TP4] Load weight begin. avail mem=187.27 GB
[2025-10-24 15:30:58 TP3] Load weight begin. avail mem=187.22 GB
[2025-10-24 15:30:58 TP2] Load weight begin. avail mem=187.21 GB
[2025-10-24 15:30:58 TP1] Load weight begin. avail mem=187.21 GB
[2025-10-24 15:30:58 TP0] Load weight begin. avail mem=187.63 GB
[2025-10-24 15:30:58 TP0] Detected fp8 checkpoint.
[2025-10-24 15:30:58 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:45,  3.59it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:55,  2.89it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:44,  3.57it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:01<00:42,  3.78it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:01<00:41,  3.84it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:01<00:28,  5.38it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:01<00:29,  5.23it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:01<00:26,  5.86it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:02<00:26,  5.80it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:02<00:44,  3.42it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:02<00:37,  4.03it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:02<00:31,  4.69it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:03<00:31,  4.75it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:03<00:27,  5.44it/s]
Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:03<00:24,  6.00it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:03<00:23,  6.25it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:03<00:25,  5.79it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:04<00:28,  5.03it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:04<00:25,  5.63it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:04<00:22,  6.32it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:04<00:24,  5.78it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:04<00:22,  6.19it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:04<00:21,  6.56it/s]
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:04<00:19,  6.97it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:04<00:19,  6.97it/s]
Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:05<00:24,  5.55it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:05<00:23,  5.80it/s]
Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:05<00:21,  6.20it/s]
Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:05<00:19,  6.66it/s]
Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:06<00:27,  4.86it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:06<00:19,  6.65it/s]
Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:06<00:19,  6.65it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:06<00:27,  4.59it/s]
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:06<00:26,  4.75it/s]
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:07<00:39,  3.22it/s]
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:07<00:32,  3.87it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:07<00:30,  4.02it/s]
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:08<00:27,  4.49it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:08<00:28,  4.31it/s]
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:08<00:26,  4.49it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:08<00:29,  4.10it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:08<00:19,  6.04it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:09<00:11,  9.99it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:09<00:08, 13.07it/s]
Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:09<00:06, 15.98it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:09<00:05, 17.69it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:09<00:05, 19.04it/s]
Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:09<00:04, 21.60it/s]
Loading safetensors checkpoint shards:  41% Completed | 67/163 [00:09<00:04, 21.79it/s]
Loading safetensors checkpoint shards:  43% Completed | 70/163 [00:10<00:09,  9.42it/s]
Loading safetensors checkpoint shards:  44% Completed | 72/163 [00:10<00:08, 10.49it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:10<00:07, 12.39it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:10<00:05, 15.15it/s]
Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:11<00:04, 17.38it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:11<00:04, 18.73it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:11<00:03, 20.51it/s]
Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:11<00:03, 22.24it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:11<00:02, 24.57it/s]
Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:11<00:02, 24.28it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:11<00:02, 23.25it/s]
Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:11<00:02, 24.66it/s]
Loading safetensors checkpoint shards:  65% Completed | 106/163 [00:12<00:05,  9.60it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:12<00:05, 10.60it/s]
Loading safetensors checkpoint shards:  68% Completed | 111/163 [00:12<00:04, 12.89it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:13<00:03, 15.50it/s]
Loading safetensors checkpoint shards:  72% Completed | 117/163 [00:13<00:02, 17.91it/s]
Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:13<00:02, 19.92it/s]
Loading safetensors checkpoint shards:  75% Completed | 123/163 [00:13<00:01, 21.16it/s]
Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:13<00:01, 22.76it/s]
Loading safetensors checkpoint shards:  80% Completed | 130/163 [00:13<00:01, 23.39it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:13<00:01, 17.12it/s]
Loading safetensors checkpoint shards:  84% Completed | 137/163 [00:14<00:01, 20.25it/s]
Loading safetensors checkpoint shards:  86% Completed | 140/163 [00:14<00:01, 20.99it/s]
Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:14<00:00, 21.57it/s]
Loading safetensors checkpoint shards:  90% Completed | 146/163 [00:14<00:00, 22.65it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:14<00:00, 22.85it/s]
Loading safetensors checkpoint shards:  93% Completed | 152/163 [00:14<00:00, 23.20it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:15<00:00, 10.65it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:15<00:00, 11.78it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:15<00:00, 13.02it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:15<00:00, 14.17it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:15<00:00, 13.22it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:15<00:00, 10.30it/s]

[2025-10-24 15:32:04 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.64 GB, mem usage=79.56 GB.
[2025-10-24 15:32:04 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.65 GB, mem usage=79.56 GB.
[2025-10-24 15:32:04 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.06 GB, mem usage=79.56 GB.
[2025-10-24 15:32:04 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.65 GB, mem usage=79.56 GB.
[2025-10-24 15:32:04 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.78 GB, mem usage=79.56 GB.
[2025-10-24 15:32:04 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.70 GB, mem usage=79.56 GB.
[2025-10-24 15:32:04 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.77 GB, mem usage=79.56 GB.
[2025-10-24 15:32:04 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.79 GB, mem usage=79.56 GB.
[2025-10-24 15:32:04 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-24 15:32:04 TP6] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-24 15:32:04 TP6] Memory pool end. avail mem=43.52 GB
[2025-10-24 15:32:04 TP5] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-24 15:32:04 TP5] Memory pool end. avail mem=43.54 GB
[2025-10-24 15:32:04 TP7] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-24 15:32:04 TP7] Memory pool end. avail mem=43.53 GB
[2025-10-24 15:32:04 TP3] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-24 15:32:04 TP3] Memory pool end. avail mem=43.40 GB
[2025-10-24 15:32:04 TP0] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-24 15:32:04 TP0] Memory pool end. avail mem=43.81 GB
[2025-10-24 15:32:04 TP1] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-24 15:32:04 TP1] Memory pool end. avail mem=43.40 GB
[2025-10-24 15:32:04 TP2] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-24 15:32:04 TP2] Memory pool end. avail mem=43.39 GB
[2025-10-24 15:32:04 TP4] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-24 15:32:04 TP4] Memory pool end. avail mem=43.45 GB
[2025-10-24 15:32:07 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.33 GB
[2025-10-24 15:32:07 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.19 GB
[2025-10-24 15:32:07 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.19 GB
[2025-10-24 15:32:07 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.33 GB
[2025-10-24 15:32:07 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.20 GB
[2025-10-24 15:32:07 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.61 GB
[2025-10-24 15:32:07 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-10-24 15:32:07 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.32 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.97 GB):   0%|          | 0/52 [00:00<?, ?it/s][2025-10-24 15:32:07 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.25 GB
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 15:32:09 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 15:32:09 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 15:32:09 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 15:32:09 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:09 TP6] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:09 TP5] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:09 TP7] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:09 TP4] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 15:32:10 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 15:32:10 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 15:32:10 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 15:32:10 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:10 TP6] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:10 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:10 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:10 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:10 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:10 TP5] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:10 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:10 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:10 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:10 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:10 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:10 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:11 TP7] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:11 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:11 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:11 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:11 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:11 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 15:32:11 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:11 TP1] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:11 TP4] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:11 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:11 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:11 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:11 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:11 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 15:32:13 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 15:32:13 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:13 TP0] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 15:32:13 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 15:32:14 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:14 TP1] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:14 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:14 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:14 TP2] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:14 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:14 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:14 TP3] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:14 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 15:32:15 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP0] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:16 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 15:32:16 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 15:32:16 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP3] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP2] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:16 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:16 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:16 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
Capturing batches (bs=512 avail_mem=42.97 GB):   2%|         | 1/52 [00:09<07:59,  9.40s/it]Capturing batches (bs=496 avail_mem=42.31 GB):   2%|         | 1/52 [00:09<07:59,  9.40s/it][aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP6] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP7] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP5] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP4] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP1] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP0] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP3] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP2] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=496 avail_mem=42.31 GB):   4%|         | 2/52 [00:10<03:39,  4.40s/it]Capturing batches (bs=480 avail_mem=42.30 GB):   4%|         | 2/52 [00:10<03:39,  4.40s/it][aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP1] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP3] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP2] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP7] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP6] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP5] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP4] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:17 TP0] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=480 avail_mem=42.30 GB):   6%|         | 3/52 [00:10<02:05,  2.57s/it]Capturing batches (bs=464 avail_mem=42.30 GB):   6%|         | 3/52 [00:10<02:05,  2.57s/it][aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP7] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP6] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP5] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP4] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP2] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP1] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP0] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP3] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=464 avail_mem=42.30 GB):   8%|         | 4/52 [00:11<01:22,  1.71s/it]Capturing batches (bs=448 avail_mem=42.29 GB):   8%|         | 4/52 [00:11<01:22,  1.71s/it][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:18 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=448 avail_mem=42.29 GB):  10%|         | 5/52 [00:11<00:58,  1.24s/it]Capturing batches (bs=432 avail_mem=42.29 GB):  10%|         | 5/52 [00:11<00:58,  1.24s/it][aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP3] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP5] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP6] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP7] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP0] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP1] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP2] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP4] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=432 avail_mem=42.29 GB):  12%|        | 6/52 [00:11<00:43,  1.05it/s]Capturing batches (bs=416 avail_mem=42.28 GB):  12%|        | 6/52 [00:11<00:43,  1.05it/s][aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP1] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP2] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP3] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP7] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP6] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP0] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP5] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP4] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=416 avail_mem=42.28 GB):  13%|        | 7/52 [00:12<00:34,  1.29it/s]Capturing batches (bs=400 avail_mem=42.28 GB):  13%|        | 7/52 [00:12<00:34,  1.29it/s][aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP4] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP6] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP2] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP1] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP3] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP5] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP7] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:19 TP0] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=400 avail_mem=42.28 GB):  15%|        | 8/52 [00:12<00:28,  1.52it/s]Capturing batches (bs=384 avail_mem=42.27 GB):  15%|        | 8/52 [00:12<00:28,  1.52it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=384 avail_mem=42.27 GB):  17%|        | 9/52 [00:12<00:22,  1.88it/s]Capturing batches (bs=368 avail_mem=42.27 GB):  17%|        | 9/52 [00:12<00:22,  1.88it/s][aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP3] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP1] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP0] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP5] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP2] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP4] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP6] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP7] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=368 avail_mem=42.27 GB):  19%|        | 10/52 [00:13<00:20,  2.03it/s]Capturing batches (bs=352 avail_mem=42.27 GB):  19%|        | 10/52 [00:13<00:20,  2.03it/s][aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP0] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP1] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP5] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:20 TP4] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP6] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP3] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP2] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP7] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=352 avail_mem=42.27 GB):  21%|        | 11/52 [00:13<00:19,  2.14it/s]Capturing batches (bs=336 avail_mem=42.26 GB):  21%|        | 11/52 [00:13<00:19,  2.14it/s][aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP2] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP6] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP3] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP0] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP1] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP7] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP4] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP5] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=336 avail_mem=42.26 GB):  23%|       | 12/52 [00:14<00:18,  2.22it/s]Capturing batches (bs=320 avail_mem=42.26 GB):  23%|       | 12/52 [00:14<00:18,  2.22it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:21 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=320 avail_mem=42.26 GB):  25%|       | 13/52 [00:14<00:15,  2.55it/s]Capturing batches (bs=304 avail_mem=42.25 GB):  25%|       | 13/52 [00:14<00:15,  2.55it/s][aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP6] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP7] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP4] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP5] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP1] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP2] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP3] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP0] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=304 avail_mem=42.25 GB):  27%|       | 14/52 [00:14<00:15,  2.52it/s]Capturing batches (bs=288 avail_mem=42.25 GB):  27%|       | 14/52 [00:14<00:15,  2.52it/s][aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP0] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP2] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP6] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP3] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP1] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP5] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP7] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP4] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=288 avail_mem=42.25 GB):  29%|       | 15/52 [00:15<00:13,  2.82it/s]Capturing batches (bs=272 avail_mem=42.24 GB):  29%|       | 15/52 [00:15<00:13,  2.82it/s][aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP5] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP3] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP0] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP2] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP4] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP1] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP7] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:22 TP6] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=272 avail_mem=42.24 GB):  31%|       | 16/52 [00:15<00:13,  2.69it/s]Capturing batches (bs=256 avail_mem=42.24 GB):  31%|       | 16/52 [00:15<00:13,  2.69it/s][aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP0] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP3] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP1] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP2] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP5] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP7] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP4] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP6] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP0] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP1] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:23 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP2] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:23 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:23 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP7] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:23 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP6] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:23 TP3] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:23 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP5] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:23 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP4] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:23 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:23 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=256 avail_mem=42.24 GB):  33%|      | 17/52 [00:15<00:13,  2.62it/s]Capturing batches (bs=248 avail_mem=42.23 GB):  33%|      | 17/52 [00:15<00:13,  2.62it/s][aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP3] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP6] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP2] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP1] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP5] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP4] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP7] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP0] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=248 avail_mem=42.23 GB):  35%|      | 18/52 [00:16<00:13,  2.55it/s]Capturing batches (bs=240 avail_mem=42.23 GB):  35%|      | 18/52 [00:16<00:13,  2.55it/s][aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP1] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP3] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP0] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP2] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP5] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP4] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP6] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:23 TP7] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=240 avail_mem=42.23 GB):  37%|      | 19/52 [00:16<00:13,  2.53it/s]Capturing batches (bs=232 avail_mem=42.22 GB):  37%|      | 19/52 [00:16<00:13,  2.53it/s][aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP5] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP6] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP4] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP1] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP7] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP0] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP2] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP3] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=232 avail_mem=42.22 GB):  38%|      | 20/52 [00:17<00:12,  2.51it/s]Capturing batches (bs=224 avail_mem=42.22 GB):  38%|      | 20/52 [00:17<00:12,  2.51it/s][aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP0] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP1] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP2] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP4] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP7] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP3] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP6] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:24 TP5] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=224 avail_mem=42.22 GB):  40%|      | 21/52 [00:17<00:12,  2.50it/s]Capturing batches (bs=216 avail_mem=42.21 GB):  40%|      | 21/52 [00:17<00:12,  2.50it/s][aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP7] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP0] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP4] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP2] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP3] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP1] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP6] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP5] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=216 avail_mem=42.21 GB):  42%|     | 22/52 [00:17<00:12,  2.48it/s]Capturing batches (bs=208 avail_mem=42.21 GB):  42%|     | 22/52 [00:17<00:12,  2.48it/s][aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP2] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP0] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP5] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP3] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP6] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP1] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP4] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP7] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=208 avail_mem=42.21 GB):  44%|     | 23/52 [00:18<00:10,  2.75it/s]Capturing batches (bs=200 avail_mem=42.20 GB):  44%|     | 23/52 [00:18<00:10,  2.75it/s][aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP1] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP6] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP5] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP4] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP0] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP3] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP2] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:25 TP7] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=200 avail_mem=42.20 GB):  46%|     | 24/52 [00:18<00:10,  2.65it/s]Capturing batches (bs=192 avail_mem=42.20 GB):  46%|     | 24/52 [00:18<00:10,  2.65it/s][aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=192 avail_mem=42.20 GB):  48%|     | 25/52 [00:18<00:09,  2.89it/s]Capturing batches (bs=184 avail_mem=42.20 GB):  48%|     | 25/52 [00:18<00:09,  2.89it/s][aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP0] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP2] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP3] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP1] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP7] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP5] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP4] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP6] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=184 avail_mem=42.20 GB):  50%|     | 26/52 [00:19<00:08,  3.12it/s]Capturing batches (bs=176 avail_mem=42.19 GB):  50%|     | 26/52 [00:19<00:08,  3.12it/s][aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP0] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP3] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP1] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP2] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP7] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP5] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP6] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:26 TP4] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=176 avail_mem=42.19 GB):  52%|    | 27/52 [00:19<00:07,  3.29it/s]Capturing batches (bs=168 avail_mem=42.19 GB):  52%|    | 27/52 [00:19<00:07,  3.29it/s][aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP3] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP1] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP0] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP2] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP7] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP4] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP6] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP5] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=168 avail_mem=42.19 GB):  54%|    | 28/52 [00:19<00:08,  2.99it/s]Capturing batches (bs=160 avail_mem=42.19 GB):  54%|    | 28/52 [00:19<00:08,  2.99it/s][aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP0] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP2] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP3] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP5] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP6] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP1] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP7] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP4] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=160 avail_mem=42.19 GB):  56%|    | 29/52 [00:20<00:07,  3.20it/s]Capturing batches (bs=152 avail_mem=42.18 GB):  56%|    | 29/52 [00:20<00:07,  3.20it/s][aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP1] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP0] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP6] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP5] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP2] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP7] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP3] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:27 TP4] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=152 avail_mem=42.18 GB):  58%|    | 30/52 [00:20<00:07,  2.94it/s]Capturing batches (bs=144 avail_mem=42.18 GB):  58%|    | 30/52 [00:20<00:07,  2.94it/s][aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP0] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP2] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP3] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP6] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP7] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP5] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP1] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP4] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=144 avail_mem=42.18 GB):  60%|    | 31/52 [00:20<00:06,  3.13it/s]Capturing batches (bs=136 avail_mem=42.17 GB):  60%|    | 31/52 [00:20<00:06,  3.13it/s][aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP0] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP2] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP3] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP1] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP6] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP7] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP5] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP4] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=136 avail_mem=42.17 GB):  62%|   | 32/52 [00:21<00:06,  3.32it/s]Capturing batches (bs=128 avail_mem=42.17 GB):  62%|   | 32/52 [00:21<00:06,  3.32it/s][aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:28 TP3] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:28 TP1] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:28 TP0] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:28 TP2] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:28 TP6] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:28 TP7] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:28 TP5] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:28 TP4] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP3] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP0] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP1] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP2] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP6] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP7] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP5] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP4] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:28 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:28 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:28 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:28 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:28 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:28 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:28 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:28 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:28 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=128 avail_mem=42.17 GB):  63%|   | 33/52 [00:21<00:05,  3.43it/s]Capturing batches (bs=120 avail_mem=42.17 GB):  63%|   | 33/52 [00:21<00:05,  3.43it/s][aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP3] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP4] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP5] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP7] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP0] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP6] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP2] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:28 TP1] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.17 GB):  65%|   | 34/52 [00:21<00:05,  3.07it/s]Capturing batches (bs=112 avail_mem=42.16 GB):  65%|   | 34/52 [00:21<00:05,  3.07it/s][aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP0] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP2] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP1] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP3] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP7] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP6] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP4] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP5] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.16 GB):  67%|   | 35/52 [00:21<00:05,  3.27it/s]Capturing batches (bs=104 avail_mem=42.16 GB):  67%|   | 35/52 [00:21<00:05,  3.27it/s][aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP3] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP5] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP1] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP6] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP7] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP0] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP2] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP4] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.16 GB):  69%|   | 36/52 [00:22<00:05,  2.98it/s]Capturing batches (bs=96 avail_mem=42.16 GB):  69%|   | 36/52 [00:22<00:05,  2.98it/s] [aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:29 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.16 GB):  71%|   | 37/52 [00:22<00:04,  3.19it/s]Capturing batches (bs=88 avail_mem=42.15 GB):  71%|   | 37/52 [00:22<00:04,  3.19it/s][aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP1] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP2] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP0] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP4] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP7] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP5] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP6] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP3] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.15 GB):  73%|  | 38/52 [00:23<00:04,  2.93it/s]Capturing batches (bs=80 avail_mem=42.15 GB):  73%|  | 38/52 [00:23<00:04,  2.93it/s][aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP0] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP2] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP3] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP1] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP6] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP5] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP4] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP7] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.15 GB):  75%|  | 39/52 [00:23<00:04,  3.15it/s]Capturing batches (bs=72 avail_mem=42.14 GB):  75%|  | 39/52 [00:23<00:04,  3.15it/s][aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP4] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP6] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP7] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP5] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP0] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP3] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP2] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:30 TP1] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.14 GB):  77%|  | 40/52 [00:23<00:04,  2.90it/s]Capturing batches (bs=64 avail_mem=42.14 GB):  77%|  | 40/52 [00:23<00:04,  2.90it/s][aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:31 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:31 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:31 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:31 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:31 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:31 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:31 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:32:31 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:31 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:31 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:31 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:31 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:31 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:31 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:31 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:31 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:32:31 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.14 GB):  79%|  | 41/52 [00:23<00:03,  3.12it/s]Capturing batches (bs=56 avail_mem=42.13 GB):  79%|  | 41/52 [00:23<00:03,  3.12it/s][aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP6] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP5] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP2] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP3] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP0] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP7] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP4] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP1] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.13 GB):  81%|  | 42/52 [00:24<00:03,  2.86it/s]Capturing batches (bs=48 avail_mem=42.13 GB):  81%|  | 42/52 [00:24<00:03,  2.86it/s][aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP0] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP2] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP1] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP3] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP6] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP5] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP7] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:31 TP4] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.13 GB):  83%| | 43/52 [00:24<00:02,  3.10it/s]Capturing batches (bs=40 avail_mem=42.12 GB):  83%| | 43/52 [00:24<00:02,  3.10it/s][aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP3] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP0] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP1] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP2] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP6] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP4] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP5] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP7] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.12 GB):  85%| | 44/52 [00:25<00:02,  2.88it/s]Capturing batches (bs=32 avail_mem=42.12 GB):  85%| | 44/52 [00:25<00:02,  2.88it/s][aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:32 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=32 avail_mem=42.12 GB):  87%| | 45/52 [00:25<00:02,  3.10it/s]Capturing batches (bs=24 avail_mem=42.11 GB):  87%| | 45/52 [00:25<00:02,  3.10it/s][aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP6] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP4] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP3] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP5] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP7] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP2] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP1] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:32 TP0] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=24 avail_mem=42.11 GB):  88%| | 46/52 [00:25<00:02,  2.87it/s]Capturing batches (bs=16 avail_mem=42.11 GB):  88%| | 46/52 [00:25<00:02,  2.87it/s][aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:32:33 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=16 avail_mem=42.11 GB):  90%| | 47/52 [00:26<00:01,  3.09it/s]Capturing batches (bs=12 avail_mem=42.11 GB):  90%| | 47/52 [00:26<00:01,  3.09it/s][aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP0] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP2] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP3] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP1] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP6] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP5] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP4] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP7] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=12 avail_mem=42.11 GB):  92%|| 48/52 [00:26<00:01,  3.26it/s]Capturing batches (bs=8 avail_mem=42.10 GB):  92%|| 48/52 [00:26<00:01,  3.26it/s] [aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:33 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=8 avail_mem=42.10 GB):  94%|| 49/52 [00:26<00:00,  3.44it/s]Capturing batches (bs=4 avail_mem=42.10 GB):  94%|| 49/52 [00:26<00:00,  3.44it/s][aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP0] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP2] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP6] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP3] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP1] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP5] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP7] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP4] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=4 avail_mem=42.10 GB):  96%|| 50/52 [00:26<00:00,  3.57it/s]Capturing batches (bs=2 avail_mem=42.10 GB):  96%|| 50/52 [00:26<00:00,  3.57it/s][aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP0] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP2] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP3] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP1] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP6] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP5] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP7] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP4] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=2 avail_mem=42.10 GB):  98%|| 51/52 [00:27<00:00,  3.67it/s]Capturing batches (bs=1 avail_mem=42.09 GB):  98%|| 51/52 [00:27<00:00,  3.67it/s][aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:34 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:35 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:35 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:35 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:35 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:35 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:35 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=1 avail_mem=42.09 GB): 100%|| 52/52 [00:27<00:00,  2.41it/s]Capturing batches (bs=1 avail_mem=42.09 GB): 100%|| 52/52 [00:27<00:00,  1.87it/s]
[2025-10-24 15:32:35 TP0] Registering 6396 cuda graph addresses
[2025-10-24 15:32:36 TP0] Capture cuda graph end. Time elapsed: 28.82 s. mem usage=1.52 GB. avail mem=42.09 GB.
[2025-10-24 15:32:36 TP3] Capture cuda graph end. Time elapsed: 28.85 s. mem usage=1.52 GB. avail mem=41.68 GB.
[2025-10-24 15:32:36 TP6] Capture cuda graph end. Time elapsed: 28.82 s. mem usage=1.52 GB. avail mem=41.79 GB.
[2025-10-24 15:32:36 TP1] Capture cuda graph end. Time elapsed: 28.87 s. mem usage=1.52 GB. avail mem=41.67 GB.
[2025-10-24 15:32:36 TP2] Capture cuda graph end. Time elapsed: 28.94 s. mem usage=1.52 GB. avail mem=41.66 GB.
[2025-10-24 15:32:36 TP7] Capture cuda graph end. Time elapsed: 28.93 s. mem usage=1.52 GB. avail mem=41.80 GB.
[2025-10-24 15:32:36 TP4] Capture cuda graph end. Time elapsed: 28.49 s. mem usage=1.52 GB. avail mem=41.72 GB.
[2025-10-24 15:32:36 TP5] Capture cuda graph end. Time elapsed: 29.04 s. mem usage=1.52 GB. avail mem=41.81 GB.
[2025-10-24 15:32:36 TP0] max_total_num_tokens=971748, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=42.09 GB
[2025-10-24 15:32:36] INFO:     Started server process [47]
[2025-10-24 15:32:36] INFO:     Waiting for application startup.
[2025-10-24 15:32:36] INFO:     Application startup complete.
[2025-10-24 15:32:36] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-24 15:32:37] INFO:     127.0.0.1:43316 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-24 15:32:37 TP0] Prefill batch [1], #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:32:38] INFO:     127.0.0.1:43326 - "GET /get_model_info HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:38 TP6] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:38 TP7] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:38 TP5] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:38 TP3] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:38 TP0] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:38 TP1] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:38 TP2] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:38 TP4] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:42] INFO:     127.0.0.1:43318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:32:42] The server is fired up and ready to roll!
[2025-10-24 15:32:46] INFO:     127.0.0.1:36562 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-24 15:32:46 TP0] Prefill batch [10], #new-seq: 1, #new-token: 666, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP0] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP3] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP1] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP2] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP6] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP5] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP7] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP4] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46] INFO:     127.0.0.1:36578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:32:46 TP0] Prefill batch [11], #new-seq: 1, #new-token: 67, #cached-token: 667, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP3] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP7] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP6] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP0] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP5] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP4] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP2] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP1] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP0] Prefill batch [12], #new-seq: 45, #new-token: 2700, #cached-token: 30015, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:32:46 TP0] Prefill batch [13], #new-seq: 131, #new-token: 8110, #cached-token: 87639, token usage: 0.00, #running-req: 46, #queue-req: 0, 
[2025-10-24 15:32:46 TP0] Prefill batch [14], #new-seq: 135, #new-token: 7886, #cached-token: 90364, token usage: 0.01, #running-req: 177, #queue-req: 0, 
[2025-10-24 15:32:48 TP0] Prefill batch [15], #new-seq: 278, #new-token: 16380, #cached-token: 186116, token usage: 0.02, #running-req: 312, #queue-req: 46, 
[2025-10-24 15:32:49 TP0] Prefill batch [16], #new-seq: 274, #new-token: 16353, #cached-token: 183503, token usage: 0.04, #running-req: 590, #queue-req: 100, 
[2025-10-24 15:33:03 TP0] Prefill batch [17], #new-seq: 160, #new-token: 9988, #cached-token: 107165, token usage: 0.05, #running-req: 864, #queue-req: 295, 
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP6] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP6] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP1] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP1] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP2] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP2] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP3] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP3] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP0] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP0] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP5] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP5] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP4] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP4] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP7] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP7] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 15:33:04 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:04 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:33:08] INFO:     127.0.0.1:39324 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP2] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP4] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP6] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP0] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP3] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP7] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP1] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP5] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP0] Prefill batch [46], #new-seq: 1, #new-token: 43, #cached-token: 669, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP2] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP6] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP4] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP7] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP3] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP0] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP1] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP5] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:08 TP0] Decode batch [51], #running-req: 1024, #token: 95516, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1011.17, #queue-req: 294, 
[2025-10-24 15:33:09] INFO:     127.0.0.1:39378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:09 TP0] Prefill batch [55], #new-seq: 1, #new-token: 41, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP4] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP1] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP3] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP7] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP5] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP2] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP0] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP6] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09] INFO:     127.0.0.1:37128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:09] INFO:     127.0.0.1:40096 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP3] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP7] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP4] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP0] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP1] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP5] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP2] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP6] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09] INFO:     127.0.0.1:36612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:09] INFO:     127.0.0.1:37840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:09] INFO:     127.0.0.1:41160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:09] INFO:     127.0.0.1:42288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:09] INFO:     127.0.0.1:45018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:09 TP0] Prefill batch [61], #new-seq: 2, #new-token: 135, #cached-token: 1339, token usage: 0.11, #running-req: 1022, #queue-req: 291, 
[aiter] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP3] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP7] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP4] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP0] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP1] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP5] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP2] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:09 TP6] [fused_moe] using default for (135, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:37452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:38694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:39194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:42266 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP4] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP3] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP7] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP1] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP5] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP0] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP2] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP6] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP0] Prefill batch [63], #new-seq: 10, #new-token: 734, #cached-token: 6701, token usage: 0.11, #running-req: 1014, #queue-req: 281, 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP3] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP7] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP5] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP0] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP1] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP2] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP4] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP6] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10] INFO:     127.0.0.1:37484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:40930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10 TP0] Prefill batch [65], #new-seq: 2, #new-token: 139, #cached-token: 1340, token usage: 0.11, #running-req: 1022, #queue-req: 279, 
[aiter] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP0] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP3] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP7] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP4] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP1] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP5] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP2] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP6] [fused_moe] using default for (139, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10] INFO:     127.0.0.1:36782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:37120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:37230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:37252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:38892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:40868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:41312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:41390 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP4] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP0] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP3] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP7] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP1] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP5] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP2] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP6] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP0] Prefill batch [67], #new-seq: 8, #new-token: 450, #cached-token: 5363, token usage: 0.11, #running-req: 1016, #queue-req: 271, 
[aiter] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP2] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP7] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP1] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP0] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP3] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP5] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP6] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP4] [fused_moe] using default for (450, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10] INFO:     127.0.0.1:38128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:39760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:41530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:43378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:45158 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP0] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP4] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP3] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP1] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP7] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP5] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP2] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP6] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP0] Prefill batch [69], #new-seq: 5, #new-token: 231, #cached-token: 3351, token usage: 0.11, #running-req: 1019, #queue-req: 266, 
[aiter] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP0] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP1] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP7] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP3] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP5] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP4] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP2] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP6] [fused_moe] using default for (231, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10] INFO:     127.0.0.1:39624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:40258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:40992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:41738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10] INFO:     127.0.0.1:45148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:10 TP0] Prefill batch [71], #new-seq: 5, #new-token: 287, #cached-token: 3350, token usage: 0.11, #running-req: 1019, #queue-req: 261, 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP0] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP1] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP2] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP5] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP3] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP6] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP4] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:10 TP7] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11] INFO:     127.0.0.1:40858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:42550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11 TP0] Prefill batch [73], #new-seq: 2, #new-token: 89, #cached-token: 1338, token usage: 0.11, #running-req: 1022, #queue-req: 259, 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP1] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP2] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP7] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP0] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP3] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP6] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP5] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP4] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:36992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:37828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:39748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:39882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:41308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:44288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:44656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:45214 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP0] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP4] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP1] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP3] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP7] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP5] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP2] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP6] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP0] Prefill batch [75], #new-seq: 9, #new-token: 471, #cached-token: 6031, token usage: 0.11, #running-req: 1015, #queue-req: 250, 
[aiter] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP1] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP2] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP0] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP3] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP7] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP5] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP6] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP4] [fused_moe] using default for (471, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11] INFO:     127.0.0.1:36602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:37204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:38966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:39102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:39302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:42848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:43908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:44170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:44442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:44738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:44996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:45656 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP0] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP4] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP1] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP3] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP7] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP5] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP2] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP6] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP0] Prefill batch [77], #new-seq: 12, #new-token: 875, #cached-token: 8039, token usage: 0.11, #running-req: 1012, #queue-req: 238, 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP1] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP0] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP7] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP3] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP5] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP4] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP2] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP6] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP1] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP0] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP3] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP7] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP4] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP5] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP2] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP6] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11] INFO:     127.0.0.1:37662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:41638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:43680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:44174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11 TP0] Prefill batch [79], #new-seq: 4, #new-token: 203, #cached-token: 2681, token usage: 0.11, #running-req: 1020, #queue-req: 234, 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP3] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP7] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP0] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP4] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP1] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP2] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP5] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11 TP6] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:11] INFO:     127.0.0.1:37632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:40390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:40788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:40840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:41412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:42148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:44094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:11] INFO:     127.0.0.1:45490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12 TP0] Prefill batch [81], #new-seq: 8, #new-token: 537, #cached-token: 5362, token usage: 0.11, #running-req: 1016, #queue-req: 226, 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP0] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP7] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP1] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP3] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP2] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP5] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP4] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP6] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12] INFO:     127.0.0.1:38938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:40654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:40870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:42014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:42358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:43036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:44504 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP0] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP4] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP3] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP7] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP1] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP5] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP2] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP6] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP0] Prefill batch [83], #new-seq: 7, #new-token: 444, #cached-token: 4690, token usage: 0.12, #running-req: 1017, #queue-req: 219, 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP0] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP7] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP3] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP4] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP1] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP5] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP2] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP6] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12] INFO:     127.0.0.1:37324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:37396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:38602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:41752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:42982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:43860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:44446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12 TP0] Prefill batch [85], #new-seq: 8, #new-token: 399, #cached-token: 5355, token usage: 0.12, #running-req: 1016, #queue-req: 211, 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP0] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP3] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP7] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP4] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP1] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP2] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP5] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP6] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12] INFO:     127.0.0.1:37284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:37900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:40572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:42440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:43516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:44818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12 TP0] Prefill batch [87], #new-seq: 7, #new-token: 362, #cached-token: 4688, token usage: 0.12, #running-req: 1017, #queue-req: 204, 
[aiter] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP5] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP4] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP2] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP0] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP1] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP6] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP3] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP7] [fused_moe] using default for (362, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12] INFO:     127.0.0.1:37432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:38166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:41024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:42716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:43338 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP0] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP4] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP1] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP5] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP2] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP3] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP6] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP7] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:12 TP0] Prefill batch [89], #new-seq: 6, #new-token: 336, #cached-token: 4021, token usage: 0.12, #running-req: 1018, #queue-req: 198, 
[2025-10-24 15:33:12] INFO:     127.0.0.1:38226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:38262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:38300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:38458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:42572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:43038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:43468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:12] INFO:     127.0.0.1:45592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13 TP0] Prefill batch [91], #new-seq: 9, #new-token: 531, #cached-token: 6028, token usage: 0.12, #running-req: 1015, #queue-req: 189, 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP1] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP7] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP0] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP3] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP5] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP4] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP2] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP6] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:37956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:38374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:38404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:38556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:39156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:39830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:43566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:44310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13 TP0] Prefill batch [93], #new-seq: 9, #new-token: 552, #cached-token: 6028, token usage: 0.12, #running-req: 1015, #queue-req: 180, 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP1] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP0] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP3] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP7] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP5] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP4] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP2] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP6] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13] INFO:     127.0.0.1:37554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:38002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:39764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:40776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:41506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:42500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13 TP0] Prefill batch [95], #new-seq: 7, #new-token: 374, #cached-token: 4690, token usage: 0.12, #running-req: 1017, #queue-req: 173, 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP1] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP0] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP7] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP3] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP5] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP4] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP2] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP6] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13] INFO:     127.0.0.1:38710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:39052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:39418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:40998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:41074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:41292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:42424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:42632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:43242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:44262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13 TP0] Prefill batch [97], #new-seq: 10, #new-token: 502, #cached-token: 6698, token usage: 0.12, #running-req: 1014, #queue-req: 163, 
[aiter] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP1] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP0] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP7] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP3] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP5] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP4] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP2] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP6] [fused_moe] using default for (502, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13] INFO:     127.0.0.1:38012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:41256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:42900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:43178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:43998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13 TP0] Prefill batch [99], #new-seq: 7, #new-token: 375, #cached-token: 4690, token usage: 0.12, #running-req: 1017, #queue-req: 156, 
[aiter] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP0] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP1] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP7] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP3] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP5] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP4] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP2] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13 TP6] [fused_moe] using default for (375, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:13] INFO:     127.0.0.1:38278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:38496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:41104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:43008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:43664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:13] INFO:     127.0.0.1:44770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14 TP0] Prefill batch [101], #new-seq: 7, #new-token: 415, #cached-token: 4689, token usage: 0.12, #running-req: 1017, #queue-req: 149, 
[aiter] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP3] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP7] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP1] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP5] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP0] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP4] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP2] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP6] [fused_moe] using default for (415, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14] INFO:     127.0.0.1:36698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:36954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:38236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:38698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:39946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:41482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:42090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:42178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:42810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:42824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:43920 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP4] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP0] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP3] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP7] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP1] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP5] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP2] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP6] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP0] Prefill batch [103], #new-seq: 11, #new-token: 752, #cached-token: 7374, token usage: 0.12, #running-req: 1013, #queue-req: 138, 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP3] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP0] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP1] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP7] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP4] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP5] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP2] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP6] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14] INFO:     127.0.0.1:37982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:38268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:38682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:42930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:43512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:44124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:44478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:44524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:44692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14 TP0] Prefill batch [105], #new-seq: 9, #new-token: 454, #cached-token: 6029, token usage: 0.12, #running-req: 1015, #queue-req: 129, 
[aiter] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP5] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP4] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP1] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP3] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP7] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP2] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP0] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP6] [fused_moe] using default for (454, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14] INFO:     127.0.0.1:37056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:37480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:39044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:39144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:39262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:39440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:39798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:39888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:39980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:42576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:43460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:44550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:45042 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP0] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP1] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP4] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP3] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP7] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP5] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP2] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP6] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP0] Prefill batch [107], #new-seq: 13, #new-token: 935, #cached-token: 8710, token usage: 0.12, #running-req: 1011, #queue-req: 116, 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP7] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP0] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP3] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP4] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP1] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP5] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP2] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP6] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14] INFO:     127.0.0.1:36846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:37592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:37918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:39340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:39480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:40516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:40608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:41654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:41948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:44398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14] INFO:     127.0.0.1:44558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:14 TP0] Prefill batch [109], #new-seq: 11, #new-token: 741, #cached-token: 7373, token usage: 0.12, #running-req: 1013, #queue-req: 105, 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP7] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP3] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP1] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP0] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP5] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP4] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP2] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:14 TP6] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15] INFO:     127.0.0.1:37068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:37790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:38114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:38266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:38672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:38872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:39314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:39428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:40214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:41098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:41202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:41592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:42302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:43402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:43818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:44846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:45426 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP3] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP7] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP0] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP4] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP1] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP5] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP2] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP6] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP0] Prefill batch [111], #new-seq: 18, #new-token: 1098, #cached-token: 12062, token usage: 0.12, #running-req: 1006, #queue-req: 87, 
[2025-10-24 15:33:15] INFO:     127.0.0.1:37466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:37764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:42050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:44584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15 TP0] Prefill batch [113], #new-seq: 4, #new-token: 195, #cached-token: 2679, token usage: 0.12, #running-req: 1020, #queue-req: 83, 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP7] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP3] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP0] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP4] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP1] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP5] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP2] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP6] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15] INFO:     127.0.0.1:36744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:37162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:37756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:38828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:38906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:39974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:41068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:41374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:42122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:42240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:42254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:42352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:42916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:44546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:44630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:44750 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP3] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP7] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP4] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP0] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP1] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP5] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP2] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP6] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP0] Prefill batch [115], #new-seq: 16, #new-token: 1064, #cached-token: 10716, token usage: 0.12, #running-req: 1008, #queue-req: 67, 
[2025-10-24 15:33:15] INFO:     127.0.0.1:38206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:38440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:39834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:40938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:40986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:42076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:42284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:42496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:43408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:43838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:44302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:44364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:44866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:45432 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP1] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP2] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP0] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP3] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP7] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP4] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP6] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP5] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:15 TP0] Prefill batch [117], #new-seq: 15, #new-token: 1100, #cached-token: 10046, token usage: 0.12, #running-req: 1009, #queue-req: 52, 
[2025-10-24 15:33:15] INFO:     127.0.0.1:37320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:40366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:41326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:42164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:42396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:42514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:43938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:44952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:15] INFO:     127.0.0.1:44964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16 TP0] Prefill batch [119], #new-seq: 9, #new-token: 496, #cached-token: 6028, token usage: 0.13, #running-req: 1015, #queue-req: 43, 
[2025-10-24 15:33:16] INFO:     127.0.0.1:38572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:39616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:39658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:39782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:41564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:41876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:43612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:43790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:44462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16 TP0] Prefill batch [121], #new-seq: 9, #new-token: 506, #cached-token: 6030, token usage: 0.13, #running-req: 1015, #queue-req: 34, 
[aiter] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP2] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP7] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP6] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP0] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP3] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP4] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP5] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP1] [fused_moe] using default for (506, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16] INFO:     127.0.0.1:36818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:37616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:37678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:38768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:40490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:43060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:43444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16 TP0] Prefill batch [123], #new-seq: 8, #new-token: 429, #cached-token: 5363, token usage: 0.13, #running-req: 1016, #queue-req: 26, 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP3] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP7] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP0] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP1] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP4] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP5] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP2] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP6] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP0] Decode batch [123], #running-req: 1016, #token: 121499, token usage: 0.13, cuda graph: False, gen throughput (token/s): 5266.23, #queue-req: 26, 
[2025-10-24 15:33:16] INFO:     127.0.0.1:37192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:37986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:38318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:38584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:39462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:39556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:41212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:42722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:42752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:43206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:43236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:43306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:44428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:45468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:45618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16 TP0] Prefill batch [125], #new-seq: 15, #new-token: 1001, #cached-token: 10047, token usage: 0.13, #running-req: 1009, #queue-req: 11, 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP7] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP0] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP1] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP3] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP4] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP5] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP2] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16 TP6] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:16] INFO:     127.0.0.1:36760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:37558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:39962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:40644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:41994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:42646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16] INFO:     127.0.0.1:45302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:16 TP0] Prefill batch [127], #new-seq: 8, #new-token: 416, #cached-token: 5361, token usage: 0.13, #running-req: 1016, #queue-req: 3, 
[2025-10-24 15:33:17] INFO:     127.0.0.1:38020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:38384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:39668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:39932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:42372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:42454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:43148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:44344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:45136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:45246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17 TP0] Prefill batch [129], #new-seq: 3, #new-token: 153, #cached-token: 2013, token usage: 0.13, #running-req: 1013, #queue-req: 0, 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP1] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP3] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP7] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP0] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP5] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP4] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP2] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP6] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17] INFO:     127.0.0.1:38126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:38190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:38512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:40354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:40798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:41138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:41468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:45368 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP0] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP4] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP1] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP3] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP7] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP5] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP2] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP6] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17] INFO:     127.0.0.1:36900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:36904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:38014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:38654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:38922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:39126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:40730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:42598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:43126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:43498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:43776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:43942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:44084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:45620 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP3] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP7] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP0] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP4] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP1] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP5] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP2] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP6] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17] INFO:     127.0.0.1:37458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:37874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:38590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:39420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:41424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:41844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:44252 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP3] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP7] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP0] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP4] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP1] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP5] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP2] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP6] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17] INFO:     127.0.0.1:36864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:39140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:39576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:41548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:44412 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP7] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP3] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP4] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP0] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP1] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP5] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP2] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP6] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17] INFO:     127.0.0.1:36794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:38448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:40780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:41220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:41606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:41838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:43630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:44152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:44264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:44720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:45378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:45532 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP3] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP7] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP4] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP0] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP1] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP5] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP2] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP6] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17] INFO:     127.0.0.1:36770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:36994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:37096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:37106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:38930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:39172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:41316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:42886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:43684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:17] INFO:     127.0.0.1:44670 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP3] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP7] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP4] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP0] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP1] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP5] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP2] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:17 TP6] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18] INFO:     127.0.0.1:36916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:37448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:37528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:37544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:37896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:41774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:42662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:43016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:43898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:44940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:45014 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP3] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP7] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP4] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP0] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP1] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP5] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP2] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP6] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18] INFO:     127.0.0.1:37036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:38142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:38388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:39860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:40520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:42114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:42390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:42780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:43730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:43948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:44794 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP3] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP4] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP7] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP0] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP1] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP5] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP2] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP6] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18] INFO:     127.0.0.1:36624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:36660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:36786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:37940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:38782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:39032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:40368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:42938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:42970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:44690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:44816 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP7] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP3] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP4] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP0] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP1] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP5] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP2] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP6] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18] INFO:     127.0.0.1:37084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:37092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:38326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:38568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:39982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:42110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:43660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:44974 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP5] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP1] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP7] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP3] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP4] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP0] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP6] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP2] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18] INFO:     127.0.0.1:37168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:37588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:38028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:38736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:39476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:39730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:40108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:40322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:40408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:40540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:41166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:42436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:44016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:44500 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP7] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP4] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP1] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP5] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP3] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP6] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP2] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP0] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:39064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:39870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:40300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:41718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:41856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:44184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:45260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:18] INFO:     127.0.0.1:45456 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP1] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP2] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP3] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP0] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP7] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP6] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP5] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:18 TP4] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19] INFO:     127.0.0.1:36858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:37102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:37182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:37376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:38992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:40760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:43480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:43732 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP2] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP0] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP6] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP5] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP4] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP1] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP3] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP7] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19] INFO:     127.0.0.1:36978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:37508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:37500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:37704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:38874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:39414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:39682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:39886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:40092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:41532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:41556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:41926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:43088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:43158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:43424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:43534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:43872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44930 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP0] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP4] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP2] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP6] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP3] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP1] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP7] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP5] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19] INFO:     127.0.0.1:36632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:36690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:37628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:37812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:40284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44958 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP0] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP4] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP2] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP6] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP3] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP7] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP1] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP5] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:43442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:43940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44854 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP0] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP4] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP3] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP7] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP2] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP6] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP1] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP5] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19] INFO:     127.0.0.1:37712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:37782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:38098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:39096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:39864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:41628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:42030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:43204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:43494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44708 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP4] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP0] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP3] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP7] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP2] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP6] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP1] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP5] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19] INFO:     127.0.0.1:37020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:39282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:40700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:40954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:41870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:45068 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP4] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP0] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP3] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP7] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP6] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP2] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP1] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP5] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19] INFO:     127.0.0.1:37522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:39960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:40464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:42350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:45524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:45734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:46162 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP4] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP0] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP3] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP7] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP2] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP6] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP1] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP5] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19] INFO:     127.0.0.1:37214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:38858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:41516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:41678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:41764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:42658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:43508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44214 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP4] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP2] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP6] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP0] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP3] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP7] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP1] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP5] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19] INFO:     127.0.0.1:37212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:37266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:37976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:38350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:39610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:40418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:40824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:41984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:44266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:19] INFO:     127.0.0.1:45802 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP2] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP6] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP4] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP1] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP0] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP3] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP7] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:19 TP5] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20] INFO:     127.0.0.1:37044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:40272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:40588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:40734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:40774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:41670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:42534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:43974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:45074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:45232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:45514 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP2] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP0] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP4] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP6] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP3] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP7] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP1] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP5] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20] INFO:     127.0.0.1:39366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:40834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:41088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:41998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:42310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:43050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:45318 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP2] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP4] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP6] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP0] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP3] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP7] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP1] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP5] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20] INFO:     127.0.0.1:36748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:39448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:41966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:42234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:43028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:43616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:43930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:45198 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP4] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP2] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP6] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP0] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP3] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP7] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP1] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP5] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP4] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP2] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP6] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP3] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP7] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP0] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP1] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP5] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:41408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:44052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:45206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:45454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:45602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:46852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:37742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:39850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:40808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:41238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:43596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:44694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:45944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:46872 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP2] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP6] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP4] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP0] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP3] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP7] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP1] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP5] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20] INFO:     127.0.0.1:37622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:37758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:37972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:43138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:43370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:43514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:44110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:44468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:45112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:45652 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP4] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP2] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP6] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP0] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP3] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP7] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP1] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP5] [fused_moe] using default for (740, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20] INFO:     127.0.0.1:36676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:36834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:37144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:37390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:37410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:37600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:39646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:40290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:40312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:41190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:42474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:43154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:43192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:46302 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP4] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP2] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP6] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP0] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP3] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP7] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP1] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP5] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:36986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:37866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:38490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:38798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:40332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:40768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:42226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:20] INFO:     127.0.0.1:45090 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP4] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP2] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP6] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP0] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP3] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP7] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP1] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:20 TP5] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21] INFO:     127.0.0.1:39538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:39996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:40432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:40632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:40880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:41172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:42480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:43588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:44046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:46406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:46574 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP2] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP4] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP6] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP0] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP3] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP7] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP1] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP5] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21] INFO:     127.0.0.1:39080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:39816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:40154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:40334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:41588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:42866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:43232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:46788 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP2] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP4] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP6] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP0] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP3] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP7] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP1] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP5] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21] INFO:     127.0.0.1:37860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:41874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:41898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:44182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:44978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:46504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP2] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP4] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP6] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP0] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP3] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP7] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP1] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP5] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21] INFO:     127.0.0.1:36734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:40190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:40956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:41288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:43020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:44192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:44802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:46540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP2] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP4] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP6] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP0] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP7] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP3] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP1] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP5] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21] INFO:     127.0.0.1:36626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:37008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:37112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:38062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:39188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:41110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:42802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:43230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:43892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:46836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:47170 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP4] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP2] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP6] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP0] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP3] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP7] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP1] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP5] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21] INFO:     127.0.0.1:37574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:37702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:38178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:38592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:38816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:38884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:41344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:41860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:41914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:42198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:42562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:42628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:44980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45558 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP2] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP4] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP6] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP0] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP3] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP7] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP1] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP5] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21] INFO:     127.0.0.1:37060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:38850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:38948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:39216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:40360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:41724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:41892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:42044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:42528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:44032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:44892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP4] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP6] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP2] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP1] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP0] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP3] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP7] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP5] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21] INFO:     127.0.0.1:36926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:37358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:37418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:37798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:37876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:38810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:40744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:42462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:44162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45988 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP2] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP0] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP4] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP6] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP3] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP7] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP1] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP5] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP0] Decode batch [167], #running-req: 628, #token: 94281, token usage: 0.10, cuda graph: False, gen throughput (token/s): 6334.00, #queue-req: 0, 
[2025-10-24 15:33:21] INFO:     127.0.0.1:39338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:40618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:41880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:45584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:46252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:21] INFO:     127.0.0.1:48156 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP4] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP0] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP3] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP7] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP2] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP6] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP1] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:21 TP5] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22] INFO:     127.0.0.1:39890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:41126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:42402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:43112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:44484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:45328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:47354 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP0] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP4] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP3] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP7] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP2] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP6] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP1] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP5] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22] INFO:     127.0.0.1:36838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:38538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:38564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:43576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:44466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:44614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:45500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP4] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP0] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP7] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP3] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP1] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP2] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP6] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP5] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22] INFO:     127.0.0.1:36886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:36944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:37564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:38420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:38638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:38982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:41054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:42410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:44070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:45140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:45474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:45780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46984 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP7] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP3] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP0] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP4] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP1] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP2] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP5] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP6] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22] INFO:     127.0.0.1:37364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:39570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:39652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:39736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:42486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:43128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:43430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:43698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:44004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:44246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:45990 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP3] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP4] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP7] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP0] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP2] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP6] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP1] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP5] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22] INFO:     127.0.0.1:37152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:38216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:38742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:38962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:39518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:41076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:42058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:43504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:45946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:47112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP4] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP3] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP7] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP0] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP2] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP6] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP1] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP5] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22] INFO:     127.0.0.1:36666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:36700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:38238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:42100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:42610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:42838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:42854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:45878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:47580 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP4] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP2] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP6] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP1] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP0] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP3] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP5] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP7] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22] INFO:     127.0.0.1:37308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:38144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:39400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:39490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:44784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:47376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:47516 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP0] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP4] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP7] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP3] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP2] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP6] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP1] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP5] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22] INFO:     127.0.0.1:38342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:44230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:45200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46978 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP4] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP0] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP7] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP3] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP2] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP6] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP1] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22 TP5] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:22] INFO:     127.0.0.1:39298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:41498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:42788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:42954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:42968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:43674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:36642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:38796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:39220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:41406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:44804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:38246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:39198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:41178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:41226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:42880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:38464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:39012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:40542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:43216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:43272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:44254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:44414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:44842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:46394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:22] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:37218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:39770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:42624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:42678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:43914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:43990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:44916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:39724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:40594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:42580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:46056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:46648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:46726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:46960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:37040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:44208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:44536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:45688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:39170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:41272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:43950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:43962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:46098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:46670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:38322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:38546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:38870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:42376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:44136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:45856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:46910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:36608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:38842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:44204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:44598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:45984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:40512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:41968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:44728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:45174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:39598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:41278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:41458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:45218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:46294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:46632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:46714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:48284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:40084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:41246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:45052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:45978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:48110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:41330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:41784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:42074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:42336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:42750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:45352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:48294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:37848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:39248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:39284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:39904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:40140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:40394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:42192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:42278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:43656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:46892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:48174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:40242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:40348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:42766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:48266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:38290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:40186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:43546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:44338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:44976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:45444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:38288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:39110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:39268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:39698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:41622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:41964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:43120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:43830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:44342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:44820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:46466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:23] INFO:     127.0.0.1:47610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:44684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:48000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:38402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:40336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:43354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:43978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:38544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:43166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:36726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:40670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:43484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:48212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:37306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:36964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:42774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:44900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:37156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:38010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:38382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:38656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:42694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:38734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:41274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:41356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:37552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:38164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:39522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:39806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:42700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:48298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:37240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:37924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:37908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:39916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:40228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:40452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:42186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24 TP0] Decode batch [207], #running-req: 284, #token: 52750, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6069.35, #queue-req: 0, 
[2025-10-24 15:33:24] INFO:     127.0.0.1:37970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:38756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:41340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:42818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:37184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:38364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:38622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:38704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:41710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:42738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:48082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:46028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:48130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:48344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:45898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:47304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:24] INFO:     127.0.0.1:48326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:38764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:43852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:46518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:37780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:38072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:42478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:45788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:37268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:37294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:41702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:45258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:46128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:38498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:48166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:48230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:40158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:42304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:43288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:46010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:39584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:43102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:43746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:45548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:40038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:46906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:39504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:48142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:38150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:46018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:38480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:39384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:40862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:43394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:48078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:38052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:39006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:40374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:43762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:46044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:46694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:39026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:43108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:46732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:40436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:45710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:38684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:39070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:41802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:45858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:36636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:42018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:46764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:40972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:41646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:41828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:46826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:36848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:42400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:46704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:47588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:25] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:39636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:48050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:39048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:41816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:44390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:48104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:48252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:37730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:40716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:48094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:39404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:36684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:38314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:40118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:48180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:42134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:43256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:45928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:41910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:44436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26 TP0] Decode batch [247], #running-req: 123, #token: 29882, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3931.03, #queue-req: 0, 
[2025-10-24 15:33:26] INFO:     127.0.0.1:42322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:38616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:38952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:41692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:42756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:48312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:41150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:45660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:45770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:40524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:47524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:48154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:48336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:37884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:45720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:48360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:44904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:26] INFO:     127.0.0.1:45484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:48242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:45940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:48196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:37340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:39116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:41436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:36686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:45912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:46190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:45826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:43330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:43804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:47286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:47426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:40500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:41156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:41796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:37686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:39232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:45958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:47668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:46740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:36972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:44830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:46488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:47714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:40304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:47636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:48280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:37352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:43994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:47088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:48370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:42966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:37646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:43586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:43874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:46454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:46346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:47882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:46590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:27] INFO:     127.0.0.1:45338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:47320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28 TP0] Decode batch [287], #running-req: 46, #token: 13013, token usage: 0.01, cuda graph: True, gen throughput (token/s): 2011.30, #queue-req: 0, 
[2025-10-24 15:33:28] INFO:     127.0.0.1:44876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:47868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:37272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:43052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:46332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:37912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:48214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:38088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:40478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:40172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:41940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:48260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:46948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:46986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:43878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:28] INFO:     127.0.0.1:45506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:47024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:48138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:37722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:39422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:36936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:46262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:46416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:47962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29 TP0] Decode batch [327], #running-req: 14, #token: 4936, token usage: 0.01, cuda graph: True, gen throughput (token/s): 953.11, #queue-req: 0, 
[2025-10-24 15:33:29] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:46282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:47892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:46182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:43520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:47046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:47278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:45412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:29] INFO:     127.0.0.1:45510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:30] INFO:     127.0.0.1:38340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:30] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:30 TP0] Decode batch [367], #running-req: 2, #token: 960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 287.31, #queue-req: 0, 
[2025-10-24 15:33:30] INFO:     127.0.0.1:46672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:30] INFO:     127.0.0.1:47146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:43] INFO:     127.0.0.1:46628 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-24 15:33:43 TP0] Prefill batch [376], #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:33:43] INFO:     127.0.0.1:46632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:43 TP0] Prefill batch [377], #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:33:43 TP0] Prefill batch [378], #new-seq: 40, #new-token: 40, #cached-token: 28927, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:33:43 TP0] Prefill batch [379], #new-seq: 44, #new-token: 44, #cached-token: 32115, token usage: 0.01, #running-req: 41, #queue-req: 0, 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP1] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP2] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP3] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP0] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP4] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP7] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP5] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP6] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP0] Prefill batch [380], #new-seq: 50, #new-token: 50, #cached-token: 36531, token usage: 0.01, #running-req: 85, #queue-req: 0, 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP1] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP0] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP2] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP3] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP6] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP4] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP7] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP5] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP0] Prefill batch [381], #new-seq: 51, #new-token: 51, #cached-token: 37324, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP3] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP2] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP1] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP0] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP4] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP7] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP5] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:43 TP6] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP0] Prefill batch [382], #new-seq: 58, #new-token: 58, #cached-token: 42113, token usage: 0.02, #running-req: 186, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP0] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP1] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP2] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP3] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP6] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP7] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP5] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP4] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP0] Prefill batch [383], #new-seq: 58, #new-token: 58, #cached-token: 42152, token usage: 0.02, #running-req: 244, #queue-req: 0, 
[2025-10-24 15:33:44 TP0] Prefill batch [384], #new-seq: 63, #new-token: 63, #cached-token: 45895, token usage: 0.02, #running-req: 302, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP6] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP7] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP4] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP5] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP3] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP2] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP1] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP0] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP0] Prefill batch [385], #new-seq: 67, #new-token: 67, #cached-token: 48913, token usage: 0.03, #running-req: 365, #queue-req: 0, 
[2025-10-24 15:33:44 TP0] Prefill batch [386], #new-seq: 67, #new-token: 67, #cached-token: 48581, token usage: 0.03, #running-req: 432, #queue-req: 0, 
[2025-10-24 15:33:44 TP0] Prefill batch [387], #new-seq: 74, #new-token: 74, #cached-token: 53602, token usage: 0.04, #running-req: 499, #queue-req: 0, 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP2] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP1] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP3] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP0] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP7] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP6] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP4] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP5] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:44 TP0] Prefill batch [388], #new-seq: 72, #new-token: 72, #cached-token: 52463, token usage: 0.04, #running-req: 573, #queue-req: 0, 
[2025-10-24 15:33:44 TP0] Prefill batch [389], #new-seq: 84, #new-token: 84, #cached-token: 61167, token usage: 0.04, #running-req: 645, #queue-req: 0, 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP2] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP1] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP0] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP3] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP7] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP4] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP5] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP6] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP0] Prefill batch [390], #new-seq: 72, #new-token: 72, #cached-token: 52458, token usage: 0.05, #running-req: 729, #queue-req: 0, 
[2025-10-24 15:33:45 TP0] Prefill batch [391], #new-seq: 90, #new-token: 90, #cached-token: 65522, token usage: 0.05, #running-req: 801, #queue-req: 0, 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP2] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP1] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP3] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP0] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP6] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP7] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP4] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP5] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP0] Prefill batch [392], #new-seq: 76, #new-token: 76, #cached-token: 55657, token usage: 0.06, #running-req: 891, #queue-req: 0, 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP0] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP2] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP1] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP3] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP6] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP7] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP4] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP5] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP0] Prefill batch [393], #new-seq: 8, #new-token: 8, #cached-token: 5802, token usage: 0.06, #running-req: 967, #queue-req: 0, 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP0] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP3] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP1] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP2] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP4] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP5] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP7] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP6] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP0] Prefill batch [395], #new-seq: 31, #new-token: 31, #cached-token: 22739, token usage: 0.06, #running-req: 975, #queue-req: 0, 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP0] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP2] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP1] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP3] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP4] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP5] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP7] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP6] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP0] Prefill batch [396], #new-seq: 18, #new-token: 18, #cached-token: 13312, token usage: 0.06, #running-req: 1006, #queue-req: 33, 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP0] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP1] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP3] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP4] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP7] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP5] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP2] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:45 TP6] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:49] INFO:     127.0.0.1:49470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:49 TP0] Prefill batch [424], #new-seq: 1, #new-token: 1, #cached-token: 756, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-24 15:33:49 TP0] Decode batch [428], #running-req: 1024, #token: 94563, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1619.16, #queue-req: 294, 
[2025-10-24 15:33:50] INFO:     127.0.0.1:49510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:50 TP0] Prefill batch [433], #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-24 15:33:50] INFO:     127.0.0.1:46894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:50] INFO:     127.0.0.1:46660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:50] INFO:     127.0.0.1:48024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:50] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:50] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:50] INFO:     127.0.0.1:55068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:50 TP0] Prefill batch [439], #new-seq: 1, #new-token: 1, #cached-token: 716, token usage: 0.11, #running-req: 1023, #queue-req: 292, 
[2025-10-24 15:33:51] INFO:     127.0.0.1:47740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:47830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:48964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:49310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51 TP0] Prefill batch [441], #new-seq: 11, #new-token: 11, #cached-token: 8059, token usage: 0.11, #running-req: 1013, #queue-req: 281, 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP3] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP1] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP0] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP7] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP5] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP4] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP2] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP6] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:49450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51 TP0] Prefill batch [443], #new-seq: 2, #new-token: 2, #cached-token: 1473, token usage: 0.11, #running-req: 1022, #queue-req: 279, 
[2025-10-24 15:33:51] INFO:     127.0.0.1:46878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:46904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:47278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:47822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:53812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51 TP0] Prefill batch [445], #new-seq: 9, #new-token: 9, #cached-token: 6523, token usage: 0.11, #running-req: 1015, #queue-req: 270, 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP1] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP3] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP2] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP0] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP6] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP7] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP5] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP4] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:49834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:51514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:52002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:55202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51 TP0] Prefill batch [447], #new-seq: 5, #new-token: 5, #cached-token: 3623, token usage: 0.11, #running-req: 1019, #queue-req: 265, 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP1] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP0] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP5] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP2] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP4] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP3] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP6] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51 TP7] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:51] INFO:     127.0.0.1:48788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:49714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:50266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:51704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:55206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:51] INFO:     127.0.0.1:55306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52 TP0] Prefill batch [449], #new-seq: 7, #new-token: 7, #cached-token: 5023, token usage: 0.11, #running-req: 1017, #queue-req: 258, 
[2025-10-24 15:33:52] INFO:     127.0.0.1:51004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:52518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52 TP0] Prefill batch [451], #new-seq: 4, #new-token: 4, #cached-token: 2897, token usage: 0.11, #running-req: 1020, #queue-req: 254, 
[2025-10-24 15:33:52] INFO:     127.0.0.1:46846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:46960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:48488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:49950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:51290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52 TP0] Prefill batch [453], #new-seq: 9, #new-token: 9, #cached-token: 6579, token usage: 0.11, #running-req: 1015, #queue-req: 245, 
[2025-10-24 15:33:52] INFO:     127.0.0.1:46642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:48580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:48590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:49426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:53870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:54172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:54454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52 TP0] Prefill batch [455], #new-seq: 9, #new-token: 9, #cached-token: 6639, token usage: 0.11, #running-req: 1015, #queue-req: 236, 
[2025-10-24 15:33:52] INFO:     127.0.0.1:47554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:54164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:55684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52 TP0] Prefill batch [457], #new-seq: 5, #new-token: 5, #cached-token: 3660, token usage: 0.11, #running-req: 1019, #queue-req: 231, 
[2025-10-24 15:33:52] INFO:     127.0.0.1:48152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:50846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:52] INFO:     127.0.0.1:52136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53 TP0] Prefill batch [459], #new-seq: 6, #new-token: 6, #cached-token: 4395, token usage: 0.12, #running-req: 1018, #queue-req: 225, 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP3] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP0] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP1] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP7] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP4] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP5] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP2] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP6] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53] INFO:     127.0.0.1:47238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:50644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:51630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:51978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:52794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:54384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:54528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53 TP0] Prefill batch [461], #new-seq: 9, #new-token: 9, #cached-token: 6578, token usage: 0.12, #running-req: 1015, #queue-req: 216, 
[2025-10-24 15:33:53] INFO:     127.0.0.1:47574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:47968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:48330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:48626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:49662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:54472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53 TP0] Prefill batch [463], #new-seq: 9, #new-token: 9, #cached-token: 6457, token usage: 0.12, #running-req: 1015, #queue-req: 207, 
[2025-10-24 15:33:53] INFO:     127.0.0.1:47046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:47644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:48264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:50100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:50570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:53014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:55064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53 TP0] Prefill batch [465], #new-seq: 11, #new-token: 11, #cached-token: 7980, token usage: 0.12, #running-req: 1013, #queue-req: 196, 
[2025-10-24 15:33:53] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:53308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53 TP0] Prefill batch [467], #new-seq: 10, #new-token: 10, #cached-token: 7262, token usage: 0.12, #running-req: 1014, #queue-req: 186, 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP3] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP1] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP7] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP5] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP4] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP0] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP2] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53 TP6] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:53] INFO:     127.0.0.1:48294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:51108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:52882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:53] INFO:     127.0.0.1:54594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54 TP0] Prefill batch [469], #new-seq: 6, #new-token: 6, #cached-token: 4381, token usage: 0.12, #running-req: 1018, #queue-req: 180, 
[2025-10-24 15:33:54] INFO:     127.0.0.1:46640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:47390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:49126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:49262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:49900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:52252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54 TP0] Prefill batch [471], #new-seq: 10, #new-token: 10, #cached-token: 7251, token usage: 0.12, #running-req: 1014, #queue-req: 170, 
[2025-10-24 15:33:54] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:48530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:49546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:49864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:53434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:53806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54 TP0] Prefill batch [473], #new-seq: 8, #new-token: 8, #cached-token: 5702, token usage: 0.12, #running-req: 1016, #queue-req: 162, 
[2025-10-24 15:33:54] INFO:     127.0.0.1:48796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:52382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:52616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:53118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:53886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54 TP0] Prefill batch [475], #new-seq: 6, #new-token: 6, #cached-token: 4362, token usage: 0.12, #running-req: 1018, #queue-req: 156, 
[2025-10-24 15:33:54] INFO:     127.0.0.1:47366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:48430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:48558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:51226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:51494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54 TP0] Prefill batch [477], #new-seq: 6, #new-token: 6, #cached-token: 4364, token usage: 0.12, #running-req: 1018, #queue-req: 150, 
[2025-10-24 15:33:54] INFO:     127.0.0.1:47976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:48310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:52196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:54778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:54] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55 TP0] Prefill batch [479], #new-seq: 11, #new-token: 11, #cached-token: 8134, token usage: 0.12, #running-req: 1013, #queue-req: 139, 
[2025-10-24 15:33:55] INFO:     127.0.0.1:46776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:46996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:47576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:51278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:52050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:54828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55 TP0] Prefill batch [481], #new-seq: 9, #new-token: 9, #cached-token: 6461, token usage: 0.12, #running-req: 1015, #queue-req: 130, 
[2025-10-24 15:33:55] INFO:     127.0.0.1:47378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:48306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:51640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:52814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:53432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:54120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55 TP0] Prefill batch [483], #new-seq: 11, #new-token: 11, #cached-token: 8205, token usage: 0.12, #running-req: 1013, #queue-req: 119, 
[2025-10-24 15:33:55] INFO:     127.0.0.1:48004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:48218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:49256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:49596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:50002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:53500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:54016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55 TP0] Prefill batch [485], #new-seq: 12, #new-token: 12, #cached-token: 8803, token usage: 0.12, #running-req: 1012, #queue-req: 107, 
[2025-10-24 15:33:55] INFO:     127.0.0.1:47796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:49620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:50508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:50612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:51886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:54400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55 TP0] Prefill batch [487], #new-seq: 6, #new-token: 6, #cached-token: 4465, token usage: 0.12, #running-req: 1018, #queue-req: 101, 
[2025-10-24 15:33:55] INFO:     127.0.0.1:46992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:47316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:47340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:48298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:48646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:48862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:49436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:49574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:50782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:54378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:55] INFO:     127.0.0.1:54894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56 TP0] Prefill batch [489], #new-seq: 16, #new-token: 16, #cached-token: 11554, token usage: 0.12, #running-req: 1008, #queue-req: 85, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:33:56] INFO:     127.0.0.1:47898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:52216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP5] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP1] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP4] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP3] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP7] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP0] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP2] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP6] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP0] Prefill batch [491], #new-seq: 3, #new-token: 3, #cached-token: 2172, token usage: 0.13, #running-req: 1021, #queue-req: 82, 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP1] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP3] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP7] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP4] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP5] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP0] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP2] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56 TP6] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:56] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:47748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:49612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:52296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:53392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56 TP0] Prefill batch [493], #new-seq: 12, #new-token: 12, #cached-token: 8792, token usage: 0.13, #running-req: 1012, #queue-req: 70, 
[2025-10-24 15:33:56] INFO:     127.0.0.1:48246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:48638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:48886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:52028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:52096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56 TP0] Prefill batch [495], #new-seq: 11, #new-token: 11, #cached-token: 8070, token usage: 0.13, #running-req: 1013, #queue-req: 59, 
[2025-10-24 15:33:56] INFO:     127.0.0.1:47602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:48192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:48454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:50022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:50386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:51320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:52120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:52348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:52490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56] INFO:     127.0.0.1:55592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:56 TP0] Prefill batch [497], #new-seq: 12, #new-token: 12, #cached-token: 8890, token usage: 0.13, #running-req: 1012, #queue-req: 47, 
[2025-10-24 15:33:57] INFO:     127.0.0.1:48352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:49268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:49386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:49704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:49738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:50220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:51834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:51948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:53586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:53864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP1] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP5] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP4] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP0] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP3] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP7] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP2] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP6] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP0] Prefill batch [499], #new-seq: 14, #new-token: 14, #cached-token: 10161, token usage: 0.13, #running-req: 1010, #queue-req: 33, 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP4] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP5] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP1] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP3] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP7] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP2] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP0] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP6] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP0] Decode batch [499], #running-req: 1010, #token: 122020, token usage: 0.13, cuda graph: False, gen throughput (token/s): 5495.44, #queue-req: 33, 
[2025-10-24 15:33:57] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:49678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:50480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:51790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:53430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57 TP0] Prefill batch [501], #new-seq: 7, #new-token: 7, #cached-token: 5073, token usage: 0.13, #running-req: 1017, #queue-req: 26, 
[2025-10-24 15:33:57] INFO:     127.0.0.1:46858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:46880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:46916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:48854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:50816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:52522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:52694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:52708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:53190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:55458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57 TP0] Prefill batch [503], #new-seq: 13, #new-token: 13, #cached-token: 9520, token usage: 0.13, #running-req: 1011, #queue-req: 13, 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP0] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP3] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP1] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP7] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP4] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP5] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP2] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57 TP6] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:57] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:49148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:50648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:52648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57 TP0] Prefill batch [505], #new-seq: 10, #new-token: 10, #cached-token: 7282, token usage: 0.13, #running-req: 1014, #queue-req: 3, 
[2025-10-24 15:33:57] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:47406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:52410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:52688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:53132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:53682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:53772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:55010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:55276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57] INFO:     127.0.0.1:55488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:57 TP0] Prefill batch [507], #new-seq: 3, #new-token: 3, #cached-token: 2163, token usage: 0.13, #running-req: 1012, #queue-req: 0, 
[2025-10-24 15:33:58] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:47628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:48994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:50822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:50942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:53094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP1] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP5] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP4] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP3] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP7] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP0] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP2] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP6] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58] INFO:     127.0.0.1:46982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:48086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:48972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:49234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP5] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP1] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP4] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP0] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP3] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP7] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP2] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP6] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58] INFO:     127.0.0.1:46844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:47708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:48040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:48238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:48418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:49556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:52338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:52972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54224 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP4] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP5] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP1] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP3] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP7] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP0] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP2] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP6] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58] INFO:     127.0.0.1:46962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:48280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:49224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:49692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:50772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:51564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP1] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP0] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP4] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP5] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP3] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP7] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP2] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP6] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58] INFO:     127.0.0.1:47020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:47062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:47550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:51598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:51780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP5] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP4] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP1] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP0] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP3] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP7] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP2] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP6] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58] INFO:     127.0.0.1:47650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:48594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:50520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:51662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:52854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP4] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP5] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP1] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP3] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP7] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP0] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP2] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP6] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:52916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:58] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP1] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP3] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP4] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP5] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP7] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP0] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP2] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:58 TP6] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59] INFO:     127.0.0.1:46926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:48460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:52080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:53946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:55416 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP1] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP5] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP4] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP3] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP7] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP0] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP2] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP6] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59] INFO:     127.0.0.1:46666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:47096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:47758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:47964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:48910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:50758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:51028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:52756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:52932 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP1] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP4] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP5] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP3] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP7] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP0] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP2] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP6] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59] INFO:     127.0.0.1:47150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:48384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:48514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:51534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:53632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:53902 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP1] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP3] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP4] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP5] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP7] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP0] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP2] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP6] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:46704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:47008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:47204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:49088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:49614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:50324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:53440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:55780 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP1] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP3] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP4] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP5] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP7] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP0] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP2] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP6] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59] INFO:     127.0.0.1:46942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:47872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:47990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:49008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:50378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:51614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:51786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP1] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP3] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP4] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP5] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP7] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP0] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP2] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP6] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59] INFO:     127.0.0.1:46988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:47718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:48952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:50282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:52956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:55310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP1] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP3] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP5] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP7] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP4] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP0] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP2] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP6] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59] INFO:     127.0.0.1:47068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:47930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:51374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:51522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:51868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:53076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:53136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:53380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54968 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP1] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP5] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP3] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP4] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP7] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP0] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP2] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP6] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59] INFO:     127.0.0.1:47298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:47832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:48666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:50542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:48100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:53416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:33:59] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP1] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP3] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP4] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP5] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP7] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP0] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP2] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:33:59 TP6] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:47534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:48012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:48768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:53166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:53460 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP4] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP5] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP7] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP6] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP1] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP3] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP0] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP2] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:49936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:50006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:50026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:50686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP1] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP3] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP5] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP7] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP4] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP0] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP2] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP6] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00] INFO:     127.0.0.1:50470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:53910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP1] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP3] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP0] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP2] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP4] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP7] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP5] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP6] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:48404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:51134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:51506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:52632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:56382 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP1] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP0] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP4] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP3] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP2] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP5] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP6] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP7] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:48038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:49536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:50436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:52662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:55800 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP1] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP5] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP0] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP4] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP3] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP7] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP2] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP6] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00] INFO:     127.0.0.1:46902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:47678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:49974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:50272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:50598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:50714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:50970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:52162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:52512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:53986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:55862 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP1] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP5] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP0] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP4] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP3] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP7] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP2] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP6] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00] INFO:     127.0.0.1:48834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:49494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:50860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:50890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:51030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:55436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP1] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP5] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP4] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP0] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP3] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP7] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP2] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP6] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00] INFO:     127.0.0.1:46818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:51658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:52288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:53600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:00] INFO:     127.0.0.1:56860 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP1] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP5] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP4] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP3] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP7] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP0] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP2] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:00 TP6] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:54398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:48114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:48136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:51212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:52434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:53786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:54488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP3] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP1] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP7] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP5] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP0] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP4] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP2] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP6] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01] INFO:     127.0.0.1:47218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:47694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:48582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:48786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:50298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:56308 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP1] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP5] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP4] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP3] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP7] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP0] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP2] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP6] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:48526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:49114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:49124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:50338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:50642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:51100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:52172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:53128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:53162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:53274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:54046 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP1] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP5] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP4] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP3] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP7] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP0] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP2] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP6] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01] INFO:     127.0.0.1:46860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:47086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:47304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:50446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:56590 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP1] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP5] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP4] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP3] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP7] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP0] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP2] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP6] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01] INFO:     127.0.0.1:49776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:49922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:52462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:54032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:56810 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP1] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP3] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP4] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP7] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP5] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP0] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP2] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP6] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:53210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:54292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:56464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:46750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:46764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:51848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:53848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP1] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP5] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP4] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP3] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP7] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP0] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP2] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP6] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01] INFO:     127.0.0.1:46802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:48180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:48748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:49160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:49330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:54194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:54796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:56516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:01] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP1] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP4] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP5] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP3] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP7] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP0] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP2] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:01 TP6] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02] INFO:     127.0.0.1:47770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:47960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:49294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:51186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:52788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:52818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP4] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP5] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP7] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP6] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP1] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP3] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP0] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP2] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:47206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:48482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:51798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:52156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:52364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:52534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:52678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP1] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP3] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP5] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP4] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP7] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP0] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP2] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP6] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02] INFO:     127.0.0.1:47454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:48564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:49346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:49644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:51692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:52504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:55816 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP1] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP5] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP4] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP0] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP3] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP7] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP2] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP6] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP0] Decode batch [544], #running-req: 649, #token: 95788, token usage: 0.10, cuda graph: False, gen throughput (token/s): 6552.41, #queue-req: 0, 
[2025-10-24 15:34:02] INFO:     127.0.0.1:46780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:47104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:47178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:48120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:49176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:53204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:56846 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP1] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP5] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP0] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP4] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP3] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP7] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP2] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP6] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02] INFO:     127.0.0.1:47876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:49460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:51826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:53408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:55542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:55766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:56966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP1] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP5] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP0] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP4] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP3] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP7] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP2] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP6] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02] INFO:     127.0.0.1:47016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:51842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:54170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:54526 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP5] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP1] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP0] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP4] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP3] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP7] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP2] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP6] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:49002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP5] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP1] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP3] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP7] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP4] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP2] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP0] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP6] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02] INFO:     127.0.0.1:48166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:49190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:49666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:52420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP1] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP5] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP0] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP3] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP4] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP7] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP2] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP6] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:49076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:49684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:53112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:55976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:57120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:02] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP1] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP5] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP3] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP7] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP0] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP4] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP2] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:02 TP6] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03] INFO:     127.0.0.1:47294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:47330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:49068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:49622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:49744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:52018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:52436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP1] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP5] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP3] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP7] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP0] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP4] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP2] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP6] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:48252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:49790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:50156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:51604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:52060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:55132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP1] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP5] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP4] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP0] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP3] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP7] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP2] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP6] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03] INFO:     127.0.0.1:49526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:50964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:55226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:57532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:46778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:49944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:50640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:52780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:55246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP1] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP4] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP3] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP5] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP7] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP0] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP2] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP6] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03] INFO:     127.0.0.1:49420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:50372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:50580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:50802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:52940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:54250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP1] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP3] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP4] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP5] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP7] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP0] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP2] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03 TP6] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:03] INFO:     127.0.0.1:48986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:54222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:55764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:49324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:48386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:49016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:49218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:50664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:46720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:49846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:52322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:46976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:47260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:48708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:51234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:57164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:48368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:50314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:51450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:55846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:03] INFO:     127.0.0.1:56936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:46652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:46682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:52282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:52576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:46832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:49694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:51926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:52832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:54214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:49876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:54948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:47664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:50128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:51340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:51740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:52042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:56890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:48284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:53338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:55640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:58162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:51678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:53790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:54784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:47264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:47472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:48328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:49812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:53520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:48720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:48948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:51218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:52748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:54004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:47466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:49374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:49750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:50682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:56448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:56838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:48288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:56386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:51910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:56526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:56542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:47458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:55322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:56288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:04] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:54422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:46788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:49278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:55554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:48658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:48922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:50068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:51368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:50624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:47326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:47910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05 TP0] Decode batch [584], #running-req: 304, #token: 57581, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6113.10, #queue-req: 0, 
[2025-10-24 15:34:05] INFO:     127.0.0.1:48692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:52108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:47254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:52810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:47742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:48434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:49090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:52718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:54802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:47356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:52556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:55360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:47442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:48338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:50236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:50448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:55582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:55744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:48784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:49516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:53082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:58344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:49762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:51668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:52248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:56350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:05] INFO:     127.0.0.1:57732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:49044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:52354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:54618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:47108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:55214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:55302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:48610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:49688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:58000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:47338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:50400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:50452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:51892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:53756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:46698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:54572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:51236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:54008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:48936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:51990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:53066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:47024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:55522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:48148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:51148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:57302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:48464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:49024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:53750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:58092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:48198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:06] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:48754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:49518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:55412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:47192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:57682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07 TP0] Decode batch [624], #running-req: 123, #token: 29654, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4028.61, #queue-req: 0, 
[2025-10-24 15:34:07] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:49206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:52736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:46734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:47116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:58354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:47088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:50918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:49734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:50510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:55530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:47480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:51438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:47920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:57284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:53970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:50774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:51776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:47592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:07] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:50318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:56688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:52274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:53814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:57480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:47164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:49422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:57812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:56976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:49570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:47988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:46968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:57690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:47756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:56566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:57904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:52924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:53292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:55380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:53706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:53778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:54898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:57984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08 TP0] Decode batch [664], #running-req: 43, #token: 12248, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1980.99, #queue-req: 0, 
[2025-10-24 15:34:08] INFO:     127.0.0.1:51858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:47418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:57742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:08] INFO:     127.0.0.1:55430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:53050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:56368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:50176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:58258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:56954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:47772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:49376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:57250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:57136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:57592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:56998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:47512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:56276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:09] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:10 TP0] Decode batch [704], #running-req: 15, #token: 5274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 953.59, #queue-req: 0, 
[2025-10-24 15:34:10] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:10] INFO:     127.0.0.1:55736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:10] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:10] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:10] INFO:     127.0.0.1:50460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:10] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:10] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:10] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:10] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:10] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:10] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:10 TP0] Decode batch [744], #running-req: 4, #token: 2059, token usage: 0.00, cuda graph: True, gen throughput (token/s): 331.16, #queue-req: 0, 
[2025-10-24 15:34:11] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:11] INFO:     127.0.0.1:55706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:11] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:11 TP0] Decode batch [784], #running-req: 1, #token: 1121, token usage: 0.00, cuda graph: True, gen throughput (token/s): 77.59, #queue-req: 0, 
[2025-10-24 15:34:12 TP0] Decode batch [824], #running-req: 1, #token: 1161, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.41, #queue-req: 0, 
[2025-10-24 15:34:13 TP0] Decode batch [864], #running-req: 1, #token: 1201, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.40, #queue-req: 0, 
[2025-10-24 15:34:14 TP0] Decode batch [904], #running-req: 1, #token: 1241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.39, #queue-req: 0, 
[2025-10-24 15:34:14 TP0] Decode batch [944], #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.36, #queue-req: 0, 
[2025-10-24 15:34:14] INFO:     127.0.0.1:51034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:27] INFO:     127.0.0.1:46034 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-24 15:34:27 TP0] Prefill batch [945], #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:34:27] INFO:     127.0.0.1:46046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:27 TP0] Prefill batch [946], #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:34:27 TP0] Prefill batch [947], #new-seq: 39, #new-token: 39, #cached-token: 28348, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:27 TP0] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:27 TP2] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:27 TP4] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:27 TP3] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:27 TP6] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:27 TP1] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:27 TP5] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:27 TP7] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP0] Prefill batch [948], #new-seq: 47, #new-token: 47, #cached-token: 34111, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP0] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP1] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP3] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP4] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP5] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP7] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP2] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP6] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP0] Prefill batch [949], #new-seq: 48, #new-token: 48, #cached-token: 35136, token usage: 0.01, #running-req: 87, #queue-req: 0, 
[2025-10-24 15:34:28 TP0] Prefill batch [950], #new-seq: 53, #new-token: 53, #cached-token: 38709, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP3] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP2] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP0] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP1] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP5] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP4] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP6] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP7] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP0] Prefill batch [951], #new-seq: 54, #new-token: 54, #cached-token: 39181, token usage: 0.02, #running-req: 188, #queue-req: 0, 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP2] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP3] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP0] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP7] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP1] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP4] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP5] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP6] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP0] Prefill batch [952], #new-seq: 58, #new-token: 58, #cached-token: 42196, token usage: 0.02, #running-req: 242, #queue-req: 0, 
[2025-10-24 15:34:28 TP0] Prefill batch [953], #new-seq: 57, #new-token: 57, #cached-token: 41490, token usage: 0.02, #running-req: 300, #queue-req: 0, 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP2] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP3] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP0] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP1] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP6] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP7] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP4] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP5] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP0] Prefill batch [954], #new-seq: 64, #new-token: 64, #cached-token: 46657, token usage: 0.03, #running-req: 357, #queue-req: 0, 
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP2] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP0] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP3] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP1] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP7] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP6] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP5] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP4] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:34:28 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:34:28 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:34:28 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:34:28 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:34:28 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:34:28 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:34:28 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-24 15:34:28 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:34:28 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 15:34:28 TP0] Prefill batch [955], #new-seq: 62, #new-token: 62, #cached-token: 45164, token usage: 0.03, #running-req: 421, #queue-req: 0, 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP3] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP2] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP0] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP1] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP5] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP7] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP4] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP6] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP0] Prefill batch [956], #new-seq: 75, #new-token: 75, #cached-token: 54309, token usage: 0.03, #running-req: 483, #queue-req: 0, 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP2] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP3] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP0] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP1] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP7] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP6] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP4] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:28 TP5] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP0] Prefill batch [957], #new-seq: 61, #new-token: 61, #cached-token: 44474, token usage: 0.04, #running-req: 558, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP2] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP0] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP3] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP1] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP6] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP7] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP4] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP5] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP0] Prefill batch [958], #new-seq: 79, #new-token: 79, #cached-token: 57448, token usage: 0.04, #running-req: 619, #queue-req: 0, 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP0] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP3] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP2] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP1] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP6] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP4] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP7] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP5] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP0] Prefill batch [959], #new-seq: 69, #new-token: 69, #cached-token: 50162, token usage: 0.05, #running-req: 698, #queue-req: 0, 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP3] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP0] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP1] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP2] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP6] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP7] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP5] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP4] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP0] Prefill batch [960], #new-seq: 42, #new-token: 42, #cached-token: 30681, token usage: 0.05, #running-req: 767, #queue-req: 0, 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP3] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP0] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP2] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP1] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP6] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP7] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP4] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP5] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP0] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP1] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP3] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP4] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP5] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP7] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP2] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP6] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:29 TP0] Prefill batch [963], #new-seq: 50, #new-token: 50, #cached-token: 36526, token usage: 0.05, #running-req: 809, #queue-req: 0, 
[2025-10-24 15:34:30 TP0] Prefill batch [964], #new-seq: 49, #new-token: 49, #cached-token: 35474, token usage: 0.06, #running-req: 859, #queue-req: 0, 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:30 TP2] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:30 TP3] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:30 TP1] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:30 TP0] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:30 TP5] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:30 TP6] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:30 TP7] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:30 TP4] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:30 TP0] Prefill batch [965], #new-seq: 58, #new-token: 58, #cached-token: 42576, token usage: 0.06, #running-req: 908, #queue-req: 0, 
[2025-10-24 15:34:30 TP0] Prefill batch [966], #new-seq: 54, #new-token: 54, #cached-token: 39739, token usage: 0.07, #running-req: 966, #queue-req: 0, 
[2025-10-24 15:34:30 TP0] Prefill batch [967], #new-seq: 4, #new-token: 4, #cached-token: 2919, token usage: 0.07, #running-req: 1020, #queue-req: 61, 
[2025-10-24 15:34:33] INFO:     127.0.0.1:48792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:33 TP0] Prefill batch [994], #new-seq: 1, #new-token: 1, #cached-token: 701, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-24 15:34:34] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:34 TP0] Prefill batch [1003], #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-24 15:34:35 TP0] Decode batch [1007], #running-req: 1024, #token: 102081, token usage: 0.11, cuda graph: False, gen throughput (token/s): 1957.18, #queue-req: 293, 
[2025-10-24 15:34:35] INFO:     127.0.0.1:46734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:46074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:47298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:50166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35 TP0] Prefill batch [1009], #new-seq: 3, #new-token: 3, #cached-token: 2204, token usage: 0.11, #running-req: 1021, #queue-req: 290, 
[2025-10-24 15:34:35] INFO:     127.0.0.1:46970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:48410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:48670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35 TP0] Prefill batch [1011], #new-seq: 6, #new-token: 6, #cached-token: 4423, token usage: 0.11, #running-req: 1018, #queue-req: 284, 
[2025-10-24 15:34:35] INFO:     127.0.0.1:46856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:54418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35 TP0] Prefill batch [1013], #new-seq: 2, #new-token: 2, #cached-token: 1420, token usage: 0.11, #running-req: 1022, #queue-req: 282, 
[2025-10-24 15:34:35] INFO:     127.0.0.1:46352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:46620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35 TP0] Prefill batch [1015], #new-seq: 5, #new-token: 5, #cached-token: 3682, token usage: 0.11, #running-req: 1019, #queue-req: 277, 
[2025-10-24 15:34:35] INFO:     127.0.0.1:47584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:50876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:51408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:35] INFO:     127.0.0.1:52700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36 TP0] Prefill batch [1017], #new-seq: 4, #new-token: 4, #cached-token: 2856, token usage: 0.11, #running-req: 1020, #queue-req: 273, 
[2025-10-24 15:34:36] INFO:     127.0.0.1:46280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:48178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:48212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:49062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:51044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36 TP0] Prefill batch [1019], #new-seq: 6, #new-token: 6, #cached-token: 4349, token usage: 0.11, #running-req: 1018, #queue-req: 267, 
[2025-10-24 15:34:36] INFO:     127.0.0.1:50262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:50266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:50302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:50594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:51904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36 TP0] Prefill batch [1021], #new-seq: 7, #new-token: 7, #cached-token: 5058, token usage: 0.11, #running-req: 1017, #queue-req: 260, 
[2025-10-24 15:34:36] INFO:     127.0.0.1:46558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:47060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:48204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:49280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36 TP0] Prefill batch [1023], #new-seq: 7, #new-token: 7, #cached-token: 5029, token usage: 0.11, #running-req: 1017, #queue-req: 253, 
[2025-10-24 15:34:36] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:48142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36] INFO:     127.0.0.1:48762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:36 TP0] Prefill batch [1025], #new-seq: 3, #new-token: 3, #cached-token: 2214, token usage: 0.11, #running-req: 1021, #queue-req: 250, 
[2025-10-24 15:34:37 TP0] Prefill batch [1027], #new-seq: 8, #new-token: 8, #cached-token: 5807, token usage: 0.11, #running-req: 1016, #queue-req: 242, 
[2025-10-24 15:34:37] INFO:     127.0.0.1:47042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:48426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:49160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:53432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:53700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:54008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:47186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:50296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:51498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:53838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37 TP0] Prefill batch [1029], #new-seq: 12, #new-token: 12, #cached-token: 8900, token usage: 0.12, #running-req: 1012, #queue-req: 230, 
[2025-10-24 15:34:37] INFO:     127.0.0.1:46516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:49962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:50992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:51364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37 TP0] Prefill batch [1031], #new-seq: 8, #new-token: 8, #cached-token: 5876, token usage: 0.12, #running-req: 1016, #queue-req: 222, 
[2025-10-24 15:34:37] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:46898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:48958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:51058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:51774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:52212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:52302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37 TP0] Prefill batch [1033], #new-seq: 7, #new-token: 7, #cached-token: 5094, token usage: 0.12, #running-req: 1017, #queue-req: 215, 
[2025-10-24 15:34:37] INFO:     127.0.0.1:46572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:47682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:49456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:51708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:53784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37] INFO:     127.0.0.1:54774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:37 TP0] Prefill batch [1035], #new-seq: 11, #new-token: 11, #cached-token: 7837, token usage: 0.12, #running-req: 1013, #queue-req: 204, 
[2025-10-24 15:34:38] INFO:     127.0.0.1:47002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:47262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:47624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:48050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:50476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:52252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:52644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:53134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:53988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38 TP0] Prefill batch [1037], #new-seq: 9, #new-token: 9, #cached-token: 6586, token usage: 0.12, #running-req: 1015, #queue-req: 195, 
[2025-10-24 15:34:38] INFO:     127.0.0.1:47328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:47700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:47722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:47958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:50274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:52748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:53864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38 TP0] Prefill batch [1039], #new-seq: 10, #new-token: 10, #cached-token: 7290, token usage: 0.12, #running-req: 1014, #queue-req: 185, 
[2025-10-24 15:34:38] INFO:     127.0.0.1:46056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:47906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:48528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:49218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38 TP0] Prefill batch [1041], #new-seq: 6, #new-token: 6, #cached-token: 4360, token usage: 0.12, #running-req: 1018, #queue-req: 179, 
[2025-10-24 15:34:38] INFO:     127.0.0.1:47220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:47466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:48420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:48848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:49180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:51862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:54174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38 TP0] Prefill batch [1043], #new-seq: 8, #new-token: 8, #cached-token: 5814, token usage: 0.12, #running-req: 1016, #queue-req: 171, 
[2025-10-24 15:34:38] INFO:     127.0.0.1:48060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:51768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:52370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:38] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39 TP0] Prefill batch [1045], #new-seq: 5, #new-token: 5, #cached-token: 3557, token usage: 0.12, #running-req: 1019, #queue-req: 166, 
[2025-10-24 15:34:39] INFO:     127.0.0.1:47850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:52242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:52478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39 TP0] Prefill batch [1047], #new-seq: 6, #new-token: 6, #cached-token: 4315, token usage: 0.12, #running-req: 1018, #queue-req: 160, 
[2025-10-24 15:34:39] INFO:     127.0.0.1:47738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:48114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:49312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:52958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:54544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39 TP0] Prefill batch [1049], #new-seq: 8, #new-token: 8, #cached-token: 5755, token usage: 0.12, #running-req: 1016, #queue-req: 152, 
[2025-10-24 15:34:39] INFO:     127.0.0.1:46166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:47202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:47490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:48314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:49348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:50832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:51456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:51518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:52182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:52946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39 TP0] Prefill batch [1051], #new-seq: 12, #new-token: 12, #cached-token: 8884, token usage: 0.12, #running-req: 1012, #queue-req: 140, 
[2025-10-24 15:34:39] INFO:     127.0.0.1:46760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:47232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:50282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:50994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:52250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:53852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:54138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39 TP0] Prefill batch [1053], #new-seq: 8, #new-token: 8, #cached-token: 5806, token usage: 0.12, #running-req: 1016, #queue-req: 132, 
[2025-10-24 15:34:39] INFO:     127.0.0.1:48162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:48642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:50190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:51294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:51626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:53220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:39] INFO:     127.0.0.1:54404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40 TP0] Prefill batch [1055], #new-seq: 9, #new-token: 9, #cached-token: 6610, token usage: 0.12, #running-req: 1015, #queue-req: 123, 
[2025-10-24 15:34:40] INFO:     127.0.0.1:46492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:48800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:48890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:48928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:49360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:53458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:53916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40 TP0] Prefill batch [1057], #new-seq: 13, #new-token: 13, #cached-token: 9517, token usage: 0.12, #running-req: 1011, #queue-req: 110, 
[2025-10-24 15:34:40] INFO:     127.0.0.1:46500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:47206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:47564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:48430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:49608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:50928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:51650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:52194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40 TP0] Prefill batch [1059], #new-seq: 16, #new-token: 16, #cached-token: 11811, token usage: 0.12, #running-req: 1008, #queue-req: 94, 
[2025-10-24 15:34:40] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:48582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:53798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40 TP0] Prefill batch [1061], #new-seq: 5, #new-token: 5, #cached-token: 3585, token usage: 0.12, #running-req: 1019, #queue-req: 89, 
[2025-10-24 15:34:40] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:46254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:46732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:47236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:47406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:47870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:48018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:49368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:50726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:51484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:51592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:51692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:51934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:52230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP3] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP4] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP5] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP1] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP7] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP0] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP2] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP6] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP0] Prefill batch [1063], #new-seq: 19, #new-token: 19, #cached-token: 13876, token usage: 0.12, #running-req: 1005, #queue-req: 70, 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP3] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP0] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP5] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP4] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP7] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP1] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP2] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40 TP6] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:40] INFO:     127.0.0.1:46940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:47668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:48126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:51586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:51622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:51850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:52020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:52704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:40] INFO:     127.0.0.1:53004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41 TP0] Prefill batch [1065], #new-seq: 13, #new-token: 13, #cached-token: 9566, token usage: 0.13, #running-req: 1011, #queue-req: 57, 
[2025-10-24 15:34:41] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:50520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:50674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:51520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:51732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:52534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41 TP0] Prefill batch [1067], #new-seq: 9, #new-token: 9, #cached-token: 6669, token usage: 0.13, #running-req: 1015, #queue-req: 48, 
[2025-10-24 15:34:41] INFO:     127.0.0.1:46836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:48370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:48730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:48986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:49050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:50910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:51336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:52918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:53768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:53786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:55008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41 TP0] Prefill batch [1069], #new-seq: 17, #new-token: 17, #cached-token: 12351, token usage: 0.13, #running-req: 1007, #queue-req: 31, 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:41 TP3] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:41 TP7] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:41 TP4] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:41 TP5] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:41 TP0] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:41 TP1] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:41 TP2] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:41 TP6] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:41] INFO:     127.0.0.1:46486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:47020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:47716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:48970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:49194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:49832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:50054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:52400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:52630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:52726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41 TP0] Prefill batch [1071], #new-seq: 12, #new-token: 12, #cached-token: 8752, token usage: 0.13, #running-req: 1012, #queue-req: 19, 
[2025-10-24 15:34:41] INFO:     127.0.0.1:46830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:48284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:52096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:52590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41] INFO:     127.0.0.1:53196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:41 TP0] Prefill batch [1073], #new-seq: 8, #new-token: 8, #cached-token: 5889, token usage: 0.13, #running-req: 1016, #queue-req: 11, 
[2025-10-24 15:34:42] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:47482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:55034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42 TP0] Prefill batch [1075], #new-seq: 4, #new-token: 4, #cached-token: 2894, token usage: 0.13, #running-req: 1020, #queue-req: 7, 
[2025-10-24 15:34:42] INFO:     127.0.0.1:46222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:51798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:52484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:55040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42 TP0] Prefill batch [1077], #new-seq: 7, #new-token: 7, #cached-token: 5038, token usage: 0.13, #running-req: 1016, #queue-req: 0, 
[2025-10-24 15:34:42] INFO:     127.0.0.1:47572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:47652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:50158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:50316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:50330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:52440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:54844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:54954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:47500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:48014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:50104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:50722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:51960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:52924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:53626 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP4] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP0] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP1] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP5] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP3] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP7] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP2] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP6] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:46650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:46932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:48610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:50236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:51138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:51724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:52316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP4] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP1] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP5] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP3] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP7] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP0] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP2] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP6] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42] INFO:     127.0.0.1:46340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:47174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:48994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:42] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP4] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP3] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP5] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP1] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP7] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP0] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP2] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:42 TP6] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP0] Decode batch [1082], #running-req: 983, #token: 122221, token usage: 0.13, cuda graph: False, gen throughput (token/s): 5147.96, #queue-req: 0, 
[2025-10-24 15:34:43] INFO:     127.0.0.1:46776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:47928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:50872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:52770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:52972 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP4] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP3] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP5] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP1] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP7] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP0] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP2] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP6] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43] INFO:     127.0.0.1:46418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:46576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:47090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:48646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:50658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:52284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:53806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:54072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP3] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP1] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP5] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP4] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP7] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP0] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP2] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP6] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43] INFO:     127.0.0.1:47362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:52332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:53642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:54092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:54662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:54796 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP3] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP4] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP5] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP1] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP7] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP0] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP2] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP6] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:47588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:47884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:49924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:54032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:46130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:48026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:48170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:48488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:48544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:50768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:54394 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP4] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP5] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP3] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP7] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP1] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP0] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP2] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP6] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:46504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:49380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP4] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP3] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP1] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP5] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP7] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP0] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP2] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP6] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43] INFO:     127.0.0.1:46090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:46542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:47516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:49494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:49720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:50154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:52756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:53396 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP4] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP5] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP1] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP3] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP7] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP0] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP2] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP6] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43] INFO:     127.0.0.1:47322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:48264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:48802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:49296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:49672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:50852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:50970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:51164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:43] INFO:     127.0.0.1:53230 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP4] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP5] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP3] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP1] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP7] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP0] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP6] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:43 TP2] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44] INFO:     127.0.0.1:46268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:46440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:46902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:48454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:49658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:53540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:53902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:47132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:47332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:49116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:49506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:50416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:50888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:51948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:52414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:52504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:52694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:52840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54898 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP3] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP1] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP4] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP5] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP7] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP0] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP2] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP6] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44] INFO:     127.0.0.1:46480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:46746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:47062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:49246 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP4] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP5] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP1] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP3] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP7] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP0] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP2] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP6] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44] INFO:     127.0.0.1:48386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:51004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:52306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:53360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP4] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP5] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP1] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP3] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP7] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP0] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP2] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP6] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44] INFO:     127.0.0.1:47048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:48128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:52526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP3] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP5] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP4] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP1] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP7] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP0] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP2] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP6] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:46982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:47004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:50090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:50552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:52794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:53694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54222 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP3] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP4] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP5] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP1] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP7] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP0] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP2] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP6] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54456 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP3] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP1] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP4] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP5] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP7] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP0] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP2] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP6] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:47832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:48160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:49138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:51682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:48842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:50432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:50508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:51170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:53680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:53908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:53972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:44] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP4] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP3] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP5] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP1] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:44 TP7] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP0] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP2] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP6] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45] INFO:     127.0.0.1:46080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:46236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:46594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:49322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:49650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:51402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:51562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:51892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP3] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP4] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP5] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP1] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP7] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP0] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP2] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP6] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:47258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:47430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:50220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:51016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:51062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP4] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP3] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP5] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP1] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP7] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP0] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP2] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP6] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45] INFO:     127.0.0.1:46206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:48698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:49776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:50528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:51580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:52358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:52920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:53294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:54468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:54646 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP3] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP4] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP1] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP5] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP7] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP0] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP2] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP6] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45] INFO:     127.0.0.1:50748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:56108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:47122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:49394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:50124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:50576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:52176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:52902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:55670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:56110 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP4] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP1] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP5] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP3] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP7] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP0] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP2] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP6] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45] INFO:     127.0.0.1:48824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:49036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:53388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:54378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:54618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:54820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:55216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:55308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:46122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:46348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:47266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:48030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:48270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:48518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:48722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:48818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:49076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:49708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:50368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:51820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:52462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:52518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:52632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP4] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP3] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP1] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP5] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP7] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP0] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP2] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP6] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45] INFO:     127.0.0.1:46682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:47078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:45] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP4] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP1] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP5] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP0] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP3] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP7] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP2] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:45 TP6] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:49234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP4] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP5] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP1] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP3] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP0] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP7] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP2] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP6] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46] INFO:     127.0.0.1:47426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:47796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:52854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:53346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:53822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:53878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP4] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP1] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP5] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP3] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP7] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP0] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP2] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP6] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46] INFO:     127.0.0.1:46164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:47014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:48214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:51180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:51348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:51570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP4] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP3] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP5] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP1] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP0] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP7] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP2] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP6] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46] INFO:     127.0.0.1:46192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:47022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:47560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:50624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:57170 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP4] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP3] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP1] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP5] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP0] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP7] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP2] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP6] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46] INFO:     127.0.0.1:46304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:48654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:48710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:49448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:49738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:50198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:56370 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP4] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP3] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP1] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP5] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP7] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP0] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP2] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP6] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46] INFO:     127.0.0.1:46070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:46756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:48308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:48796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:49430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:51152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:52108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:52572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:53556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP4] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP5] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP1] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP3] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP7] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP0] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP6] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP2] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46] INFO:     127.0.0.1:47390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:51880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:53180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:55846 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP4] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP3] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP1] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP5] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP7] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP0] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP2] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP6] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46] INFO:     127.0.0.1:46178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:47160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:47640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:47966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:50206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:55162 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP4] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP5] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP3] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP7] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP1] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP0] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP2] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP6] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:46700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:46784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:50032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:46] INFO:     127.0.0.1:57286 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP4] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP3] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP1] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP5] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP7] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP0] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP2] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:46 TP6] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47] INFO:     127.0.0.1:47040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:47280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:47600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:48146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:48558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:48568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:48964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:49268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:51764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:54526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:55340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:56570 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP4] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP3] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP1] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP5] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP7] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP0] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP2] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP6] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47] INFO:     127.0.0.1:46242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:48362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:51204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:54850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:56200 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP4] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP5] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP7] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP6] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP3] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP1] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP0] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP2] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47] INFO:     127.0.0.1:47016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:47916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:51760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:53904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:48312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:49148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:49208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:50068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:51826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP4] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP2] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP6] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP0] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP3] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP1] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP5] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP7] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47] INFO:     127.0.0.1:47678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:47694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:49534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:51334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:55434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:56588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:57210 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP4] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP0] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP2] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP1] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP5] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP6] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP3] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP7] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47] INFO:     127.0.0.1:46170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:47082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:49136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:51968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:53334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:55206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47 TP0] Decode batch [1122], #running-req: 563, #token: 87096, token usage: 0.09, cuda graph: False, gen throughput (token/s): 6620.65, #queue-req: 0, 
[2025-10-24 15:34:47] INFO:     127.0.0.1:48922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:49094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:49638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:50942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP4] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP5] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP1] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP0] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP2] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP6] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP3] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP7] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47] INFO:     127.0.0.1:46606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:54858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:56154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP4] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP5] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP1] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP0] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP2] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP3] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP6] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP7] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47] INFO:     127.0.0.1:47548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:48412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:49752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:53506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:56190 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP4] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP5] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP1] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP0] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP3] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP2] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP7] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47 TP6] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:34:47] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:47710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:48438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:49520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:49992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:50376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:53792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:54612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:55642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:47] INFO:     127.0.0.1:55802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:46718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:47990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:48684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:49524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:49630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:46614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:47822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:49944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:53274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:53576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:47756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:53606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:49964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:51946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:51982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:53644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:55406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:48768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:50402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:52774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:48660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:49552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:50596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:51722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:47768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:48076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:48248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:51782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:53452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:46064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:46092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:48198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:49008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:49896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:50622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:51326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:49470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:52214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:57444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:49186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:49202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:52558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:52852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:49616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:50690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:51428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:52124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:53992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:55338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:57458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:48756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:54880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:54992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:46646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:47862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:51028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:55718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:56646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:57434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:48602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:50802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:48] INFO:     127.0.0.1:53112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:48742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:52074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:52660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:54152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:49420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:52146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:53756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:54582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:54170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:57040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:46474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:48040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:46182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:47472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:52430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:53740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:51296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:48496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:47372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:54530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:46690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:47998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:46910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:52166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:47932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:54282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:54964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:48294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:57460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:57512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:46870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:47346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:48230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:51522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:57540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:52188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:47860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:51020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:54438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:55458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:56656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:49] INFO:     127.0.0.1:57532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:49024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:48932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:47250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:48094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:48954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:52618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:54698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:46678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:49128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:52426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:52758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50 TP0] Decode batch [1162], #running-req: 259, #token: 50247, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6042.65, #queue-req: 0, 
[2025-10-24 15:34:50] INFO:     127.0.0.1:47936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:54758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:47976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:48838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:47120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:47764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:48280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:47606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:49338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:49854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:51284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:51746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:52682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:47950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:48570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:50252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:46322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:52428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:51090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:51388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:50] INFO:     127.0.0.1:57238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:49820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:50606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:50982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:55246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:57324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:53070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:49070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:50574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:47532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:48338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:53788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:57208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:46814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:57410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:46916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:46338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:46466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:47894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:48850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:51098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:46152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:46094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:51512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:52588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:54976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:57070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:48596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:49550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:51008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:51236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:51666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:52130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:51] INFO:     127.0.0.1:57090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:49880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:47106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52 TP0] Decode batch [1202], #running-req: 100, #token: 24315, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3609.12, #queue-req: 0, 
[2025-10-24 15:34:52] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:50774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:54392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:50400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:52934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:55544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:46402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:49692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:55778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:46574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:54942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:55194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:52268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:46996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:46386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:57514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:48884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:52] INFO:     127.0.0.1:56838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:47288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:52634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:57364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:57276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:55684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:56414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:56518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:56308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:56732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:53020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:54204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53 TP0] Decode batch [1242], #running-req: 31, #token: 9147, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1709.31, #queue-req: 0, 
[2025-10-24 15:34:53] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:55080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:52382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:49572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:55700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:56500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:48724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:47034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:49560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:54822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:53] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:55892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:55622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:55640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:55758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:57056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:52838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:53166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:55276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54 TP0] Decode batch [1282], #running-req: 8, #token: 3212, token usage: 0.00, cuda graph: True, gen throughput (token/s): 678.66, #queue-req: 0, 
[2025-10-24 15:34:54] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:55006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:55068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:54] INFO:     127.0.0.1:56342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:55] INFO:     127.0.0.1:55952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:34:55] INFO:     127.0.0.1:50424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:07] INFO:     127.0.0.1:56174 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-24 15:35:07 TP0] Prefill batch [1321], #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:35:08] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:08 TP0] Prefill batch [1322], #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:35:08 TP0] Prefill batch [1323], #new-seq: 35, #new-token: 35, #cached-token: 25413, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:08 TP2] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:08 TP4] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:08 TP6] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:08 TP0] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:08 TP1] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:08 TP3] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:08 TP7] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:08 TP5] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:08 TP0] Prefill batch [1324], #new-seq: 47, #new-token: 47, #cached-token: 34297, token usage: 0.01, #running-req: 36, #queue-req: 0, 
[2025-10-24 15:35:08 TP0] Prefill batch [1325], #new-seq: 48, #new-token: 48, #cached-token: 34911, token usage: 0.01, #running-req: 83, #queue-req: 0, 
[2025-10-24 15:35:08 TP0] Prefill batch [1326], #new-seq: 57, #new-token: 57, #cached-token: 41683, token usage: 0.01, #running-req: 131, #queue-req: 0, 
[2025-10-24 15:35:08 TP0] Prefill batch [1327], #new-seq: 54, #new-token: 54, #cached-token: 39115, token usage: 0.02, #running-req: 188, #queue-req: 0, 
[2025-10-24 15:35:08 TP0] Prefill batch [1328], #new-seq: 69, #new-token: 69, #cached-token: 50273, token usage: 0.02, #running-req: 242, #queue-req: 0, 
[2025-10-24 15:35:08 TP0] Prefill batch [1329], #new-seq: 53, #new-token: 53, #cached-token: 38643, token usage: 0.02, #running-req: 311, #queue-req: 0, 
[2025-10-24 15:35:08 TP0] Prefill batch [1330], #new-seq: 75, #new-token: 75, #cached-token: 54693, token usage: 0.03, #running-req: 364, #queue-req: 0, 
[2025-10-24 15:35:09 TP0] Prefill batch [1331], #new-seq: 64, #new-token: 64, #cached-token: 46457, token usage: 0.03, #running-req: 439, #queue-req: 0, 
[2025-10-24 15:35:09 TP0] Prefill batch [1332], #new-seq: 75, #new-token: 75, #cached-token: 54422, token usage: 0.04, #running-req: 503, #queue-req: 0, 
[2025-10-24 15:35:09 TP0] Prefill batch [1333], #new-seq: 69, #new-token: 69, #cached-token: 50130, token usage: 0.04, #running-req: 578, #queue-req: 0, 
[2025-10-24 15:35:09 TP0] Prefill batch [1334], #new-seq: 3, #new-token: 3, #cached-token: 2198, token usage: 0.04, #running-req: 647, #queue-req: 0, 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP3] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP7] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP0] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP2] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP1] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP6] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP4] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP5] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP0] Prefill batch [1336], #new-seq: 26, #new-token: 26, #cached-token: 18920, token usage: 0.04, #running-req: 650, #queue-req: 0, 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP2] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP6] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP4] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP1] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP0] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP3] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP7] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP5] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:09 TP0] Decode batch [1336], #running-req: 650, #token: 41174, token usage: 0.04, cuda graph: False, gen throughput (token/s): 52.06, #queue-req: 0, 
[2025-10-24 15:35:09 TP0] Prefill batch [1337], #new-seq: 51, #new-token: 51, #cached-token: 37168, token usage: 0.05, #running-req: 676, #queue-req: 0, 
[2025-10-24 15:35:09 TP0] Prefill batch [1338], #new-seq: 49, #new-token: 49, #cached-token: 35620, token usage: 0.05, #running-req: 727, #queue-req: 0, 
[2025-10-24 15:35:10 TP0] Prefill batch [1339], #new-seq: 61, #new-token: 61, #cached-token: 44725, token usage: 0.05, #running-req: 776, #queue-req: 0, 
[2025-10-24 15:35:10 TP0] Prefill batch [1340], #new-seq: 56, #new-token: 56, #cached-token: 40543, token usage: 0.06, #running-req: 837, #queue-req: 0, 
[2025-10-24 15:35:10 TP0] Prefill batch [1341], #new-seq: 68, #new-token: 68, #cached-token: 49714, token usage: 0.06, #running-req: 893, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:10 TP2] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:10 TP3] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:10 TP1] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:10 TP7] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:10 TP0] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:10 TP6] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:10 TP4] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:10 TP5] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:10 TP0] Prefill batch [1342], #new-seq: 62, #new-token: 62, #cached-token: 45640, token usage: 0.06, #running-req: 961, #queue-req: 0, 
[2025-10-24 15:35:10 TP0] Prefill batch [1343], #new-seq: 1, #new-token: 1, #cached-token: 717, token usage: 0.06, #running-req: 1023, #queue-req: 74, 
[2025-10-24 15:35:13] INFO:     127.0.0.1:59026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:14 TP0] Prefill batch [1371], #new-seq: 1, #new-token: 1, #cached-token: 743, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-24 15:35:14] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:15 TP0] Prefill batch [1380], #new-seq: 1, #new-token: 1, #cached-token: 729, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-24 15:35:15] INFO:     127.0.0.1:56210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:15] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:15] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:15] INFO:     127.0.0.1:56990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:15] INFO:     127.0.0.1:60756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:15] INFO:     127.0.0.1:33650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:15 TP0] Prefill batch [1386], #new-seq: 3, #new-token: 3, #cached-token: 2221, token usage: 0.11, #running-req: 1021, #queue-req: 290, 
[2025-10-24 15:35:15 TP0] Decode batch [1386], #running-req: 1021, #token: 102789, token usage: 0.11, cuda graph: False, gen throughput (token/s): 6814.79, #queue-req: 290, 
[2025-10-24 15:35:15] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:15] INFO:     127.0.0.1:58656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:15] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:15] INFO:     127.0.0.1:33610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:15] INFO:     127.0.0.1:36378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:15 TP0] Prefill batch [1388], #new-seq: 8, #new-token: 8, #cached-token: 5823, token usage: 0.11, #running-req: 1016, #queue-req: 282, 
[2025-10-24 15:35:16] INFO:     127.0.0.1:56900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16 TP0] Prefill batch [1390], #new-seq: 1, #new-token: 1, #cached-token: 716, token usage: 0.11, #running-req: 1023, #queue-req: 281, 
[2025-10-24 15:35:16] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:57052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:57068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:60472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:60926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:32768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16 TP0] Prefill batch [1392], #new-seq: 6, #new-token: 6, #cached-token: 4391, token usage: 0.11, #running-req: 1018, #queue-req: 275, 
[2025-10-24 15:35:16] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:32894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:35118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:37032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16 TP0] Prefill batch [1394], #new-seq: 4, #new-token: 4, #cached-token: 2891, token usage: 0.11, #running-req: 1020, #queue-req: 271, 
[2025-10-24 15:35:16] INFO:     127.0.0.1:56628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:59318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:59938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:34706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:36472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:36632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16 TP0] Prefill batch [1396], #new-seq: 8, #new-token: 8, #cached-token: 5789, token usage: 0.11, #running-req: 1016, #queue-req: 263, 
[2025-10-24 15:35:16] INFO:     127.0.0.1:60482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:60586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:35772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:16] INFO:     127.0.0.1:36492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17 TP0] Prefill batch [1398], #new-seq: 4, #new-token: 4, #cached-token: 2845, token usage: 0.11, #running-req: 1020, #queue-req: 259, 
[2025-10-24 15:35:17] INFO:     127.0.0.1:57004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:57090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:60908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:33916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17 TP0] Prefill batch [1400], #new-seq: 6, #new-token: 6, #cached-token: 4325, token usage: 0.11, #running-req: 1018, #queue-req: 253, 
[2025-10-24 15:35:17] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:58178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:59016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:35364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:35598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:35922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:36544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17 TP0] Prefill batch [1402], #new-seq: 9, #new-token: 9, #cached-token: 6583, token usage: 0.11, #running-req: 1015, #queue-req: 244, 
[2025-10-24 15:35:17] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:59464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:33036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:34170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:35170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:35458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:35742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:35998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:36764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17 TP0] Prefill batch [1404], #new-seq: 10, #new-token: 10, #cached-token: 7367, token usage: 0.11, #running-req: 1014, #queue-req: 234, 
[2025-10-24 15:35:17] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:58624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:32778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:33522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:34986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:35444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17 TP0] Prefill batch [1406], #new-seq: 8, #new-token: 8, #cached-token: 5891, token usage: 0.11, #running-req: 1016, #queue-req: 226, 
[2025-10-24 15:35:17] INFO:     127.0.0.1:58772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:60294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:60648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:33372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:17] INFO:     127.0.0.1:36834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18 TP0] Prefill batch [1408], #new-seq: 6, #new-token: 6, #cached-token: 4391, token usage: 0.12, #running-req: 1018, #queue-req: 220, 
[2025-10-24 15:35:18] INFO:     127.0.0.1:57304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:60086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:34140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:35654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:35806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:36716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18 TP0] Prefill batch [1410], #new-seq: 9, #new-token: 9, #cached-token: 6482, token usage: 0.12, #running-req: 1015, #queue-req: 211, 
[2025-10-24 15:35:18] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:59730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:60914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:34296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:35730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18 TP0] Prefill batch [1412], #new-seq: 9, #new-token: 9, #cached-token: 6459, token usage: 0.12, #running-req: 1015, #queue-req: 202, 
[2025-10-24 15:35:18] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:60404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:60600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:33794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:34006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18 TP0] Prefill batch [1414], #new-seq: 5, #new-token: 5, #cached-token: 3698, token usage: 0.12, #running-req: 1019, #queue-req: 197, 
[2025-10-24 15:35:18] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:59400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:59508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:34234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:34236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:34682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18 TP0] Prefill batch [1416], #new-seq: 9, #new-token: 9, #cached-token: 6534, token usage: 0.12, #running-req: 1015, #queue-req: 188, 
[2025-10-24 15:35:18] INFO:     127.0.0.1:56184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:57610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:34054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:34856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:18] INFO:     127.0.0.1:36670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19 TP0] Prefill batch [1418], #new-seq: 11, #new-token: 11, #cached-token: 7987, token usage: 0.12, #running-req: 1013, #queue-req: 177, 
[2025-10-24 15:35:19] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:58908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:59476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:60700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19 TP0] Prefill batch [1420], #new-seq: 6, #new-token: 6, #cached-token: 4386, token usage: 0.12, #running-req: 1018, #queue-req: 171, 
[2025-10-24 15:35:19] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:33880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:35874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19 TP0] Prefill batch [1422], #new-seq: 5, #new-token: 5, #cached-token: 3521, token usage: 0.12, #running-req: 1019, #queue-req: 166, 
[2025-10-24 15:35:19] INFO:     127.0.0.1:56586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:58334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:60442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:60862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:33786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:35104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19 TP0] Prefill batch [1424], #new-seq: 9, #new-token: 9, #cached-token: 6540, token usage: 0.12, #running-req: 1015, #queue-req: 157, 
[2025-10-24 15:35:19] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:60570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:32876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:34482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:34526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19] INFO:     127.0.0.1:35294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:19 TP0] Prefill batch [1426], #new-seq: 7, #new-token: 7, #cached-token: 5069, token usage: 0.12, #running-req: 1017, #queue-req: 150, 
[2025-10-24 15:35:20] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:58246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:59636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:60594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:33446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:33528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:34972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:35564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:36054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20 TP0] Prefill batch [1428], #new-seq: 12, #new-token: 12, #cached-token: 8838, token usage: 0.12, #running-req: 1012, #queue-req: 138, 
[2025-10-24 15:35:20] INFO:     127.0.0.1:56858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:34158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:35186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:35736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:36070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20 TP0] Prefill batch [1430], #new-seq: 6, #new-token: 6, #cached-token: 4334, token usage: 0.12, #running-req: 1018, #queue-req: 132, 
[2025-10-24 15:35:20] INFO:     127.0.0.1:58538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:58894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:58992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:59916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:34780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:34834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:35828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:35950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20 TP0] Prefill batch [1432], #new-seq: 11, #new-token: 11, #cached-token: 8049, token usage: 0.12, #running-req: 1013, #queue-req: 121, 
[2025-10-24 15:35:20] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:56822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:57576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:59052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:59154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:59186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:59650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:60184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:33302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:33948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:34774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:34930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:35858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:35992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:36382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20 TP0] Prefill batch [1434], #new-seq: 18, #new-token: 18, #cached-token: 13315, token usage: 0.12, #running-req: 1006, #queue-req: 103, 
[2025-10-24 15:35:20] INFO:     127.0.0.1:56908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:57418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:58274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:58298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:59024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:33674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:35676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:35852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20] INFO:     127.0.0.1:36266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:20 TP0] Prefill batch [1436], #new-seq: 14, #new-token: 14, #cached-token: 10174, token usage: 0.12, #running-req: 1010, #queue-req: 89, 
[2025-10-24 15:35:21] INFO:     127.0.0.1:57590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:36136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21 TP0] Prefill batch [1438], #new-seq: 3, #new-token: 3, #cached-token: 2191, token usage: 0.12, #running-req: 1021, #queue-req: 86, 
[2025-10-24 15:35:21] INFO:     127.0.0.1:56380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:58848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:59172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:60984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:35198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21 TP0] Prefill batch [1440], #new-seq: 12, #new-token: 12, #cached-token: 8707, token usage: 0.13, #running-req: 1012, #queue-req: 74, 
[2025-10-24 15:35:21] INFO:     127.0.0.1:56694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:57888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:59528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:35848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21 TP0] Prefill batch [1442], #new-seq: 12, #new-token: 12, #cached-token: 8908, token usage: 0.13, #running-req: 1012, #queue-req: 62, 
[2025-10-24 15:35:21] INFO:     127.0.0.1:60062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:60942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:34218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:34714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:35600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:35648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:35652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:36160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:36802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21 TP0] Prefill batch [1444], #new-seq: 14, #new-token: 14, #cached-token: 10305, token usage: 0.13, #running-req: 1010, #queue-req: 48, 
[2025-10-24 15:35:21] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:59306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:33890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:34372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:34546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:21] INFO:     127.0.0.1:36290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22 TP0] Prefill batch [1446], #new-seq: 11, #new-token: 11, #cached-token: 7964, token usage: 0.13, #running-req: 1013, #queue-req: 37, 
[2025-10-24 15:35:22] INFO:     127.0.0.1:56678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:58232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:59242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:34256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:34756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:34922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22 TP0] Prefill batch [1448], #new-seq: 13, #new-token: 13, #cached-token: 9447, token usage: 0.13, #running-req: 1011, #queue-req: 24, 
[2025-10-24 15:35:22] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:56494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:59246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:34418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:37024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22 TP0] Prefill batch [1450], #new-seq: 9, #new-token: 9, #cached-token: 6621, token usage: 0.13, #running-req: 1015, #queue-req: 15, 
[2025-10-24 15:35:22] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:59638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:60304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:33798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:34608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:35058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:35720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:36096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:36914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22 TP0] Prefill batch [1452], #new-seq: 10, #new-token: 10, #cached-token: 7274, token usage: 0.13, #running-req: 1014, #queue-req: 5, 
[2025-10-24 15:35:22] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:59356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:34650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:36728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22 TP0] Prefill batch [1454], #new-seq: 5, #new-token: 5, #cached-token: 3618, token usage: 0.13, #running-req: 1016, #queue-req: 0, 
[2025-10-24 15:35:22] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:58676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:60622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:60738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:32834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:33204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:33812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:34052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:34488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:22] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:58642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:58880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:60350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:60606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:33748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:36014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:36026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:36068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:56402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:56952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:58528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:58850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:60674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:33968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:34804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:35210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:57532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:32962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:34312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:34558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:35544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:36338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:56516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:58740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:60640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:32922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:33006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:33944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:34458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:35708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:35904 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP2] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP4] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP6] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP0] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP3] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP7] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP1] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP5] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:58318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:60934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:32792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:33084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:34802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:34998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:35042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:35420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:35980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:36614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23 TP0] Decode batch [1461], #running-req: 965, #token: 120174, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5076.51, #queue-req: 0, 
[2025-10-24 15:35:23] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:60050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:33138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:33710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:34202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:35354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:35926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:35952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:36748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:56852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:60198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:34030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:23] INFO:     127.0.0.1:35322 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP2] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP3] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP0] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP6] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP7] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP4] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP1] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:23 TP5] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24] INFO:     127.0.0.1:56512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:57598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:60078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:34614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:36042 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP2] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP6] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP4] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP0] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP3] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP7] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP1] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP5] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:59762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:60776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35524 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP2] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP4] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP6] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP0] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP3] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP7] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP1] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP5] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24] INFO:     127.0.0.1:56280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:57260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:58756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:59996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:34104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:36276 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP2] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP6] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP4] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP0] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP3] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP7] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP1] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP5] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:33196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:33324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:34788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:36296 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP2] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP4] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP6] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP0] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP3] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP7] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP1] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP5] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:56826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:58638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:60664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:36556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:36986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:37036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:57106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:58506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:58696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:59120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:59386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:59750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:60986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:32908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:32936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:33286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:36624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:56230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:60708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:34428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:34514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:34726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:34884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:36250 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP2] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP6] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP4] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP3] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP0] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP7] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP1] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24 TP5] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:24] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:34302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:35606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:36104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:24] INFO:     127.0.0.1:36260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:58492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:59542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:33238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:33360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:35190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:35594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:56964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:59006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:37106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:34404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:34964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:36206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:36398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:37482 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP2] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP6] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP4] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP3] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP0] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP7] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP1] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP5] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25] INFO:     127.0.0.1:57386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:32880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:33992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:34744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:34816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:35580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:35814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:35888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:37702 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP2] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP6] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP4] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP0] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP3] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP7] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP1] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP5] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25] INFO:     127.0.0.1:59292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:33126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:33210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:34024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:33180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:33564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:35572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:37128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:58818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:59684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:33690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:33964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:35256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:36410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP2] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP4] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP6] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP0] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP3] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP7] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP1] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP5] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:33594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:34926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:35306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:36130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:36434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:25] INFO:     127.0.0.1:36654 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP2] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP4] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP6] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP7] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP0] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP3] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP1] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:25 TP5] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP2] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP4] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP6] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP0] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP3] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP7] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP1] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP5] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26] INFO:     127.0.0.1:34358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:35958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:36682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:36990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:38212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:56868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:60846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:35336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:35702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:36572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:36818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:36840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:38216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:57076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:57226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:34912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:35756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:36476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:37086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:37240 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP2] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP6] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP4] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP0] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP3] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP7] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP1] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP5] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:59992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:33208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:33550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:35382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:36440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:37646 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP2] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP4] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP6] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP3] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP0] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP7] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP1] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP5] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26] INFO:     127.0.0.1:56220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:60008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:34476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP2] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP4] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP6] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP0] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP3] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP7] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP1] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP5] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:57292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:59218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:60262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:60356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:60760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:33260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:34130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:34696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:35784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:37008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:37182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:37718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:37886 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP2] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP4] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP6] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP7] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP0] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP3] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP1] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP5] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:32790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:32974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:33852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:35074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:35342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:38138 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP2] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP4] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP6] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP1] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP0] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP3] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP7] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP5] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:60646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:34192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:34582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:37040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:37804 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP2] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP0] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP6] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP4] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP7] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP3] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP1] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP5] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:59890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:35462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:36858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:37006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:37836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:38326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:26] INFO:     127.0.0.1:39260 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP2] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP6] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP4] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP0] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP7] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP3] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP1] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:26 TP5] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:34342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:36082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:38182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:38434 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP2] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP6] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP4] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP3] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP7] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP0] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP1] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP5] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:56934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:58216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:35148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:37602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:37858 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP2] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP6] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP4] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP3] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP0] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP7] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP1] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP5] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27] INFO:     127.0.0.1:57272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:58920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:33394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:34572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:36188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:36310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:37120 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP2] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP4] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP6] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP3] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP0] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP7] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP1] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP5] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:57188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:33292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:33894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:36982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:37022 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP2] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP4] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP6] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP0] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP3] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP7] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP1] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP5] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27] INFO:     127.0.0.1:56580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:36214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:36244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:36444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:37058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:39418 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP2] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP4] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP6] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP0] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP3] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP7] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP1] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP5] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:60216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:33212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:33244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:34766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:34828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:37700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:38638 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP2] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP4] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP6] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP7] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP0] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP3] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP1] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP5] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:35666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:36938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:37742 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP2] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP6] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP4] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP3] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP0] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP7] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP1] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP5] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27] INFO:     127.0.0.1:56550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:58348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:33838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:34160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:34462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:34850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:34900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:35912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:38298 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP2] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP4] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP6] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP0] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP3] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP7] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP1] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP5] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:57410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:33782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:33814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:35362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:27] INFO:     127.0.0.1:37294 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP2] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP4] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP6] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP3] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP0] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP7] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP1] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:27 TP5] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:59200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:33404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:35302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:35536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:38392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:39290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP2] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP6] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP4] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP0] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP3] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP7] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP1] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP5] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28] INFO:     127.0.0.1:56302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:33828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:35798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37148 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP6] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP4] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP2] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP0] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP7] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP3] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP1] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP5] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:59368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:33770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:33970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:36416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:38656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:38796 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP2] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP4] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP6] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP0] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP3] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP7] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP1] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP5] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:34940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:35440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:36034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:38278 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP2] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP4] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP6] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP0] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP3] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP7] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP1] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP5] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP0] Decode batch [1501], #running-req: 545, #token: 85459, token usage: 0.09, cuda graph: False, gen throughput (token/s): 6460.62, #queue-req: 0, 
[2025-10-24 15:35:28] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:36532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:38158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:38310 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP2] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP6] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP4] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP3] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP7] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP0] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP1] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP5] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:57976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:58962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:34260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:34982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:38196 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP2] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP6] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP4] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP1] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP3] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP0] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP5] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28 TP7] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:28] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:59928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:34114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:34172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:35254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:36506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:34184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:34290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:34564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:39276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:33744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:33960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:35532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:35538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:36144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:60836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:35178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:38092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:38908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:57030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:59598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:38816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:35502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:36856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:28] INFO:     127.0.0.1:37986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:60236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:32812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:34038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:34042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:35512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:57054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:36732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:59288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:60178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:33328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:33696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:60886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:35428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:58736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:59414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:59480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:32818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:33100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:35276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:36588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:56588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:60958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:33132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:33416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:34440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:35396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:56462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:58460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:36892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:57318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:35088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:36782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:57764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:58994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:33028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:33622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:34874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:35622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:36064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:36088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:60860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:34446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:34700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:57736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:33162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:34090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:34956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:35034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:35936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:56742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:58476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:34652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:36446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:34500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:35222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:38634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:39646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:58080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:58526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:60960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:33984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:29] INFO:     127.0.0.1:37846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:34082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:56418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:59792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:36204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:35036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:59694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:60970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:34110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:57518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:57846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:60258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:58926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:36974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:34634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:36366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:33530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:34148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:34072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:36358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:34436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:36648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:59924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:35114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:56810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:33664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:33780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:37468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:35066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:39580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30 TP0] Decode batch [1541], #running-req: 236, #token: 46817, token usage: 0.05, cuda graph: True, gen throughput (token/s): 5880.03, #queue-req: 0, 
[2025-10-24 15:35:30] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:30] INFO:     127.0.0.1:38966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:37372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:34792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:36952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:39216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:39200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:36622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:37356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:37472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:39096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:57164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:58258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:39224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:39306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:39454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:37384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:39386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:35280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:60870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:34730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:35890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:58438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:37168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:60084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:36696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:37054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:39034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:39374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:60884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:37566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:33178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:35040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:36516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:36882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:37434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:33154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:35692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:34426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:31] INFO:     127.0.0.1:36798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:39086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:59328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:37224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:37412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:37428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:37630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:35842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:39336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:34084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:39152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:39460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:37594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:59780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:33274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:36960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:37216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:37446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:37810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:33070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:35718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:39166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:60214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:37112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:37492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:39056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:60138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:36176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:39486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:39612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:36924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:57152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:32806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32 TP0] Decode batch [1581], #running-req: 93, #token: 22593, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3419.89, #queue-req: 0, 
[2025-10-24 15:35:32] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:34668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:58888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:36768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:38702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:39278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:39332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:36786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:37542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:32] INFO:     127.0.0.1:37994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:60164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:37772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:38314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:56966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:37208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:38380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:39462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:56724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:35128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:37258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:34904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:39080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:38978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:39078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:36324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:37132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:38078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:39642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:38928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:37552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:39140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:37902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:38734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:39008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:34262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:36948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:39124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:39210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:33] INFO:     127.0.0.1:39478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:35010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:36120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:38382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:57544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:35238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:37670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:34400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34 TP0] Decode batch [1621], #running-req: 33, #token: 9809, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1594.13, #queue-req: 0, 
[2025-10-24 15:35:34] INFO:     127.0.0.1:59862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:38568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:38952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:39398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:39518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:38280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:33144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:38328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:38500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:56250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:36850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:37608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:37734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:37914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:34] INFO:     127.0.0.1:56688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:35] INFO:     127.0.0.1:34860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:35] INFO:     127.0.0.1:37692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:35] INFO:     127.0.0.1:38642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:35] INFO:     127.0.0.1:37626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:35] INFO:     127.0.0.1:38836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:35] INFO:     127.0.0.1:37534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:35 TP0] Decode batch [1661], #running-req: 8, #token: 3001, token usage: 0.00, cuda graph: True, gen throughput (token/s): 702.78, #queue-req: 0, 
[2025-10-24 15:35:35] INFO:     127.0.0.1:38320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:35] INFO:     127.0.0.1:36866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:35] INFO:     127.0.0.1:38554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:35] INFO:     127.0.0.1:37984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:35] INFO:     127.0.0.1:35140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:36] INFO:     127.0.0.1:38430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:36 TP0] Decode batch [1701], #running-req: 2, #token: 1073, token usage: 0.00, cuda graph: True, gen throughput (token/s): 174.66, #queue-req: 0, 
[2025-10-24 15:35:36 TP0] Decode batch [1741], #running-req: 1, #token: 1113, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.51, #queue-req: 0, 
[2025-10-24 15:35:37 TP0] Decode batch [1781], #running-req: 1, #token: 1153, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.40, #queue-req: 0, 
[2025-10-24 15:35:38 TP0] Decode batch [1821], #running-req: 1, #token: 1193, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.40, #queue-req: 0, 
[2025-10-24 15:35:39 TP0] Decode batch [1861], #running-req: 1, #token: 1233, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.40, #queue-req: 0, 
[2025-10-24 15:35:39] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:52] INFO:     127.0.0.1:38628 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-24 15:35:52 TP0] Prefill batch [1892], #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:35:52] INFO:     127.0.0.1:38630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:52 TP0] Prefill batch [1893], #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:35:52 TP0] Prefill batch [1894], #new-seq: 39, #new-token: 39, #cached-token: 28345, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:35:53 TP0] Prefill batch [1895], #new-seq: 46, #new-token: 46, #cached-token: 33356, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP0] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP2] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP3] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP1] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP7] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP6] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP4] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP5] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP0] Prefill batch [1896], #new-seq: 49, #new-token: 49, #cached-token: 35822, token usage: 0.01, #running-req: 86, #queue-req: 0, 
[2025-10-24 15:35:53 TP0] Prefill batch [1897], #new-seq: 54, #new-token: 54, #cached-token: 39516, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[2025-10-24 15:35:53 TP0] Prefill batch [1898], #new-seq: 61, #new-token: 61, #cached-token: 44361, token usage: 0.02, #running-req: 189, #queue-req: 0, 
[2025-10-24 15:35:53 TP0] Prefill batch [1899], #new-seq: 56, #new-token: 56, #cached-token: 40632, token usage: 0.02, #running-req: 250, #queue-req: 0, 
[2025-10-24 15:35:53 TP0] Prefill batch [1900], #new-seq: 64, #new-token: 64, #cached-token: 46659, token usage: 0.02, #running-req: 306, #queue-req: 0, 
[2025-10-24 15:35:53 TP0] Prefill batch [1901], #new-seq: 61, #new-token: 61, #cached-token: 44541, token usage: 0.03, #running-req: 370, #queue-req: 0, 
[2025-10-24 15:35:53 TP0] Prefill batch [1902], #new-seq: 15, #new-token: 15, #cached-token: 10958, token usage: 0.03, #running-req: 431, #queue-req: 0, 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP2] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP3] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP0] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP1] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP6] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP7] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP4] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:53 TP5] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP0] Prefill batch [1906], #new-seq: 12, #new-token: 12, #cached-token: 8694, token usage: 0.03, #running-req: 446, #queue-req: 0, 
[2025-10-24 15:35:54 TP0] Prefill batch [1907], #new-seq: 78, #new-token: 78, #cached-token: 56555, token usage: 0.03, #running-req: 458, #queue-req: 0, 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP0] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP4] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP6] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP1] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP3] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP2] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP7] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP5] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP0] Prefill batch [1908], #new-seq: 50, #new-token: 50, #cached-token: 36227, token usage: 0.04, #running-req: 536, #queue-req: 0, 
[2025-10-24 15:35:54 TP0] Prefill batch [1909], #new-seq: 90, #new-token: 90, #cached-token: 65489, token usage: 0.04, #running-req: 586, #queue-req: 0, 
[2025-10-24 15:35:54 TP0] Prefill batch [1910], #new-seq: 53, #new-token: 53, #cached-token: 38634, token usage: 0.05, #running-req: 676, #queue-req: 0, 
[2025-10-24 15:35:54 TP0] Prefill batch [1911], #new-seq: 99, #new-token: 99, #cached-token: 72277, token usage: 0.05, #running-req: 729, #queue-req: 0, 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP0] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP2] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP3] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP1] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP7] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP6] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP4] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP5] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:54 TP0] Prefill batch [1912], #new-seq: 56, #new-token: 56, #cached-token: 40682, token usage: 0.06, #running-req: 828, #queue-req: 0, 
[2025-10-24 15:35:55 TP0] Prefill batch [1913], #new-seq: 105, #new-token: 105, #cached-token: 76604, token usage: 0.06, #running-req: 884, #queue-req: 0, 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:55 TP2] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:55 TP0] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:55 TP7] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:55 TP6] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:55 TP3] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:55 TP1] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:55 TP4] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:55 TP5] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:35:55 TP0] Prefill batch [1914], #new-seq: 35, #new-token: 35, #cached-token: 25803, token usage: 0.06, #running-req: 989, #queue-req: 24, 
[2025-10-24 15:35:56 TP0] Decode batch [1921], #running-req: 1024, #token: 70235, token usage: 0.07, cuda graph: False, gen throughput (token/s): 440.86, #queue-req: 295, 
[2025-10-24 15:35:58] INFO:     127.0.0.1:41550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:58 TP0] Prefill batch [1942], #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-24 15:35:59] INFO:     127.0.0.1:41582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:35:59 TP0] Prefill batch [1949], #new-seq: 1, #new-token: 1, #cached-token: 723, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-24 15:36:00] INFO:     127.0.0.1:39054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:00] INFO:     127.0.0.1:42238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:00] INFO:     127.0.0.1:38662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:00] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:00 TP0] Prefill batch [1955], #new-seq: 2, #new-token: 2, #cached-token: 1458, token usage: 0.10, #running-req: 1022, #queue-req: 291, 
[2025-10-24 15:36:00] INFO:     127.0.0.1:39496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:00] INFO:     127.0.0.1:40862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:00] INFO:     127.0.0.1:41430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:00 TP0] Prefill batch [1957], #new-seq: 5, #new-token: 5, #cached-token: 3790, token usage: 0.11, #running-req: 1019, #queue-req: 286, 
[2025-10-24 15:36:00] INFO:     127.0.0.1:39398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:00 TP0] Prefill batch [1959], #new-seq: 1, #new-token: 1, #cached-token: 726, token usage: 0.11, #running-req: 1023, #queue-req: 285, 
[2025-10-24 15:36:00] INFO:     127.0.0.1:38914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:00] INFO:     127.0.0.1:38924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:00] INFO:     127.0.0.1:39070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:00] INFO:     127.0.0.1:43296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:00] INFO:     127.0.0.1:44496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:00 TP0] Prefill batch [1961], #new-seq: 5, #new-token: 5, #cached-token: 3674, token usage: 0.11, #running-req: 1019, #queue-req: 280, 
[2025-10-24 15:36:01] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:41940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01 TP0] Prefill batch [1963], #new-seq: 3, #new-token: 3, #cached-token: 2178, token usage: 0.11, #running-req: 1021, #queue-req: 277, 
[2025-10-24 15:36:01] INFO:     127.0.0.1:40804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:41834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:42410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:43262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:47326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01 TP0] Prefill batch [1965], #new-seq: 5, #new-token: 5, #cached-token: 3612, token usage: 0.11, #running-req: 1019, #queue-req: 272, 
[2025-10-24 15:36:01] INFO:     127.0.0.1:43134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:43180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:43586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:46094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:46618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01 TP0] Prefill batch [1967], #new-seq: 5, #new-token: 5, #cached-token: 3583, token usage: 0.11, #running-req: 1019, #queue-req: 267, 
[2025-10-24 15:36:01] INFO:     127.0.0.1:39038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:39950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:42058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:43684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:44282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:45606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:47414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:47566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01 TP0] Prefill batch [1969], #new-seq: 8, #new-token: 8, #cached-token: 5794, token usage: 0.11, #running-req: 1016, #queue-req: 259, 
[2025-10-24 15:36:01] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:38874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:40926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:40982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:41536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:43908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:46786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01] INFO:     127.0.0.1:47426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:01 TP0] Prefill batch [1971], #new-seq: 8, #new-token: 8, #cached-token: 5765, token usage: 0.11, #running-req: 1016, #queue-req: 251, 
[2025-10-24 15:36:02 TP0] Decode batch [1971], #running-req: 1016, #token: 108548, token usage: 0.11, cuda graph: False, gen throughput (token/s): 7194.30, #queue-req: 251, 
[2025-10-24 15:36:02] INFO:     127.0.0.1:39066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:39826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:43090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:44788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:47008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02 TP0] Prefill batch [1973], #new-seq: 5, #new-token: 5, #cached-token: 3699, token usage: 0.11, #running-req: 1019, #queue-req: 246, 
[2025-10-24 15:36:02] INFO:     127.0.0.1:39576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:40706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:42958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:47480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02 TP0] Prefill batch [1975], #new-seq: 6, #new-token: 6, #cached-token: 4469, token usage: 0.11, #running-req: 1018, #queue-req: 240, 
[2025-10-24 15:36:02] INFO:     127.0.0.1:41260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:43100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:45078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:46148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:46750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:47828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02 TP0] Prefill batch [1977], #new-seq: 7, #new-token: 7, #cached-token: 5040, token usage: 0.11, #running-req: 1017, #queue-req: 233, 
[2025-10-24 15:36:02] INFO:     127.0.0.1:39372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:39462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:40016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:40584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:41776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:42540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:45134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:45932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:46454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02 TP0] Prefill batch [1979], #new-seq: 9, #new-token: 9, #cached-token: 6667, token usage: 0.12, #running-req: 1015, #queue-req: 224, 
[2025-10-24 15:36:02] INFO:     127.0.0.1:39322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:39906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:40348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:41934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:43498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:43618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:02] INFO:     127.0.0.1:44362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03 TP0] Prefill batch [1981], #new-seq: 9, #new-token: 9, #cached-token: 6576, token usage: 0.12, #running-req: 1015, #queue-req: 215, 
[2025-10-24 15:36:03] INFO:     127.0.0.1:38826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:39408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:40308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:40970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:42834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:43804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:44226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:46818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03 TP0] Prefill batch [1983], #new-seq: 8, #new-token: 8, #cached-token: 5715, token usage: 0.12, #running-req: 1016, #queue-req: 207, 
[2025-10-24 15:36:03] INFO:     127.0.0.1:40386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:40404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:40460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:43924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:45220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:46764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:47910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03 TP0] Prefill batch [1985], #new-seq: 7, #new-token: 7, #cached-token: 5043, token usage: 0.12, #running-req: 1017, #queue-req: 200, 
[2025-10-24 15:36:03] INFO:     127.0.0.1:38638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:40110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:40638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:41384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:41982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:42740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:42932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:44554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:44674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:44910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03 TP0] Prefill batch [1987], #new-seq: 11, #new-token: 11, #cached-token: 8009, token usage: 0.12, #running-req: 1013, #queue-req: 189, 
[2025-10-24 15:36:03] INFO:     127.0.0.1:39838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:40144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:40912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:41620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:41944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:43370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:44312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:45142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:45574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03 TP0] Prefill batch [1989], #new-seq: 9, #new-token: 9, #cached-token: 6571, token usage: 0.12, #running-req: 1015, #queue-req: 180, 
[2025-10-24 15:36:03] INFO:     127.0.0.1:40718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:44942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:45262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:45744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:03] INFO:     127.0.0.1:47598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04 TP0] Prefill batch [1991], #new-seq: 5, #new-token: 5, #cached-token: 3600, token usage: 0.12, #running-req: 1019, #queue-req: 175, 
[2025-10-24 15:36:04] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:41182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:43192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:44790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:45800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04 TP0] Prefill batch [1993], #new-seq: 6, #new-token: 6, #cached-token: 4353, token usage: 0.12, #running-req: 1018, #queue-req: 169, 
[2025-10-24 15:36:04] INFO:     127.0.0.1:40416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:41114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:43664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:44752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:46632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04 TP0] Prefill batch [1995], #new-seq: 5, #new-token: 5, #cached-token: 3560, token usage: 0.12, #running-req: 1019, #queue-req: 164, 
[2025-10-24 15:36:04] INFO:     127.0.0.1:38750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:39638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:40152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:40998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:42094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:43574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:44652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:45510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:46288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:46772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04 TP0] Prefill batch [1997], #new-seq: 13, #new-token: 13, #cached-token: 9445, token usage: 0.12, #running-req: 1011, #queue-req: 151, 
[2025-10-24 15:36:04] INFO:     127.0.0.1:39326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:43150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:43152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:44456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:45380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04] INFO:     127.0.0.1:47748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:04 TP0] Prefill batch [1999], #new-seq: 7, #new-token: 7, #cached-token: 5131, token usage: 0.12, #running-req: 1017, #queue-req: 144, 
[2025-10-24 15:36:05] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:41374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:41486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:41970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:42056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:43510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:45934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:47040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05 TP0] Prefill batch [2001], #new-seq: 9, #new-token: 9, #cached-token: 6626, token usage: 0.12, #running-req: 1015, #queue-req: 135, 
[2025-10-24 15:36:05] INFO:     127.0.0.1:38882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:39884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:41562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:41710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:42130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:43682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:44318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:44384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:44894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:45038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:46162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:47296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05 TP0] Prefill batch [2003], #new-seq: 15, #new-token: 15, #cached-token: 10938, token usage: 0.12, #running-req: 1009, #queue-req: 120, 
[2025-10-24 15:36:05] INFO:     127.0.0.1:39124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:39200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:39510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:39792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:40232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:40392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:40870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:40948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:41546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:41654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:41692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:42352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:43816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:45158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:45690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:45728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:46828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:46960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05 TP0] Prefill batch [2005], #new-seq: 19, #new-token: 19, #cached-token: 14064, token usage: 0.12, #running-req: 1005, #queue-req: 101, 
[2025-10-24 15:36:05] INFO:     127.0.0.1:39764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:40094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:41318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:43026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:44684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:44808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:45064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:45666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:46194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:46854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:47334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05 TP0] Prefill batch [2007], #new-seq: 12, #new-token: 12, #cached-token: 8725, token usage: 0.12, #running-req: 1012, #queue-req: 89, 
[2025-10-24 15:36:05] INFO:     127.0.0.1:38824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:39040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:41270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:41400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:42158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:42650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:43472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05] INFO:     127.0.0.1:46860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:05 TP0] Prefill batch [2009], #new-seq: 11, #new-token: 11, #cached-token: 7945, token usage: 0.12, #running-req: 1013, #queue-req: 78, 
[2025-10-24 15:36:06] INFO:     127.0.0.1:40332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:41278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:41998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:43758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:44506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:45948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:47164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:47870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06 TP0] Prefill batch [2011], #new-seq: 10, #new-token: 10, #cached-token: 7372, token usage: 0.12, #running-req: 1014, #queue-req: 68, 
[2025-10-24 15:36:06] INFO:     127.0.0.1:39392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:39688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:40578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:42506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:46874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06 TP0] Prefill batch [2013], #new-seq: 5, #new-token: 5, #cached-token: 3665, token usage: 0.13, #running-req: 1019, #queue-req: 63, 
[2025-10-24 15:36:06] INFO:     127.0.0.1:40896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:41792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:41828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:41958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:42348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:43458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:43582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:44446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:44540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:46206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:46844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:47024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06 TP0] Prefill batch [2015], #new-seq: 12, #new-token: 12, #cached-token: 8923, token usage: 0.13, #running-req: 1012, #queue-req: 51, 
[2025-10-24 15:36:06] INFO:     127.0.0.1:39432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:40126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:41784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:42610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:44348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:44486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:44736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:44762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:44930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:45608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:45640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:46620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:46686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06 TP0] Prefill batch [2017], #new-seq: 16, #new-token: 16, #cached-token: 11562, token usage: 0.13, #running-req: 1008, #queue-req: 35, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 15:36:06] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:39546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:40472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:41632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:43422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:43540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:44124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:44376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:44598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:45456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:46066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:06] INFO:     127.0.0.1:47264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07 TP0] Prefill batch [2019], #new-seq: 14, #new-token: 14, #cached-token: 10257, token usage: 0.13, #running-req: 1010, #queue-req: 21, 
[2025-10-24 15:36:07] INFO:     127.0.0.1:39424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:42126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:43722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:44080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:44210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:45848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:46138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:47620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07 TP0] Prefill batch [2021], #new-seq: 9, #new-token: 9, #cached-token: 6523, token usage: 0.13, #running-req: 1015, #queue-req: 12, 
[2025-10-24 15:36:07] INFO:     127.0.0.1:40174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:41860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:42838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:45122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:45278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:45548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:45664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07 TP0] Prefill batch [2023], #new-seq: 8, #new-token: 8, #cached-token: 5845, token usage: 0.13, #running-req: 1016, #queue-req: 4, 
[2025-10-24 15:36:07] INFO:     127.0.0.1:40262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:40746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:42492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:44956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:44960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:45498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:46744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:47720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:47768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07 TP0] Prefill batch [2025], #new-seq: 4, #new-token: 4, #cached-token: 2890, token usage: 0.13, #running-req: 1014, #queue-req: 0, 
[2025-10-24 15:36:07] INFO:     127.0.0.1:39100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:40160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:40620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:40934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:44880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:45200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:47382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:39490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:39616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:39678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:40328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:40532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:40778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:44566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:45366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:46028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:47540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:07] INFO:     127.0.0.1:47780 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:07 TP2] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:07 TP4] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:07 TP6] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:07 TP0] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:07 TP3] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:07 TP7] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:07 TP1] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:07 TP5] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08] INFO:     127.0.0.1:39078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:39112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:41142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:41344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:43116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:43490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:44008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:46996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:47082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP2] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP6] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP4] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP0] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP3] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP7] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP1] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP5] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08] INFO:     127.0.0.1:38978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:41198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:41810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:42504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:43264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:43772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:44834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:45714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:45868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:46016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:47852 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP2] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP6] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP4] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP0] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP3] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP7] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP1] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP5] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08] INFO:     127.0.0.1:39012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:40026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:40760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:41064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:42868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:43360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:43648 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP2] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP6] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP4] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP0] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP3] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP7] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP1] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP5] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08] INFO:     127.0.0.1:39476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:39668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:43484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:43572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:43632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:43694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:43732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:46716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:46982 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP4] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP2] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP6] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP1] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP0] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP3] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP7] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP5] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08] INFO:     127.0.0.1:38930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:39190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:40562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:43996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:46596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:47028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP2] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP0] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP4] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP6] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP3] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP7] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP1] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP5] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08] INFO:     127.0.0.1:38724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:38910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:39386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:40844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:41272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:42514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:42686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:42920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:46942 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP4] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP2] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP6] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP0] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP3] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP7] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP1] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP5] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08] INFO:     127.0.0.1:39146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:39272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:40494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:41052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:42140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:42250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:45464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:46330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:47660 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP4] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP6] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP7] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP2] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP5] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP0] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP3] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08 TP1] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:08] INFO:     127.0.0.1:38678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:39090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:39238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:40188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:40678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:41230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:41696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:42466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:42666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:44338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:44994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:45982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:46526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:47062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:08] INFO:     127.0.0.1:48002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:39174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:40764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:42028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:42054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:42438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:45164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:46358 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP2] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP0] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP4] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP6] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP3] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP7] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP1] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP5] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09] INFO:     127.0.0.1:38994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:39276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:39592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:39912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:40124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:41020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:44334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:44916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:46178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:46672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:47268 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP0] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP4] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP2] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP6] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP3] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP7] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP1] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP5] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09] INFO:     127.0.0.1:39924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:41880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:42246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:42784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:43692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:44662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:45910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:46292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:46488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:46794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09 TP0] Decode batch [2039], #running-req: 887, #token: 116325, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5383.41, #queue-req: 0, 
[2025-10-24 15:36:09] INFO:     127.0.0.1:39932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:42420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:43884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:44020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:45986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:47256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:47888 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP2] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP6] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP4] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP0] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP3] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP7] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP1] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP5] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09] INFO:     127.0.0.1:39604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:45676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:39628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:39666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:39896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:40252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:41062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:41898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:42098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:43504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:43580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:43708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:44146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:44624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:45288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:45628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:45774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:46098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:47218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:38946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:39750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:40080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:40292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:41502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:42014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:42026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:44830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:46866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:47126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:47228 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP2] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP6] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP4] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP0] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP3] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP7] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP1] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP5] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09] INFO:     127.0.0.1:42584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:43676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:46612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:47104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:09] INFO:     127.0.0.1:47154 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP2] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP6] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP4] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP0] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP3] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP7] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP1] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:09 TP5] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10] INFO:     127.0.0.1:42702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:45340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:45450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:45712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:46972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:38894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:39262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:41622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:42494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:42864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:42986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:45286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:45724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:47212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:47344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:47728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:48390 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP2] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP6] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP4] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP0] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP3] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP7] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP1] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP5] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10] INFO:     127.0.0.1:38676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:38844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:39648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:40530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:42070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:42418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:46606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:46826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:47706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:48616 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP2] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP6] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP4] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP0] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP3] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP7] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP1] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP5] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10] INFO:     127.0.0.1:40708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:48016 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP2] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP6] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP4] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP1] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP0] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP3] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP7] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP5] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10] INFO:     127.0.0.1:38818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:38938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:39464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:44194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:48096 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP2] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP0] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP6] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP4] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP3] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP7] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP1] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP5] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP2] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP6] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP4] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP0] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP3] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP7] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP1] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP5] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10] INFO:     127.0.0.1:42472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:42776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:42884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:44418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:44538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:44786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:45232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:47866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:39556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:39672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:39948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:44270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:44508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:47592 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP2] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP6] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP4] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP1] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP0] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP3] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP7] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP5] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10] INFO:     127.0.0.1:40116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:42152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:43464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:44432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:45258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:45856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:46252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:46984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:48658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:10] INFO:     127.0.0.1:49068 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP2] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP0] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP4] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP6] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP3] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP7] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP1] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:10 TP5] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11] INFO:     127.0.0.1:38716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:38862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:39662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:39740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:40954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:41262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:41854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:42456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:42568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:43610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:44050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:45392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:46356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:47516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:47860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:49078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:38962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:39872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:41290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:41594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:42484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:43094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:43258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:43846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:44086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:45026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:46218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:48158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:48542 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP2] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP6] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP4] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP0] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP3] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP7] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP1] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP5] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11] INFO:     127.0.0.1:41782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:42172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:42434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:44280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:46412 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP2] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP4] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP6] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP0] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP3] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP7] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP1] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP5] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11] INFO:     127.0.0.1:40668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:41976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:42302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:43266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:44698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:45364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:45448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:45536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:46210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:47182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:48774 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP2] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP6] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP4] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP0] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP3] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP7] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP1] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP5] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11] INFO:     127.0.0.1:38766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:41070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:42558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:45590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:46320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:46346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:46780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:46782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:49004 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP2] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP6] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP4] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP0] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP3] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP7] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP1] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP5] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11] INFO:     127.0.0.1:38802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:40806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:41440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:41670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:42338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:43408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:44724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:45652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:45824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:46062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:47066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:48692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:49740 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP2] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP6] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP4] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP0] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP7] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP3] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP1] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP5] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11] INFO:     127.0.0.1:38998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:39524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:40298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:40732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:43746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:45108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:45492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:47628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:49208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:50186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:38674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:39882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:41018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:42198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:44108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:44242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:45322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:47450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:48766 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP2] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP4] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP6] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP0] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP3] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP7] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP1] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11 TP5] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:11] INFO:     127.0.0.1:39318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:40208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:41424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:41454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:41914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:42340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:42946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:43222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:43400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:44430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:47376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:11] INFO:     127.0.0.1:47812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:38776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:39340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:39346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:39442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:39572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:41146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:42810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:43900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:47794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:48788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:39368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:42040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:43170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:44034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:44800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:44866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:45476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:47272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:48010 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP2] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP4] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP6] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP0] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP3] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP7] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP1] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP5] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12] INFO:     127.0.0.1:38772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:39966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:41442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:41766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:42184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:43280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:44772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:48502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP2] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP6] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP4] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP0] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP3] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP7] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP1] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP5] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12] INFO:     127.0.0.1:38868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:40614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:43064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:44688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:47622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:49586 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP2] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP4] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP6] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP0] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP3] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP7] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP1] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP5] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12] INFO:     127.0.0.1:39530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:40590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:40652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:42824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:43638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP2] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP6] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP4] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP0] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP3] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP7] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP1] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP5] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12] INFO:     127.0.0.1:38730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:39212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:39726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:39992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:40372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:41224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:41808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:42218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:42638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:42728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:45832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:46440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:47762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:48224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:48660 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP2] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP6] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP4] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP0] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP3] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP7] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP1] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP5] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12] INFO:     127.0.0.1:39168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:40342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:41044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:41210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:41416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:41726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:41868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:44098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:45722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:45760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:45818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:46802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:49378 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP4] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP6] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP2] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP0] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP3] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP7] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP1] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP5] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12] INFO:     127.0.0.1:38842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:40378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:42272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:44402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:45088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:46370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:48248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:48316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:49168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:50212 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP4] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP6] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP2] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP0] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP3] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP7] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP1] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP5] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:41598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:42396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:42892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:43444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:45356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:45950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:46122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:46308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:46558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:46652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:48076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:48174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:49310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:12] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP2] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP6] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP4] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP0] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP3] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP7] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP1] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:12 TP5] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13] INFO:     127.0.0.1:40516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:43052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:44298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:44326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:44740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:46474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:46704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:47958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:49592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:49716 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP2] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP4] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP6] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP0] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP3] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP7] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP1] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP5] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13] INFO:     127.0.0.1:40850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:43830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:44638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:46542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:47030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48854 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP2] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP6] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP4] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP0] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP3] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP7] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP1] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13 TP5] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 15:36:13] INFO:     127.0.0.1:38712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:40432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:40790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:49130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:40324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:42260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:42382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:45072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:47460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:47464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:39002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:39748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:41126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:42760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:43652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:45020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:45204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:45916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:40076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:41950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:42322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:42850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:43600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:47050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:49062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:41552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:41912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:44820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:49092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:40656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:41436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:41532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:42718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:44564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:47148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:47924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:47988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:44912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:46158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:49156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13 TP0] Decode batch [2079], #running-req: 470, #token: 77568, token usage: 0.08, cuda graph: True, gen throughput (token/s): 6363.86, #queue-req: 0, 
[2025-10-24 15:36:13] INFO:     127.0.0.1:40482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:42750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:42984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:44852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:38654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:43000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:46506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:46832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:47838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:41812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:42002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:42232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:43396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:45300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:46240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:41298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:43644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:40192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:42476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:44528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:44980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:48512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:13] INFO:     127.0.0.1:50450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:42972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:44180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:44920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:45338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:46458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:39704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:41516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:42288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:45470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:46262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:46520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:50470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:46422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:46902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:42904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:43942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:44310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:50432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:39982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:40446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:41356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:41472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:41498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:44974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:45592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:50252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:41922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:45648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:47674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:39196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:40552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:44462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:45778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:46050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:46650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:41166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:43784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:44986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:45896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:47354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:47438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:38792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:45988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:47314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:41878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:46258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:47284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:47506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:40834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:42852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:45420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:47986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:39022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:41968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:44158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:45512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:46726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:50164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:40132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:40312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:40618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:41302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:14] INFO:     127.0.0.1:49560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:41050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:40062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:42368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:42592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:43568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:40096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:42794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:47400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:40548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:40694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:44364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:45052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:45170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:40854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:43518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:45008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:47298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:39808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:40884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:41752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:42202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:38858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:40606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:46082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:47942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:41606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:41820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:44710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:39512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:40200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:41882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:40720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:41246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:46044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:46278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:41740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:43870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:44494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:45534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:47932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:39304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:41098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:42082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:49748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:40300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:42526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:47552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:48926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:15] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:40216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:40974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:42578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16 TP0] Decode batch [2119], #running-req: 213, #token: 43512, token usage: 0.04, cuda graph: True, gen throughput (token/s): 5483.90, #queue-req: 0, 
[2025-10-24 15:36:16] INFO:     127.0.0.1:44118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:44388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:48912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:44140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:45294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:45696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:40978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:43132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:42866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:43864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:48084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:43970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:45314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:48488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:41838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:44258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:40868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:47912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:48968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:40012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:43164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:43806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:43986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:46008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:39134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:48402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:41612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:48888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:41158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:38700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:40464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:46696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:40814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:41334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:43980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:38736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:44608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:48528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:42252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:49118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:16] INFO:     127.0.0.1:50074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:45496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:48372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:48962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:39778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:48132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:46738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:47978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:39048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:44992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:39228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:48984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:43856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:47814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:42598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:50318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:50516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:42678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:48450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:48746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:40048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:47200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:43636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:41456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:45564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:42624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:50376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:47610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17 TP0] Decode batch [2159], #running-req: 89, #token: 22932, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3153.92, #queue-req: 0, 
[2025-10-24 15:36:17] INFO:     127.0.0.1:43246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:48180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:38728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:44522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:39164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:39720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:42442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:40236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:17] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:39456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:49174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:48952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:39288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:49282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:50452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:50538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:39324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:39858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:43954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:48464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:45810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:50064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:50052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:47138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:48802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:39388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:41366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:45184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:43228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:45994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:45972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:47114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:42300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:50408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:47178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:40042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:47644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:48120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:48562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:47966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:18] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:49510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:48574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:45612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:42306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19 TP0] Decode batch [2199], #running-req: 29, #token: 9098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1540.47, #queue-req: 0, 
[2025-10-24 15:36:19] INFO:     127.0.0.1:50424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:49746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:39846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:49442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:41640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:42276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:48518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:46110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:49214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:19] INFO:     127.0.0.1:50072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:20] INFO:     127.0.0.1:49196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:20] INFO:     127.0.0.1:49498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:20] INFO:     127.0.0.1:45750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:20 TP0] Decode batch [2239], #running-req: 11, #token: 4084, token usage: 0.00, cuda graph: True, gen throughput (token/s): 690.38, #queue-req: 0, 
[2025-10-24 15:36:20] INFO:     127.0.0.1:47974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:20] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:20] INFO:     127.0.0.1:48408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:20] INFO:     127.0.0.1:47672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:20] INFO:     127.0.0.1:49268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:20] INFO:     127.0.0.1:49296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:20] INFO:     127.0.0.1:48876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:20] INFO:     127.0.0.1:49342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:21 TP0] Decode batch [2279], #running-req: 2, #token: 1508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 230.60, #queue-req: 0, 
[2025-10-24 15:36:21 TP0] Decode batch [2319], #running-req: 2, #token: 1588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 101.16, #queue-req: 0, 
[2025-10-24 15:36:22 TP0] Decode batch [2359], #running-req: 2, #token: 1668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 101.15, #queue-req: 0, 
[2025-10-24 15:36:23 TP0] Decode batch [2399], #running-req: 2, #token: 1748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 101.10, #queue-req: 0, 
[2025-10-24 15:36:24] INFO:     127.0.0.1:40510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:24 TP0] Decode batch [2439], #running-req: 1, #token: 1256, token usage: 0.00, cuda graph: True, gen throughput (token/s): 98.37, #queue-req: 0, 
[2025-10-24 15:36:24] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:35] INFO:     127.0.0.1:49550 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-24 15:36:42] INFO:     127.0.0.1:49556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:42 TP0] Prefill batch [2446], #new-seq: 1, #new-token: 3200, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:36:44] INFO:     127.0.0.1:49558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44 TP0] Prefill batch [2479], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:36:44] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44 TP0] Prefill batch [2480], #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 6, 
[2025-10-24 15:36:44] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44 TP0] Prefill batch [2481], #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.02, #running-req: 6, #queue-req: 18, 
[2025-10-24 15:36:44] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:44] INFO:     127.0.0.1:50166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45] INFO:     127.0.0.1:50664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:36:45 TP0] Prefill batch [2482], #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-10-24 15:36:46 TP0] Prefill batch [2483], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-10-24 15:36:47 TP0] Prefill batch [2484], #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-10-24 15:36:47 TP0] Prefill batch [2485], #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.09, #running-req: 26, #queue-req: 97, 
[2025-10-24 15:36:48 TP0] Prefill batch [2486], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-10-24 15:36:49 TP0] Prefill batch [2487], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-10-24 15:36:50 TP0] Prefill batch [2488], #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-10-24 15:36:51 TP0] Prefill batch [2489], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-10-24 15:36:52 TP0] Prefill batch [2490], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-10-24 15:36:52 TP0] Prefill batch [2491], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-10-24 15:36:53 TP0] Prefill batch [2492], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.20, #running-req: 61, #queue-req: 62, 
[2025-10-24 15:36:54 TP0] Prefill batch [2493], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.22, #running-req: 66, #queue-req: 57, 
[2025-10-24 15:36:55 TP0] Prefill batch [2494], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.23, #running-req: 71, #queue-req: 52, 
[2025-10-24 15:36:56 TP0] Prefill batch [2495], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.25, #running-req: 76, #queue-req: 47, 
[2025-10-24 15:36:56 TP0] Prefill batch [2496], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.27, #running-req: 81, #queue-req: 42, 
[2025-10-24 15:36:57 TP0] Prefill batch [2497], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.28, #running-req: 86, #queue-req: 37, 
[2025-10-24 15:36:58 TP0] Prefill batch [2498], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.30, #running-req: 91, #queue-req: 32, 
[2025-10-24 15:36:59 TP0] Prefill batch [2499], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.32, #running-req: 96, #queue-req: 27, 
[2025-10-24 15:37:00 TP0] Prefill batch [2500], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.33, #running-req: 101, #queue-req: 22, 
[2025-10-24 15:37:00 TP0] Prefill batch [2501], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.35, #running-req: 106, #queue-req: 17, 
[2025-10-24 15:37:01 TP0] Prefill batch [2502], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.37, #running-req: 111, #queue-req: 12, 
[2025-10-24 15:37:02 TP0] Prefill batch [2503], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.38, #running-req: 116, #queue-req: 7, 
[2025-10-24 15:37:03 TP0] Prefill batch [2504], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.40, #running-req: 121, #queue-req: 2, 
[2025-10-24 15:37:04 TP0] Prefill batch [2505], #new-seq: 2, #new-token: 6394, #cached-token: 8, token usage: 0.41, #running-req: 126, #queue-req: 0, 
[2025-10-24 15:37:05 TP0] Decode batch [2507], #running-req: 128, #token: 409631, token usage: 0.42, cuda graph: True, gen throughput (token/s): 4.08, #queue-req: 0, 
[2025-10-24 15:37:07 TP0] Decode batch [2547], #running-req: 128, #token: 414751, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2562.72, #queue-req: 0, 
[2025-10-24 15:37:09 TP0] Decode batch [2587], #running-req: 128, #token: 419871, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2524.66, #queue-req: 0, 
[2025-10-24 15:37:11 TP0] Decode batch [2627], #running-req: 128, #token: 424991, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2507.60, #queue-req: 0, 
[2025-10-24 15:37:13 TP0] Decode batch [2667], #running-req: 128, #token: 430111, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2475.93, #queue-req: 0, 
[2025-10-24 15:37:15 TP0] Decode batch [2707], #running-req: 128, #token: 435231, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2487.50, #queue-req: 0, 
[2025-10-24 15:37:17 TP0] Decode batch [2747], #running-req: 128, #token: 440351, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2479.46, #queue-req: 0, 
[2025-10-24 15:37:19 TP0] Decode batch [2787], #running-req: 128, #token: 445471, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2474.77, #queue-req: 0, 
[2025-10-24 15:37:21 TP0] Decode batch [2827], #running-req: 128, #token: 450591, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2468.94, #queue-req: 0, 
[2025-10-24 15:37:23 TP0] Decode batch [2867], #running-req: 128, #token: 455711, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2460.64, #queue-req: 0, 
[2025-10-24 15:37:25 TP0] Decode batch [2907], #running-req: 128, #token: 460831, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2457.90, #queue-req: 0, 
[2025-10-24 15:37:27 TP0] Decode batch [2947], #running-req: 128, #token: 465951, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2453.52, #queue-req: 0, 
[2025-10-24 15:37:29 TP0] Decode batch [2987], #running-req: 128, #token: 471071, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2448.56, #queue-req: 0, 
[2025-10-24 15:37:32 TP0] Decode batch [3027], #running-req: 128, #token: 476191, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2441.17, #queue-req: 0, 
[2025-10-24 15:37:34 TP0] Decode batch [3067], #running-req: 128, #token: 481311, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2436.21, #queue-req: 0, 
[2025-10-24 15:37:36 TP0] Decode batch [3107], #running-req: 128, #token: 486431, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2431.27, #queue-req: 0, 
[2025-10-24 15:37:38 TP0] Decode batch [3147], #running-req: 128, #token: 491551, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2426.02, #queue-req: 0, 
[2025-10-24 15:37:40 TP0] Decode batch [3187], #running-req: 128, #token: 496671, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2420.83, #queue-req: 0, 
[2025-10-24 15:37:42 TP0] Decode batch [3227], #running-req: 128, #token: 501791, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2418.86, #queue-req: 0, 
[2025-10-24 15:37:44 TP0] Decode batch [3267], #running-req: 128, #token: 506911, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2414.56, #queue-req: 0, 
[2025-10-24 15:37:46] INFO:     127.0.0.1:46972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:46 TP0] Prefill batch [3306], #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:37:47 TP0] Decode batch [3308], #running-req: 1, #token: 3203, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2117.53, #queue-req: 0, 
[2025-10-24 15:37:47] INFO:     127.0.0.1:46976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:46986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:46992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47 TP0] Prefill batch [3314], #new-seq: 3, #new-token: 9595, #cached-token: 8, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:37:47] INFO:     127.0.0.1:47018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47 TP0] Prefill batch [3315], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.01, #running-req: 4, #queue-req: 15, 
[2025-10-24 15:37:47] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47 TP0] Prefill batch [3316], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.03, #running-req: 9, #queue-req: 93, 
[2025-10-24 15:37:47] INFO:     127.0.0.1:47942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:47990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:48012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:48018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:48026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:48028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:48060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:48064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:48078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:48080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:47] INFO:     127.0.0.1:48088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:37:48 TP0] Prefill batch [3317], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 14, #queue-req: 109, 
[2025-10-24 15:37:49 TP0] Prefill batch [3318], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.06, #running-req: 19, #queue-req: 104, 
[2025-10-24 15:37:50 TP0] Prefill batch [3319], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.08, #running-req: 24, #queue-req: 99, 
[2025-10-24 15:37:51 TP0] Prefill batch [3320], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.10, #running-req: 29, #queue-req: 94, 
[2025-10-24 15:37:51 TP0] Prefill batch [3321], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.11, #running-req: 34, #queue-req: 89, 
[2025-10-24 15:37:52 TP0] Prefill batch [3322], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 39, #queue-req: 84, 
[2025-10-24 15:37:53 TP0] Prefill batch [3323], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.14, #running-req: 44, #queue-req: 79, 
[2025-10-24 15:37:54 TP0] Prefill batch [3324], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.16, #running-req: 49, #queue-req: 74, 
[2025-10-24 15:37:55 TP0] Prefill batch [3325], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.18, #running-req: 54, #queue-req: 69, 
[2025-10-24 15:37:56 TP0] Prefill batch [3326], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.19, #running-req: 59, #queue-req: 64, 
[2025-10-24 15:37:56 TP0] Prefill batch [3327], #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.21, #running-req: 64, #queue-req: 58, 
[2025-10-24 15:37:57 TP0] Prefill batch [3328], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.23, #running-req: 70, #queue-req: 53, 
[2025-10-24 15:37:58 TP0] Prefill batch [3329], #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.24, #running-req: 75, #queue-req: 48, 
[2025-10-24 15:37:59 TP0] Prefill batch [3330], #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.26, #running-req: 80, #queue-req: 43, 
[2025-10-24 15:38:00 TP0] Prefill batch [3331], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.28, #running-req: 85, #queue-req: 38, 
[2025-10-24 15:38:00 TP0] Prefill batch [3332], #new-seq: 6, #new-token: 15992, #cached-token: 3214, token usage: 0.30, #running-req: 90, #queue-req: 32, 
[2025-10-24 15:38:01 TP0] Prefill batch [3333], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.31, #running-req: 96, #queue-req: 27, 
[2025-10-24 15:38:02 TP0] Prefill batch [3334], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.33, #running-req: 101, #queue-req: 22, 
[2025-10-24 15:38:03 TP0] Prefill batch [3335], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.35, #running-req: 106, #queue-req: 17, 
[2025-10-24 15:38:04 TP0] Prefill batch [3336], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.36, #running-req: 111, #queue-req: 12, 
[2025-10-24 15:38:05 TP0] Prefill batch [3337], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.38, #running-req: 116, #queue-req: 7, 
[2025-10-24 15:38:05 TP0] Prefill batch [3338], #new-seq: 6, #new-token: 15984, #cached-token: 3222, token usage: 0.39, #running-req: 121, #queue-req: 1, 
[2025-10-24 15:38:06 TP0] Prefill batch [3339], #new-seq: 1, #new-token: 3198, #cached-token: 3, token usage: 0.41, #running-req: 127, #queue-req: 0, 
[2025-10-24 15:38:09 TP0] Decode batch [3374], #running-req: 128, #token: 407460, token usage: 0.42, cuda graph: True, gen throughput (token/s): 196.14, #queue-req: 0, 
[2025-10-24 15:38:11 TP0] Decode batch [3414], #running-req: 128, #token: 412580, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2509.25, #queue-req: 0, 
[2025-10-24 15:38:13 TP0] Decode batch [3454], #running-req: 128, #token: 417700, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2491.54, #queue-req: 0, 
[2025-10-24 15:38:15 TP0] Decode batch [3494], #running-req: 128, #token: 422820, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2483.50, #queue-req: 0, 
[2025-10-24 15:38:17 TP0] Decode batch [3534], #running-req: 128, #token: 427940, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2478.56, #queue-req: 0, 
[2025-10-24 15:38:19 TP0] Decode batch [3574], #running-req: 128, #token: 433060, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2466.85, #queue-req: 0, 
[2025-10-24 15:38:21 TP0] Decode batch [3614], #running-req: 128, #token: 438180, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2464.01, #queue-req: 0, 
[2025-10-24 15:38:23 TP0] Decode batch [3654], #running-req: 128, #token: 443300, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2460.01, #queue-req: 0, 
[2025-10-24 15:38:25 TP0] Decode batch [3694], #running-req: 128, #token: 448420, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2456.39, #queue-req: 0, 
[2025-10-24 15:38:27 TP0] Decode batch [3734], #running-req: 128, #token: 453540, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2450.55, #queue-req: 0, 
[2025-10-24 15:38:30 TP0] Decode batch [3774], #running-req: 128, #token: 458660, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2447.59, #queue-req: 0, 
[2025-10-24 15:38:32 TP0] Decode batch [3814], #running-req: 128, #token: 463780, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2442.77, #queue-req: 0, 
[2025-10-24 15:38:34 TP0] Decode batch [3854], #running-req: 128, #token: 468900, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2438.10, #queue-req: 0, 
[2025-10-24 15:38:36 TP0] Decode batch [3894], #running-req: 128, #token: 474020, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2429.78, #queue-req: 0, 
[2025-10-24 15:38:38 TP0] Decode batch [3934], #running-req: 128, #token: 479140, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2426.42, #queue-req: 0, 
[2025-10-24 15:38:40 TP0] Decode batch [3974], #running-req: 128, #token: 484260, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2419.81, #queue-req: 0, 
[2025-10-24 15:38:42 TP0] Decode batch [4014], #running-req: 128, #token: 489380, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2415.82, #queue-req: 0, 
[2025-10-24 15:38:44 TP0] Decode batch [4054], #running-req: 128, #token: 494500, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2412.94, #queue-req: 0, 
[2025-10-24 15:38:46 TP0] Decode batch [4094], #running-req: 128, #token: 499620, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2407.61, #queue-req: 0, 
[2025-10-24 15:38:48] INFO:     127.0.0.1:48618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49 TP0] Prefill batch [4134], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.52, #running-req: 127, #queue-req: 0, 
[2025-10-24 15:38:49 TP0] Decode batch [4134], #running-req: 127, #token: 503817, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2291.06, #queue-req: 0, 
[2025-10-24 15:38:49] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49 TP0] Prefill batch [4145], #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:38:49] INFO:     127.0.0.1:48640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49 TP0] Prefill batch [4146], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.01, #running-req: 2, #queue-req: 13, 
[2025-10-24 15:38:49] INFO:     127.0.0.1:48758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:48994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:49000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:49] INFO:     127.0.0.1:49014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50 TP0] Prefill batch [4147], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.02, #running-req: 7, #queue-req: 40, 
[2025-10-24 15:38:50] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50] INFO:     127.0.0.1:49690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:38:50 TP0] Prefill batch [4148], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.04, #running-req: 12, #queue-req: 111, 
[2025-10-24 15:38:51 TP0] Prefill batch [4149], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.06, #running-req: 17, #queue-req: 106, 
[2025-10-24 15:38:52 TP0] Prefill batch [4150], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.07, #running-req: 22, #queue-req: 101, 
[2025-10-24 15:38:53 TP0] Prefill batch [4151], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.09, #running-req: 27, #queue-req: 96, 
[2025-10-24 15:38:54 TP0] Prefill batch [4152], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.11, #running-req: 32, #queue-req: 91, 
[2025-10-24 15:38:54 TP0] Prefill batch [4153], #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.13, #running-req: 37, #queue-req: 85, 
[2025-10-24 15:38:55 TP0] Prefill batch [4154], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.14, #running-req: 43, #queue-req: 80, 
[2025-10-24 15:38:56 TP0] Prefill batch [4155], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.16, #running-req: 48, #queue-req: 75, 
[2025-10-24 15:38:57 TP0] Prefill batch [4156], #new-seq: 5, #new-token: 15978, #cached-token: 27, token usage: 0.17, #running-req: 53, #queue-req: 70, 
[2025-10-24 15:38:58 TP0] Prefill batch [4157], #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.19, #running-req: 58, #queue-req: 64, 
[2025-10-24 15:38:59 TP0] Prefill batch [4158], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.21, #running-req: 64, #queue-req: 59, 
[2025-10-24 15:38:59 TP0] Prefill batch [4159], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.23, #running-req: 69, #queue-req: 54, 
[2025-10-24 15:39:00 TP0] Prefill batch [4160], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.24, #running-req: 74, #queue-req: 49, 
[2025-10-24 15:39:01 TP0] Prefill batch [4161], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.26, #running-req: 79, #queue-req: 44, 
[2025-10-24 15:39:02 TP0] Prefill batch [4162], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.28, #running-req: 84, #queue-req: 39, 
[2025-10-24 15:39:03 TP0] Prefill batch [4163], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.29, #running-req: 89, #queue-req: 34, 
[2025-10-24 15:39:03 TP0] Prefill batch [4164], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.31, #running-req: 94, #queue-req: 29, 
[2025-10-24 15:39:04 TP0] Prefill batch [4165], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.33, #running-req: 99, #queue-req: 24, 
[2025-10-24 15:39:05 TP0] Prefill batch [4166], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.34, #running-req: 104, #queue-req: 19, 
[2025-10-24 15:39:06 TP0] Prefill batch [4167], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.36, #running-req: 109, #queue-req: 14, 
[2025-10-24 15:39:07 TP0] Prefill batch [4168], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.38, #running-req: 114, #queue-req: 9, 
[2025-10-24 15:39:07 TP0] Prefill batch [4169], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.39, #running-req: 119, #queue-req: 4, 
[2025-10-24 15:39:08 TP0] Prefill batch [4170], #new-seq: 4, #new-token: 12795, #cached-token: 9, token usage: 0.41, #running-req: 124, #queue-req: 0, 
[2025-10-24 15:39:11 TP0] Decode batch [4201], #running-req: 128, #token: 413350, token usage: 0.43, cuda graph: True, gen throughput (token/s): 205.33, #queue-req: 0, 
[2025-10-24 15:39:13 TP0] Decode batch [4241], #running-req: 128, #token: 418470, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2515.65, #queue-req: 0, 
[2025-10-24 15:39:15 TP0] Decode batch [4281], #running-req: 128, #token: 423590, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2493.83, #queue-req: 0, 
[2025-10-24 15:39:17 TP0] Decode batch [4321], #running-req: 128, #token: 428710, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2477.44, #queue-req: 0, 
[2025-10-24 15:39:19 TP0] Decode batch [4361], #running-req: 128, #token: 433830, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2478.98, #queue-req: 0, 
[2025-10-24 15:39:21 TP0] Decode batch [4401], #running-req: 128, #token: 438950, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2470.97, #queue-req: 0, 
[2025-10-24 15:39:24 TP0] Decode batch [4441], #running-req: 128, #token: 444070, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2462.19, #queue-req: 0, 
[2025-10-24 15:39:26 TP0] Decode batch [4481], #running-req: 128, #token: 449190, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2459.90, #queue-req: 0, 
[2025-10-24 15:39:28 TP0] Decode batch [4521], #running-req: 128, #token: 454310, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2446.78, #queue-req: 0, 
[2025-10-24 15:39:30 TP0] Decode batch [4561], #running-req: 128, #token: 459430, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2453.02, #queue-req: 0, 
[2025-10-24 15:39:32 TP0] Decode batch [4601], #running-req: 128, #token: 464550, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2450.29, #queue-req: 0, 
[2025-10-24 15:39:34 TP0] Decode batch [4641], #running-req: 128, #token: 469670, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2448.77, #queue-req: 0, 
[2025-10-24 15:39:36 TP0] Decode batch [4681], #running-req: 128, #token: 474790, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2444.00, #queue-req: 0, 
[2025-10-24 15:39:38 TP0] Decode batch [4721], #running-req: 128, #token: 479910, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2437.75, #queue-req: 0, 
[2025-10-24 15:39:40 TP0] Decode batch [4761], #running-req: 128, #token: 485030, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2432.81, #queue-req: 0, 
[2025-10-24 15:39:42 TP0] Decode batch [4801], #running-req: 128, #token: 490150, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2429.28, #queue-req: 0, 
[2025-10-24 15:39:44 TP0] Decode batch [4841], #running-req: 128, #token: 495270, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2427.77, #queue-req: 0, 
[2025-10-24 15:39:47 TP0] Decode batch [4881], #running-req: 128, #token: 500390, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2423.90, #queue-req: 0, 
[2025-10-24 15:39:49 TP0] Decode batch [4921], #running-req: 128, #token: 505510, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2420.63, #queue-req: 0, 
[2025-10-24 15:39:51] INFO:     127.0.0.1:35526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:51 TP0] Decode batch [4961], #running-req: 128, #token: 506630, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2412.26, #queue-req: 0, 
[2025-10-24 15:39:51 TP0] Prefill batch [4962], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.52, #running-req: 127, #queue-req: 0, 
[2025-10-24 15:39:52] INFO:     127.0.0.1:35540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52 TP0] Prefill batch [4976], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:39:52] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52 TP0] Prefill batch [4977], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.01, #running-req: 2, #queue-req: 13, 
[2025-10-24 15:39:52] INFO:     127.0.0.1:35734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:35998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52 TP0] Prefill batch [4978], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 7, #queue-req: 42, 
[2025-10-24 15:39:52] INFO:     127.0.0.1:36064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:52] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:39:53 TP0] Prefill batch [4979], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 12, #queue-req: 99, 
[2025-10-24 15:39:54 TP0] Prefill batch [4980], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.06, #running-req: 17, #queue-req: 94, 
[2025-10-24 15:39:54 TP0] Prefill batch [4981], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 22, #queue-req: 89, 
[2025-10-24 15:39:55 TP0] Prefill batch [4982], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 27, #queue-req: 84, 
[2025-10-24 15:39:56 TP0] Prefill batch [4983], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.11, #running-req: 32, #queue-req: 79, 
[2025-10-24 15:39:57 TP0] Prefill batch [4984], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.12, #running-req: 37, #queue-req: 74, 
[2025-10-24 15:39:58 TP0] Prefill batch [4985], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.14, #running-req: 42, #queue-req: 69, 
[2025-10-24 15:39:58 TP0] Prefill batch [4986], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.15, #running-req: 47, #queue-req: 64, 
[2025-10-24 15:39:59 TP0] Prefill batch [4987], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.17, #running-req: 52, #queue-req: 59, 
[2025-10-24 15:40:00 TP0] Prefill batch [4988], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.19, #running-req: 57, #queue-req: 54, 
[2025-10-24 15:40:01 TP0] Prefill batch [4989], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.20, #running-req: 62, #queue-req: 49, 
[2025-10-24 15:40:02 TP0] Prefill batch [4990], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.22, #running-req: 67, #queue-req: 44, 
[2025-10-24 15:40:03 TP0] Prefill batch [4991], #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.24, #running-req: 72, #queue-req: 38, 
[2025-10-24 15:40:03 TP0] Prefill batch [4992], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.26, #running-req: 78, #queue-req: 33, 
[2025-10-24 15:40:04 TP0] Prefill batch [4993], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.27, #running-req: 83, #queue-req: 28, 
[2025-10-24 15:40:05 TP0] Prefill batch [4994], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.29, #running-req: 88, #queue-req: 23, 
[2025-10-24 15:40:06 TP0] Prefill batch [4995], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.31, #running-req: 93, #queue-req: 18, 
[2025-10-24 15:40:07 TP0] Prefill batch [4996], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.32, #running-req: 98, #queue-req: 13, 
[2025-10-24 15:40:07 TP0] Prefill batch [4997], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.34, #running-req: 103, #queue-req: 8, 
[2025-10-24 15:40:08 TP0] Prefill batch [4998], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.36, #running-req: 108, #queue-req: 3, 
[2025-10-24 15:40:09 TP0] Prefill batch [4999], #new-seq: 3, #new-token: 9590, #cached-token: 13, token usage: 0.37, #running-req: 113, #queue-req: 0, 
[2025-10-24 15:40:12 TP0] Decode batch [5026], #running-req: 116, #token: 374143, token usage: 0.39, cuda graph: True, gen throughput (token/s): 207.26, #queue-req: 0, 
[2025-10-24 15:40:14 TP0] Decode batch [5066], #running-req: 116, #token: 378783, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2339.49, #queue-req: 0, 
[2025-10-24 15:40:16 TP0] Decode batch [5106], #running-req: 116, #token: 383423, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2326.78, #queue-req: 0, 
[2025-10-24 15:40:18 TP0] Decode batch [5146], #running-req: 116, #token: 388063, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2313.97, #queue-req: 0, 
[2025-10-24 15:40:20 TP0] Decode batch [5186], #running-req: 116, #token: 392703, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2305.75, #queue-req: 0, 
[2025-10-24 15:40:22 TP0] Decode batch [5226], #running-req: 116, #token: 397343, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2299.53, #queue-req: 0, 
[2025-10-24 15:40:24 TP0] Decode batch [5266], #running-req: 116, #token: 401983, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2295.49, #queue-req: 0, 
[2025-10-24 15:40:26 TP0] Decode batch [5306], #running-req: 116, #token: 406623, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2290.67, #queue-req: 0, 
[2025-10-24 15:40:28 TP0] Decode batch [5346], #running-req: 116, #token: 411263, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2285.72, #queue-req: 0, 
[2025-10-24 15:40:30 TP0] Decode batch [5386], #running-req: 116, #token: 415903, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2283.18, #queue-req: 0, 
[2025-10-24 15:40:32 TP0] Decode batch [5426], #running-req: 116, #token: 420543, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2279.69, #queue-req: 0, 
[2025-10-24 15:40:34 TP0] Decode batch [5466], #running-req: 116, #token: 425183, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2280.15, #queue-req: 0, 
[2025-10-24 15:40:36 TP0] Decode batch [5506], #running-req: 116, #token: 429823, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2274.89, #queue-req: 0, 
[2025-10-24 15:40:38 TP0] Decode batch [5546], #running-req: 116, #token: 434463, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2272.54, #queue-req: 0, 
[2025-10-24 15:40:40 TP0] Decode batch [5586], #running-req: 116, #token: 439103, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2264.31, #queue-req: 0, 
[2025-10-24 15:40:42 TP0] Decode batch [5626], #running-req: 116, #token: 443743, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2261.10, #queue-req: 0, 
[2025-10-24 15:40:44 TP0] Decode batch [5666], #running-req: 116, #token: 448383, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2257.24, #queue-req: 0, 
[2025-10-24 15:40:46 TP0] Decode batch [5706], #running-req: 116, #token: 453023, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2253.10, #queue-req: 0, 
[2025-10-24 15:40:48 TP0] Decode batch [5746], #running-req: 116, #token: 457663, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2247.27, #queue-req: 0, 
[2025-10-24 15:40:50 TP0] Decode batch [5786], #running-req: 116, #token: 458307, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2242.95, #queue-req: 0, 
[2025-10-24 15:40:51] INFO:     127.0.0.1:39756 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-24 15:41:09] INFO:     127.0.0.1:46516 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-24 15:41:15] INFO:     127.0.0.1:36768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:15 TP0] Prefill batch [5800], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:41:16 TP0] Decode batch [5827], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 63.55, #queue-req: 0, 
[2025-10-24 15:41:17] INFO:     127.0.0.1:36782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17 TP0] Prefill batch [5833], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:41:17] INFO:     127.0.0.1:36796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17 TP0] Prefill batch [5834], #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 6, 
[2025-10-24 15:41:17] INFO:     127.0.0.1:36910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:36986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17 TP0] Prefill batch [5835], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 19, 
[2025-10-24 15:41:17] INFO:     127.0.0.1:37112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:17] INFO:     127.0.0.1:37374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:18] INFO:     127.0.0.1:37382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:18] INFO:     127.0.0.1:37386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:18] INFO:     127.0.0.1:37394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:41:18 TP0] Prefill batch [5836], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-10-24 15:41:19 TP0] Prefill batch [5837], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-24 15:41:20 TP0] Prefill batch [5838], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-10-24 15:41:21 TP0] Prefill batch [5839], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-10-24 15:41:21 TP0] Prefill batch [5840], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-10-24 15:41:22 TP0] Prefill batch [5841], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-10-24 15:41:23 TP0] Prefill batch [5842], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-10-24 15:41:24 TP0] Prefill batch [5843], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-10-24 15:41:25 TP0] Prefill batch [5844], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-10-24 15:41:25 TP0] Prefill batch [5845], #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.19, #running-req: 56, #queue-req: 2, 
[2025-10-24 15:41:26 TP0] Prefill batch [5846], #new-seq: 2, #new-token: 6398, #cached-token: 4, token usage: 0.20, #running-req: 62, #queue-req: 0, 
[2025-10-24 15:41:29 TP0] Decode batch [5881], #running-req: 64, #token: 206952, token usage: 0.21, cuda graph: True, gen throughput (token/s): 171.29, #queue-req: 0, 
[2025-10-24 15:41:30 TP0] Decode batch [5921], #running-req: 64, #token: 209512, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1532.05, #queue-req: 0, 
[2025-10-24 15:41:32 TP0] Decode batch [5961], #running-req: 64, #token: 212072, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1520.21, #queue-req: 0, 
[2025-10-24 15:41:34 TP0] Decode batch [6001], #running-req: 64, #token: 214632, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1513.53, #queue-req: 0, 
[2025-10-24 15:41:35 TP0] Decode batch [6041], #running-req: 64, #token: 217192, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1511.15, #queue-req: 0, 
[2025-10-24 15:41:37 TP0] Decode batch [6081], #running-req: 64, #token: 219752, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1502.51, #queue-req: 0, 
[2025-10-24 15:41:39 TP0] Decode batch [6121], #running-req: 64, #token: 222312, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1498.88, #queue-req: 0, 
[2025-10-24 15:41:41 TP0] Decode batch [6161], #running-req: 64, #token: 224872, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1496.08, #queue-req: 0, 
[2025-10-24 15:41:42 TP0] Decode batch [6201], #running-req: 64, #token: 227432, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1489.52, #queue-req: 0, 
[2025-10-24 15:41:44 TP0] Decode batch [6241], #running-req: 64, #token: 229992, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1489.00, #queue-req: 0, 
[2025-10-24 15:41:46 TP0] Decode batch [6281], #running-req: 64, #token: 232552, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1486.64, #queue-req: 0, 
[2025-10-24 15:41:47 TP0] Decode batch [6321], #running-req: 64, #token: 235112, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1490.39, #queue-req: 0, 
[2025-10-24 15:41:49 TP0] Decode batch [6361], #running-req: 64, #token: 237672, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1488.66, #queue-req: 0, 
[2025-10-24 15:41:51 TP0] Decode batch [6401], #running-req: 64, #token: 240232, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1486.89, #queue-req: 0, 
[2025-10-24 15:41:53 TP0] Decode batch [6441], #running-req: 64, #token: 242792, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1486.26, #queue-req: 0, 
[2025-10-24 15:41:54 TP0] Decode batch [6481], #running-req: 64, #token: 245352, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1481.90, #queue-req: 0, 
[2025-10-24 15:41:56 TP0] Decode batch [6521], #running-req: 64, #token: 247912, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1481.61, #queue-req: 0, 
[2025-10-24 15:41:58 TP0] Decode batch [6561], #running-req: 64, #token: 250472, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1478.58, #queue-req: 0, 
[2025-10-24 15:42:00 TP0] Decode batch [6601], #running-req: 64, #token: 253032, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1471.25, #queue-req: 0, 
[2025-10-24 15:42:01 TP0] Decode batch [6641], #running-req: 64, #token: 255592, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1476.90, #queue-req: 0, 
[2025-10-24 15:42:02] INFO:     127.0.0.1:42332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02 TP0] Prefill batch [6647], #new-seq: 1, #new-token: 3195, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:42:02] INFO:     127.0.0.1:42340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02 TP0] Prefill batch [6648], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-10-24 15:42:02] INFO:     127.0.0.1:42476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02 TP0] Prefill batch [6649], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 40, 
[2025-10-24 15:42:02] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:02] INFO:     127.0.0.1:42932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:03 TP0] Prefill batch [6650], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-10-24 15:42:03 TP0] Prefill batch [6651], #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.06, #running-req: 16, #queue-req: 42, 
[2025-10-24 15:42:04 TP0] Prefill batch [6652], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 22, #queue-req: 37, 
[2025-10-24 15:42:05 TP0] Prefill batch [6653], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.09, #running-req: 27, #queue-req: 32, 
[2025-10-24 15:42:06 TP0] Prefill batch [6654], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.11, #running-req: 32, #queue-req: 27, 
[2025-10-24 15:42:07 TP0] Prefill batch [6655], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.12, #running-req: 37, #queue-req: 22, 
[2025-10-24 15:42:08 TP0] Prefill batch [6656], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.14, #running-req: 42, #queue-req: 17, 
[2025-10-24 15:42:08 TP0] Prefill batch [6657], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 47, #queue-req: 12, 
[2025-10-24 15:42:09 TP0] Prefill batch [6658], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.17, #running-req: 52, #queue-req: 7, 
[2025-10-24 15:42:10 TP0] Prefill batch [6659], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.19, #running-req: 57, #queue-req: 2, 
[2025-10-24 15:42:11 TP0] Prefill batch [6660], #new-seq: 2, #new-token: 6393, #cached-token: 9, token usage: 0.20, #running-req: 62, #queue-req: 0, 
[2025-10-24 15:42:13 TP0] Decode batch [6695], #running-req: 64, #token: 206935, token usage: 0.21, cuda graph: True, gen throughput (token/s): 212.46, #queue-req: 0, 
[2025-10-24 15:42:15 TP0] Decode batch [6735], #running-req: 64, #token: 209495, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1526.90, #queue-req: 0, 
[2025-10-24 15:42:17 TP0] Decode batch [6775], #running-req: 64, #token: 212055, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1516.84, #queue-req: 0, 
[2025-10-24 15:42:18 TP0] Decode batch [6815], #running-req: 64, #token: 214615, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1511.40, #queue-req: 0, 
[2025-10-24 15:42:20 TP0] Decode batch [6855], #running-req: 64, #token: 217175, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1506.82, #queue-req: 0, 
[2025-10-24 15:42:22 TP0] Decode batch [6895], #running-req: 64, #token: 219735, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1499.14, #queue-req: 0, 
[2025-10-24 15:42:23 TP0] Decode batch [6935], #running-req: 64, #token: 222295, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1497.16, #queue-req: 0, 
[2025-10-24 15:42:25 TP0] Decode batch [6975], #running-req: 64, #token: 224855, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1496.65, #queue-req: 0, 
[2025-10-24 15:42:27 TP0] Decode batch [7015], #running-req: 64, #token: 227415, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1493.26, #queue-req: 0, 
[2025-10-24 15:42:29 TP0] Decode batch [7055], #running-req: 64, #token: 229975, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1490.64, #queue-req: 0, 
[2025-10-24 15:42:30 TP0] Decode batch [7095], #running-req: 64, #token: 232535, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1491.11, #queue-req: 0, 
[2025-10-24 15:42:32 TP0] Decode batch [7135], #running-req: 64, #token: 235095, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1494.42, #queue-req: 0, 
[2025-10-24 15:42:34 TP0] Decode batch [7175], #running-req: 64, #token: 237655, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1492.77, #queue-req: 0, 
[2025-10-24 15:42:35 TP0] Decode batch [7215], #running-req: 64, #token: 240215, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1492.45, #queue-req: 0, 
[2025-10-24 15:42:37 TP0] Decode batch [7255], #running-req: 64, #token: 242775, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1489.74, #queue-req: 0, 
[2025-10-24 15:42:39 TP0] Decode batch [7295], #running-req: 64, #token: 245335, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1488.39, #queue-req: 0, 
[2025-10-24 15:42:41 TP0] Decode batch [7335], #running-req: 64, #token: 247895, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1484.05, #queue-req: 0, 
[2025-10-24 15:42:42 TP0] Decode batch [7375], #running-req: 64, #token: 250455, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1483.70, #queue-req: 0, 
[2025-10-24 15:42:44 TP0] Decode batch [7415], #running-req: 64, #token: 253015, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1482.49, #queue-req: 0, 
[2025-10-24 15:42:46 TP0] Decode batch [7455], #running-req: 64, #token: 255575, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1483.59, #queue-req: 0, 
[2025-10-24 15:42:46] INFO:     127.0.0.1:35402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46 TP0] Prefill batch [7461], #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:42:46] INFO:     127.0.0.1:35418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46 TP0] Prefill batch [7462], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-10-24 15:42:46] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46 TP0] Prefill batch [7463], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 39, 
[2025-10-24 15:42:46] INFO:     127.0.0.1:35822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:46] INFO:     127.0.0.1:35910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:47] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:47] INFO:     127.0.0.1:35924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:42:47 TP0] Prefill batch [7464], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-10-24 15:42:48 TP0] Prefill batch [7465], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-24 15:42:49 TP0] Prefill batch [7466], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-10-24 15:42:50 TP0] Prefill batch [7467], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-10-24 15:42:50 TP0] Prefill batch [7468], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-10-24 15:42:51 TP0] Prefill batch [7469], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-10-24 15:42:52 TP0] Prefill batch [7470], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-10-24 15:42:53 TP0] Prefill batch [7471], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-10-24 15:42:54 TP0] Prefill batch [7472], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-10-24 15:42:55 TP0] Prefill batch [7473], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-10-24 15:42:55 TP0] Prefill batch [7474], #new-seq: 3, #new-token: 9592, #cached-token: 11, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-10-24 15:42:58 TP0] Decode batch [7509], #running-req: 64, #token: 206949, token usage: 0.21, cuda graph: True, gen throughput (token/s): 211.28, #queue-req: 0, 
[2025-10-24 15:43:00 TP0] Decode batch [7549], #running-req: 64, #token: 209509, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1521.08, #queue-req: 0, 
[2025-10-24 15:43:01 TP0] Decode batch [7589], #running-req: 64, #token: 212069, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1509.61, #queue-req: 0, 
[2025-10-24 15:43:03 TP0] Decode batch [7629], #running-req: 64, #token: 214629, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1500.98, #queue-req: 0, 
[2025-10-24 15:43:05 TP0] Decode batch [7669], #running-req: 64, #token: 217189, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1495.24, #queue-req: 0, 
[2025-10-24 15:43:06 TP0] Decode batch [7709], #running-req: 64, #token: 219749, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1490.93, #queue-req: 0, 
[2025-10-24 15:43:08 TP0] Decode batch [7749], #running-req: 64, #token: 222309, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1487.79, #queue-req: 0, 
[2025-10-24 15:43:10 TP0] Decode batch [7789], #running-req: 64, #token: 224869, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1482.56, #queue-req: 0, 
[2025-10-24 15:43:12 TP0] Decode batch [7829], #running-req: 64, #token: 227429, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1481.94, #queue-req: 0, 
[2025-10-24 15:43:13 TP0] Decode batch [7869], #running-req: 64, #token: 229989, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1481.96, #queue-req: 0, 
[2025-10-24 15:43:15 TP0] Decode batch [7909], #running-req: 64, #token: 232549, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1481.52, #queue-req: 0, 
[2025-10-24 15:43:17 TP0] Decode batch [7949], #running-req: 64, #token: 235109, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1483.23, #queue-req: 0, 
[2025-10-24 15:43:19 TP0] Decode batch [7989], #running-req: 64, #token: 237669, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1483.44, #queue-req: 0, 
[2025-10-24 15:43:20 TP0] Decode batch [8029], #running-req: 64, #token: 240229, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1483.34, #queue-req: 0, 
[2025-10-24 15:43:22 TP0] Decode batch [8069], #running-req: 64, #token: 242789, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1481.07, #queue-req: 0, 
[2025-10-24 15:43:24 TP0] Decode batch [8109], #running-req: 64, #token: 245349, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1477.45, #queue-req: 0, 
[2025-10-24 15:43:25 TP0] Decode batch [8149], #running-req: 64, #token: 247909, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1472.95, #queue-req: 0, 
[2025-10-24 15:43:27 TP0] Decode batch [8189], #running-req: 64, #token: 250469, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1471.96, #queue-req: 0, 
[2025-10-24 15:43:29 TP0] Decode batch [8229], #running-req: 64, #token: 253029, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1470.45, #queue-req: 0, 
[2025-10-24 15:43:31 TP0] Decode batch [8269], #running-req: 64, #token: 255589, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1472.60, #queue-req: 0, 
[2025-10-24 15:43:31] INFO:     127.0.0.1:57636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31 TP0] Prefill batch [8275], #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:43:31] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31 TP0] Prefill batch [8276], #new-seq: 6, #new-token: 15987, #cached-token: 3219, token usage: 0.01, #running-req: 1, #queue-req: 8, 
[2025-10-24 15:43:31] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31 TP0] Prefill batch [8277], #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.02, #running-req: 7, #queue-req: 38, 
[2025-10-24 15:43:31] INFO:     127.0.0.1:58102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:31] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:43:32 TP0] Prefill batch [8278], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.04, #running-req: 12, #queue-req: 47, 
[2025-10-24 15:43:33 TP0] Prefill batch [8279], #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.06, #running-req: 17, #queue-req: 42, 
[2025-10-24 15:43:34 TP0] Prefill batch [8280], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 22, #queue-req: 37, 
[2025-10-24 15:43:35 TP0] Prefill batch [8281], #new-seq: 7, #new-token: 15993, #cached-token: 6414, token usage: 0.10, #running-req: 27, #queue-req: 30, 
[2025-10-24 15:43:35 TP0] Prefill batch [8282], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.11, #running-req: 34, #queue-req: 25, 
[2025-10-24 15:43:36 TP0] Prefill batch [8283], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.13, #running-req: 39, #queue-req: 20, 
[2025-10-24 15:43:37 TP0] Prefill batch [8284], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.14, #running-req: 44, #queue-req: 15, 
[2025-10-24 15:43:38 TP0] Prefill batch [8285], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.16, #running-req: 49, #queue-req: 10, 
[2025-10-24 15:43:39 TP0] Prefill batch [8286], #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.18, #running-req: 54, #queue-req: 5, 
[2025-10-24 15:43:39 TP0] Prefill batch [8287], #new-seq: 5, #new-token: 12792, #cached-token: 3213, token usage: 0.19, #running-req: 59, #queue-req: 0, 
[2025-10-24 15:43:42 TP0] Decode batch [8322], #running-req: 64, #token: 203738, token usage: 0.21, cuda graph: True, gen throughput (token/s): 221.51, #queue-req: 0, 
[2025-10-24 15:43:44 TP0] Decode batch [8362], #running-req: 64, #token: 206298, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1518.50, #queue-req: 0, 
[2025-10-24 15:43:46 TP0] Decode batch [8402], #running-req: 64, #token: 208858, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1505.60, #queue-req: 0, 
[2025-10-24 15:43:47 TP0] Decode batch [8442], #running-req: 64, #token: 211418, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1505.24, #queue-req: 0, 
[2025-10-24 15:43:49 TP0] Decode batch [8482], #running-req: 64, #token: 213978, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1500.72, #queue-req: 0, 
[2025-10-24 15:43:51 TP0] Decode batch [8522], #running-req: 64, #token: 216538, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1499.55, #queue-req: 0, 
[2025-10-24 15:43:52 TP0] Decode batch [8562], #running-req: 64, #token: 219098, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1500.68, #queue-req: 0, 
[2025-10-24 15:43:54 TP0] Decode batch [8602], #running-req: 64, #token: 221658, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1497.51, #queue-req: 0, 
[2025-10-24 15:43:56 TP0] Decode batch [8642], #running-req: 64, #token: 224218, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1493.27, #queue-req: 0, 
[2025-10-24 15:43:58 TP0] Decode batch [8682], #running-req: 64, #token: 226778, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1497.08, #queue-req: 0, 
[2025-10-24 15:43:59 TP0] Decode batch [8722], #running-req: 64, #token: 229338, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1493.97, #queue-req: 0, 
[2025-10-24 15:44:01 TP0] Decode batch [8762], #running-req: 64, #token: 231898, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1495.56, #queue-req: 0, 
[2025-10-24 15:44:03 TP0] Decode batch [8802], #running-req: 64, #token: 234458, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1493.93, #queue-req: 0, 
[2025-10-24 15:44:04 TP0] Decode batch [8842], #running-req: 64, #token: 237018, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1491.22, #queue-req: 0, 
[2025-10-24 15:44:06 TP0] Decode batch [8882], #running-req: 64, #token: 239578, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1493.70, #queue-req: 0, 
[2025-10-24 15:44:08 TP0] Decode batch [8922], #running-req: 64, #token: 242138, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1489.70, #queue-req: 0, 
[2025-10-24 15:44:10 TP0] Decode batch [8962], #running-req: 64, #token: 244698, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1488.42, #queue-req: 0, 
[2025-10-24 15:44:11 TP0] Decode batch [9002], #running-req: 64, #token: 247258, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1485.53, #queue-req: 0, 
[2025-10-24 15:44:13 TP0] Decode batch [9042], #running-req: 64, #token: 249818, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1486.47, #queue-req: 0, 
[2025-10-24 15:44:15 TP0] Decode batch [9082], #running-req: 64, #token: 252378, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1488.95, #queue-req: 0, 
[2025-10-24 15:44:15] INFO:     127.0.0.1:45164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15 TP0] Prefill batch [9088], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:44:15] INFO:     127.0.0.1:45168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15 TP0] Prefill batch [9089], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-10-24 15:44:15] INFO:     127.0.0.1:45328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15 TP0] Prefill batch [9090], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.02, #running-req: 6, #queue-req: 36, 
[2025-10-24 15:44:15] INFO:     127.0.0.1:45562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:15] INFO:     127.0.0.1:45666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:16 TP0] Prefill batch [9091], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-10-24 15:44:17 TP0] Prefill batch [9092], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-24 15:44:18 TP0] Prefill batch [9093], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-10-24 15:44:19 TP0] Prefill batch [9094], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-10-24 15:44:19 TP0] Prefill batch [9095], #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-10-24 15:44:20 TP0] Prefill batch [9096], #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.12, #running-req: 36, #queue-req: 22, 
[2025-10-24 15:44:21 TP0] Prefill batch [9097], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.14, #running-req: 42, #queue-req: 17, 
[2025-10-24 15:44:22 TP0] Prefill batch [9098], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.15, #running-req: 47, #queue-req: 12, 
[2025-10-24 15:44:23 TP0] Prefill batch [9099], #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.17, #running-req: 52, #queue-req: 7, 
[2025-10-24 15:44:24 TP0] Prefill batch [9100], #new-seq: 6, #new-token: 15985, #cached-token: 3221, token usage: 0.19, #running-req: 57, #queue-req: 1, 
[2025-10-24 15:44:24 TP0] Prefill batch [9101], #new-seq: 1, #new-token: 3193, #cached-token: 8, token usage: 0.21, #running-req: 63, #queue-req: 0, 
[2025-10-24 15:44:27 TP0] Decode batch [9136], #running-req: 64, #token: 206938, token usage: 0.21, cuda graph: True, gen throughput (token/s): 214.21, #queue-req: 0, 
[2025-10-24 15:44:28 TP0] Decode batch [9176], #running-req: 64, #token: 209498, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1519.51, #queue-req: 0, 
[2025-10-24 15:44:30 TP0] Decode batch [9216], #running-req: 64, #token: 212058, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1510.96, #queue-req: 0, 
[2025-10-24 15:44:32 TP0] Decode batch [9256], #running-req: 64, #token: 214618, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1508.58, #queue-req: 0, 
[2025-10-24 15:44:33 TP0] Decode batch [9296], #running-req: 64, #token: 217178, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1501.92, #queue-req: 0, 
[2025-10-24 15:44:35 TP0] Decode batch [9336], #running-req: 64, #token: 219738, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1498.21, #queue-req: 0, 
[2025-10-24 15:44:37 TP0] Decode batch [9376], #running-req: 64, #token: 222298, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1495.63, #queue-req: 0, 
[2025-10-24 15:44:39 TP0] Decode batch [9416], #running-req: 64, #token: 224858, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1492.98, #queue-req: 0, 
[2025-10-24 15:44:40 TP0] Decode batch [9456], #running-req: 64, #token: 227418, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1492.96, #queue-req: 0, 
[2025-10-24 15:44:42 TP0] Decode batch [9496], #running-req: 64, #token: 229978, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1492.98, #queue-req: 0, 
[2025-10-24 15:44:44 TP0] Decode batch [9536], #running-req: 64, #token: 232538, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1494.50, #queue-req: 0, 
[2025-10-24 15:44:45 TP0] Decode batch [9576], #running-req: 64, #token: 235098, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1493.64, #queue-req: 0, 
[2025-10-24 15:44:47 TP0] Decode batch [9616], #running-req: 64, #token: 237658, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1494.40, #queue-req: 0, 
[2025-10-24 15:44:49 TP0] Decode batch [9656], #running-req: 64, #token: 240218, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1496.07, #queue-req: 0, 
[2025-10-24 15:44:51 TP0] Decode batch [9696], #running-req: 64, #token: 242778, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1493.38, #queue-req: 0, 
[2025-10-24 15:44:52 TP0] Decode batch [9736], #running-req: 64, #token: 245338, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1493.69, #queue-req: 0, 
[2025-10-24 15:44:54 TP0] Decode batch [9776], #running-req: 64, #token: 247898, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1490.10, #queue-req: 0, 
[2025-10-24 15:44:56 TP0] Decode batch [9816], #running-req: 64, #token: 250458, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1497.15, #queue-req: 0, 
[2025-10-24 15:44:57 TP0] Decode batch [9856], #running-req: 64, #token: 253018, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1487.82, #queue-req: 0, 
[2025-10-24 15:44:59 TP0] Decode batch [9896], #running-req: 64, #token: 255578, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1483.76, #queue-req: 0, 
[2025-10-24 15:44:59] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:59 TP0] Prefill batch [9902], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:44:59] INFO:     127.0.0.1:58024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:59] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:44:59] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00 TP0] Prefill batch [9903], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-10-24 15:45:00] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00 TP0] Prefill batch [9904], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 36, 
[2025-10-24 15:45:00] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:00] INFO:     127.0.0.1:58646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:01 TP0] Prefill batch [9905], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-10-24 15:45:01 TP0] Prefill batch [9906], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-24 15:45:02 TP0] Prefill batch [9907], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-10-24 15:45:03 TP0] Prefill batch [9908], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-10-24 15:45:04 TP0] Prefill batch [9909], #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-10-24 15:45:05 TP0] Prefill batch [9910], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-10-24 15:45:05 TP0] Prefill batch [9911], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-10-24 15:45:06 TP0] Prefill batch [9912], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-10-24 15:45:07 TP0] Prefill batch [9913], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-10-24 15:45:08 TP0] Prefill batch [9914], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-10-24 15:45:09 TP0] Prefill batch [9915], #new-seq: 3, #new-token: 9597, #cached-token: 6, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-10-24 15:45:11 TP0] Decode batch [9950], #running-req: 64, #token: 206952, token usage: 0.21, cuda graph: True, gen throughput (token/s): 210.07, #queue-req: 0, 
[2025-10-24 15:45:13 TP0] Decode batch [9990], #running-req: 64, #token: 209512, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1519.68, #queue-req: 0, 
[2025-10-24 15:45:15 TP0] Decode batch [10030], #running-req: 64, #token: 212072, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1509.68, #queue-req: 0, 
[2025-10-24 15:45:16 TP0] Decode batch [10070], #running-req: 64, #token: 214632, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1503.97, #queue-req: 0, 
[2025-10-24 15:45:18 TP0] Decode batch [10110], #running-req: 64, #token: 217192, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1502.37, #queue-req: 0, 
[2025-10-24 15:45:20 TP0] Decode batch [10150], #running-req: 64, #token: 219752, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1499.33, #queue-req: 0, 
[2025-10-24 15:45:22 TP0] Decode batch [10190], #running-req: 64, #token: 222312, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1497.28, #queue-req: 0, 
[2025-10-24 15:45:23 TP0] Decode batch [10230], #running-req: 64, #token: 224872, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1497.00, #queue-req: 0, 
[2025-10-24 15:45:25 TP0] Decode batch [10270], #running-req: 64, #token: 227432, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1495.53, #queue-req: 0, 
[2025-10-24 15:45:27 TP0] Decode batch [10310], #running-req: 64, #token: 229992, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1495.26, #queue-req: 0, 
[2025-10-24 15:45:28 TP0] Decode batch [10350], #running-req: 64, #token: 232552, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1495.10, #queue-req: 0, 
[2025-10-24 15:45:30 TP0] Decode batch [10390], #running-req: 64, #token: 235112, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1500.95, #queue-req: 0, 
[2025-10-24 15:45:32 TP0] Decode batch [10430], #running-req: 64, #token: 237672, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1495.16, #queue-req: 0, 
[2025-10-24 15:45:34 TP0] Decode batch [10470], #running-req: 64, #token: 240232, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1495.98, #queue-req: 0, 
[2025-10-24 15:45:35 TP0] Decode batch [10510], #running-req: 64, #token: 242792, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1491.47, #queue-req: 0, 
[2025-10-24 15:45:37 TP0] Decode batch [10550], #running-req: 64, #token: 245352, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1493.53, #queue-req: 0, 
[2025-10-24 15:45:39 TP0] Decode batch [10590], #running-req: 64, #token: 247912, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1484.29, #queue-req: 0, 
[2025-10-24 15:45:40 TP0] Decode batch [10630], #running-req: 64, #token: 250472, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1483.10, #queue-req: 0, 
[2025-10-24 15:45:42 TP0] Decode batch [10670], #running-req: 64, #token: 253032, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1484.78, #queue-req: 0, 
[2025-10-24 15:45:44 TP0] Decode batch [10710], #running-req: 64, #token: 255592, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1481.81, #queue-req: 0, 
[2025-10-24 15:45:44] INFO:     127.0.0.1:43760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44 TP0] Prefill batch [10716], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:45:44] INFO:     127.0.0.1:43762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44 TP0] Prefill batch [10717], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-10-24 15:45:44] INFO:     127.0.0.1:43892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:43994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44] INFO:     127.0.0.1:44174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:44 TP0] Prefill batch [10718], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 36, 
[2025-10-24 15:45:44] INFO:     127.0.0.1:44184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45] INFO:     127.0.0.1:44294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:45:45 TP0] Prefill batch [10719], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-10-24 15:45:46 TP0] Prefill batch [10720], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-24 15:45:47 TP0] Prefill batch [10721], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-10-24 15:45:48 TP0] Prefill batch [10722], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-10-24 15:45:49 TP0] Prefill batch [10723], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-10-24 15:45:49 TP0] Prefill batch [10724], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-10-24 15:45:50 TP0] Prefill batch [10725], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-10-24 15:45:51 TP0] Prefill batch [10726], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-10-24 15:45:52 TP0] Prefill batch [10727], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-10-24 15:45:53 TP0] Prefill batch [10728], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-10-24 15:45:53 TP0] Prefill batch [10729], #new-seq: 3, #new-token: 9593, #cached-token: 10, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-10-24 15:45:56 TP0] Decode batch [10764], #running-req: 64, #token: 206942, token usage: 0.21, cuda graph: True, gen throughput (token/s): 210.50, #queue-req: 0, 
[2025-10-24 15:45:58 TP0] Decode batch [10804], #running-req: 64, #token: 209502, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1527.88, #queue-req: 0, 
[2025-10-24 15:45:59 TP0] Decode batch [10844], #running-req: 64, #token: 212062, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1514.63, #queue-req: 0, 
[2025-10-24 15:46:01 TP0] Decode batch [10884], #running-req: 64, #token: 214622, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1506.97, #queue-req: 0, 
[2025-10-24 15:46:03 TP0] Decode batch [10924], #running-req: 64, #token: 217182, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1502.19, #queue-req: 0, 
[2025-10-24 15:46:05 TP0] Decode batch [10964], #running-req: 64, #token: 219742, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1496.97, #queue-req: 0, 
[2025-10-24 15:46:06 TP0] Decode batch [11004], #running-req: 64, #token: 222302, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1492.86, #queue-req: 0, 
[2025-10-24 15:46:08 TP0] Decode batch [11044], #running-req: 64, #token: 224862, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1490.47, #queue-req: 0, 
[2025-10-24 15:46:10 TP0] Decode batch [11084], #running-req: 64, #token: 227422, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1488.48, #queue-req: 0, 
[2025-10-24 15:46:11 TP0] Decode batch [11124], #running-req: 64, #token: 229982, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1491.27, #queue-req: 0, 
[2025-10-24 15:46:13 TP0] Decode batch [11164], #running-req: 64, #token: 232542, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1494.61, #queue-req: 0, 
[2025-10-24 15:46:15 TP0] Decode batch [11204], #running-req: 64, #token: 235102, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1491.71, #queue-req: 0, 
[2025-10-24 15:46:17 TP0] Decode batch [11244], #running-req: 64, #token: 237662, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1488.89, #queue-req: 0, 
[2025-10-24 15:46:18 TP0] Decode batch [11284], #running-req: 64, #token: 240222, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1489.20, #queue-req: 0, 
[2025-10-24 15:46:20 TP0] Decode batch [11324], #running-req: 64, #token: 242782, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1484.43, #queue-req: 0, 
[2025-10-24 15:46:22 TP0] Decode batch [11364], #running-req: 64, #token: 245342, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1481.48, #queue-req: 0, 
[2025-10-24 15:46:23 TP0] Decode batch [11404], #running-req: 64, #token: 247902, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1480.00, #queue-req: 0, 
[2025-10-24 15:46:25 TP0] Decode batch [11444], #running-req: 64, #token: 250462, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1477.93, #queue-req: 0, 
[2025-10-24 15:46:27 TP0] Decode batch [11484], #running-req: 64, #token: 253022, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1477.31, #queue-req: 0, 
[2025-10-24 15:46:29 TP0] Decode batch [11524], #running-req: 64, #token: 255582, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1476.98, #queue-req: 0, 
[2025-10-24 15:46:29] INFO:     127.0.0.1:51802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29 TP0] Prefill batch [11530], #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:46:29] INFO:     127.0.0.1:51806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29 TP0] Prefill batch [11531], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-10-24 15:46:29] INFO:     127.0.0.1:51928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29 TP0] Prefill batch [11532], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 6, #queue-req: 37, 
[2025-10-24 15:46:29] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:29] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:46:30 TP0] Prefill batch [11533], #new-seq: 6, #new-token: 15988, #cached-token: 3218, token usage: 0.04, #running-req: 11, #queue-req: 35, 
[2025-10-24 15:46:31 TP0] Prefill batch [11534], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.06, #running-req: 17, #queue-req: 30, 
[2025-10-24 15:46:32 TP0] Prefill batch [11535], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 22, #queue-req: 25, 
[2025-10-24 15:46:33 TP0] Prefill batch [11536], #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.09, #running-req: 27, #queue-req: 20, 
[2025-10-24 15:46:33 TP0] Prefill batch [11537], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.11, #running-req: 32, #queue-req: 15, 
[2025-10-24 15:46:34 TP0] Prefill batch [11538], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 37, #queue-req: 10, 
[2025-10-24 15:46:35 TP0] Prefill batch [11539], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.14, #running-req: 42, #queue-req: 5, 
[2025-10-24 15:46:36 TP0] Prefill batch [11540], #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.15, #running-req: 47, #queue-req: 0, 
[2025-10-24 15:46:39 TP0] Decode batch [11575], #running-req: 52, #token: 168142, token usage: 0.17, cuda graph: True, gen throughput (token/s): 215.84, #queue-req: 0, 
[2025-10-24 15:46:40 TP0] Decode batch [11615], #running-req: 52, #token: 170222, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1280.53, #queue-req: 0, 
[2025-10-24 15:46:42 TP0] Decode batch [11655], #running-req: 52, #token: 172302, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1274.32, #queue-req: 0, 
[2025-10-24 15:46:44 TP0] Decode batch [11695], #running-req: 52, #token: 174382, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1268.48, #queue-req: 0, 
[2025-10-24 15:46:45 TP0] Decode batch [11735], #running-req: 52, #token: 176462, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1265.92, #queue-req: 0, 
[2025-10-24 15:46:47 TP0] Decode batch [11775], #running-req: 52, #token: 178542, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1260.10, #queue-req: 0, 
[2025-10-24 15:46:48 TP0] Decode batch [11815], #running-req: 52, #token: 180622, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1261.13, #queue-req: 0, 
[2025-10-24 15:46:50 TP0] Decode batch [11855], #running-req: 52, #token: 182702, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1261.72, #queue-req: 0, 
[2025-10-24 15:46:52 TP0] Decode batch [11895], #running-req: 52, #token: 184782, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1257.07, #queue-req: 0, 
[2025-10-24 15:46:53 TP0] Decode batch [11935], #running-req: 52, #token: 186862, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1262.61, #queue-req: 0, 
[2025-10-24 15:46:55 TP0] Decode batch [11975], #running-req: 52, #token: 188942, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1257.90, #queue-req: 0, 
[2025-10-24 15:46:57 TP0] Decode batch [12015], #running-req: 52, #token: 191022, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1257.58, #queue-req: 0, 
[2025-10-24 15:46:58 TP0] Decode batch [12055], #running-req: 52, #token: 193102, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1256.32, #queue-req: 0, 
[2025-10-24 15:47:00 TP0] Decode batch [12095], #running-req: 52, #token: 195182, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1253.59, #queue-req: 0, 
[2025-10-24 15:47:02 TP0] Decode batch [12135], #running-req: 52, #token: 197262, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1251.43, #queue-req: 0, 
[2025-10-24 15:47:03 TP0] Decode batch [12175], #running-req: 52, #token: 199342, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1249.12, #queue-req: 0, 
[2025-10-24 15:47:05 TP0] Decode batch [12215], #running-req: 52, #token: 201422, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1246.23, #queue-req: 0, 
[2025-10-24 15:47:07 TP0] Decode batch [12255], #running-req: 52, #token: 203502, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1238.20, #queue-req: 0, 
[2025-10-24 15:47:08 TP0] Decode batch [12295], #running-req: 52, #token: 205582, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1247.38, #queue-req: 0, 
[2025-10-24 15:47:10 TP0] Decode batch [12335], #running-req: 52, #token: 207662, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1246.76, #queue-req: 0, 
[2025-10-24 15:47:10] INFO:     127.0.0.1:44564 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-24 15:47:28] INFO:     127.0.0.1:49090 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-24 15:47:33] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:34 TP0] Prefill batch [12341], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:47:35] INFO:     127.0.0.1:40714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:35] INFO:     127.0.0.1:40730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:35 TP0] Prefill batch [12374], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:47:35] INFO:     127.0.0.1:40740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:35] INFO:     127.0.0.1:40750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:35] INFO:     127.0.0.1:40754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:35] INFO:     127.0.0.1:40766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:35] INFO:     127.0.0.1:40776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:35] INFO:     127.0.0.1:40792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:35] INFO:     127.0.0.1:40806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:35] INFO:     127.0.0.1:40822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:35] INFO:     127.0.0.1:40838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:35] INFO:     127.0.0.1:40840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:36] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:36] INFO:     127.0.0.1:40848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:36] INFO:     127.0.0.1:40858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:36] INFO:     127.0.0.1:40874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:36 TP0] Prefill batch [12375], #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-10-24 15:47:36 TP0] Prefill batch [12376], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-24 15:47:36 TP0] Prefill batch [12377], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-24 15:47:38 TP0] Decode batch [12380], #running-req: 16, #token: 51234, token usage: 0.05, cuda graph: True, gen throughput (token/s): 13.45, #queue-req: 0, 
[2025-10-24 15:47:39 TP0] Decode batch [12420], #running-req: 16, #token: 51874, token usage: 0.05, cuda graph: True, gen throughput (token/s): 622.78, #queue-req: 0, 
[2025-10-24 15:47:40 TP0] Decode batch [12460], #running-req: 16, #token: 52514, token usage: 0.05, cuda graph: True, gen throughput (token/s): 618.46, #queue-req: 0, 
[2025-10-24 15:47:41 TP0] Decode batch [12500], #running-req: 16, #token: 53154, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.46, #queue-req: 0, 
[2025-10-24 15:47:42 TP0] Decode batch [12540], #running-req: 16, #token: 53794, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.20, #queue-req: 0, 
[2025-10-24 15:47:43 TP0] Decode batch [12580], #running-req: 16, #token: 54434, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.46, #queue-req: 0, 
[2025-10-24 15:47:44 TP0] Decode batch [12620], #running-req: 16, #token: 55074, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.70, #queue-req: 0, 
[2025-10-24 15:47:45 TP0] Decode batch [12660], #running-req: 16, #token: 55714, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.88, #queue-req: 0, 
[2025-10-24 15:47:46 TP0] Decode batch [12700], #running-req: 16, #token: 56354, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.69, #queue-req: 0, 
[2025-10-24 15:47:47 TP0] Decode batch [12740], #running-req: 16, #token: 56994, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.89, #queue-req: 0, 
[2025-10-24 15:47:48 TP0] Decode batch [12780], #running-req: 16, #token: 57634, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.86, #queue-req: 0, 
[2025-10-24 15:47:49 TP0] Decode batch [12820], #running-req: 16, #token: 58274, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.25, #queue-req: 0, 
[2025-10-24 15:47:51 TP0] Decode batch [12860], #running-req: 16, #token: 58914, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.05, #queue-req: 0, 
[2025-10-24 15:47:52 TP0] Decode batch [12900], #running-req: 16, #token: 59554, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.54, #queue-req: 0, 
[2025-10-24 15:47:53 TP0] Decode batch [12940], #running-req: 16, #token: 60194, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.55, #queue-req: 0, 
[2025-10-24 15:47:54 TP0] Decode batch [12980], #running-req: 16, #token: 60834, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.44, #queue-req: 0, 
[2025-10-24 15:47:55 TP0] Decode batch [13020], #running-req: 16, #token: 61474, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.48, #queue-req: 0, 
[2025-10-24 15:47:56 TP0] Decode batch [13060], #running-req: 16, #token: 62114, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.38, #queue-req: 0, 
[2025-10-24 15:47:57 TP0] Decode batch [13100], #running-req: 16, #token: 62754, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.38, #queue-req: 0, 
[2025-10-24 15:47:58 TP0] Decode batch [13140], #running-req: 16, #token: 63394, token usage: 0.07, cuda graph: True, gen throughput (token/s): 608.58, #queue-req: 0, 
[2025-10-24 15:47:59] INFO:     127.0.0.1:39580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59 TP0] Prefill batch [13178], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:47:59] INFO:     127.0.0.1:39600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59] INFO:     127.0.0.1:39750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:47:59 TP0] Prefill batch [13179], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-24 15:47:59 TP0] Prefill batch [13180], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-24 15:48:00 TP0] Prefill batch [13181], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-24 15:48:02 TP0] Decode batch [13184], #running-req: 16, #token: 51228, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.65, #queue-req: 0, 
[2025-10-24 15:48:03 TP0] Decode batch [13224], #running-req: 16, #token: 51868, token usage: 0.05, cuda graph: True, gen throughput (token/s): 620.09, #queue-req: 0, 
[2025-10-24 15:48:04 TP0] Decode batch [13264], #running-req: 16, #token: 52508, token usage: 0.05, cuda graph: True, gen throughput (token/s): 613.48, #queue-req: 0, 
[2025-10-24 15:48:05 TP0] Decode batch [13304], #running-req: 16, #token: 53148, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.66, #queue-req: 0, 
[2025-10-24 15:48:06 TP0] Decode batch [13344], #running-req: 16, #token: 53788, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.44, #queue-req: 0, 
[2025-10-24 15:48:07 TP0] Decode batch [13384], #running-req: 16, #token: 54428, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.32, #queue-req: 0, 
[2025-10-24 15:48:08 TP0] Decode batch [13424], #running-req: 16, #token: 55068, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.62, #queue-req: 0, 
[2025-10-24 15:48:09 TP0] Decode batch [13464], #running-req: 16, #token: 55708, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.11, #queue-req: 0, 
[2025-10-24 15:48:10 TP0] Decode batch [13504], #running-req: 16, #token: 56348, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.51, #queue-req: 0, 
[2025-10-24 15:48:11 TP0] Decode batch [13544], #running-req: 16, #token: 56988, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.53, #queue-req: 0, 
[2025-10-24 15:48:12 TP0] Decode batch [13584], #running-req: 16, #token: 57628, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.73, #queue-req: 0, 
[2025-10-24 15:48:13 TP0] Decode batch [13624], #running-req: 16, #token: 58268, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.47, #queue-req: 0, 
[2025-10-24 15:48:14 TP0] Decode batch [13664], #running-req: 16, #token: 58908, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.94, #queue-req: 0, 
[2025-10-24 15:48:15 TP0] Decode batch [13704], #running-req: 16, #token: 59548, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.24, #queue-req: 0, 
[2025-10-24 15:48:16 TP0] Decode batch [13744], #running-req: 16, #token: 60188, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.42, #queue-req: 0, 
[2025-10-24 15:48:17 TP0] Decode batch [13784], #running-req: 16, #token: 60828, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.51, #queue-req: 0, 
[2025-10-24 15:48:18 TP0] Decode batch [13824], #running-req: 16, #token: 61468, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.24, #queue-req: 0, 
[2025-10-24 15:48:19 TP0] Decode batch [13864], #running-req: 16, #token: 62108, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.20, #queue-req: 0, 
[2025-10-24 15:48:21 TP0] Decode batch [13904], #running-req: 16, #token: 62748, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.94, #queue-req: 0, 
[2025-10-24 15:48:22 TP0] Decode batch [13944], #running-req: 16, #token: 63388, token usage: 0.07, cuda graph: True, gen throughput (token/s): 604.58, #queue-req: 0, 
[2025-10-24 15:48:23] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:55970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23 TP0] Prefill batch [13982], #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:48:23] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:55980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:55992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:55998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:56008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:56024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:23 TP0] Prefill batch [13983], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-24 15:48:23 TP0] Prefill batch [13984], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-24 15:48:24 TP0] Prefill batch [13985], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-24 15:48:25 TP0] Decode batch [13988], #running-req: 16, #token: 51230, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.59, #queue-req: 0, 
[2025-10-24 15:48:26 TP0] Decode batch [14028], #running-req: 16, #token: 51870, token usage: 0.05, cuda graph: True, gen throughput (token/s): 624.29, #queue-req: 0, 
[2025-10-24 15:48:27 TP0] Decode batch [14068], #running-req: 16, #token: 52510, token usage: 0.05, cuda graph: True, gen throughput (token/s): 624.52, #queue-req: 0, 
[2025-10-24 15:48:28 TP0] Decode batch [14108], #running-req: 16, #token: 53150, token usage: 0.05, cuda graph: True, gen throughput (token/s): 618.15, #queue-req: 0, 
[2025-10-24 15:48:29 TP0] Decode batch [14148], #running-req: 16, #token: 53790, token usage: 0.06, cuda graph: True, gen throughput (token/s): 618.04, #queue-req: 0, 
[2025-10-24 15:48:30 TP0] Decode batch [14188], #running-req: 16, #token: 54430, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.34, #queue-req: 0, 
[2025-10-24 15:48:31 TP0] Decode batch [14228], #running-req: 16, #token: 55070, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.22, #queue-req: 0, 
[2025-10-24 15:48:33 TP0] Decode batch [14268], #running-req: 16, #token: 55710, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.36, #queue-req: 0, 
[2025-10-24 15:48:34 TP0] Decode batch [14308], #running-req: 16, #token: 56350, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.42, #queue-req: 0, 
[2025-10-24 15:48:35 TP0] Decode batch [14348], #running-req: 16, #token: 56990, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.39, #queue-req: 0, 
[2025-10-24 15:48:36 TP0] Decode batch [14388], #running-req: 16, #token: 57630, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.40, #queue-req: 0, 
[2025-10-24 15:48:37 TP0] Decode batch [14428], #running-req: 16, #token: 58270, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.55, #queue-req: 0, 
[2025-10-24 15:48:38 TP0] Decode batch [14468], #running-req: 16, #token: 58910, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.63, #queue-req: 0, 
[2025-10-24 15:48:39 TP0] Decode batch [14508], #running-req: 16, #token: 59550, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.49, #queue-req: 0, 
[2025-10-24 15:48:40 TP0] Decode batch [14548], #running-req: 16, #token: 60190, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.60, #queue-req: 0, 
[2025-10-24 15:48:41 TP0] Decode batch [14588], #running-req: 16, #token: 60830, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.92, #queue-req: 0, 
[2025-10-24 15:48:42 TP0] Decode batch [14628], #running-req: 16, #token: 61470, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.85, #queue-req: 0, 
[2025-10-24 15:48:43 TP0] Decode batch [14668], #running-req: 16, #token: 62110, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.75, #queue-req: 0, 
[2025-10-24 15:48:44 TP0] Decode batch [14708], #running-req: 16, #token: 62750, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.67, #queue-req: 0, 
[2025-10-24 15:48:45 TP0] Decode batch [14748], #running-req: 16, #token: 63390, token usage: 0.07, cuda graph: True, gen throughput (token/s): 608.09, #queue-req: 0, 
[2025-10-24 15:48:46] INFO:     127.0.0.1:52978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46 TP0] Prefill batch [14786], #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:48:46] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:53042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:53086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:53108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:48:46 TP0] Prefill batch [14787], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-24 15:48:46 TP0] Prefill batch [14788], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-24 15:48:47 TP0] Prefill batch [14789], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-24 15:48:49 TP0] Decode batch [14792], #running-req: 16, #token: 51233, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.76, #queue-req: 0, 
[2025-10-24 15:48:50 TP0] Decode batch [14832], #running-req: 16, #token: 51873, token usage: 0.05, cuda graph: True, gen throughput (token/s): 618.94, #queue-req: 0, 
[2025-10-24 15:48:51 TP0] Decode batch [14872], #running-req: 16, #token: 52513, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.27, #queue-req: 0, 
[2025-10-24 15:48:52 TP0] Decode batch [14912], #running-req: 16, #token: 53153, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.73, #queue-req: 0, 
[2025-10-24 15:48:53 TP0] Decode batch [14952], #running-req: 16, #token: 53793, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.15, #queue-req: 0, 
[2025-10-24 15:48:54 TP0] Decode batch [14992], #running-req: 16, #token: 54433, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.33, #queue-req: 0, 
[2025-10-24 15:48:55 TP0] Decode batch [15032], #running-req: 16, #token: 55073, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.96, #queue-req: 0, 
[2025-10-24 15:48:56 TP0] Decode batch [15072], #running-req: 16, #token: 55713, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.35, #queue-req: 0, 
[2025-10-24 15:48:57 TP0] Decode batch [15112], #running-req: 16, #token: 56353, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.95, #queue-req: 0, 
[2025-10-24 15:48:58 TP0] Decode batch [15152], #running-req: 16, #token: 56993, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.60, #queue-req: 0, 
[2025-10-24 15:48:59 TP0] Decode batch [15192], #running-req: 16, #token: 57633, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.66, #queue-req: 0, 
[2025-10-24 15:49:00 TP0] Decode batch [15232], #running-req: 16, #token: 58273, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.00, #queue-req: 0, 
[2025-10-24 15:49:01 TP0] Decode batch [15272], #running-req: 16, #token: 58913, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.80, #queue-req: 0, 
[2025-10-24 15:49:02 TP0] Decode batch [15312], #running-req: 16, #token: 59553, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.92, #queue-req: 0, 
[2025-10-24 15:49:04 TP0] Decode batch [15352], #running-req: 16, #token: 60193, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.36, #queue-req: 0, 
[2025-10-24 15:49:05 TP0] Decode batch [15392], #running-req: 16, #token: 60833, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.46, #queue-req: 0, 
[2025-10-24 15:49:06 TP0] Decode batch [15432], #running-req: 16, #token: 61473, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.25, #queue-req: 0, 
[2025-10-24 15:49:07 TP0] Decode batch [15472], #running-req: 16, #token: 62113, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.28, #queue-req: 0, 
[2025-10-24 15:49:08 TP0] Decode batch [15512], #running-req: 16, #token: 62753, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.95, #queue-req: 0, 
[2025-10-24 15:49:09 TP0] Decode batch [15552], #running-req: 16, #token: 63393, token usage: 0.07, cuda graph: True, gen throughput (token/s): 610.15, #queue-req: 0, 
[2025-10-24 15:49:10] INFO:     127.0.0.1:34586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10 TP0] Prefill batch [15590], #new-seq: 1, #new-token: 3195, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:49:10] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10] INFO:     127.0.0.1:34692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:10 TP0] Prefill batch [15591], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-24 15:49:10 TP0] Prefill batch [15592], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-24 15:49:11 TP0] Prefill batch [15593], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-24 15:49:12 TP0] Decode batch [15596], #running-req: 16, #token: 51228, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.95, #queue-req: 0, 
[2025-10-24 15:49:13 TP0] Decode batch [15636], #running-req: 16, #token: 51868, token usage: 0.05, cuda graph: True, gen throughput (token/s): 624.28, #queue-req: 0, 
[2025-10-24 15:49:15 TP0] Decode batch [15676], #running-req: 16, #token: 52508, token usage: 0.05, cuda graph: True, gen throughput (token/s): 621.75, #queue-req: 0, 
[2025-10-24 15:49:16 TP0] Decode batch [15716], #running-req: 16, #token: 53148, token usage: 0.05, cuda graph: True, gen throughput (token/s): 621.02, #queue-req: 0, 
[2025-10-24 15:49:17 TP0] Decode batch [15756], #running-req: 16, #token: 53788, token usage: 0.06, cuda graph: True, gen throughput (token/s): 618.12, #queue-req: 0, 
[2025-10-24 15:49:18 TP0] Decode batch [15796], #running-req: 16, #token: 54428, token usage: 0.06, cuda graph: True, gen throughput (token/s): 617.27, #queue-req: 0, 
[2025-10-24 15:49:19 TP0] Decode batch [15836], #running-req: 16, #token: 55068, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.60, #queue-req: 0, 
[2025-10-24 15:49:20 TP0] Decode batch [15876], #running-req: 16, #token: 55708, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.44, #queue-req: 0, 
[2025-10-24 15:49:21 TP0] Decode batch [15916], #running-req: 16, #token: 56348, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.67, #queue-req: 0, 
[2025-10-24 15:49:22 TP0] Decode batch [15956], #running-req: 16, #token: 56988, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.38, #queue-req: 0, 
[2025-10-24 15:49:23 TP0] Decode batch [15996], #running-req: 16, #token: 57628, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.11, #queue-req: 0, 
[2025-10-24 15:49:24 TP0] Decode batch [16036], #running-req: 16, #token: 58268, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.49, #queue-req: 0, 
[2025-10-24 15:49:25 TP0] Decode batch [16076], #running-req: 16, #token: 58908, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.82, #queue-req: 0, 
[2025-10-24 15:49:26 TP0] Decode batch [16116], #running-req: 16, #token: 59548, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.57, #queue-req: 0, 
[2025-10-24 15:49:27 TP0] Decode batch [16156], #running-req: 16, #token: 60188, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.62, #queue-req: 0, 
[2025-10-24 15:49:28 TP0] Decode batch [16196], #running-req: 16, #token: 60828, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.33, #queue-req: 0, 
[2025-10-24 15:49:29 TP0] Decode batch [16236], #running-req: 16, #token: 61468, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.01, #queue-req: 0, 
[2025-10-24 15:49:30 TP0] Decode batch [16276], #running-req: 16, #token: 62108, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.30, #queue-req: 0, 
[2025-10-24 15:49:31 TP0] Decode batch [16316], #running-req: 16, #token: 62748, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.99, #queue-req: 0, 
[2025-10-24 15:49:32 TP0] Decode batch [16356], #running-req: 16, #token: 63388, token usage: 0.07, cuda graph: True, gen throughput (token/s): 612.29, #queue-req: 0, 
[2025-10-24 15:49:33] INFO:     127.0.0.1:43722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33 TP0] Prefill batch [16394], #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:49:33] INFO:     127.0.0.1:43740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33] INFO:     127.0.0.1:43884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:33 TP0] Prefill batch [16395], #new-seq: 5, #new-token: 15978, #cached-token: 27, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-24 15:49:34 TP0] Prefill batch [16396], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-24 15:49:34 TP0] Prefill batch [16397], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-24 15:49:36 TP0] Decode batch [16400], #running-req: 16, #token: 51228, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.92, #queue-req: 0, 
[2025-10-24 15:49:37 TP0] Decode batch [16440], #running-req: 16, #token: 51868, token usage: 0.05, cuda graph: True, gen throughput (token/s): 623.69, #queue-req: 0, 
[2025-10-24 15:49:38 TP0] Decode batch [16480], #running-req: 16, #token: 52508, token usage: 0.05, cuda graph: True, gen throughput (token/s): 618.35, #queue-req: 0, 
[2025-10-24 15:49:39 TP0] Decode batch [16520], #running-req: 16, #token: 53148, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.16, #queue-req: 0, 
[2025-10-24 15:49:40 TP0] Decode batch [16560], #running-req: 16, #token: 53788, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.29, #queue-req: 0, 
[2025-10-24 15:49:41 TP0] Decode batch [16600], #running-req: 16, #token: 54428, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.01, #queue-req: 0, 
[2025-10-24 15:49:42 TP0] Decode batch [16640], #running-req: 16, #token: 55068, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.33, #queue-req: 0, 
[2025-10-24 15:49:43 TP0] Decode batch [16680], #running-req: 16, #token: 55708, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.57, #queue-req: 0, 
[2025-10-24 15:49:44 TP0] Decode batch [16720], #running-req: 16, #token: 56348, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.66, #queue-req: 0, 
[2025-10-24 15:49:45 TP0] Decode batch [16760], #running-req: 16, #token: 56988, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.81, #queue-req: 0, 
[2025-10-24 15:49:46 TP0] Decode batch [16800], #running-req: 16, #token: 57628, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.35, #queue-req: 0, 
[2025-10-24 15:49:47 TP0] Decode batch [16840], #running-req: 16, #token: 58268, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.11, #queue-req: 0, 
[2025-10-24 15:49:48 TP0] Decode batch [16880], #running-req: 16, #token: 58908, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.81, #queue-req: 0, 
[2025-10-24 15:49:50 TP0] Decode batch [16920], #running-req: 16, #token: 59548, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.07, #queue-req: 0, 
[2025-10-24 15:49:51 TP0] Decode batch [16960], #running-req: 16, #token: 60188, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.31, #queue-req: 0, 
[2025-10-24 15:49:52 TP0] Decode batch [17000], #running-req: 16, #token: 60828, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.26, #queue-req: 0, 
[2025-10-24 15:49:53 TP0] Decode batch [17040], #running-req: 16, #token: 61468, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.81, #queue-req: 0, 
[2025-10-24 15:49:54 TP0] Decode batch [17080], #running-req: 16, #token: 62108, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.92, #queue-req: 0, 
[2025-10-24 15:49:55 TP0] Decode batch [17120], #running-req: 16, #token: 62748, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.32, #queue-req: 0, 
[2025-10-24 15:49:56 TP0] Decode batch [17160], #running-req: 16, #token: 63388, token usage: 0.07, cuda graph: True, gen throughput (token/s): 609.64, #queue-req: 0, 
[2025-10-24 15:49:57] INFO:     127.0.0.1:59762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57 TP0] Prefill batch [17198], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:49:57] INFO:     127.0.0.1:59780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57] INFO:     127.0.0.1:59914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:49:57 TP0] Prefill batch [17199], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-24 15:49:57 TP0] Prefill batch [17200], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-24 15:49:58 TP0] Prefill batch [17201], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-24 15:50:00 TP0] Decode batch [17204], #running-req: 16, #token: 51230, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.26, #queue-req: 0, 
[2025-10-24 15:50:01 TP0] Decode batch [17244], #running-req: 16, #token: 51870, token usage: 0.05, cuda graph: True, gen throughput (token/s): 619.92, #queue-req: 0, 
[2025-10-24 15:50:02 TP0] Decode batch [17284], #running-req: 16, #token: 52510, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.73, #queue-req: 0, 
[2025-10-24 15:50:03 TP0] Decode batch [17324], #running-req: 16, #token: 53150, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.58, #queue-req: 0, 
[2025-10-24 15:50:04 TP0] Decode batch [17364], #running-req: 16, #token: 53790, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.88, #queue-req: 0, 
[2025-10-24 15:50:05 TP0] Decode batch [17404], #running-req: 16, #token: 54430, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.91, #queue-req: 0, 
[2025-10-24 15:50:06 TP0] Decode batch [17444], #running-req: 16, #token: 55070, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.31, #queue-req: 0, 
[2025-10-24 15:50:07 TP0] Decode batch [17484], #running-req: 16, #token: 55710, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.17, #queue-req: 0, 
[2025-10-24 15:50:08 TP0] Decode batch [17524], #running-req: 16, #token: 56350, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.88, #queue-req: 0, 
[2025-10-24 15:50:09 TP0] Decode batch [17564], #running-req: 16, #token: 56990, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.50, #queue-req: 0, 
[2025-10-24 15:50:10 TP0] Decode batch [17604], #running-req: 16, #token: 57630, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.06, #queue-req: 0, 
[2025-10-24 15:50:11 TP0] Decode batch [17644], #running-req: 16, #token: 58270, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.91, #queue-req: 0, 
[2025-10-24 15:50:12 TP0] Decode batch [17684], #running-req: 16, #token: 58910, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.63, #queue-req: 0, 
[2025-10-24 15:50:13 TP0] Decode batch [17724], #running-req: 16, #token: 59550, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.07, #queue-req: 0, 
[2025-10-24 15:50:14 TP0] Decode batch [17764], #running-req: 16, #token: 60190, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.23, #queue-req: 0, 
[2025-10-24 15:50:15 TP0] Decode batch [17804], #running-req: 16, #token: 60830, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.07, #queue-req: 0, 
[2025-10-24 15:50:16 TP0] Decode batch [17844], #running-req: 16, #token: 61470, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.39, #queue-req: 0, 
[2025-10-24 15:50:17 TP0] Decode batch [17884], #running-req: 16, #token: 62110, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.11, #queue-req: 0, 
[2025-10-24 15:50:18 TP0] Decode batch [17924], #running-req: 16, #token: 62750, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.65, #queue-req: 0, 
[2025-10-24 15:50:19 TP0] Decode batch [17964], #running-req: 16, #token: 63390, token usage: 0.07, cuda graph: True, gen throughput (token/s): 606.08, #queue-req: 0, 
[2025-10-24 15:50:20] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:20] INFO:     127.0.0.1:52628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:20 TP0] Prefill batch [18002], #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:50:21] INFO:     127.0.0.1:52630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21] INFO:     127.0.0.1:52644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21] INFO:     127.0.0.1:52656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21] INFO:     127.0.0.1:52670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21] INFO:     127.0.0.1:52678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21] INFO:     127.0.0.1:52718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21] INFO:     127.0.0.1:52720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21] INFO:     127.0.0.1:52726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:50:21 TP0] Prefill batch [18003], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-24 15:50:21 TP0] Prefill batch [18004], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-24 15:50:22 TP0] Prefill batch [18005], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-24 15:50:23 TP0] Decode batch [18008], #running-req: 16, #token: 51231, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.07, #queue-req: 0, 
[2025-10-24 15:50:24 TP0] Decode batch [18048], #running-req: 16, #token: 51871, token usage: 0.05, cuda graph: True, gen throughput (token/s): 618.27, #queue-req: 0, 
[2025-10-24 15:50:25 TP0] Decode batch [18088], #running-req: 16, #token: 52511, token usage: 0.05, cuda graph: True, gen throughput (token/s): 613.13, #queue-req: 0, 
[2025-10-24 15:50:26 TP0] Decode batch [18128], #running-req: 16, #token: 53151, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.47, #queue-req: 0, 
[2025-10-24 15:50:27 TP0] Decode batch [18168], #running-req: 16, #token: 53791, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.32, #queue-req: 0, 
[2025-10-24 15:50:28 TP0] Decode batch [18208], #running-req: 16, #token: 54431, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.44, #queue-req: 0, 
[2025-10-24 15:50:29 TP0] Decode batch [18248], #running-req: 16, #token: 55071, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.15, #queue-req: 0, 
[2025-10-24 15:50:31 TP0] Decode batch [18288], #running-req: 16, #token: 55711, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.23, #queue-req: 0, 
[2025-10-24 15:50:32 TP0] Decode batch [18328], #running-req: 16, #token: 56351, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.40, #queue-req: 0, 
[2025-10-24 15:50:33 TP0] Decode batch [18368], #running-req: 16, #token: 56991, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.63, #queue-req: 0, 
[2025-10-24 15:50:34 TP0] Decode batch [18408], #running-req: 16, #token: 57631, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.29, #queue-req: 0, 
[2025-10-24 15:50:35 TP0] Decode batch [18448], #running-req: 16, #token: 58271, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.52, #queue-req: 0, 
[2025-10-24 15:50:36 TP0] Decode batch [18488], #running-req: 16, #token: 58911, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.67, #queue-req: 0, 
[2025-10-24 15:50:37 TP0] Decode batch [18528], #running-req: 16, #token: 59551, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.67, #queue-req: 0, 
[2025-10-24 15:50:38 TP0] Decode batch [18568], #running-req: 16, #token: 60191, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.08, #queue-req: 0, 
[2025-10-24 15:50:39 TP0] Decode batch [18608], #running-req: 16, #token: 60831, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.24, #queue-req: 0, 
[2025-10-24 15:50:40 TP0] Decode batch [18648], #running-req: 16, #token: 61471, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.32, #queue-req: 0, 
[2025-10-24 15:50:41 TP0] Decode batch [18688], #running-req: 16, #token: 62111, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.80, #queue-req: 0, 
[2025-10-24 15:50:42 TP0] Decode batch [18728], #running-req: 16, #token: 62751, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.17, #queue-req: 0, 
[2025-10-24 15:50:43 TP0] Decode batch [18768], #running-req: 16, #token: 63391, token usage: 0.07, cuda graph: True, gen throughput (token/s): 605.69, #queue-req: 0, 
[2025-10-24 15:50:44] INFO:     127.0.0.1:56572 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-24 15:51:01] INFO:     127.0.0.1:36796 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-24 15:51:07] INFO:     127.0.0.1:35082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:07 TP0] Prefill batch [18806], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:51:07 TP0] Decode batch [18809], #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 25.72, #queue-req: 0, 
[2025-10-24 15:51:09] INFO:     127.0.0.1:35098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:09] INFO:     127.0.0.1:35102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:09 TP0] Prefill batch [18839], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:51:09] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:09] INFO:     127.0.0.1:35112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:09 TP0] Prefill batch [18840], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:51:09 TP0] Decode batch [18851], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 35.08, #queue-req: 0, 
[2025-10-24 15:51:10 TP0] Decode batch [18891], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-10-24 15:51:11 TP0] Decode batch [18931], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-24 15:51:11 TP0] Decode batch [18971], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-24 15:51:12 TP0] Decode batch [19011], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:51:13 TP0] Decode batch [19051], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-24 15:51:14 TP0] Decode batch [19091], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:51:15 TP0] Decode batch [19131], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:51:16 TP0] Decode batch [19171], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:51:16 TP0] Decode batch [19211], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-24 15:51:17 TP0] Decode batch [19251], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:51:18 TP0] Decode batch [19291], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:51:19 TP0] Decode batch [19331], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:51:20 TP0] Decode batch [19371], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:51:20 TP0] Decode batch [19411], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:51:21 TP0] Decode batch [19451], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:51:22 TP0] Decode batch [19491], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:51:23 TP0] Decode batch [19531], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:51:24 TP0] Decode batch [19571], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:51:25 TP0] Decode batch [19611], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:51:25] INFO:     127.0.0.1:53820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:25] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:25] INFO:     127.0.0.1:53838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:25 TP0] Prefill batch [19641], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:51:25] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:25 TP0] Prefill batch [19642], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:51:26 TP0] Decode batch [19653], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.44, #queue-req: 0, 
[2025-10-24 15:51:26 TP0] Decode batch [19693], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-24 15:51:27 TP0] Decode batch [19733], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-24 15:51:28 TP0] Decode batch [19773], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-24 15:51:29 TP0] Decode batch [19813], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-24 15:51:30 TP0] Decode batch [19853], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-24 15:51:31 TP0] Decode batch [19893], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-24 15:51:31 TP0] Decode batch [19933], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-24 15:51:32 TP0] Decode batch [19973], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-24 15:51:33 TP0] Decode batch [20013], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-24 15:51:34 TP0] Decode batch [20053], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:51:35 TP0] Decode batch [20093], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:51:36 TP0] Decode batch [20133], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:51:36 TP0] Decode batch [20173], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-24 15:51:37 TP0] Decode batch [20213], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:51:38 TP0] Decode batch [20253], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:51:39 TP0] Decode batch [20293], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:51:40 TP0] Decode batch [20333], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-24 15:51:40 TP0] Decode batch [20373], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:51:41 TP0] Decode batch [20413], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:51:42] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:42] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:42] INFO:     127.0.0.1:50086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:42 TP0] Prefill batch [20443], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:51:42] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:42 TP0] Prefill batch [20444], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:51:42 TP0] Decode batch [20455], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.68, #queue-req: 0, 
[2025-10-24 15:51:43 TP0] Decode batch [20495], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-10-24 15:51:44 TP0] Decode batch [20535], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-24 15:51:45 TP0] Decode batch [20575], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-24 15:51:46 TP0] Decode batch [20615], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-24 15:51:46 TP0] Decode batch [20655], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-24 15:51:47 TP0] Decode batch [20695], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-24 15:51:48 TP0] Decode batch [20735], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-24 15:51:49 TP0] Decode batch [20775], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-24 15:51:50 TP0] Decode batch [20815], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-24 15:51:51 TP0] Decode batch [20855], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:51:51 TP0] Decode batch [20895], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:51:52 TP0] Decode batch [20935], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-24 15:51:53 TP0] Decode batch [20975], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-24 15:51:54 TP0] Decode batch [21015], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-24 15:51:55 TP0] Decode batch [21055], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:51:56 TP0] Decode batch [21095], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:51:56 TP0] Decode batch [21135], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-24 15:51:57 TP0] Decode batch [21175], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:51:58 TP0] Decode batch [21215], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:51:59] INFO:     127.0.0.1:54828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:59] INFO:     127.0.0.1:54844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:59] INFO:     127.0.0.1:54850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:59 TP0] Prefill batch [21245], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:51:59] INFO:     127.0.0.1:54858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:51:59 TP0] Prefill batch [21246], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:51:59 TP0] Decode batch [21257], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.85, #queue-req: 0, 
[2025-10-24 15:52:00 TP0] Decode batch [21297], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-24 15:52:01 TP0] Decode batch [21337], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-24 15:52:02 TP0] Decode batch [21377], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-24 15:52:02 TP0] Decode batch [21417], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-24 15:52:03 TP0] Decode batch [21457], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:52:04 TP0] Decode batch [21497], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-24 15:52:05 TP0] Decode batch [21537], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:52:06 TP0] Decode batch [21577], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:52:06 TP0] Decode batch [21617], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:52:07 TP0] Decode batch [21657], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:52:08 TP0] Decode batch [21697], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:52:09 TP0] Decode batch [21737], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:52:10 TP0] Decode batch [21777], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:52:11 TP0] Decode batch [21817], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:52:11 TP0] Decode batch [21857], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:52:12 TP0] Decode batch [21897], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:52:13 TP0] Decode batch [21937], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:52:14 TP0] Decode batch [21977], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:52:15 TP0] Decode batch [22017], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:52:15] INFO:     127.0.0.1:34096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:52:15] INFO:     127.0.0.1:34108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:52:15] INFO:     127.0.0.1:34116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:52:15 TP0] Prefill batch [22047], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:52:15] INFO:     127.0.0.1:34120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:52:15 TP0] Prefill batch [22048], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:52:16 TP0] Decode batch [22059], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.86, #queue-req: 0, 
[2025-10-24 15:52:17 TP0] Decode batch [22099], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-24 15:52:17 TP0] Decode batch [22139], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-24 15:52:18 TP0] Decode batch [22179], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-24 15:52:19 TP0] Decode batch [22219], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-24 15:52:20 TP0] Decode batch [22259], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-24 15:52:21 TP0] Decode batch [22299], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-24 15:52:22 TP0] Decode batch [22339], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-24 15:52:22 TP0] Decode batch [22379], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-24 15:52:23 TP0] Decode batch [22419], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-24 15:52:24 TP0] Decode batch [22459], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:52:25 TP0] Decode batch [22499], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:52:26 TP0] Decode batch [22539], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-24 15:52:26 TP0] Decode batch [22579], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:52:27 TP0] Decode batch [22619], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:52:28 TP0] Decode batch [22659], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:52:29 TP0] Decode batch [22699], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-24 15:52:30 TP0] Decode batch [22739], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:52:31 TP0] Decode batch [22779], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:52:31 TP0] Decode batch [22819], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-24 15:52:32] INFO:     127.0.0.1:36920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:52:32] INFO:     127.0.0.1:36934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:52:32 TP0] Prefill batch [22849], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:52:32] INFO:     127.0.0.1:36948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:52:32] INFO:     127.0.0.1:36954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:52:32 TP0] Prefill batch [22850], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:52:32 TP0] Decode batch [22861], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.99, #queue-req: 0, 
[2025-10-24 15:52:33 TP0] Decode batch [22901], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-10-24 15:52:34 TP0] Decode batch [22941], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-24 15:52:35 TP0] Decode batch [22981], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 191.62, #queue-req: 0, 
[2025-10-24 15:52:36 TP0] Decode batch [23021], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-24 15:52:37 TP0] Decode batch [23061], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-24 15:52:37 TP0] Decode batch [23101], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-24 15:52:38 TP0] Decode batch [23141], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:52:39 TP0] Decode batch [23181], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-24 15:52:40 TP0] Decode batch [23221], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:52:41 TP0] Decode batch [23261], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-24 15:52:42 TP0] Decode batch [23301], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:52:42 TP0] Decode batch [23341], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:52:43 TP0] Decode batch [23381], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:52:44 TP0] Decode batch [23421], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:52:45 TP0] Decode batch [23461], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:52:46 TP0] Decode batch [23501], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:52:46 TP0] Decode batch [23541], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:52:47 TP0] Decode batch [23581], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:52:48 TP0] Decode batch [23621], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:52:49] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:52:49] INFO:     127.0.0.1:42128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:52:49] INFO:     127.0.0.1:42144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:52:49 TP0] Prefill batch [23651], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:52:49] INFO:     127.0.0.1:42146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:52:49 TP0] Prefill batch [23652], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:52:49 TP0] Decode batch [23663], #running-req: 4, #token: 12840, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.36, #queue-req: 0, 
[2025-10-24 15:52:50 TP0] Decode batch [23703], #running-req: 4, #token: 13000, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-24 15:52:51 TP0] Decode batch [23743], #running-req: 4, #token: 13160, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-24 15:52:52 TP0] Decode batch [23783], #running-req: 4, #token: 13320, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-24 15:52:52 TP0] Decode batch [23823], #running-req: 4, #token: 13480, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-24 15:52:53 TP0] Decode batch [23863], #running-req: 4, #token: 13640, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-24 15:52:54 TP0] Decode batch [23903], #running-req: 4, #token: 13800, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-24 15:52:55 TP0] Decode batch [23943], #running-req: 4, #token: 13960, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-24 15:52:56 TP0] Decode batch [23983], #running-req: 4, #token: 14120, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:52:57 TP0] Decode batch [24023], #running-req: 4, #token: 14280, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:52:57 TP0] Decode batch [24063], #running-req: 4, #token: 14440, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:52:58 TP0] Decode batch [24103], #running-req: 4, #token: 14600, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:52:59 TP0] Decode batch [24143], #running-req: 4, #token: 14760, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:53:00 TP0] Decode batch [24183], #running-req: 4, #token: 14920, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:53:01 TP0] Decode batch [24223], #running-req: 4, #token: 15080, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:53:02 TP0] Decode batch [24263], #running-req: 4, #token: 15240, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:53:02 TP0] Decode batch [24303], #running-req: 4, #token: 15400, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:53:03 TP0] Decode batch [24343], #running-req: 4, #token: 15560, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:53:04 TP0] Decode batch [24383], #running-req: 4, #token: 15720, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:53:05 TP0] Decode batch [24423], #running-req: 4, #token: 15880, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-24 15:53:05] INFO:     127.0.0.1:35306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:05] INFO:     127.0.0.1:35308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:05] INFO:     127.0.0.1:35324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:05 TP0] Prefill batch [24453], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:53:05] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:06 TP0] Prefill batch [24454], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:53:06 TP0] Decode batch [24465], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.76, #queue-req: 0, 
[2025-10-24 15:53:07 TP0] Decode batch [24505], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:53:08 TP0] Decode batch [24545], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-24 15:53:08 TP0] Decode batch [24585], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:53:09 TP0] Decode batch [24625], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-24 15:53:10 TP0] Decode batch [24665], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-24 15:53:11 TP0] Decode batch [24705], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-24 15:53:12 TP0] Decode batch [24745], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-24 15:53:12 TP0] Decode batch [24785], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-24 15:53:13 TP0] Decode batch [24825], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-24 15:53:14 TP0] Decode batch [24865], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-24 15:53:15 TP0] Decode batch [24905], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-24 15:53:16 TP0] Decode batch [24945], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-24 15:53:17 TP0] Decode batch [24985], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.12, #queue-req: 0, 
[2025-10-24 15:53:17 TP0] Decode batch [25025], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.14, #queue-req: 0, 
[2025-10-24 15:53:18 TP0] Decode batch [25065], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.11, #queue-req: 0, 
[2025-10-24 15:53:19 TP0] Decode batch [25105], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.09, #queue-req: 0, 
[2025-10-24 15:53:20 TP0] Decode batch [25145], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.09, #queue-req: 0, 
[2025-10-24 15:53:21 TP0] Decode batch [25185], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.10, #queue-req: 0, 
[2025-10-24 15:53:22 TP0] Decode batch [25225], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.09, #queue-req: 0, 
[2025-10-24 15:53:22] INFO:     127.0.0.1:34706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:22] INFO:     127.0.0.1:34710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:22] INFO:     127.0.0.1:34724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:22 TP0] Prefill batch [25255], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:53:22] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:22 TP0] Prefill batch [25256], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:53:23 TP0] Decode batch [25267], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.71, #queue-req: 0, 
[2025-10-24 15:53:23 TP0] Decode batch [25307], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-10-24 15:53:24 TP0] Decode batch [25347], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-24 15:53:25 TP0] Decode batch [25387], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-24 15:53:26 TP0] Decode batch [25427], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-24 15:53:27 TP0] Decode batch [25467], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-24 15:53:28 TP0] Decode batch [25507], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-24 15:53:28 TP0] Decode batch [25547], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-24 15:53:29 TP0] Decode batch [25587], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-24 15:53:30 TP0] Decode batch [25627], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:53:31 TP0] Decode batch [25667], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:53:32 TP0] Decode batch [25707], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:53:33 TP0] Decode batch [25747], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:53:33 TP0] Decode batch [25787], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:53:34 TP0] Decode batch [25827], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:53:35 TP0] Decode batch [25867], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:53:36 TP0] Decode batch [25907], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:53:37 TP0] Decode batch [25947], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:53:37 TP0] Decode batch [25987], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-24 15:53:38 TP0] Decode batch [26027], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:53:39] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:39] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:39] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:39 TP0] Prefill batch [26057], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:53:39] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:39 TP0] Prefill batch [26058], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:53:39 TP0] Decode batch [26069], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.88, #queue-req: 0, 
[2025-10-24 15:53:40 TP0] Decode batch [26109], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-24 15:53:41 TP0] Decode batch [26149], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-24 15:53:42 TP0] Decode batch [26189], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-24 15:53:43 TP0] Decode batch [26229], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-24 15:53:43 TP0] Decode batch [26269], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:53:44 TP0] Decode batch [26309], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:53:45 TP0] Decode batch [26349], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:53:46 TP0] Decode batch [26389], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:53:47 TP0] Decode batch [26429], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:53:48 TP0] Decode batch [26469], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:53:48 TP0] Decode batch [26509], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-24 15:53:49 TP0] Decode batch [26549], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:53:50 TP0] Decode batch [26589], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:53:51 TP0] Decode batch [26629], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:53:52 TP0] Decode batch [26669], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:53:53 TP0] Decode batch [26709], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:53:53 TP0] Decode batch [26749], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:53:54 TP0] Decode batch [26789], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:53:55 TP0] Decode batch [26829], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:53:56] INFO:     127.0.0.1:37962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:56] INFO:     127.0.0.1:37964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:56] INFO:     127.0.0.1:37966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:56 TP0] Prefill batch [26859], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:53:56] INFO:     127.0.0.1:37972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:53:56 TP0] Prefill batch [26860], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:53:56 TP0] Decode batch [26871], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.65, #queue-req: 0, 
[2025-10-24 15:53:57 TP0] Decode batch [26911], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:53:58 TP0] Decode batch [26951], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-24 15:53:58 TP0] Decode batch [26991], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-24 15:53:59 TP0] Decode batch [27031], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-24 15:54:00 TP0] Decode batch [27071], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:54:01 TP0] Decode batch [27111], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:54:02 TP0] Decode batch [27151], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:54:03 TP0] Decode batch [27191], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:54:03 TP0] Decode batch [27231], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:54:04 TP0] Decode batch [27271], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:54:05 TP0] Decode batch [27311], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 15:54:06 TP0] Decode batch [27351], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:54:07 TP0] Decode batch [27391], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-24 15:54:08 TP0] Decode batch [27431], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:54:08 TP0] Decode batch [27471], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-24 15:54:09 TP0] Decode batch [27511], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-24 15:54:10 TP0] Decode batch [27551], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-24 15:54:11 TP0] Decode batch [27591], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-24 15:54:12 TP0] Decode batch [27631], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:54:12] INFO:     127.0.0.1:40192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:54:12] INFO:     127.0.0.1:40198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:54:12] INFO:     127.0.0.1:40208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:54:12 TP0] Prefill batch [27661], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:54:12] INFO:     127.0.0.1:40222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:54:12 TP0] Prefill batch [27662], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:54:13 TP0] Decode batch [27673], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.71, #queue-req: 0, 
[2025-10-24 15:54:14 TP0] Decode batch [27713], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-24 15:54:14 TP0] Decode batch [27753], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-10-24 15:54:15 TP0] Decode batch [27793], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-24 15:54:16 TP0] Decode batch [27833], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-24 15:54:17 TP0] Decode batch [27873], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-24 15:54:18 TP0] Decode batch [27913], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-24 15:54:19 TP0] Decode batch [27953], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-24 15:54:19 TP0] Decode batch [27993], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-24 15:54:20 TP0] Decode batch [28033], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-24 15:54:21 TP0] Decode batch [28073], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:54:22 TP0] Decode batch [28113], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:54:23 TP0] Decode batch [28153], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-24 15:54:23 TP0] Decode batch [28193], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:54:24 TP0] Decode batch [28233], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:54:25 TP0] Decode batch [28273], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:54:26 TP0] Decode batch [28313], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:54:27 TP0] Decode batch [28353], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:54:28 TP0] Decode batch [28393], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 187.82, #queue-req: 0, 
[2025-10-24 15:54:28 TP0] Decode batch [28433], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:54:29] INFO:     127.0.0.1:35258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:54:29] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:54:29] INFO:     127.0.0.1:35278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:54:29 TP0] Prefill batch [28463], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:54:29] INFO:     127.0.0.1:35286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:54:29 TP0] Prefill batch [28464], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:54:29 TP0] Decode batch [28475], #running-req: 4, #token: 12841, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.84, #queue-req: 0, 
[2025-10-24 15:54:30 TP0] Decode batch [28515], #running-req: 4, #token: 13001, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:54:31 TP0] Decode batch [28555], #running-req: 4, #token: 13161, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-24 15:54:32 TP0] Decode batch [28595], #running-req: 4, #token: 13321, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 15:54:33 TP0] Decode batch [28635], #running-req: 4, #token: 13481, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-24 15:54:34 TP0] Decode batch [28675], #running-req: 4, #token: 13641, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-24 15:54:34 TP0] Decode batch [28715], #running-req: 4, #token: 13801, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-24 15:54:35 TP0] Decode batch [28755], #running-req: 4, #token: 13961, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-24 15:54:36 TP0] Decode batch [28795], #running-req: 4, #token: 14121, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-24 15:54:37 TP0] Decode batch [28835], #running-req: 4, #token: 14281, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:54:38 TP0] Decode batch [28875], #running-req: 4, #token: 14441, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:54:39 TP0] Decode batch [28915], #running-req: 4, #token: 14601, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-24 15:54:39 TP0] Decode batch [28955], #running-req: 4, #token: 14761, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-24 15:54:40 TP0] Decode batch [28995], #running-req: 4, #token: 14921, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-24 15:54:41 TP0] Decode batch [29035], #running-req: 4, #token: 15081, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:54:42 TP0] Decode batch [29075], #running-req: 4, #token: 15241, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-24 15:54:43 TP0] Decode batch [29115], #running-req: 4, #token: 15401, token usage: 0.02, cuda graph: True, gen throughput (token/s): 192.07, #queue-req: 0, 
[2025-10-24 15:54:44 TP0] Decode batch [29155], #running-req: 4, #token: 15561, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.14, #queue-req: 0, 
[2025-10-24 15:54:44 TP0] Decode batch [29195], #running-req: 4, #token: 15721, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-24 15:54:45 TP0] Decode batch [29235], #running-req: 4, #token: 15881, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-24 15:54:46] INFO:     127.0.0.1:54756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:54:46] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:54:46] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:54:46 TP0] Prefill batch [29265], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:54:46] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:54:46 TP0] Prefill batch [29266], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:54:46 TP0] Decode batch [29277], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.42, #queue-req: 0, 
[2025-10-24 15:54:47 TP0] Decode batch [29317], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-24 15:54:48 TP0] Decode batch [29357], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:54:49 TP0] Decode batch [29397], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:54:50 TP0] Decode batch [29437], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-24 15:54:50 TP0] Decode batch [29477], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:54:51 TP0] Decode batch [29517], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-24 15:54:52 TP0] Decode batch [29557], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-24 15:54:53 TP0] Decode batch [29597], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-24 15:54:54 TP0] Decode batch [29637], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.12, #queue-req: 0, 
[2025-10-24 15:54:54 TP0] Decode batch [29677], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-24 15:54:55 TP0] Decode batch [29717], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-24 15:54:56 TP0] Decode batch [29757], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-24 15:54:57 TP0] Decode batch [29797], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.12, #queue-req: 0, 
[2025-10-24 15:54:58 TP0] Decode batch [29837], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-24 15:54:59 TP0] Decode batch [29877], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.10, #queue-req: 0, 
[2025-10-24 15:54:59 TP0] Decode batch [29917], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.12, #queue-req: 0, 
[2025-10-24 15:55:00 TP0] Decode batch [29957], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-24 15:55:01 TP0] Decode batch [29997], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.11, #queue-req: 0, 
[2025-10-24 15:55:02 TP0] Decode batch [30037], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.11, #queue-req: 0, 
[2025-10-24 15:55:03] INFO:     127.0.0.1:43544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:03] INFO:     127.0.0.1:43546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:03 TP0] Prefill batch [30067], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:55:03] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:03] INFO:     127.0.0.1:43566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:03 TP0] Prefill batch [30068], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:55:03 TP0] Decode batch [30079], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.71, #queue-req: 0, 
[2025-10-24 15:55:04 TP0] Decode batch [30119], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:55:05 TP0] Decode batch [30159], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-24 15:55:05 TP0] Decode batch [30199], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:55:06 TP0] Decode batch [30239], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-24 15:55:07 TP0] Decode batch [30279], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-24 15:55:08 TP0] Decode batch [30319], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-24 15:55:09 TP0] Decode batch [30359], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-24 15:55:10 TP0] Decode batch [30399], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-24 15:55:10 TP0] Decode batch [30439], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:55:11 TP0] Decode batch [30479], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-24 15:55:12 TP0] Decode batch [30519], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:55:13 TP0] Decode batch [30559], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:55:14 TP0] Decode batch [30599], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:55:15 TP0] Decode batch [30639], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:55:15 TP0] Decode batch [30679], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:55:16 TP0] Decode batch [30719], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-24 15:55:17 TP0] Decode batch [30759], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-24 15:55:18 TP0] Decode batch [30799], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-24 15:55:19 TP0] Decode batch [30839], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-24 15:55:19] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:19] INFO:     127.0.0.1:51542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:19] INFO:     127.0.0.1:51554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:19 TP0] Prefill batch [30869], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:55:19] INFO:     127.0.0.1:51564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:19 TP0] Prefill batch [30870], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:55:20 TP0] Decode batch [30881], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.96, #queue-req: 0, 
[2025-10-24 15:55:20 TP0] Decode batch [30921], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-24 15:55:21 TP0] Decode batch [30961], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 15:55:22 TP0] Decode batch [31001], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-24 15:55:23 TP0] Decode batch [31041], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:55:24 TP0] Decode batch [31081], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:55:25 TP0] Decode batch [31121], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-24 15:55:25 TP0] Decode batch [31161], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-24 15:55:26 TP0] Decode batch [31201], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-24 15:55:27 TP0] Decode batch [31241], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-24 15:55:28 TP0] Decode batch [31281], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:55:29 TP0] Decode batch [31321], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-24 15:55:30 TP0] Decode batch [31361], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:55:30 TP0] Decode batch [31401], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:55:31 TP0] Decode batch [31441], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-24 15:55:32 TP0] Decode batch [31481], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-24 15:55:33 TP0] Decode batch [31521], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-24 15:55:34 TP0] Decode batch [31561], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:55:35 TP0] Decode batch [31601], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-24 15:55:35 TP0] Decode batch [31641], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-24 15:55:36] INFO:     127.0.0.1:40710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:36] INFO:     127.0.0.1:40724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:36] INFO:     127.0.0.1:40732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:36 TP0] Prefill batch [31671], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:55:36] INFO:     127.0.0.1:40746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:36 TP0] Prefill batch [31672], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:55:36 TP0] Decode batch [31683], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.44, #queue-req: 0, 
[2025-10-24 15:55:37 TP0] Decode batch [31723], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-24 15:55:38 TP0] Decode batch [31763], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-24 15:55:39 TP0] Decode batch [31803], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-24 15:55:40 TP0] Decode batch [31843], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:55:41 TP0] Decode batch [31883], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:55:41 TP0] Decode batch [31923], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:55:42 TP0] Decode batch [31963], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-24 15:55:43 TP0] Decode batch [32003], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-24 15:55:44 TP0] Decode batch [32043], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-24 15:55:45 TP0] Decode batch [32083], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-24 15:55:45 TP0] Decode batch [32123], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-24 15:55:46 TP0] Decode batch [32163], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-24 15:55:47 TP0] Decode batch [32203], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-24 15:55:48 TP0] Decode batch [32243], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 15:55:49 TP0] Decode batch [32283], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 15:55:50 TP0] Decode batch [32323], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 15:55:50 TP0] Decode batch [32363], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:55:51 TP0] Decode batch [32403], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:55:52 TP0] Decode batch [32443], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-24 15:55:53] INFO:     127.0.0.1:38778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:53] INFO:     127.0.0.1:38780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:53 TP0] Prefill batch [32473], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:55:53] INFO:     127.0.0.1:38794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:53] INFO:     127.0.0.1:38808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:55:53 TP0] Prefill batch [32474], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:55:53 TP0] Decode batch [32485], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.76, #queue-req: 0, 
[2025-10-24 15:55:54 TP0] Decode batch [32525], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-24 15:55:55 TP0] Decode batch [32565], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-24 15:55:56 TP0] Decode batch [32605], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-24 15:55:56 TP0] Decode batch [32645], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-24 15:55:57 TP0] Decode batch [32685], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-24 15:55:58 TP0] Decode batch [32725], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:55:59 TP0] Decode batch [32765], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:56:00 TP0] Decode batch [32805], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:56:01 TP0] Decode batch [32845], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-24 15:56:01 TP0] Decode batch [32885], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:56:02 TP0] Decode batch [32925], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:56:03 TP0] Decode batch [32965], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:56:04 TP0] Decode batch [33005], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:56:05 TP0] Decode batch [33045], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-24 15:56:06 TP0] Decode batch [33085], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:56:06 TP0] Decode batch [33125], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:56:07 TP0] Decode batch [33165], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:56:08 TP0] Decode batch [33205], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:56:09 TP0] Decode batch [33245], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:56:09] INFO:     127.0.0.1:45620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:56:09] INFO:     127.0.0.1:45626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:56:09] INFO:     127.0.0.1:45636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:56:09 TP0] Prefill batch [33275], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:56:09] INFO:     127.0.0.1:45640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:56:10 TP0] Prefill batch [33276], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:56:10 TP0] Decode batch [33287], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.74, #queue-req: 0, 
[2025-10-24 15:56:11 TP0] Decode batch [33327], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:56:11 TP0] Decode batch [33367], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:56:12 TP0] Decode batch [33407], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:56:13 TP0] Decode batch [33447], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:56:14 TP0] Decode batch [33487], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:56:15 TP0] Decode batch [33527], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-24 15:56:16 TP0] Decode batch [33567], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:56:16 TP0] Decode batch [33607], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-24 15:56:17 TP0] Decode batch [33647], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-24 15:56:18 TP0] Decode batch [33687], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:56:19 TP0] Decode batch [33727], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-24 15:56:20 TP0] Decode batch [33767], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-24 15:56:21 TP0] Decode batch [33807], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:56:21 TP0] Decode batch [33847], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:56:22 TP0] Decode batch [33887], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-24 15:56:23 TP0] Decode batch [33927], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-24 15:56:24 TP0] Decode batch [33967], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-24 15:56:25 TP0] Decode batch [34007], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-24 15:56:26 TP0] Decode batch [34047], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-24 15:56:26] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:56:26] INFO:     127.0.0.1:49702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:56:26 TP0] Prefill batch [34077], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:56:26] INFO:     127.0.0.1:49706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:56:26] INFO:     127.0.0.1:49720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:56:26 TP0] Prefill batch [34078], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:56:27 TP0] Decode batch [34089], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.74, #queue-req: 0, 
[2025-10-24 15:56:27 TP0] Decode batch [34129], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:56:28 TP0] Decode batch [34169], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:56:29 TP0] Decode batch [34209], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:56:30 TP0] Decode batch [34249], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.09, #queue-req: 0, 
[2025-10-24 15:56:31 TP0] Decode batch [34289], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:56:31 TP0] Decode batch [34329], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-24 15:56:32 TP0] Decode batch [34369], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-24 15:56:33 TP0] Decode batch [34409], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.12, #queue-req: 0, 
[2025-10-24 15:56:34 TP0] Decode batch [34449], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-24 15:56:35 TP0] Decode batch [34489], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.08, #queue-req: 0, 
[2025-10-24 15:56:36 TP0] Decode batch [34529], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.10, #queue-req: 0, 
[2025-10-24 15:56:36 TP0] Decode batch [34569], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.07, #queue-req: 0, 
[2025-10-24 15:56:37 TP0] Decode batch [34609], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.09, #queue-req: 0, 
[2025-10-24 15:56:38 TP0] Decode batch [34649], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.02, #queue-req: 0, 
[2025-10-24 15:56:39 TP0] Decode batch [34689], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.01, #queue-req: 0, 
[2025-10-24 15:56:40 TP0] Decode batch [34729], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.02, #queue-req: 0, 
[2025-10-24 15:56:41 TP0] Decode batch [34769], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 192.98, #queue-req: 0, 
[2025-10-24 15:56:41 TP0] Decode batch [34809], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 192.99, #queue-req: 0, 
[2025-10-24 15:56:42 TP0] Decode batch [34849], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 192.98, #queue-req: 0, 
[2025-10-24 15:56:43] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:56:43] INFO:     127.0.0.1:40918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:56:43] INFO:     127.0.0.1:40932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:56:43 TP0] Prefill batch [34879], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:56:43] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:56:43 TP0] Prefill batch [34880], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:56:43 TP0] Decode batch [34891], #running-req: 4, #token: 12841, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.53, #queue-req: 0, 
[2025-10-24 15:56:44 TP0] Decode batch [34931], #running-req: 4, #token: 13001, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:56:45 TP0] Decode batch [34971], #running-req: 4, #token: 13161, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:56:46 TP0] Decode batch [35011], #running-req: 4, #token: 13321, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:56:47 TP0] Decode batch [35051], #running-req: 4, #token: 13481, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-24 15:56:47 TP0] Decode batch [35091], #running-req: 4, #token: 13641, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:56:48 TP0] Decode batch [35131], #running-req: 4, #token: 13801, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:56:49 TP0] Decode batch [35171], #running-req: 4, #token: 13961, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-24 15:56:50 TP0] Decode batch [35211], #running-req: 4, #token: 14121, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 15:56:51 TP0] Decode batch [35251], #running-req: 4, #token: 14281, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:56:52 TP0] Decode batch [35291], #running-req: 4, #token: 14441, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 15:56:52 TP0] Decode batch [35331], #running-req: 4, #token: 14601, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-24 15:56:53 TP0] Decode batch [35371], #running-req: 4, #token: 14761, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:56:54 TP0] Decode batch [35411], #running-req: 4, #token: 14921, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-24 15:56:55 TP0] Decode batch [35451], #running-req: 4, #token: 15081, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:56:56 TP0] Decode batch [35491], #running-req: 4, #token: 15241, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-24 15:56:57 TP0] Decode batch [35531], #running-req: 4, #token: 15401, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-24 15:56:57 TP0] Decode batch [35571], #running-req: 4, #token: 15561, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 15:56:58 TP0] Decode batch [35611], #running-req: 4, #token: 15721, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-24 15:56:59 TP0] Decode batch [35651], #running-req: 4, #token: 15881, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:57:00] INFO:     127.0.0.1:60276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:00] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:00 TP0] Prefill batch [35681], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:57:00] INFO:     127.0.0.1:60302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:00] INFO:     127.0.0.1:60308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:00 TP0] Prefill batch [35682], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:57:00 TP0] Decode batch [35693], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.83, #queue-req: 0, 
[2025-10-24 15:57:01 TP0] Decode batch [35733], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-24 15:57:02 TP0] Decode batch [35773], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-24 15:57:02 TP0] Decode batch [35813], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-24 15:57:03 TP0] Decode batch [35853], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-24 15:57:04 TP0] Decode batch [35893], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-24 15:57:05 TP0] Decode batch [35933], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-24 15:57:06 TP0] Decode batch [35973], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-24 15:57:07 TP0] Decode batch [36013], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-24 15:57:07 TP0] Decode batch [36053], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-24 15:57:08 TP0] Decode batch [36093], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-24 15:57:09 TP0] Decode batch [36133], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-24 15:57:10 TP0] Decode batch [36173], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:57:11 TP0] Decode batch [36213], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:57:12 TP0] Decode batch [36253], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-24 15:57:12 TP0] Decode batch [36293], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:57:13 TP0] Decode batch [36333], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-24 15:57:14 TP0] Decode batch [36373], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:57:15 TP0] Decode batch [36413], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:57:16 TP0] Decode batch [36453], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:57:16] INFO:     127.0.0.1:54784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:16] INFO:     127.0.0.1:54790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:16] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:16 TP0] Prefill batch [36483], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:57:16] INFO:     127.0.0.1:54806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:16 TP0] Prefill batch [36484], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:57:17 TP0] Decode batch [36495], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.79, #queue-req: 0, 
[2025-10-24 15:57:18 TP0] Decode batch [36535], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-24 15:57:18 TP0] Decode batch [36575], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-24 15:57:19 TP0] Decode batch [36615], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:57:20 TP0] Decode batch [36655], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-24 15:57:21 TP0] Decode batch [36695], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-24 15:57:22 TP0] Decode batch [36735], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 191.22, #queue-req: 0, 
[2025-10-24 15:57:22 TP0] Decode batch [36775], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:57:23 TP0] Decode batch [36815], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:57:24 TP0] Decode batch [36855], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-24 15:57:25 TP0] Decode batch [36895], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-24 15:57:26 TP0] Decode batch [36935], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:57:27 TP0] Decode batch [36975], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-24 15:57:27 TP0] Decode batch [37015], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-24 15:57:28 TP0] Decode batch [37055], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-24 15:57:29 TP0] Decode batch [37095], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-24 15:57:30 TP0] Decode batch [37135], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-24 15:57:31 TP0] Decode batch [37175], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-24 15:57:32 TP0] Decode batch [37215], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 191.10, #queue-req: 0, 
[2025-10-24 15:57:32 TP0] Decode batch [37255], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-24 15:57:33] INFO:     127.0.0.1:37598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:33] INFO:     127.0.0.1:37602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:33] INFO:     127.0.0.1:37612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:33 TP0] Prefill batch [37285], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:57:33] INFO:     127.0.0.1:37616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:33 TP0] Prefill batch [37286], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:57:33 TP0] Decode batch [37297], #running-req: 4, #token: 12841, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.51, #queue-req: 0, 
[2025-10-24 15:57:34 TP0] Decode batch [37337], #running-req: 4, #token: 13001, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:57:35 TP0] Decode batch [37377], #running-req: 4, #token: 13161, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-24 15:57:36 TP0] Decode batch [37417], #running-req: 4, #token: 13321, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-24 15:57:37 TP0] Decode batch [37457], #running-req: 4, #token: 13481, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:57:38 TP0] Decode batch [37497], #running-req: 4, #token: 13641, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:57:38 TP0] Decode batch [37537], #running-req: 4, #token: 13801, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-24 15:57:39 TP0] Decode batch [37577], #running-req: 4, #token: 13961, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:57:40 TP0] Decode batch [37617], #running-req: 4, #token: 14121, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-24 15:57:41 TP0] Decode batch [37657], #running-req: 4, #token: 14281, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-24 15:57:42 TP0] Decode batch [37697], #running-req: 4, #token: 14441, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-24 15:57:43 TP0] Decode batch [37737], #running-req: 4, #token: 14601, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-24 15:57:43 TP0] Decode batch [37777], #running-req: 4, #token: 14761, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-24 15:57:44 TP0] Decode batch [37817], #running-req: 4, #token: 14921, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:57:45 TP0] Decode batch [37857], #running-req: 4, #token: 15081, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:57:46 TP0] Decode batch [37897], #running-req: 4, #token: 15241, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-24 15:57:47 TP0] Decode batch [37937], #running-req: 4, #token: 15401, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-24 15:57:48 TP0] Decode batch [37977], #running-req: 4, #token: 15561, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-24 15:57:48 TP0] Decode batch [38017], #running-req: 4, #token: 15721, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-24 15:57:49 TP0] Decode batch [38057], #running-req: 4, #token: 15881, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-24 15:57:50] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:50] INFO:     127.0.0.1:36438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:50] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:50 TP0] Prefill batch [38087], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:57:50] INFO:     127.0.0.1:36444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:57:50 TP0] Prefill batch [38088], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-24 15:57:50 TP0] Decode batch [38099], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.08, #queue-req: 0, 
[2025-10-24 15:57:51 TP0] Decode batch [38139], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-24 15:57:52 TP0] Decode batch [38179], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-24 15:57:53 TP0] Decode batch [38219], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-24 15:57:53 TP0] Decode batch [38259], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:57:54 TP0] Decode batch [38299], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:57:55 TP0] Decode batch [38339], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:57:56 TP0] Decode batch [38379], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:57:57 TP0] Decode batch [38419], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:57:58 TP0] Decode batch [38459], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-24 15:57:58 TP0] Decode batch [38499], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-24 15:57:59 TP0] Decode batch [38539], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-24 15:58:00 TP0] Decode batch [38579], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-24 15:58:01 TP0] Decode batch [38619], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-24 15:58:02 TP0] Decode batch [38659], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.14, #queue-req: 0, 
[2025-10-24 15:58:03 TP0] Decode batch [38699], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.14, #queue-req: 0, 
[2025-10-24 15:58:03 TP0] Decode batch [38739], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.14, #queue-req: 0, 
[2025-10-24 15:58:04 TP0] Decode batch [38779], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-24 15:58:05 TP0] Decode batch [38819], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.11, #queue-req: 0, 
[2025-10-24 15:58:06 TP0] Decode batch [38859], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.14, #queue-req: 0, 
[2025-10-24 15:58:06] INFO:     127.0.0.1:47976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:07] INFO:     127.0.0.1:47982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:07] INFO:     127.0.0.1:47998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:07 TP0] Prefill batch [38889], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:58:07] INFO:     127.0.0.1:48004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:07 TP0] Prefill batch [38890], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:58:07 TP0] Decode batch [38901], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.11, #queue-req: 0, 
[2025-10-24 15:58:08 TP0] Decode batch [38941], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-24 15:58:09 TP0] Decode batch [38981], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 15:58:09 TP0] Decode batch [39021], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:58:10 TP0] Decode batch [39061], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:58:11 TP0] Decode batch [39101], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:58:12 TP0] Decode batch [39141], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-24 15:58:13 TP0] Decode batch [39181], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:58:14 TP0] Decode batch [39221], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-24 15:58:14 TP0] Decode batch [39261], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:58:15 TP0] Decode batch [39301], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-24 15:58:16 TP0] Decode batch [39341], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-24 15:58:17 TP0] Decode batch [39381], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-24 15:58:18 TP0] Decode batch [39421], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-24 15:58:18 TP0] Decode batch [39461], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:58:19 TP0] Decode batch [39501], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:58:20 TP0] Decode batch [39541], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:58:21 TP0] Decode batch [39581], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-24 15:58:22 TP0] Decode batch [39621], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:58:23 TP0] Decode batch [39661], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:58:23] INFO:     127.0.0.1:35244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:23] INFO:     127.0.0.1:35256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:23] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:23 TP0] Prefill batch [39691], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:58:23] INFO:     127.0.0.1:35280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:23 TP0] Prefill batch [39692], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:58:24 TP0] Decode batch [39703], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.83, #queue-req: 0, 
[2025-10-24 15:58:24 TP0] Decode batch [39743], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:58:25 TP0] Decode batch [39783], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:58:26 TP0] Decode batch [39823], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-24 15:58:27 TP0] Decode batch [39863], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-24 15:58:28 TP0] Decode batch [39903], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-24 15:58:29 TP0] Decode batch [39943], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-24 15:58:29 TP0] Decode batch [39983], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-24 15:58:30 TP0] Decode batch [40023], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 15:58:31 TP0] Decode batch [40063], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:58:32 TP0] Decode batch [40103], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:58:33 TP0] Decode batch [40143], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:58:34 TP0] Decode batch [40183], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:58:34 TP0] Decode batch [40223], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-24 15:58:35 TP0] Decode batch [40263], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:58:36 TP0] Decode batch [40303], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-24 15:58:37 TP0] Decode batch [40343], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-24 15:58:38 TP0] Decode batch [40383], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:58:39 TP0] Decode batch [40423], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-24 15:58:39 TP0] Decode batch [40463], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-24 15:58:40] INFO:     127.0.0.1:39370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:40] INFO:     127.0.0.1:39372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:40] INFO:     127.0.0.1:39386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:40 TP0] Prefill batch [40493], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:58:40] INFO:     127.0.0.1:39400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:40 TP0] Prefill batch [40494], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:58:40 TP0] Decode batch [40505], #running-req: 4, #token: 12841, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.78, #queue-req: 0, 
[2025-10-24 15:58:41 TP0] Decode batch [40545], #running-req: 4, #token: 13001, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-24 15:58:42 TP0] Decode batch [40585], #running-req: 4, #token: 13161, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-24 15:58:43 TP0] Decode batch [40625], #running-req: 4, #token: 13321, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:58:44 TP0] Decode batch [40665], #running-req: 4, #token: 13481, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:58:44 TP0] Decode batch [40705], #running-req: 4, #token: 13641, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-24 15:58:45 TP0] Decode batch [40745], #running-req: 4, #token: 13801, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-24 15:58:46 TP0] Decode batch [40785], #running-req: 4, #token: 13961, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 15:58:47 TP0] Decode batch [40825], #running-req: 4, #token: 14121, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-24 15:58:48 TP0] Decode batch [40865], #running-req: 4, #token: 14281, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:58:49 TP0] Decode batch [40905], #running-req: 4, #token: 14441, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:58:49 TP0] Decode batch [40945], #running-req: 4, #token: 14601, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-24 15:58:50 TP0] Decode batch [40985], #running-req: 4, #token: 14761, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:58:51 TP0] Decode batch [41025], #running-req: 4, #token: 14921, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-24 15:58:52 TP0] Decode batch [41065], #running-req: 4, #token: 15081, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:58:53 TP0] Decode batch [41105], #running-req: 4, #token: 15241, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-24 15:58:54 TP0] Decode batch [41145], #running-req: 4, #token: 15401, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-24 15:58:54 TP0] Decode batch [41185], #running-req: 4, #token: 15561, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:58:55 TP0] Decode batch [41225], #running-req: 4, #token: 15721, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-24 15:58:56 TP0] Decode batch [41265], #running-req: 4, #token: 15881, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-24 15:58:57] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:57] INFO:     127.0.0.1:50402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:57 TP0] Prefill batch [41295], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:58:57] INFO:     127.0.0.1:50408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:57] INFO:     127.0.0.1:50416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:58:57 TP0] Prefill batch [41296], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:58:57 TP0] Decode batch [41307], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.83, #queue-req: 0, 
[2025-10-24 15:58:58 TP0] Decode batch [41347], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-24 15:58:59 TP0] Decode batch [41387], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:59:00 TP0] Decode batch [41427], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:59:00 TP0] Decode batch [41467], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:59:01 TP0] Decode batch [41507], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 15:59:02 TP0] Decode batch [41547], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:59:03 TP0] Decode batch [41587], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:59:04 TP0] Decode batch [41627], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-24 15:59:05 TP0] Decode batch [41667], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:59:05 TP0] Decode batch [41707], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:59:06 TP0] Decode batch [41747], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-24 15:59:07 TP0] Decode batch [41787], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-24 15:59:08 TP0] Decode batch [41827], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-24 15:59:09 TP0] Decode batch [41867], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-24 15:59:09 TP0] Decode batch [41907], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-24 15:59:10 TP0] Decode batch [41947], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-24 15:59:11 TP0] Decode batch [41987], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-24 15:59:12 TP0] Decode batch [42027], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-24 15:59:13 TP0] Decode batch [42067], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-24 15:59:13] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:59:13] INFO:     127.0.0.1:59348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:59:13 TP0] Prefill batch [42097], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:59:13] INFO:     127.0.0.1:59362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:59:13] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:59:13 TP0] Prefill batch [42098], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:59:14 TP0] Decode batch [42109], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.77, #queue-req: 0, 
[2025-10-24 15:59:15 TP0] Decode batch [42149], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-24 15:59:15 TP0] Decode batch [42189], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-24 15:59:16 TP0] Decode batch [42229], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-24 15:59:17 TP0] Decode batch [42269], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-24 15:59:18 TP0] Decode batch [42309], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-24 15:59:19 TP0] Decode batch [42349], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-24 15:59:20 TP0] Decode batch [42389], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-24 15:59:20 TP0] Decode batch [42429], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-24 15:59:21 TP0] Decode batch [42469], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-24 15:59:22 TP0] Decode batch [42509], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-24 15:59:23 TP0] Decode batch [42549], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-24 15:59:24 TP0] Decode batch [42589], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-24 15:59:25 TP0] Decode batch [42629], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-24 15:59:25 TP0] Decode batch [42669], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-24 15:59:26 TP0] Decode batch [42709], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-24 15:59:27 TP0] Decode batch [42749], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-24 15:59:28 TP0] Decode batch [42789], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-24 15:59:29 TP0] Decode batch [42829], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-24 15:59:30 TP0] Decode batch [42869], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-24 15:59:30] INFO:     127.0.0.1:42540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:59:30] INFO:     127.0.0.1:42548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:59:30 TP0] Prefill batch [42899], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:59:30] INFO:     127.0.0.1:42558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:59:30] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:59:30 TP0] Prefill batch [42900], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:59:31 TP0] Decode batch [42911], #running-req: 4, #token: 12842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.83, #queue-req: 0, 
[2025-10-24 15:59:31 TP0] Decode batch [42951], #running-req: 4, #token: 13002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-24 15:59:32 TP0] Decode batch [42991], #running-req: 4, #token: 13162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-24 15:59:33 TP0] Decode batch [43031], #running-req: 4, #token: 13322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-24 15:59:34 TP0] Decode batch [43071], #running-req: 4, #token: 13482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-24 15:59:35 TP0] Decode batch [43111], #running-req: 4, #token: 13642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-24 15:59:35 TP0] Decode batch [43151], #running-req: 4, #token: 13802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:59:36 TP0] Decode batch [43191], #running-req: 4, #token: 13962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-24 15:59:37 TP0] Decode batch [43231], #running-req: 4, #token: 14122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-24 15:59:38 TP0] Decode batch [43271], #running-req: 4, #token: 14282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-24 15:59:39 TP0] Decode batch [43311], #running-req: 4, #token: 14442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 15:59:40 TP0] Decode batch [43351], #running-req: 4, #token: 14602, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-24 15:59:40 TP0] Decode batch [43391], #running-req: 4, #token: 14762, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-24 15:59:41 TP0] Decode batch [43431], #running-req: 4, #token: 14922, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:59:42 TP0] Decode batch [43471], #running-req: 4, #token: 15082, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-24 15:59:43 TP0] Decode batch [43511], #running-req: 4, #token: 15242, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 15:59:44 TP0] Decode batch [43551], #running-req: 4, #token: 15402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-24 15:59:45 TP0] Decode batch [43591], #running-req: 4, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-24 15:59:45 TP0] Decode batch [43631], #running-req: 4, #token: 15722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-24 15:59:46 TP0] Decode batch [43671], #running-req: 4, #token: 15882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-24 15:59:47] INFO:     127.0.0.1:49576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:59:47] INFO:     127.0.0.1:49582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:59:47 TP0] Prefill batch [43701], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 15:59:47] INFO:     127.0.0.1:49594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:59:47] INFO:     127.0.0.1:49608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 15:59:47 TP0] Prefill batch [43702], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-24 15:59:47 TP0] Decode batch [43713], #running-req: 4, #token: 12840, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.76, #queue-req: 0, 
[2025-10-24 15:59:48 TP0] Decode batch [43753], #running-req: 4, #token: 13000, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-24 15:59:49 TP0] Decode batch [43793], #running-req: 4, #token: 13160, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-24 15:59:50 TP0] Decode batch [43833], #running-req: 4, #token: 13320, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-24 15:59:51 TP0] Decode batch [43873], #running-req: 4, #token: 13480, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-24 15:59:51 TP0] Decode batch [43913], #running-req: 4, #token: 13640, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-24 15:59:52 TP0] Decode batch [43953], #running-req: 4, #token: 13800, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-24 15:59:53 TP0] Decode batch [43993], #running-req: 4, #token: 13960, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-24 15:59:54 TP0] Decode batch [44033], #running-req: 4, #token: 14120, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:59:55 TP0] Decode batch [44073], #running-req: 4, #token: 14280, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-24 15:59:55 TP0] Decode batch [44113], #running-req: 4, #token: 14440, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-24 15:59:56 TP0] Decode batch [44153], #running-req: 4, #token: 14600, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-24 15:59:57 TP0] Decode batch [44193], #running-req: 4, #token: 14760, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-24 15:59:58 TP0] Decode batch [44233], #running-req: 4, #token: 14920, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-24 15:59:59 TP0] Decode batch [44273], #running-req: 4, #token: 15080, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-24 16:00:00 TP0] Decode batch [44313], #running-req: 4, #token: 15240, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-24 16:00:00 TP0] Decode batch [44353], #running-req: 4, #token: 15400, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-24 16:00:01 TP0] Decode batch [44393], #running-req: 4, #token: 15560, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 16:00:02 TP0] Decode batch [44433], #running-req: 4, #token: 15720, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 16:00:03 TP0] Decode batch [44473], #running-req: 4, #token: 15880, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-24 16:00:04] INFO:     127.0.0.1:57198 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-24 16:00:21] INFO:     127.0.0.1:57848 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-24 16:00:26] INFO:     127.0.0.1:60774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:00:26 TP0] Prefill batch [44503], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:00:27 TP0] Decode batch [44514], #running-req: 1, #token: 3212, token usage: 0.00, cuda graph: True, gen throughput (token/s): 5.48, #queue-req: 0, 
[2025-10-24 16:00:28] INFO:     127.0.0.1:60778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:00:28 TP0] Prefill batch [44536], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:00:29 TP0] Decode batch [44555], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.07, #queue-req: 0, 
[2025-10-24 16:00:29 TP0] Decode batch [44595], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:00:30 TP0] Decode batch [44635], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:00:31 TP0] Decode batch [44675], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:00:32 TP0] Decode batch [44715], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:00:33 TP0] Decode batch [44755], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:00:33 TP0] Decode batch [44795], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:00:34 TP0] Decode batch [44835], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:00:35 TP0] Decode batch [44875], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:00:36 TP0] Decode batch [44915], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:00:37 TP0] Decode batch [44955], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:00:38 TP0] Decode batch [44995], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:00:38 TP0] Decode batch [45035], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:00:39 TP0] Decode batch [45075], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:00:40 TP0] Decode batch [45115], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:00:41 TP0] Decode batch [45155], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:00:42 TP0] Decode batch [45195], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:00:43 TP0] Decode batch [45235], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:00:43 TP0] Decode batch [45275], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:00:44 TP0] Decode batch [45315], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:00:45] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:00:45 TP0] Prefill batch [45337], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:00:45 TP0] Decode batch [45356], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.11, #queue-req: 0, 
[2025-10-24 16:00:46 TP0] Decode batch [45396], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:00:47 TP0] Decode batch [45436], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:00:48 TP0] Decode batch [45476], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:00:48 TP0] Decode batch [45516], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:00:49 TP0] Decode batch [45556], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:00:50 TP0] Decode batch [45596], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:00:51 TP0] Decode batch [45636], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:00:52 TP0] Decode batch [45676], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:00:52 TP0] Decode batch [45716], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:00:53 TP0] Decode batch [45756], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:00:54 TP0] Decode batch [45796], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:00:55 TP0] Decode batch [45836], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:00:56 TP0] Decode batch [45876], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:00:57 TP0] Decode batch [45916], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:00:57 TP0] Decode batch [45956], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:00:58 TP0] Decode batch [45996], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:00:59 TP0] Decode batch [46036], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:01:00 TP0] Decode batch [46076], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:01:01 TP0] Decode batch [46116], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:01:01] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:01:01 TP0] Prefill batch [46138], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:01:02 TP0] Decode batch [46157], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-10-24 16:01:02 TP0] Decode batch [46197], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-24 16:01:03 TP0] Decode batch [46237], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-24 16:01:04 TP0] Decode batch [46277], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:01:05 TP0] Decode batch [46317], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:01:06 TP0] Decode batch [46357], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:01:06 TP0] Decode batch [46397], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:01:07 TP0] Decode batch [46437], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:01:08 TP0] Decode batch [46477], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:01:09 TP0] Decode batch [46517], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:01:10 TP0] Decode batch [46557], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:01:11 TP0] Decode batch [46597], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:01:11 TP0] Decode batch [46637], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:01:12 TP0] Decode batch [46677], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:01:13 TP0] Decode batch [46717], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:01:14 TP0] Decode batch [46757], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:01:15 TP0] Decode batch [46797], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:01:15 TP0] Decode batch [46837], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:01:16 TP0] Decode batch [46877], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:01:17 TP0] Decode batch [46917], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:01:18] INFO:     127.0.0.1:43582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:01:18 TP0] Prefill batch [46939], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:01:18 TP0] Decode batch [46958], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:01:19 TP0] Decode batch [46998], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:01:20 TP0] Decode batch [47038], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:01:20 TP0] Decode batch [47078], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:01:21 TP0] Decode batch [47118], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:01:22 TP0] Decode batch [47158], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:01:23 TP0] Decode batch [47198], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:01:24 TP0] Decode batch [47238], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:01:25 TP0] Decode batch [47278], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:01:25 TP0] Decode batch [47318], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:01:26 TP0] Decode batch [47358], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:01:27 TP0] Decode batch [47398], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:01:28 TP0] Decode batch [47438], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:01:29 TP0] Decode batch [47478], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:01:30 TP0] Decode batch [47518], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:01:30 TP0] Decode batch [47558], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:01:31 TP0] Decode batch [47598], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:01:32 TP0] Decode batch [47638], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:01:33 TP0] Decode batch [47678], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:01:34 TP0] Decode batch [47718], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:01:34] INFO:     127.0.0.1:47046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:01:34 TP0] Prefill batch [47740], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:01:35 TP0] Decode batch [47759], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:01:35 TP0] Decode batch [47799], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:01:36 TP0] Decode batch [47839], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:01:37 TP0] Decode batch [47879], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:01:38 TP0] Decode batch [47919], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:01:39 TP0] Decode batch [47959], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:01:39 TP0] Decode batch [47999], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:01:40 TP0] Decode batch [48039], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:01:41 TP0] Decode batch [48079], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:01:42 TP0] Decode batch [48119], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:01:43 TP0] Decode batch [48159], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:01:44 TP0] Decode batch [48199], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:01:44 TP0] Decode batch [48239], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:01:45 TP0] Decode batch [48279], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:01:46 TP0] Decode batch [48319], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:01:47 TP0] Decode batch [48359], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:01:48 TP0] Decode batch [48399], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:01:48 TP0] Decode batch [48439], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:01:49 TP0] Decode batch [48479], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:01:50 TP0] Decode batch [48519], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:01:51] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:01:51 TP0] Prefill batch [48541], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:01:51 TP0] Decode batch [48560], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-24 16:01:52 TP0] Decode batch [48600], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:01:53 TP0] Decode batch [48640], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:01:53 TP0] Decode batch [48680], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:01:54 TP0] Decode batch [48720], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:01:55 TP0] Decode batch [48760], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:01:56 TP0] Decode batch [48800], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:01:57 TP0] Decode batch [48840], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:01:58 TP0] Decode batch [48880], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:01:58 TP0] Decode batch [48920], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:01:59 TP0] Decode batch [48960], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:02:00 TP0] Decode batch [49000], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:02:01 TP0] Decode batch [49040], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:02:02 TP0] Decode batch [49080], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:02:03 TP0] Decode batch [49120], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:02:03 TP0] Decode batch [49160], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:02:04 TP0] Decode batch [49200], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:02:05 TP0] Decode batch [49240], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:02:06 TP0] Decode batch [49280], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:02:07 TP0] Decode batch [49320], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:02:07] INFO:     127.0.0.1:49330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:02:07 TP0] Prefill batch [49342], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:02:08 TP0] Decode batch [49361], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:02:08 TP0] Decode batch [49401], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:02:09 TP0] Decode batch [49441], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:02:10 TP0] Decode batch [49481], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:02:11 TP0] Decode batch [49521], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:02:12 TP0] Decode batch [49561], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:02:12 TP0] Decode batch [49601], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:02:13 TP0] Decode batch [49641], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:02:14 TP0] Decode batch [49681], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:02:15 TP0] Decode batch [49721], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:02:16 TP0] Decode batch [49761], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:02:17 TP0] Decode batch [49801], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:02:17 TP0] Decode batch [49841], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:02:18 TP0] Decode batch [49881], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:02:19 TP0] Decode batch [49921], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:02:20 TP0] Decode batch [49961], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:02:21 TP0] Decode batch [50001], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:02:21 TP0] Decode batch [50041], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:02:22 TP0] Decode batch [50081], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:02:23 TP0] Decode batch [50121], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:02:24] INFO:     127.0.0.1:42506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:02:24 TP0] Prefill batch [50143], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:02:24 TP0] Decode batch [50162], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.11, #queue-req: 0, 
[2025-10-24 16:02:25 TP0] Decode batch [50202], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:02:26 TP0] Decode batch [50242], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:02:26 TP0] Decode batch [50282], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:02:27 TP0] Decode batch [50322], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:02:28 TP0] Decode batch [50362], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:02:29 TP0] Decode batch [50402], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:02:30 TP0] Decode batch [50442], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:02:31 TP0] Decode batch [50482], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:02:31 TP0] Decode batch [50522], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:02:32 TP0] Decode batch [50562], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:02:33 TP0] Decode batch [50602], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:02:34 TP0] Decode batch [50642], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:02:35 TP0] Decode batch [50682], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:02:35 TP0] Decode batch [50722], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:02:36 TP0] Decode batch [50762], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:02:37 TP0] Decode batch [50802], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:02:38 TP0] Decode batch [50842], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:02:39 TP0] Decode batch [50882], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:02:40 TP0] Decode batch [50922], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:02:40] INFO:     127.0.0.1:46284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:02:40 TP0] Prefill batch [50944], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:02:41 TP0] Decode batch [50963], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.14, #queue-req: 0, 
[2025-10-24 16:02:41 TP0] Decode batch [51003], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:02:42 TP0] Decode batch [51043], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:02:43 TP0] Decode batch [51083], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:02:44 TP0] Decode batch [51123], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:02:45 TP0] Decode batch [51163], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:02:45 TP0] Decode batch [51203], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:02:46 TP0] Decode batch [51243], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:02:47 TP0] Decode batch [51283], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:02:48 TP0] Decode batch [51323], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:02:49 TP0] Decode batch [51363], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:02:50 TP0] Decode batch [51403], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:02:50 TP0] Decode batch [51443], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:02:51 TP0] Decode batch [51483], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:02:52 TP0] Decode batch [51523], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:02:53 TP0] Decode batch [51563], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:02:54 TP0] Decode batch [51603], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:02:54 TP0] Decode batch [51643], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:02:55 TP0] Decode batch [51683], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:02:56 TP0] Decode batch [51723], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:02:57] INFO:     127.0.0.1:42012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:02:57 TP0] Prefill batch [51745], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:02:57 TP0] Decode batch [51764], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-24 16:02:58 TP0] Decode batch [51804], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:02:59 TP0] Decode batch [51844], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:02:59 TP0] Decode batch [51884], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:03:00 TP0] Decode batch [51924], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:03:01 TP0] Decode batch [51964], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:03:02 TP0] Decode batch [52004], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:03 TP0] Decode batch [52044], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:03:04 TP0] Decode batch [52084], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:04 TP0] Decode batch [52124], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:03:05 TP0] Decode batch [52164], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:03:06 TP0] Decode batch [52204], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:03:07 TP0] Decode batch [52244], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:08 TP0] Decode batch [52284], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:03:08 TP0] Decode batch [52324], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:09 TP0] Decode batch [52364], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:03:10 TP0] Decode batch [52404], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:11 TP0] Decode batch [52444], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:12 TP0] Decode batch [52484], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:03:13 TP0] Decode batch [52524], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:03:13] INFO:     127.0.0.1:38738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:03:13 TP0] Prefill batch [52546], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:03:14 TP0] Decode batch [52565], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.82, #queue-req: 0, 
[2025-10-24 16:03:14 TP0] Decode batch [52605], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:03:15 TP0] Decode batch [52645], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:03:16 TP0] Decode batch [52685], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:03:17 TP0] Decode batch [52725], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:03:18 TP0] Decode batch [52765], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:03:18 TP0] Decode batch [52805], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:03:19 TP0] Decode batch [52845], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:03:20 TP0] Decode batch [52885], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:03:21 TP0] Decode batch [52925], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:03:22 TP0] Decode batch [52965], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:03:23 TP0] Decode batch [53005], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:23 TP0] Decode batch [53045], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:03:24 TP0] Decode batch [53085], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:25 TP0] Decode batch [53125], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:26 TP0] Decode batch [53165], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:27 TP0] Decode batch [53205], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:27 TP0] Decode batch [53245], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:28 TP0] Decode batch [53285], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:29 TP0] Decode batch [53325], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:03:30] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:03:30 TP0] Prefill batch [53347], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:03:30 TP0] Decode batch [53366], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.62, #queue-req: 0, 
[2025-10-24 16:03:31 TP0] Decode batch [53406], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-24 16:03:32 TP0] Decode batch [53446], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-24 16:03:32 TP0] Decode batch [53486], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-24 16:03:33 TP0] Decode batch [53526], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-24 16:03:34 TP0] Decode batch [53566], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-24 16:03:35 TP0] Decode batch [53606], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-24 16:03:36 TP0] Decode batch [53646], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:03:37 TP0] Decode batch [53686], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:03:37 TP0] Decode batch [53726], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:03:38 TP0] Decode batch [53766], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:03:39 TP0] Decode batch [53806], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:03:40 TP0] Decode batch [53846], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.54, #queue-req: 0, 
[2025-10-24 16:03:41 TP0] Decode batch [53886], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:03:41 TP0] Decode batch [53926], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:03:42 TP0] Decode batch [53966], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:03:43 TP0] Decode batch [54006], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:03:44 TP0] Decode batch [54046], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:03:45 TP0] Decode batch [54086], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:03:46 TP0] Decode batch [54126], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:03:46] INFO:     127.0.0.1:51618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:03:46 TP0] Prefill batch [54148], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:03:46 TP0] Decode batch [54167], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-10-24 16:03:47 TP0] Decode batch [54207], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:03:48 TP0] Decode batch [54247], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:03:49 TP0] Decode batch [54287], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:03:50 TP0] Decode batch [54327], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:03:51 TP0] Decode batch [54367], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:03:51 TP0] Decode batch [54407], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:03:52 TP0] Decode batch [54447], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:03:53 TP0] Decode batch [54487], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:03:54 TP0] Decode batch [54527], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:03:55 TP0] Decode batch [54567], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:03:56 TP0] Decode batch [54607], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:03:56 TP0] Decode batch [54647], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:03:57 TP0] Decode batch [54687], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:03:58 TP0] Decode batch [54727], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:03:59 TP0] Decode batch [54767], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:00 TP0] Decode batch [54807], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:00 TP0] Decode batch [54847], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:01 TP0] Decode batch [54887], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:02 TP0] Decode batch [54927], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:03] INFO:     127.0.0.1:46112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:04:03 TP0] Prefill batch [54949], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:04:03 TP0] Decode batch [54968], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:04:04 TP0] Decode batch [55008], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:04:05 TP0] Decode batch [55048], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:04:05 TP0] Decode batch [55088], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:04:06 TP0] Decode batch [55128], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:04:07 TP0] Decode batch [55168], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:04:08 TP0] Decode batch [55208], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:04:09 TP0] Decode batch [55248], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:10 TP0] Decode batch [55288], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:10 TP0] Decode batch [55328], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:11 TP0] Decode batch [55368], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:04:12 TP0] Decode batch [55408], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:04:13 TP0] Decode batch [55448], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:04:14 TP0] Decode batch [55488], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:04:14 TP0] Decode batch [55528], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:04:15 TP0] Decode batch [55568], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:04:16 TP0] Decode batch [55608], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:04:17 TP0] Decode batch [55648], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:04:18 TP0] Decode batch [55688], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:04:19 TP0] Decode batch [55728], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:04:19] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:04:19 TP0] Prefill batch [55750], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:04:19 TP0] Decode batch [55769], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:04:20 TP0] Decode batch [55809], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:04:21 TP0] Decode batch [55849], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:04:22 TP0] Decode batch [55889], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:04:23 TP0] Decode batch [55929], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:04:24 TP0] Decode batch [55969], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:04:24 TP0] Decode batch [56009], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:04:25 TP0] Decode batch [56049], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:26 TP0] Decode batch [56089], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:27 TP0] Decode batch [56129], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:04:28 TP0] Decode batch [56169], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:29 TP0] Decode batch [56209], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:29 TP0] Decode batch [56249], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:30 TP0] Decode batch [56289], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:31 TP0] Decode batch [56329], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:32 TP0] Decode batch [56369], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:33 TP0] Decode batch [56409], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:04:33 TP0] Decode batch [56449], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:34 TP0] Decode batch [56489], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:04:35 TP0] Decode batch [56529], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:04:36] INFO:     127.0.0.1:40874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:04:36 TP0] Prefill batch [56551], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:04:36 TP0] Decode batch [56570], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-24 16:04:37 TP0] Decode batch [56610], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:04:38 TP0] Decode batch [56650], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:04:38 TP0] Decode batch [56690], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:04:39 TP0] Decode batch [56730], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:04:40 TP0] Decode batch [56770], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:04:41 TP0] Decode batch [56810], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:04:42 TP0] Decode batch [56850], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:04:43 TP0] Decode batch [56890], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:04:43 TP0] Decode batch [56930], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:04:44 TP0] Decode batch [56970], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:04:45 TP0] Decode batch [57010], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:04:46 TP0] Decode batch [57050], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:04:47 TP0] Decode batch [57090], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:04:47 TP0] Decode batch [57130], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:04:48 TP0] Decode batch [57170], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:04:49 TP0] Decode batch [57210], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:04:50 TP0] Decode batch [57250], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:04:51 TP0] Decode batch [57290], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:04:52 TP0] Decode batch [57330], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:04:52] INFO:     127.0.0.1:32770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:04:52 TP0] Prefill batch [57352], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:04:52 TP0] Decode batch [57371], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-10-24 16:04:53 TP0] Decode batch [57411], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:04:54 TP0] Decode batch [57451], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:04:55 TP0] Decode batch [57491], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:04:56 TP0] Decode batch [57531], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:04:57 TP0] Decode batch [57571], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:04:57 TP0] Decode batch [57611], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:04:58 TP0] Decode batch [57651], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:04:59 TP0] Decode batch [57691], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:05:00 TP0] Decode batch [57731], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:05:01 TP0] Decode batch [57771], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:05:01 TP0] Decode batch [57811], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:05:02 TP0] Decode batch [57851], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:05:03 TP0] Decode batch [57891], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:05:04 TP0] Decode batch [57931], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:05:05 TP0] Decode batch [57971], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:05:06 TP0] Decode batch [58011], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:05:06 TP0] Decode batch [58051], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:05:07 TP0] Decode batch [58091], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:05:08 TP0] Decode batch [58131], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:05:08] INFO:     127.0.0.1:59900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:05:08 TP0] Prefill batch [58153], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:05:09 TP0] Decode batch [58172], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-24 16:05:10 TP0] Decode batch [58212], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:05:11 TP0] Decode batch [58252], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:05:11 TP0] Decode batch [58292], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:05:12 TP0] Decode batch [58332], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:05:13 TP0] Decode batch [58372], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:05:14 TP0] Decode batch [58412], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:05:15 TP0] Decode batch [58452], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:05:16 TP0] Decode batch [58492], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:05:16 TP0] Decode batch [58532], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:05:17 TP0] Decode batch [58572], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:05:18 TP0] Decode batch [58612], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:05:19 TP0] Decode batch [58652], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:05:20 TP0] Decode batch [58692], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:05:20 TP0] Decode batch [58732], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:05:21 TP0] Decode batch [58772], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:05:22 TP0] Decode batch [58812], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:05:23 TP0] Decode batch [58852], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:05:24 TP0] Decode batch [58892], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:05:25 TP0] Decode batch [58932], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:05:25] INFO:     127.0.0.1:37204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:05:25 TP0] Prefill batch [58954], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:05:25 TP0] Decode batch [58973], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:05:26 TP0] Decode batch [59013], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:05:27 TP0] Decode batch [59053], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:05:28 TP0] Decode batch [59093], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:05:29 TP0] Decode batch [59133], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:05:30 TP0] Decode batch [59173], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:05:30 TP0] Decode batch [59213], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:05:31 TP0] Decode batch [59253], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:05:32 TP0] Decode batch [59293], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:05:33 TP0] Decode batch [59333], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:05:34 TP0] Decode batch [59373], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:05:34 TP0] Decode batch [59413], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:05:35 TP0] Decode batch [59453], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:05:36 TP0] Decode batch [59493], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:05:37 TP0] Decode batch [59533], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:05:38 TP0] Decode batch [59573], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:05:39 TP0] Decode batch [59613], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:05:39 TP0] Decode batch [59653], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:05:40 TP0] Decode batch [59693], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:05:41 TP0] Decode batch [59733], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:05:41] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:05:41 TP0] Prefill batch [59755], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:05:42 TP0] Decode batch [59774], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:05:43 TP0] Decode batch [59814], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-24 16:05:44 TP0] Decode batch [59854], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:05:44 TP0] Decode batch [59894], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:05:45 TP0] Decode batch [59934], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:05:46 TP0] Decode batch [59974], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:05:47 TP0] Decode batch [60014], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:05:48 TP0] Decode batch [60054], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:05:48 TP0] Decode batch [60094], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:05:49 TP0] Decode batch [60134], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:05:50 TP0] Decode batch [60174], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:05:51 TP0] Decode batch [60214], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:05:52 TP0] Decode batch [60254], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:05:53 TP0] Decode batch [60294], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:05:53 TP0] Decode batch [60334], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:05:54 TP0] Decode batch [60374], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:05:55 TP0] Decode batch [60414], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:05:56 TP0] Decode batch [60454], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:05:57 TP0] Decode batch [60494], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:05:58 TP0] Decode batch [60534], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:05:58] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:05:58 TP0] Prefill batch [60556], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:05:58 TP0] Decode batch [60575], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-24 16:05:59 TP0] Decode batch [60615], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:06:00 TP0] Decode batch [60655], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:06:01 TP0] Decode batch [60695], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:06:02 TP0] Decode batch [60735], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:06:03 TP0] Decode batch [60775], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:06:03 TP0] Decode batch [60815], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:06:04 TP0] Decode batch [60855], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:06:05 TP0] Decode batch [60895], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:06:06 TP0] Decode batch [60935], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:06:07 TP0] Decode batch [60975], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:06:07 TP0] Decode batch [61015], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:06:08 TP0] Decode batch [61055], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:06:09 TP0] Decode batch [61095], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:06:10 TP0] Decode batch [61135], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:06:11 TP0] Decode batch [61175], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:06:12 TP0] Decode batch [61215], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:06:12 TP0] Decode batch [61255], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:06:13 TP0] Decode batch [61295], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:06:14 TP0] Decode batch [61335], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:06:14] INFO:     127.0.0.1:36730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:06:14 TP0] Prefill batch [61357], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:06:15 TP0] Decode batch [61376], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-24 16:06:16 TP0] Decode batch [61416], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:06:17 TP0] Decode batch [61456], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:06:17 TP0] Decode batch [61496], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:06:18 TP0] Decode batch [61536], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:06:19 TP0] Decode batch [61576], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:06:20 TP0] Decode batch [61616], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:06:21 TP0] Decode batch [61656], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:06:21 TP0] Decode batch [61696], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:06:22 TP0] Decode batch [61736], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:06:23 TP0] Decode batch [61776], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:06:24 TP0] Decode batch [61816], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:06:25 TP0] Decode batch [61856], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:06:26 TP0] Decode batch [61896], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:06:26 TP0] Decode batch [61936], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:06:27 TP0] Decode batch [61976], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:06:28 TP0] Decode batch [62016], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:06:29 TP0] Decode batch [62056], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:06:30 TP0] Decode batch [62096], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:06:31 TP0] Decode batch [62136], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:06:31] INFO:     127.0.0.1:55484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:06:31 TP0] Prefill batch [62158], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:06:31 TP0] Decode batch [62177], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-24 16:06:32 TP0] Decode batch [62217], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:06:33 TP0] Decode batch [62257], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:06:34 TP0] Decode batch [62297], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:06:35 TP0] Decode batch [62337], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:06:36 TP0] Decode batch [62377], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:06:36 TP0] Decode batch [62417], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:06:37 TP0] Decode batch [62457], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:06:38 TP0] Decode batch [62497], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:06:39 TP0] Decode batch [62537], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:06:40 TP0] Decode batch [62577], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:06:40 TP0] Decode batch [62617], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:06:41 TP0] Decode batch [62657], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:06:42 TP0] Decode batch [62697], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:06:43 TP0] Decode batch [62737], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:06:44 TP0] Decode batch [62777], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:06:45 TP0] Decode batch [62817], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:06:45 TP0] Decode batch [62857], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:06:46 TP0] Decode batch [62897], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:06:47 TP0] Decode batch [62937], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:06:47] INFO:     127.0.0.1:32936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:06:47 TP0] Prefill batch [62959], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:06:48 TP0] Decode batch [62978], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:06:49 TP0] Decode batch [63018], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:06:50 TP0] Decode batch [63058], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:06:50 TP0] Decode batch [63098], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:06:51 TP0] Decode batch [63138], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:06:52 TP0] Decode batch [63178], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:06:53 TP0] Decode batch [63218], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:06:54 TP0] Decode batch [63258], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:06:54 TP0] Decode batch [63298], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:06:55 TP0] Decode batch [63338], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:06:56 TP0] Decode batch [63378], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:06:57 TP0] Decode batch [63418], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:06:58 TP0] Decode batch [63458], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:06:59 TP0] Decode batch [63498], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:06:59 TP0] Decode batch [63538], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:07:00 TP0] Decode batch [63578], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:01 TP0] Decode batch [63618], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:07:02 TP0] Decode batch [63658], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:07:03 TP0] Decode batch [63698], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:03 TP0] Decode batch [63738], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:04] INFO:     127.0.0.1:44532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:07:04 TP0] Prefill batch [63760], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:07:04 TP0] Decode batch [63779], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-24 16:07:05 TP0] Decode batch [63819], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:07:06 TP0] Decode batch [63859], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:07:07 TP0] Decode batch [63899], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:07:08 TP0] Decode batch [63939], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:08 TP0] Decode batch [63979], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:07:09 TP0] Decode batch [64019], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:10 TP0] Decode batch [64059], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:11 TP0] Decode batch [64099], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:12 TP0] Decode batch [64139], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:13 TP0] Decode batch [64179], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:13 TP0] Decode batch [64219], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:14 TP0] Decode batch [64259], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:07:15 TP0] Decode batch [64299], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:07:16 TP0] Decode batch [64339], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:07:17 TP0] Decode batch [64379], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:07:18 TP0] Decode batch [64419], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:18 TP0] Decode batch [64459], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:07:19 TP0] Decode batch [64499], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:07:20 TP0] Decode batch [64539], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:20] INFO:     127.0.0.1:48656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:07:20 TP0] Prefill batch [64561], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:07:21 TP0] Decode batch [64580], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:07:22 TP0] Decode batch [64620], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:07:23 TP0] Decode batch [64660], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:07:23 TP0] Decode batch [64700], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:07:24 TP0] Decode batch [64740], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:25 TP0] Decode batch [64780], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:26 TP0] Decode batch [64820], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:27 TP0] Decode batch [64860], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:07:27 TP0] Decode batch [64900], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:28 TP0] Decode batch [64940], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:29 TP0] Decode batch [64980], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:30 TP0] Decode batch [65020], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:31 TP0] Decode batch [65060], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:32 TP0] Decode batch [65100], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:32 TP0] Decode batch [65140], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:07:33 TP0] Decode batch [65180], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:07:34 TP0] Decode batch [65220], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:07:35 TP0] Decode batch [65260], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:36 TP0] Decode batch [65300], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:07:36 TP0] Decode batch [65340], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:07:37] INFO:     127.0.0.1:37836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:07:37 TP0] Prefill batch [65362], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:07:37 TP0] Decode batch [65381], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-24 16:07:38 TP0] Decode batch [65421], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:07:39 TP0] Decode batch [65461], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:07:40 TP0] Decode batch [65501], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:07:41 TP0] Decode batch [65541], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:07:41 TP0] Decode batch [65581], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:07:42 TP0] Decode batch [65621], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:43 TP0] Decode batch [65661], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:44 TP0] Decode batch [65701], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:45 TP0] Decode batch [65741], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:46 TP0] Decode batch [65781], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:46 TP0] Decode batch [65821], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:47 TP0] Decode batch [65861], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:48 TP0] Decode batch [65901], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:49 TP0] Decode batch [65941], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:50 TP0] Decode batch [65981], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:50 TP0] Decode batch [66021], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:07:51 TP0] Decode batch [66061], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:52 TP0] Decode batch [66101], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:53 TP0] Decode batch [66141], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:07:53] INFO:     127.0.0.1:44030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:07:53 TP0] Prefill batch [66163], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:07:54 TP0] Decode batch [66182], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:07:55 TP0] Decode batch [66222], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:07:55 TP0] Decode batch [66262], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:07:56 TP0] Decode batch [66302], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:07:57 TP0] Decode batch [66342], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:07:58 TP0] Decode batch [66382], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:07:59 TP0] Decode batch [66422], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:08:00 TP0] Decode batch [66462], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:08:00 TP0] Decode batch [66502], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:08:01 TP0] Decode batch [66542], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:08:02 TP0] Decode batch [66582], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:08:03 TP0] Decode batch [66622], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:08:04 TP0] Decode batch [66662], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:08:05 TP0] Decode batch [66702], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:08:05 TP0] Decode batch [66742], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:08:06 TP0] Decode batch [66782], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:08:07 TP0] Decode batch [66822], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:08:08 TP0] Decode batch [66862], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:08:09 TP0] Decode batch [66902], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:08:09 TP0] Decode batch [66942], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:08:10] INFO:     127.0.0.1:42936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:08:10 TP0] Prefill batch [66964], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:08:10 TP0] Decode batch [66983], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:08:11 TP0] Decode batch [67023], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:08:12 TP0] Decode batch [67063], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:08:13 TP0] Decode batch [67103], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:08:14 TP0] Decode batch [67143], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:08:14 TP0] Decode batch [67183], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:08:15 TP0] Decode batch [67223], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:08:16 TP0] Decode batch [67263], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:08:17 TP0] Decode batch [67303], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:08:18 TP0] Decode batch [67343], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:08:19 TP0] Decode batch [67383], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:08:19 TP0] Decode batch [67423], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:08:20 TP0] Decode batch [67463], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:08:21 TP0] Decode batch [67503], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:08:22 TP0] Decode batch [67543], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:08:23 TP0] Decode batch [67583], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:08:23 TP0] Decode batch [67623], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:08:24 TP0] Decode batch [67663], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:08:25 TP0] Decode batch [67703], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:08:26 TP0] Decode batch [67743], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:08:26] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:08:26 TP0] Prefill batch [67765], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:08:27 TP0] Decode batch [67784], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-24 16:08:28 TP0] Decode batch [67824], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:08:28 TP0] Decode batch [67864], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:08:29 TP0] Decode batch [67904], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:08:30 TP0] Decode batch [67944], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:08:31 TP0] Decode batch [67984], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:08:32 TP0] Decode batch [68024], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:08:33 TP0] Decode batch [68064], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:08:33 TP0] Decode batch [68104], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:08:34 TP0] Decode batch [68144], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:08:35 TP0] Decode batch [68184], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:08:36 TP0] Decode batch [68224], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:08:37 TP0] Decode batch [68264], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.60, #queue-req: 0, 
[2025-10-24 16:08:38 TP0] Decode batch [68304], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.60, #queue-req: 0, 
[2025-10-24 16:08:38 TP0] Decode batch [68344], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.60, #queue-req: 0, 
[2025-10-24 16:08:39 TP0] Decode batch [68384], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.59, #queue-req: 0, 
[2025-10-24 16:08:40 TP0] Decode batch [68424], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.59, #queue-req: 0, 
[2025-10-24 16:08:41 TP0] Decode batch [68464], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-10-24 16:08:42 TP0] Decode batch [68504], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-10-24 16:08:42 TP0] Decode batch [68544], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.60, #queue-req: 0, 
[2025-10-24 16:08:43] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:08:43 TP0] Prefill batch [68566], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:08:43 TP0] Decode batch [68585], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-24 16:08:44 TP0] Decode batch [68625], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:08:45 TP0] Decode batch [68665], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:08:46 TP0] Decode batch [68705], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:08:47 TP0] Decode batch [68745], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:08:47 TP0] Decode batch [68785], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:08:48 TP0] Decode batch [68825], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:08:49 TP0] Decode batch [68865], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:08:50 TP0] Decode batch [68905], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:08:51 TP0] Decode batch [68945], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:08:52 TP0] Decode batch [68985], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:08:52 TP0] Decode batch [69025], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:08:53 TP0] Decode batch [69065], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:08:54 TP0] Decode batch [69105], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:08:55 TP0] Decode batch [69145], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:08:56 TP0] Decode batch [69185], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:08:57 TP0] Decode batch [69225], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:08:57 TP0] Decode batch [69265], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:08:58 TP0] Decode batch [69305], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:08:59 TP0] Decode batch [69345], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:08:59] INFO:     127.0.0.1:34172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:08:59 TP0] Prefill batch [69367], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:09:00 TP0] Decode batch [69386], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-24 16:09:01 TP0] Decode batch [69426], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:09:02 TP0] Decode batch [69466], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:09:02 TP0] Decode batch [69506], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:09:03 TP0] Decode batch [69546], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:09:04 TP0] Decode batch [69586], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:09:05 TP0] Decode batch [69626], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:09:06 TP0] Decode batch [69666], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:09:06 TP0] Decode batch [69706], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:09:07 TP0] Decode batch [69746], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:09:08 TP0] Decode batch [69786], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:09:09 TP0] Decode batch [69826], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:09:10 TP0] Decode batch [69866], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:09:11 TP0] Decode batch [69906], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:09:11 TP0] Decode batch [69946], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:09:12 TP0] Decode batch [69986], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:09:13 TP0] Decode batch [70026], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:09:14 TP0] Decode batch [70066], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:09:15 TP0] Decode batch [70106], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:09:15 TP0] Decode batch [70146], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:09:16] INFO:     127.0.0.1:46588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:09:16 TP0] Prefill batch [70168], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:09:16 TP0] Decode batch [70187], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-24 16:09:17 TP0] Decode batch [70227], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:09:18 TP0] Decode batch [70267], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:09:19 TP0] Decode batch [70307], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:09:20 TP0] Decode batch [70347], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:09:20 TP0] Decode batch [70387], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:09:21 TP0] Decode batch [70427], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:09:22 TP0] Decode batch [70467], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:09:23 TP0] Decode batch [70507], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:09:24 TP0] Decode batch [70547], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:09:25 TP0] Decode batch [70587], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:09:25 TP0] Decode batch [70627], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:09:26 TP0] Decode batch [70667], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:09:27 TP0] Decode batch [70707], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:09:28 TP0] Decode batch [70747], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:09:29 TP0] Decode batch [70787], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:09:30 TP0] Decode batch [70827], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:09:30 TP0] Decode batch [70867], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:09:31 TP0] Decode batch [70907], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:09:32 TP0] Decode batch [70947], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:09:32] INFO:     127.0.0.1:45302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:09:32 TP0] Prefill batch [70969], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:09:33 TP0] Decode batch [70988], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:09:34 TP0] Decode batch [71028], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:09:35 TP0] Decode batch [71068], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:09:35 TP0] Decode batch [71108], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:09:36 TP0] Decode batch [71148], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:09:37 TP0] Decode batch [71188], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:09:38 TP0] Decode batch [71228], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:09:39 TP0] Decode batch [71268], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:09:39 TP0] Decode batch [71308], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:09:40 TP0] Decode batch [71348], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:09:41 TP0] Decode batch [71388], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:09:42 TP0] Decode batch [71428], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:09:43 TP0] Decode batch [71468], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:09:44 TP0] Decode batch [71508], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:09:44 TP0] Decode batch [71548], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:09:45 TP0] Decode batch [71588], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:09:46 TP0] Decode batch [71628], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:09:47 TP0] Decode batch [71668], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:09:48 TP0] Decode batch [71708], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:09:48 TP0] Decode batch [71748], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:09:49] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:09:49 TP0] Prefill batch [71770], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:09:49 TP0] Decode batch [71789], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-24 16:09:50 TP0] Decode batch [71829], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:09:51 TP0] Decode batch [71869], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:09:52 TP0] Decode batch [71909], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:09:53 TP0] Decode batch [71949], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:09:53 TP0] Decode batch [71989], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:09:54 TP0] Decode batch [72029], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:09:55 TP0] Decode batch [72069], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:09:56 TP0] Decode batch [72109], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:09:57 TP0] Decode batch [72149], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:09:58 TP0] Decode batch [72189], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:09:58 TP0] Decode batch [72229], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:09:59 TP0] Decode batch [72269], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:10:00 TP0] Decode batch [72309], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:10:01 TP0] Decode batch [72349], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:10:02 TP0] Decode batch [72389], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:10:03 TP0] Decode batch [72429], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:10:03 TP0] Decode batch [72469], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:10:04 TP0] Decode batch [72509], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:10:05 TP0] Decode batch [72549], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:10:05] INFO:     127.0.0.1:38646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:10:05 TP0] Prefill batch [72571], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:10:06 TP0] Decode batch [72590], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-24 16:10:07 TP0] Decode batch [72630], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-24 16:10:08 TP0] Decode batch [72670], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-24 16:10:08 TP0] Decode batch [72710], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-24 16:10:09 TP0] Decode batch [72750], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-24 16:10:10 TP0] Decode batch [72790], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:10:11 TP0] Decode batch [72830], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:10:12 TP0] Decode batch [72870], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:10:12 TP0] Decode batch [72910], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:10:13 TP0] Decode batch [72950], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:10:14 TP0] Decode batch [72990], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:10:15 TP0] Decode batch [73030], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:10:16 TP0] Decode batch [73070], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:10:17 TP0] Decode batch [73110], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:10:17 TP0] Decode batch [73150], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:10:18 TP0] Decode batch [73190], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:10:19 TP0] Decode batch [73230], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:10:20 TP0] Decode batch [73270], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:10:21 TP0] Decode batch [73310], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:10:21 TP0] Decode batch [73350], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:10:22] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:10:22 TP0] Prefill batch [73372], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:10:22 TP0] Decode batch [73391], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-24 16:10:23 TP0] Decode batch [73431], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:10:24 TP0] Decode batch [73471], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:10:25 TP0] Decode batch [73511], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:10:26 TP0] Decode batch [73551], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:10:26 TP0] Decode batch [73591], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:10:27 TP0] Decode batch [73631], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:10:28 TP0] Decode batch [73671], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:10:29 TP0] Decode batch [73711], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:10:30 TP0] Decode batch [73751], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:10:31 TP0] Decode batch [73791], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:10:31 TP0] Decode batch [73831], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:10:32 TP0] Decode batch [73871], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:10:33 TP0] Decode batch [73911], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:10:34 TP0] Decode batch [73951], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:10:35 TP0] Decode batch [73991], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:10:35 TP0] Decode batch [74031], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:10:36 TP0] Decode batch [74071], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:10:37 TP0] Decode batch [74111], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:10:38 TP0] Decode batch [74151], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:10:38] INFO:     127.0.0.1:43702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:10:38 TP0] Prefill batch [74173], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:10:39 TP0] Decode batch [74192], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-24 16:10:40 TP0] Decode batch [74232], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:10:40 TP0] Decode batch [74272], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:10:41 TP0] Decode batch [74312], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:10:42 TP0] Decode batch [74352], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:10:43 TP0] Decode batch [74392], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:10:44 TP0] Decode batch [74432], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:10:45 TP0] Decode batch [74472], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:10:45 TP0] Decode batch [74512], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:10:46 TP0] Decode batch [74552], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:10:47 TP0] Decode batch [74592], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:10:48 TP0] Decode batch [74632], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:10:49 TP0] Decode batch [74672], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:10:49 TP0] Decode batch [74712], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:10:50 TP0] Decode batch [74752], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:10:51 TP0] Decode batch [74792], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:10:52 TP0] Decode batch [74832], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:10:53 TP0] Decode batch [74872], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:10:54 TP0] Decode batch [74912], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:10:54 TP0] Decode batch [74952], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:10:55] INFO:     127.0.0.1:36904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:10:55 TP0] Prefill batch [74974], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:10:55 TP0] Decode batch [74993], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:10:56 TP0] Decode batch [75033], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:10:57 TP0] Decode batch [75073], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:10:58 TP0] Decode batch [75113], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:10:59 TP0] Decode batch [75153], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:10:59 TP0] Decode batch [75193], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:11:00 TP0] Decode batch [75233], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:11:01 TP0] Decode batch [75273], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:11:02 TP0] Decode batch [75313], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:11:03 TP0] Decode batch [75353], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:11:04 TP0] Decode batch [75393], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:11:04 TP0] Decode batch [75433], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:11:05 TP0] Decode batch [75473], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:11:06 TP0] Decode batch [75513], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:11:07 TP0] Decode batch [75553], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:11:08 TP0] Decode batch [75593], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:11:08 TP0] Decode batch [75633], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:11:09 TP0] Decode batch [75673], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:11:10 TP0] Decode batch [75713], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:11:11 TP0] Decode batch [75753], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:11:11] INFO:     127.0.0.1:52414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:11:11 TP0] Prefill batch [75775], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:11:12 TP0] Decode batch [75794], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-24 16:11:13 TP0] Decode batch [75834], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:11:13 TP0] Decode batch [75874], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:11:14 TP0] Decode batch [75914], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:11:15 TP0] Decode batch [75954], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:11:16 TP0] Decode batch [75994], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:11:17 TP0] Decode batch [76034], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:11:18 TP0] Decode batch [76074], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:11:18 TP0] Decode batch [76114], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:11:19 TP0] Decode batch [76154], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:11:20 TP0] Decode batch [76194], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:11:21 TP0] Decode batch [76234], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:11:22 TP0] Decode batch [76274], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:11:22 TP0] Decode batch [76314], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:11:23 TP0] Decode batch [76354], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:11:24 TP0] Decode batch [76394], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:11:25 TP0] Decode batch [76434], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:11:26 TP0] Decode batch [76474], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:11:27 TP0] Decode batch [76514], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:11:27 TP0] Decode batch [76554], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:11:28] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:11:28 TP0] Prefill batch [76576], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:11:28 TP0] Decode batch [76595], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:11:29 TP0] Decode batch [76635], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:11:30 TP0] Decode batch [76675], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:11:31 TP0] Decode batch [76715], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:11:32 TP0] Decode batch [76755], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:11:32 TP0] Decode batch [76795], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:11:33 TP0] Decode batch [76835], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:11:34 TP0] Decode batch [76875], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:11:35 TP0] Decode batch [76915], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:11:36 TP0] Decode batch [76955], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:11:37 TP0] Decode batch [76995], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:11:37 TP0] Decode batch [77035], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:11:38 TP0] Decode batch [77075], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:11:39 TP0] Decode batch [77115], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:11:40 TP0] Decode batch [77155], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:11:41 TP0] Decode batch [77195], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:11:41 TP0] Decode batch [77235], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:11:42 TP0] Decode batch [77275], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:11:43 TP0] Decode batch [77315], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:11:44 TP0] Decode batch [77355], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:11:44] INFO:     127.0.0.1:46262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:11:44 TP0] Prefill batch [77377], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:11:45 TP0] Decode batch [77396], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-24 16:11:46 TP0] Decode batch [77436], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:11:46 TP0] Decode batch [77476], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:11:47 TP0] Decode batch [77516], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:11:48 TP0] Decode batch [77556], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:11:49 TP0] Decode batch [77596], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:11:50 TP0] Decode batch [77636], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:11:51 TP0] Decode batch [77676], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:11:51 TP0] Decode batch [77716], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:11:52 TP0] Decode batch [77756], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:11:53 TP0] Decode batch [77796], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:11:54 TP0] Decode batch [77836], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:11:55 TP0] Decode batch [77876], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:11:55 TP0] Decode batch [77916], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:11:56 TP0] Decode batch [77956], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:11:57 TP0] Decode batch [77996], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:11:58 TP0] Decode batch [78036], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:11:59 TP0] Decode batch [78076], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:12:00 TP0] Decode batch [78116], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:12:00 TP0] Decode batch [78156], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:12:01] INFO:     127.0.0.1:53486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:12:01 TP0] Prefill batch [78178], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:12:01 TP0] Decode batch [78197], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-24 16:12:02 TP0] Decode batch [78237], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:12:03 TP0] Decode batch [78277], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:12:04 TP0] Decode batch [78317], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:12:05 TP0] Decode batch [78357], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:12:05 TP0] Decode batch [78397], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:12:06 TP0] Decode batch [78437], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:12:07 TP0] Decode batch [78477], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:12:08 TP0] Decode batch [78517], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:12:09 TP0] Decode batch [78557], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:12:10 TP0] Decode batch [78597], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:12:10 TP0] Decode batch [78637], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:12:11 TP0] Decode batch [78677], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:12:12 TP0] Decode batch [78717], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:12:13 TP0] Decode batch [78757], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:12:14 TP0] Decode batch [78797], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:12:14 TP0] Decode batch [78837], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:12:15 TP0] Decode batch [78877], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:12:16 TP0] Decode batch [78917], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:12:17 TP0] Decode batch [78957], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:12:17] INFO:     127.0.0.1:38236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:12:17 TP0] Prefill batch [78979], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:12:18 TP0] Decode batch [78998], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:12:19 TP0] Decode batch [79038], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:12:19 TP0] Decode batch [79078], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:12:20 TP0] Decode batch [79118], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:21 TP0] Decode batch [79158], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:12:22 TP0] Decode batch [79198], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:12:23 TP0] Decode batch [79238], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:12:24 TP0] Decode batch [79278], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:12:24 TP0] Decode batch [79318], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:25 TP0] Decode batch [79358], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:12:26 TP0] Decode batch [79398], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:27 TP0] Decode batch [79438], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:28 TP0] Decode batch [79478], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:12:28 TP0] Decode batch [79518], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:12:29 TP0] Decode batch [79558], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:12:30 TP0] Decode batch [79598], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:12:31 TP0] Decode batch [79638], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:12:32 TP0] Decode batch [79678], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:12:33 TP0] Decode batch [79718], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:12:33 TP0] Decode batch [79758], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:12:34] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:12:34 TP0] Prefill batch [79780], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:12:34 TP0] Decode batch [79799], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-24 16:12:35 TP0] Decode batch [79839], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:12:36 TP0] Decode batch [79879], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:12:37 TP0] Decode batch [79919], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:12:38 TP0] Decode batch [79959], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:12:38 TP0] Decode batch [79999], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:12:39 TP0] Decode batch [80039], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:40 TP0] Decode batch [80079], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:12:41 TP0] Decode batch [80119], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:12:42 TP0] Decode batch [80159], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:12:42 TP0] Decode batch [80199], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:43 TP0] Decode batch [80239], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:12:44 TP0] Decode batch [80279], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:45 TP0] Decode batch [80319], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:46 TP0] Decode batch [80359], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:47 TP0] Decode batch [80399], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:47 TP0] Decode batch [80439], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:48 TP0] Decode batch [80479], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:49 TP0] Decode batch [80519], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:12:50 TP0] Decode batch [80559], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:50] INFO:     127.0.0.1:41056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:12:50 TP0] Prefill batch [80581], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:12:51 TP0] Decode batch [80600], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:12:52 TP0] Decode batch [80640], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:12:52 TP0] Decode batch [80680], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:12:53 TP0] Decode batch [80720], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:12:54 TP0] Decode batch [80760], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:55 TP0] Decode batch [80800], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:56 TP0] Decode batch [80840], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:57 TP0] Decode batch [80880], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:12:57 TP0] Decode batch [80920], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:12:58 TP0] Decode batch [80960], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:12:59 TP0] Decode batch [81000], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:13:00 TP0] Decode batch [81040], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:01 TP0] Decode batch [81080], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:13:01 TP0] Decode batch [81120], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:02 TP0] Decode batch [81160], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:13:03 TP0] Decode batch [81200], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:04 TP0] Decode batch [81240], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:13:05 TP0] Decode batch [81280], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:06 TP0] Decode batch [81320], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:13:06 TP0] Decode batch [81360], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:13:07] INFO:     127.0.0.1:37482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:13:07 TP0] Prefill batch [81382], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:13:07 TP0] Decode batch [81401], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-24 16:13:08 TP0] Decode batch [81441], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:13:09 TP0] Decode batch [81481], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:13:10 TP0] Decode batch [81521], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:13:11 TP0] Decode batch [81561], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:13:11 TP0] Decode batch [81601], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:13:12 TP0] Decode batch [81641], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:13:13 TP0] Decode batch [81681], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:13:14 TP0] Decode batch [81721], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:13:15 TP0] Decode batch [81761], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:13:15 TP0] Decode batch [81801], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:13:16 TP0] Decode batch [81841], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:17 TP0] Decode batch [81881], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:18 TP0] Decode batch [81921], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:13:19 TP0] Decode batch [81961], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:20 TP0] Decode batch [82001], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:20 TP0] Decode batch [82041], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:21 TP0] Decode batch [82081], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:22 TP0] Decode batch [82121], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:13:23 TP0] Decode batch [82161], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:23] INFO:     127.0.0.1:33880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:13:23 TP0] Prefill batch [82183], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:13:24 TP0] Decode batch [82202], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:13:25 TP0] Decode batch [82242], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:13:25 TP0] Decode batch [82282], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:13:26 TP0] Decode batch [82322], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:13:27 TP0] Decode batch [82362], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:13:28 TP0] Decode batch [82402], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:13:29 TP0] Decode batch [82442], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:13:29 TP0] Decode batch [82482], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:30 TP0] Decode batch [82522], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:31 TP0] Decode batch [82562], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:32 TP0] Decode batch [82602], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:13:33 TP0] Decode batch [82642], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:13:34 TP0] Decode batch [82682], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:13:34 TP0] Decode batch [82722], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:13:35 TP0] Decode batch [82762], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:13:36 TP0] Decode batch [82802], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:13:37 TP0] Decode batch [82842], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:13:38 TP0] Decode batch [82882], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:13:39 TP0] Decode batch [82922], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:13:39 TP0] Decode batch [82962], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:13:40] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:13:40 TP0] Prefill batch [82984], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:13:40 TP0] Decode batch [83003], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-24 16:13:41 TP0] Decode batch [83043], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:13:42 TP0] Decode batch [83083], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:13:43 TP0] Decode batch [83123], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:13:44 TP0] Decode batch [83163], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:13:44 TP0] Decode batch [83203], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:13:45 TP0] Decode batch [83243], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:13:46 TP0] Decode batch [83283], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:13:47 TP0] Decode batch [83323], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:13:48 TP0] Decode batch [83363], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:13:48 TP0] Decode batch [83403], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:13:49 TP0] Decode batch [83443], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:13:50 TP0] Decode batch [83483], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:13:51 TP0] Decode batch [83523], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:13:52 TP0] Decode batch [83563], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:13:53 TP0] Decode batch [83603], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:13:53 TP0] Decode batch [83643], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:13:54 TP0] Decode batch [83683], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:13:55 TP0] Decode batch [83723], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:13:56 TP0] Decode batch [83763], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:13:56] INFO:     127.0.0.1:49560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:13:56 TP0] Prefill batch [83785], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:13:57 TP0] Decode batch [83804], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-24 16:13:58 TP0] Decode batch [83844], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:13:58 TP0] Decode batch [83884], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:13:59 TP0] Decode batch [83924], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:14:00 TP0] Decode batch [83964], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:14:01 TP0] Decode batch [84004], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:14:02 TP0] Decode batch [84044], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:14:02 TP0] Decode batch [84084], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:14:03 TP0] Decode batch [84124], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:14:04 TP0] Decode batch [84164], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:14:05 TP0] Decode batch [84204], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:14:06 TP0] Decode batch [84244], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:14:07 TP0] Decode batch [84284], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:14:07 TP0] Decode batch [84324], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:14:08 TP0] Decode batch [84364], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:14:09 TP0] Decode batch [84404], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:14:10 TP0] Decode batch [84444], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:14:11 TP0] Decode batch [84484], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:14:12 TP0] Decode batch [84524], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:14:12 TP0] Decode batch [84564], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:14:13] INFO:     127.0.0.1:49870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:14:13 TP0] Prefill batch [84586], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:14:13 TP0] Decode batch [84605], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:14:14 TP0] Decode batch [84645], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:14:15 TP0] Decode batch [84685], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:14:16 TP0] Decode batch [84725], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:14:17 TP0] Decode batch [84765], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:14:17 TP0] Decode batch [84805], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:14:18 TP0] Decode batch [84845], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:14:19 TP0] Decode batch [84885], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:14:20 TP0] Decode batch [84925], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:14:21 TP0] Decode batch [84965], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:14:21 TP0] Decode batch [85005], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:14:22 TP0] Decode batch [85045], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:14:23 TP0] Decode batch [85085], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:14:24 TP0] Decode batch [85125], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:14:25 TP0] Decode batch [85165], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:14:26 TP0] Decode batch [85205], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:14:26 TP0] Decode batch [85245], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:14:27 TP0] Decode batch [85285], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:14:28 TP0] Decode batch [85325], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:14:29 TP0] Decode batch [85365], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:14:29] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:14:29 TP0] Prefill batch [85387], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:14:30 TP0] Decode batch [85406], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-24 16:14:31 TP0] Decode batch [85446], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:14:31 TP0] Decode batch [85486], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:14:32 TP0] Decode batch [85526], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:14:33 TP0] Decode batch [85566], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:14:34 TP0] Decode batch [85606], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:14:35 TP0] Decode batch [85646], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:14:35 TP0] Decode batch [85686], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:14:36 TP0] Decode batch [85726], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:14:37 TP0] Decode batch [85766], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:14:38 TP0] Decode batch [85806], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:14:39 TP0] Decode batch [85846], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:14:40 TP0] Decode batch [85886], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:14:40 TP0] Decode batch [85926], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:14:41 TP0] Decode batch [85966], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:14:42 TP0] Decode batch [86006], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:14:43 TP0] Decode batch [86046], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:14:44 TP0] Decode batch [86086], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:14:45 TP0] Decode batch [86126], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:14:45 TP0] Decode batch [86166], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:14:46] INFO:     127.0.0.1:38968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:14:46 TP0] Prefill batch [86188], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:14:46 TP0] Decode batch [86207], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-24 16:14:47 TP0] Decode batch [86247], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:14:48 TP0] Decode batch [86287], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:14:49 TP0] Decode batch [86327], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:14:50 TP0] Decode batch [86367], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:14:50 TP0] Decode batch [86407], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:14:51 TP0] Decode batch [86447], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:14:52 TP0] Decode batch [86487], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:14:53 TP0] Decode batch [86527], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:14:54 TP0] Decode batch [86567], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:14:54 TP0] Decode batch [86607], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:14:55 TP0] Decode batch [86647], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:14:56 TP0] Decode batch [86687], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:14:57 TP0] Decode batch [86727], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:14:58 TP0] Decode batch [86767], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:14:59 TP0] Decode batch [86807], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:14:59 TP0] Decode batch [86847], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:15:00 TP0] Decode batch [86887], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:15:01 TP0] Decode batch [86927], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:15:02 TP0] Decode batch [86967], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:15:02] INFO:     127.0.0.1:36074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:15:02 TP0] Prefill batch [86989], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:15:03 TP0] Decode batch [87008], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-24 16:15:04 TP0] Decode batch [87048], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:15:04 TP0] Decode batch [87088], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:15:05 TP0] Decode batch [87128], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:15:06 TP0] Decode batch [87168], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:15:07 TP0] Decode batch [87208], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:15:08 TP0] Decode batch [87248], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:15:09 TP0] Decode batch [87288], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:15:09 TP0] Decode batch [87328], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:15:10 TP0] Decode batch [87368], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:15:11 TP0] Decode batch [87408], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:15:12 TP0] Decode batch [87448], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:15:13 TP0] Decode batch [87488], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:15:13 TP0] Decode batch [87528], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:15:14 TP0] Decode batch [87568], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.63, #queue-req: 0, 
[2025-10-24 16:15:15 TP0] Decode batch [87608], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:15:16 TP0] Decode batch [87648], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.63, #queue-req: 0, 
[2025-10-24 16:15:17 TP0] Decode batch [87688], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:15:18 TP0] Decode batch [87728], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:15:18 TP0] Decode batch [87768], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:15:19] INFO:     127.0.0.1:39786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:15:19 TP0] Prefill batch [87790], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:15:19 TP0] Decode batch [87809], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-24 16:15:20 TP0] Decode batch [87849], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-24 16:15:21 TP0] Decode batch [87889], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-24 16:15:22 TP0] Decode batch [87929], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-24 16:15:23 TP0] Decode batch [87969], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-24 16:15:23 TP0] Decode batch [88009], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-24 16:15:24 TP0] Decode batch [88049], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-24 16:15:25 TP0] Decode batch [88089], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:15:26 TP0] Decode batch [88129], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-24 16:15:27 TP0] Decode batch [88169], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-24 16:15:27 TP0] Decode batch [88209], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-24 16:15:28 TP0] Decode batch [88249], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-24 16:15:29 TP0] Decode batch [88289], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:15:30 TP0] Decode batch [88329], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:15:31 TP0] Decode batch [88369], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:15:32 TP0] Decode batch [88409], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:15:32 TP0] Decode batch [88449], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:15:33 TP0] Decode batch [88489], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:15:34 TP0] Decode batch [88529], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:15:35 TP0] Decode batch [88569], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:15:35] INFO:     127.0.0.1:39132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:15:35 TP0] Prefill batch [88591], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:15:36 TP0] Decode batch [88610], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-24 16:15:37 TP0] Decode batch [88650], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:15:37 TP0] Decode batch [88690], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:15:38 TP0] Decode batch [88730], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:15:39 TP0] Decode batch [88770], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:15:40 TP0] Decode batch [88810], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:15:41 TP0] Decode batch [88850], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:15:41 TP0] Decode batch [88890], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:15:42 TP0] Decode batch [88930], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:15:43 TP0] Decode batch [88970], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:15:44 TP0] Decode batch [89010], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:15:45 TP0] Decode batch [89050], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:15:46 TP0] Decode batch [89090], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:15:46 TP0] Decode batch [89130], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:15:47 TP0] Decode batch [89170], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:15:48 TP0] Decode batch [89210], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:15:49 TP0] Decode batch [89250], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:15:50 TP0] Decode batch [89290], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:15:51 TP0] Decode batch [89330], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:15:51 TP0] Decode batch [89370], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:15:52] INFO:     127.0.0.1:53774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:15:52 TP0] Prefill batch [89392], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:15:52 TP0] Decode batch [89411], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-24 16:15:53 TP0] Decode batch [89451], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:15:54 TP0] Decode batch [89491], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:15:55 TP0] Decode batch [89531], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:15:56 TP0] Decode batch [89571], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:15:56 TP0] Decode batch [89611], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:15:57 TP0] Decode batch [89651], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:15:58 TP0] Decode batch [89691], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:15:59 TP0] Decode batch [89731], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:16:00 TP0] Decode batch [89771], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:16:00 TP0] Decode batch [89811], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:16:01 TP0] Decode batch [89851], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:16:02 TP0] Decode batch [89891], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:16:03 TP0] Decode batch [89931], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.63, #queue-req: 0, 
[2025-10-24 16:16:04 TP0] Decode batch [89971], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:16:05 TP0] Decode batch [90011], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:16:05 TP0] Decode batch [90051], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:16:06 TP0] Decode batch [90091], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:16:07 TP0] Decode batch [90131], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:16:08 TP0] Decode batch [90171], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:16:08] INFO:     127.0.0.1:40828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:16:08 TP0] Prefill batch [90193], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:16:09 TP0] Decode batch [90212], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-24 16:16:10 TP0] Decode batch [90252], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:16:10 TP0] Decode batch [90292], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:16:11 TP0] Decode batch [90332], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:16:12 TP0] Decode batch [90372], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:16:13 TP0] Decode batch [90412], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:16:14 TP0] Decode batch [90452], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:16:15 TP0] Decode batch [90492], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:16:15 TP0] Decode batch [90532], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:16:16 TP0] Decode batch [90572], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:16:17 TP0] Decode batch [90612], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:16:18 TP0] Decode batch [90652], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.52, #queue-req: 0, 
[2025-10-24 16:16:19 TP0] Decode batch [90692], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:16:19 TP0] Decode batch [90732], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:16:20 TP0] Decode batch [90772], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:16:21 TP0] Decode batch [90812], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:16:22 TP0] Decode batch [90852], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:16:23 TP0] Decode batch [90892], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:16:24 TP0] Decode batch [90932], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.60, #queue-req: 0, 
[2025-10-24 16:16:24 TP0] Decode batch [90972], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:16:25] INFO:     127.0.0.1:33476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:16:25 TP0] Prefill batch [90994], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:16:25 TP0] Decode batch [91013], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.69, #queue-req: 0, 
[2025-10-24 16:16:26 TP0] Decode batch [91053], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:16:27 TP0] Decode batch [91093], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:16:28 TP0] Decode batch [91133], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:16:29 TP0] Decode batch [91173], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:16:29 TP0] Decode batch [91213], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:16:30 TP0] Decode batch [91253], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:16:31 TP0] Decode batch [91293], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:16:32 TP0] Decode batch [91333], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:16:33 TP0] Decode batch [91373], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:16:34 TP0] Decode batch [91413], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:16:34 TP0] Decode batch [91453], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:16:35 TP0] Decode batch [91493], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:16:36 TP0] Decode batch [91533], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:16:37 TP0] Decode batch [91573], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:16:38 TP0] Decode batch [91613], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:16:38 TP0] Decode batch [91653], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.06, #queue-req: 0, 
[2025-10-24 16:16:39 TP0] Decode batch [91693], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:16:40 TP0] Decode batch [91733], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:16:41 TP0] Decode batch [91773], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:16:41] INFO:     127.0.0.1:44478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:16:41 TP0] Prefill batch [91795], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:16:42 TP0] Decode batch [91814], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:16:43 TP0] Decode batch [91854], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:16:43 TP0] Decode batch [91894], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:16:44 TP0] Decode batch [91934], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:16:45 TP0] Decode batch [91974], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:16:46 TP0] Decode batch [92014], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:16:47 TP0] Decode batch [92054], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:16:48 TP0] Decode batch [92094], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:16:48 TP0] Decode batch [92134], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:16:49 TP0] Decode batch [92174], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:16:50 TP0] Decode batch [92214], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:16:51 TP0] Decode batch [92254], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:16:52 TP0] Decode batch [92294], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:16:52 TP0] Decode batch [92334], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:16:53 TP0] Decode batch [92374], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:16:54 TP0] Decode batch [92414], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:16:55 TP0] Decode batch [92454], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:16:56 TP0] Decode batch [92494], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:16:57 TP0] Decode batch [92534], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:16:57 TP0] Decode batch [92574], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:16:58] INFO:     127.0.0.1:42134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:16:58 TP0] Prefill batch [92596], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:16:58 TP0] Decode batch [92615], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-24 16:16:59 TP0] Decode batch [92655], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:17:00 TP0] Decode batch [92695], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:01 TP0] Decode batch [92735], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:17:02 TP0] Decode batch [92775], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:02 TP0] Decode batch [92815], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:03 TP0] Decode batch [92855], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:04 TP0] Decode batch [92895], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:05 TP0] Decode batch [92935], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:17:06 TP0] Decode batch [92975], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:07 TP0] Decode batch [93015], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:17:07 TP0] Decode batch [93055], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:08 TP0] Decode batch [93095], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:17:09 TP0] Decode batch [93135], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:10 TP0] Decode batch [93175], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:11 TP0] Decode batch [93215], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:11 TP0] Decode batch [93255], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:12 TP0] Decode batch [93295], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:13 TP0] Decode batch [93335], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:14 TP0] Decode batch [93375], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:14] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:17:14 TP0] Prefill batch [93397], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:17:15 TP0] Decode batch [93416], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-24 16:17:16 TP0] Decode batch [93456], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:17:16 TP0] Decode batch [93496], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:17 TP0] Decode batch [93536], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:17:18 TP0] Decode batch [93576], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:17:19 TP0] Decode batch [93616], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:20 TP0] Decode batch [93656], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:17:21 TP0] Decode batch [93696], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:21 TP0] Decode batch [93736], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:22 TP0] Decode batch [93776], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:23 TP0] Decode batch [93816], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:24 TP0] Decode batch [93856], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:25 TP0] Decode batch [93896], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:25 TP0] Decode batch [93936], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:26 TP0] Decode batch [93976], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:27 TP0] Decode batch [94016], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:28 TP0] Decode batch [94056], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:29 TP0] Decode batch [94096], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:30 TP0] Decode batch [94136], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:30 TP0] Decode batch [94176], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:31] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:17:31 TP0] Prefill batch [94198], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:17:31 TP0] Decode batch [94217], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:17:32 TP0] Decode batch [94257], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:17:33 TP0] Decode batch [94297], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:34 TP0] Decode batch [94337], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:17:35 TP0] Decode batch [94377], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:35 TP0] Decode batch [94417], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:17:36 TP0] Decode batch [94457], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:17:37 TP0] Decode batch [94497], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:38 TP0] Decode batch [94537], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:17:39 TP0] Decode batch [94577], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:17:40 TP0] Decode batch [94617], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:17:40 TP0] Decode batch [94657], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:17:41 TP0] Decode batch [94697], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:17:42 TP0] Decode batch [94737], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:17:43 TP0] Decode batch [94777], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:17:44 TP0] Decode batch [94817], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:17:44 TP0] Decode batch [94857], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:17:45 TP0] Decode batch [94897], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:17:46 TP0] Decode batch [94937], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:17:47 TP0] Decode batch [94977], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:17:47] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:17:47 TP0] Prefill batch [94999], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:17:48 TP0] Decode batch [95018], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-24 16:17:49 TP0] Decode batch [95058], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:49 TP0] Decode batch [95098], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:50 TP0] Decode batch [95138], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:51 TP0] Decode batch [95178], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:17:52 TP0] Decode batch [95218], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:53 TP0] Decode batch [95258], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:54 TP0] Decode batch [95298], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:54 TP0] Decode batch [95338], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:55 TP0] Decode batch [95378], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:17:56 TP0] Decode batch [95418], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:57 TP0] Decode batch [95458], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:17:58 TP0] Decode batch [95498], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:17:59 TP0] Decode batch [95538], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:17:59 TP0] Decode batch [95578], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:18:00 TP0] Decode batch [95618], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:18:01 TP0] Decode batch [95658], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:18:02 TP0] Decode batch [95698], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:18:03 TP0] Decode batch [95738], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:18:03 TP0] Decode batch [95778], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:18:04] INFO:     127.0.0.1:48834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:18:04 TP0] Prefill batch [95800], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:18:04 TP0] Decode batch [95819], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-24 16:18:05 TP0] Decode batch [95859], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:18:06 TP0] Decode batch [95899], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:18:07 TP0] Decode batch [95939], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:18:08 TP0] Decode batch [95979], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:18:08 TP0] Decode batch [96019], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:09 TP0] Decode batch [96059], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:18:10 TP0] Decode batch [96099], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:11 TP0] Decode batch [96139], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:12 TP0] Decode batch [96179], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:18:13 TP0] Decode batch [96219], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:13 TP0] Decode batch [96259], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:14 TP0] Decode batch [96299], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:15 TP0] Decode batch [96339], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:16 TP0] Decode batch [96379], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:17 TP0] Decode batch [96419], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:17 TP0] Decode batch [96459], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:18 TP0] Decode batch [96499], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:19 TP0] Decode batch [96539], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:20 TP0] Decode batch [96579], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:20] INFO:     127.0.0.1:47374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:18:20 TP0] Prefill batch [96601], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:18:21 TP0] Decode batch [96620], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-24 16:18:22 TP0] Decode batch [96660], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:18:22 TP0] Decode batch [96700], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:18:23 TP0] Decode batch [96740], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:18:24 TP0] Decode batch [96780], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:25 TP0] Decode batch [96820], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:26 TP0] Decode batch [96860], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:27 TP0] Decode batch [96900], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:27 TP0] Decode batch [96940], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:28 TP0] Decode batch [96980], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:29 TP0] Decode batch [97020], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:30 TP0] Decode batch [97060], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:31 TP0] Decode batch [97100], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:31 TP0] Decode batch [97140], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:32 TP0] Decode batch [97180], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:33 TP0] Decode batch [97220], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:34 TP0] Decode batch [97260], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:35 TP0] Decode batch [97300], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:36 TP0] Decode batch [97340], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:36 TP0] Decode batch [97380], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:37] INFO:     127.0.0.1:55862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:18:37 TP0] Prefill batch [97402], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:18:37 TP0] Decode batch [97421], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:18:38 TP0] Decode batch [97461], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:18:39 TP0] Decode batch [97501], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:40 TP0] Decode batch [97541], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:41 TP0] Decode batch [97581], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:41 TP0] Decode batch [97621], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:42 TP0] Decode batch [97661], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:43 TP0] Decode batch [97701], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:44 TP0] Decode batch [97741], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:45 TP0] Decode batch [97781], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:46 TP0] Decode batch [97821], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:46 TP0] Decode batch [97861], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:47 TP0] Decode batch [97901], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:48 TP0] Decode batch [97941], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:49 TP0] Decode batch [97981], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:50 TP0] Decode batch [98021], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:50 TP0] Decode batch [98061], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:51 TP0] Decode batch [98101], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:52 TP0] Decode batch [98141], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:53 TP0] Decode batch [98181], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:18:53] INFO:     127.0.0.1:56726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:18:53 TP0] Prefill batch [98203], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:18:54 TP0] Decode batch [98222], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-24 16:18:55 TP0] Decode batch [98262], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:55 TP0] Decode batch [98302], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:18:56 TP0] Decode batch [98342], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:18:57 TP0] Decode batch [98382], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:58 TP0] Decode batch [98422], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:18:59 TP0] Decode batch [98462], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:00 TP0] Decode batch [98502], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:19:00 TP0] Decode batch [98542], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:01 TP0] Decode batch [98582], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:19:02 TP0] Decode batch [98622], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:03 TP0] Decode batch [98662], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:04 TP0] Decode batch [98702], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:19:04 TP0] Decode batch [98742], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:05 TP0] Decode batch [98782], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:19:06 TP0] Decode batch [98822], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:19:07 TP0] Decode batch [98862], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:19:08 TP0] Decode batch [98902], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:09 TP0] Decode batch [98942], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:19:09 TP0] Decode batch [98982], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:19:10] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:19:10 TP0] Prefill batch [99004], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:19:10 TP0] Decode batch [99023], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:19:11 TP0] Decode batch [99063], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:12 TP0] Decode batch [99103], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:13 TP0] Decode batch [99143], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:14 TP0] Decode batch [99183], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:14 TP0] Decode batch [99223], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:19:15 TP0] Decode batch [99263], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:16 TP0] Decode batch [99303], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:19:17 TP0] Decode batch [99343], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:18 TP0] Decode batch [99383], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:19:19 TP0] Decode batch [99423], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:19:19 TP0] Decode batch [99463], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:19:20 TP0] Decode batch [99503], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:19:21 TP0] Decode batch [99543], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:19:22 TP0] Decode batch [99583], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:19:23 TP0] Decode batch [99623], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:19:23 TP0] Decode batch [99663], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:19:24 TP0] Decode batch [99703], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:19:25 TP0] Decode batch [99743], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:19:26 TP0] Decode batch [99783], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:19:26] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:19:26 TP0] Prefill batch [99805], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:19:27 TP0] Decode batch [99824], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:19:28 TP0] Decode batch [99864], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:19:28 TP0] Decode batch [99904], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:19:29 TP0] Decode batch [99944], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:19:30 TP0] Decode batch [99984], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:19:31 TP0] Decode batch [100024], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:19:32 TP0] Decode batch [100064], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:33 TP0] Decode batch [100104], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:19:33 TP0] Decode batch [100144], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:19:34 TP0] Decode batch [100184], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:19:35 TP0] Decode batch [100224], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:36 TP0] Decode batch [100264], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:19:37 TP0] Decode batch [100304], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:37 TP0] Decode batch [100344], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:38 TP0] Decode batch [100384], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:19:39 TP0] Decode batch [100424], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:40 TP0] Decode batch [100464], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:41 TP0] Decode batch [100504], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:19:42 TP0] Decode batch [100544], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:42 TP0] Decode batch [100584], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:43] INFO:     127.0.0.1:33776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:19:43 TP0] Prefill batch [100606], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:19:43 TP0] Decode batch [100625], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:19:44 TP0] Decode batch [100665], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:19:45 TP0] Decode batch [100705], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:19:46 TP0] Decode batch [100745], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:47 TP0] Decode batch [100785], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:19:47 TP0] Decode batch [100825], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:19:48 TP0] Decode batch [100865], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:19:49 TP0] Decode batch [100905], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:50 TP0] Decode batch [100945], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:51 TP0] Decode batch [100985], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:52 TP0] Decode batch [101025], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:19:52 TP0] Decode batch [101065], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:53 TP0] Decode batch [101105], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:54 TP0] Decode batch [101145], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:55 TP0] Decode batch [101185], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:56 TP0] Decode batch [101225], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:56 TP0] Decode batch [101265], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:19:57 TP0] Decode batch [101305], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:58 TP0] Decode batch [101345], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:19:59 TP0] Decode batch [101385], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:19:59] INFO:     127.0.0.1:39294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:19:59 TP0] Prefill batch [101407], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:20:00 TP0] Decode batch [101426], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-24 16:20:01 TP0] Decode batch [101466], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:20:01 TP0] Decode batch [101506], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:20:02 TP0] Decode batch [101546], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:20:03 TP0] Decode batch [101586], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:20:04 TP0] Decode batch [101626], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:20:05 TP0] Decode batch [101666], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:20:06 TP0] Decode batch [101706], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:20:06 TP0] Decode batch [101746], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:20:07 TP0] Decode batch [101786], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:20:08 TP0] Decode batch [101826], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:20:09 TP0] Decode batch [101866], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:20:10 TP0] Decode batch [101906], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:20:10 TP0] Decode batch [101946], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:20:11 TP0] Decode batch [101986], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:20:12 TP0] Decode batch [102026], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:20:13 TP0] Decode batch [102066], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:20:14 TP0] Decode batch [102106], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:20:15 TP0] Decode batch [102146], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:20:15 TP0] Decode batch [102186], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:20:16] INFO:     127.0.0.1:49034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:20:16 TP0] Prefill batch [102208], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:20:16 TP0] Decode batch [102227], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-24 16:20:17 TP0] Decode batch [102267], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:20:18 TP0] Decode batch [102307], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:20:19 TP0] Decode batch [102347], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:20:20 TP0] Decode batch [102387], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:20:20 TP0] Decode batch [102427], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:20:21 TP0] Decode batch [102467], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:20:22 TP0] Decode batch [102507], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:20:23 TP0] Decode batch [102547], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:20:24 TP0] Decode batch [102587], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:20:25 TP0] Decode batch [102627], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:20:25 TP0] Decode batch [102667], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:20:26 TP0] Decode batch [102707], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:20:27 TP0] Decode batch [102747], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:20:28 TP0] Decode batch [102787], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:20:29 TP0] Decode batch [102827], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:20:29 TP0] Decode batch [102867], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:20:30 TP0] Decode batch [102907], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:20:31 TP0] Decode batch [102947], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:20:32 TP0] Decode batch [102987], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:20:32] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:20:32 TP0] Prefill batch [103009], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:20:33 TP0] Decode batch [103028], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:20:34 TP0] Decode batch [103068], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:20:34 TP0] Decode batch [103108], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:20:35 TP0] Decode batch [103148], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:20:36 TP0] Decode batch [103188], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:20:37 TP0] Decode batch [103228], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:20:38 TP0] Decode batch [103268], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:20:39 TP0] Decode batch [103308], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:20:39 TP0] Decode batch [103348], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:20:40 TP0] Decode batch [103388], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:20:41 TP0] Decode batch [103428], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:20:42 TP0] Decode batch [103468], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:20:43 TP0] Decode batch [103508], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:20:43 TP0] Decode batch [103548], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:20:44 TP0] Decode batch [103588], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:20:45 TP0] Decode batch [103628], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:20:46 TP0] Decode batch [103668], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:20:47 TP0] Decode batch [103708], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:20:48 TP0] Decode batch [103748], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:20:48 TP0] Decode batch [103788], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:20:49] INFO:     127.0.0.1:42886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:20:49 TP0] Prefill batch [103810], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:20:49 TP0] Decode batch [103829], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:20:50 TP0] Decode batch [103869], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:20:51 TP0] Decode batch [103909], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:20:52 TP0] Decode batch [103949], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:20:53 TP0] Decode batch [103989], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:20:53 TP0] Decode batch [104029], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:20:54 TP0] Decode batch [104069], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:20:55 TP0] Decode batch [104109], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:20:56 TP0] Decode batch [104149], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:20:57 TP0] Decode batch [104189], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:20:57 TP0] Decode batch [104229], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:20:58 TP0] Decode batch [104269], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:20:59 TP0] Decode batch [104309], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:21:00 TP0] Decode batch [104349], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:21:01 TP0] Decode batch [104389], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:21:02 TP0] Decode batch [104429], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:21:02 TP0] Decode batch [104469], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:21:03 TP0] Decode batch [104509], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:21:04 TP0] Decode batch [104549], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:21:05 TP0] Decode batch [104589], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:21:05] INFO:     127.0.0.1:53680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:21:05 TP0] Prefill batch [104611], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:21:06 TP0] Decode batch [104630], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.96, #queue-req: 0, 
[2025-10-24 16:21:07 TP0] Decode batch [104670], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:21:07 TP0] Decode batch [104710], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:21:08 TP0] Decode batch [104750], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:21:09 TP0] Decode batch [104790], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:21:10 TP0] Decode batch [104830], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:21:11 TP0] Decode batch [104870], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:21:12 TP0] Decode batch [104910], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:21:12 TP0] Decode batch [104950], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:21:13 TP0] Decode batch [104990], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:21:14 TP0] Decode batch [105030], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:21:15 TP0] Decode batch [105070], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:21:16 TP0] Decode batch [105110], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:21:16 TP0] Decode batch [105150], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:21:17 TP0] Decode batch [105190], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:21:18 TP0] Decode batch [105230], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:21:19 TP0] Decode batch [105270], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:21:20 TP0] Decode batch [105310], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:21:21 TP0] Decode batch [105350], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:21:21 TP0] Decode batch [105390], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:21:22] INFO:     127.0.0.1:47166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:21:22 TP0] Prefill batch [105412], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:21:22 TP0] Decode batch [105431], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-24 16:21:23 TP0] Decode batch [105471], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:21:24 TP0] Decode batch [105511], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:21:25 TP0] Decode batch [105551], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:21:26 TP0] Decode batch [105591], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:21:26 TP0] Decode batch [105631], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:21:27 TP0] Decode batch [105671], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:21:28 TP0] Decode batch [105711], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:21:29 TP0] Decode batch [105751], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:21:30 TP0] Decode batch [105791], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:21:31 TP0] Decode batch [105831], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:21:31 TP0] Decode batch [105871], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:21:32 TP0] Decode batch [105911], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:21:33 TP0] Decode batch [105951], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:21:34 TP0] Decode batch [105991], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:21:35 TP0] Decode batch [106031], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:21:35 TP0] Decode batch [106071], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:21:36 TP0] Decode batch [106111], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:21:37 TP0] Decode batch [106151], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:21:38 TP0] Decode batch [106191], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:21:38] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:21:38 TP0] Prefill batch [106213], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:21:39 TP0] Decode batch [106232], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-24 16:21:40 TP0] Decode batch [106272], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:21:40 TP0] Decode batch [106312], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:21:41 TP0] Decode batch [106352], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:21:42 TP0] Decode batch [106392], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:21:43 TP0] Decode batch [106432], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:21:44 TP0] Decode batch [106472], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:21:45 TP0] Decode batch [106512], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.13, #queue-req: 0, 
[2025-10-24 16:21:45 TP0] Decode batch [106552], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:21:46 TP0] Decode batch [106592], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:21:47 TP0] Decode batch [106632], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:21:48 TP0] Decode batch [106672], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:21:49 TP0] Decode batch [106712], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:21:50 TP0] Decode batch [106752], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.60, #queue-req: 0, 
[2025-10-24 16:21:50 TP0] Decode batch [106792], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:21:51 TP0] Decode batch [106832], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.59, #queue-req: 0, 
[2025-10-24 16:21:52 TP0] Decode batch [106872], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.60, #queue-req: 0, 
[2025-10-24 16:21:53 TP0] Decode batch [106912], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.60, #queue-req: 0, 
[2025-10-24 16:21:54 TP0] Decode batch [106952], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-10-24 16:21:54 TP0] Decode batch [106992], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.59, #queue-req: 0, 
[2025-10-24 16:21:55] INFO:     127.0.0.1:52536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:21:55 TP0] Prefill batch [107014], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:21:55 TP0] Decode batch [107033], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-24 16:21:56 TP0] Decode batch [107073], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:21:57 TP0] Decode batch [107113], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:21:58 TP0] Decode batch [107153], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:21:59 TP0] Decode batch [107193], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:21:59 TP0] Decode batch [107233], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:22:00 TP0] Decode batch [107273], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:22:01 TP0] Decode batch [107313], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:22:02 TP0] Decode batch [107353], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:22:03 TP0] Decode batch [107393], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:22:04 TP0] Decode batch [107433], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:22:04 TP0] Decode batch [107473], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:22:05 TP0] Decode batch [107513], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:22:06 TP0] Decode batch [107553], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:22:07 TP0] Decode batch [107593], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:22:08 TP0] Decode batch [107633], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:22:09 TP0] Decode batch [107673], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.63, #queue-req: 0, 
[2025-10-24 16:22:09 TP0] Decode batch [107713], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:22:10 TP0] Decode batch [107753], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:22:11 TP0] Decode batch [107793], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:22:11] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:22:11 TP0] Prefill batch [107815], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:22:12 TP0] Decode batch [107834], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-10-24 16:22:13 TP0] Decode batch [107874], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:22:14 TP0] Decode batch [107914], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:22:14 TP0] Decode batch [107954], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:22:15 TP0] Decode batch [107994], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:22:16 TP0] Decode batch [108034], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.63, #queue-req: 0, 
[2025-10-24 16:22:17 TP0] Decode batch [108074], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.63, #queue-req: 0, 
[2025-10-24 16:22:18 TP0] Decode batch [108114], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:22:18 TP0] Decode batch [108154], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:22:19 TP0] Decode batch [108194], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:22:20 TP0] Decode batch [108234], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.63, #queue-req: 0, 
[2025-10-24 16:22:21 TP0] Decode batch [108274], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:22:22 TP0] Decode batch [108314], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.60, #queue-req: 0, 
[2025-10-24 16:22:23 TP0] Decode batch [108354], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-10-24 16:22:23 TP0] Decode batch [108394], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-10-24 16:22:24 TP0] Decode batch [108434], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.59, #queue-req: 0, 
[2025-10-24 16:22:25 TP0] Decode batch [108474], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.57, #queue-req: 0, 
[2025-10-24 16:22:26 TP0] Decode batch [108514], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.59, #queue-req: 0, 
[2025-10-24 16:22:27 TP0] Decode batch [108554], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.57, #queue-req: 0, 
[2025-10-24 16:22:28 TP0] Decode batch [108594], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-10-24 16:22:28] INFO:     127.0.0.1:44656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:22:28 TP0] Prefill batch [108616], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:22:28 TP0] Decode batch [108635], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-24 16:22:29 TP0] Decode batch [108675], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:22:30 TP0] Decode batch [108715], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:22:31 TP0] Decode batch [108755], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:22:32 TP0] Decode batch [108795], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:22:33 TP0] Decode batch [108835], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:22:33 TP0] Decode batch [108875], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:22:34 TP0] Decode batch [108915], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:22:35 TP0] Decode batch [108955], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:22:36 TP0] Decode batch [108995], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:22:37 TP0] Decode batch [109035], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:22:37 TP0] Decode batch [109075], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:22:38 TP0] Decode batch [109115], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:22:39 TP0] Decode batch [109155], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:22:40 TP0] Decode batch [109195], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:22:41 TP0] Decode batch [109235], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:22:42 TP0] Decode batch [109275], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:22:42 TP0] Decode batch [109315], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:22:43 TP0] Decode batch [109355], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:22:44 TP0] Decode batch [109395], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:22:44] INFO:     127.0.0.1:41326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:22:44 TP0] Prefill batch [109417], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:22:45 TP0] Decode batch [109436], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-24 16:22:46 TP0] Decode batch [109476], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:22:47 TP0] Decode batch [109516], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:22:47 TP0] Decode batch [109556], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:22:48 TP0] Decode batch [109596], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:22:49 TP0] Decode batch [109636], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:22:50 TP0] Decode batch [109676], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:22:51 TP0] Decode batch [109716], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:22:51 TP0] Decode batch [109756], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:22:52 TP0] Decode batch [109796], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:22:53 TP0] Decode batch [109836], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:22:54 TP0] Decode batch [109876], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:22:55 TP0] Decode batch [109916], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:22:56 TP0] Decode batch [109956], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:22:56 TP0] Decode batch [109996], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:22:57 TP0] Decode batch [110036], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:22:58 TP0] Decode batch [110076], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:22:59 TP0] Decode batch [110116], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:23:00 TP0] Decode batch [110156], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:23:01 TP0] Decode batch [110196], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:23:01] INFO:     127.0.0.1:46748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:23:01 TP0] Prefill batch [110218], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:23:01 TP0] Decode batch [110237], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:23:02 TP0] Decode batch [110277], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:03 TP0] Decode batch [110317], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:04 TP0] Decode batch [110357], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:23:05 TP0] Decode batch [110397], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:06 TP0] Decode batch [110437], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:06 TP0] Decode batch [110477], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:07 TP0] Decode batch [110517], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:23:08 TP0] Decode batch [110557], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:09 TP0] Decode batch [110597], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:23:10 TP0] Decode batch [110637], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:10 TP0] Decode batch [110677], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:23:11 TP0] Decode batch [110717], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:23:12 TP0] Decode batch [110757], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:23:13 TP0] Decode batch [110797], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:23:14 TP0] Decode batch [110837], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:23:15 TP0] Decode batch [110877], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:23:15 TP0] Decode batch [110917], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:23:16 TP0] Decode batch [110957], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:23:17 TP0] Decode batch [110997], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:23:17] INFO:     127.0.0.1:36844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:23:17 TP0] Prefill batch [111019], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:23:18 TP0] Decode batch [111038], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:23:19 TP0] Decode batch [111078], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:23:20 TP0] Decode batch [111118], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:23:20 TP0] Decode batch [111158], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:23:21 TP0] Decode batch [111198], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:23:22 TP0] Decode batch [111238], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:23:23 TP0] Decode batch [111278], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:23:24 TP0] Decode batch [111318], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:23:24 TP0] Decode batch [111358], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:23:25 TP0] Decode batch [111398], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:23:26 TP0] Decode batch [111438], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:27 TP0] Decode batch [111478], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:28 TP0] Decode batch [111518], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:23:29 TP0] Decode batch [111558], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:23:29 TP0] Decode batch [111598], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:30 TP0] Decode batch [111638], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:23:31 TP0] Decode batch [111678], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:23:32 TP0] Decode batch [111718], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:33 TP0] Decode batch [111758], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:23:34 TP0] Decode batch [111798], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:34] INFO:     127.0.0.1:39750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:23:34 TP0] Prefill batch [111820], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:23:34 TP0] Decode batch [111839], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-10-24 16:23:35 TP0] Decode batch [111879], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-24 16:23:36 TP0] Decode batch [111919], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-24 16:23:37 TP0] Decode batch [111959], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-24 16:23:38 TP0] Decode batch [111999], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:23:39 TP0] Decode batch [112039], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:23:39 TP0] Decode batch [112079], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:23:40 TP0] Decode batch [112119], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:23:41 TP0] Decode batch [112159], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:23:42 TP0] Decode batch [112199], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:23:43 TP0] Decode batch [112239], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:43 TP0] Decode batch [112279], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:44 TP0] Decode batch [112319], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:45 TP0] Decode batch [112359], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:46 TP0] Decode batch [112399], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:23:47 TP0] Decode batch [112439], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:23:48 TP0] Decode batch [112479], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:23:48 TP0] Decode batch [112519], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:49 TP0] Decode batch [112559], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:23:50 TP0] Decode batch [112599], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:50] INFO:     127.0.0.1:37320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:23:50 TP0] Prefill batch [112621], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:23:51 TP0] Decode batch [112640], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:23:52 TP0] Decode batch [112680], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:23:53 TP0] Decode batch [112720], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:23:53 TP0] Decode batch [112760], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:23:54 TP0] Decode batch [112800], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:23:55 TP0] Decode batch [112840], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:56 TP0] Decode batch [112880], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:23:57 TP0] Decode batch [112920], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:23:57 TP0] Decode batch [112960], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:58 TP0] Decode batch [113000], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:23:59 TP0] Decode batch [113040], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:24:00 TP0] Decode batch [113080], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:24:01 TP0] Decode batch [113120], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:24:02 TP0] Decode batch [113160], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:24:02 TP0] Decode batch [113200], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:24:03 TP0] Decode batch [113240], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:24:04 TP0] Decode batch [113280], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:24:05 TP0] Decode batch [113320], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:24:06 TP0] Decode batch [113360], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:24:06 TP0] Decode batch [113400], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:24:07] INFO:     127.0.0.1:40534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:24:07 TP0] Prefill batch [113422], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:24:07 TP0] Decode batch [113441], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-10-24 16:24:08 TP0] Decode batch [113481], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:24:09 TP0] Decode batch [113521], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:24:10 TP0] Decode batch [113561], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:24:11 TP0] Decode batch [113601], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:24:11 TP0] Decode batch [113641], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:24:12 TP0] Decode batch [113681], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:24:13 TP0] Decode batch [113721], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:24:14 TP0] Decode batch [113761], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:24:15 TP0] Decode batch [113801], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:24:16 TP0] Decode batch [113841], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:24:16 TP0] Decode batch [113881], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:24:17 TP0] Decode batch [113921], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:18 TP0] Decode batch [113961], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:19 TP0] Decode batch [114001], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:24:20 TP0] Decode batch [114041], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:20 TP0] Decode batch [114081], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:24:21 TP0] Decode batch [114121], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:22 TP0] Decode batch [114161], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:24:23 TP0] Decode batch [114201], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:23] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:24:23 TP0] Prefill batch [114223], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:24:24 TP0] Decode batch [114242], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-24 16:24:25 TP0] Decode batch [114282], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:24:26 TP0] Decode batch [114322], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:24:26 TP0] Decode batch [114362], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:27 TP0] Decode batch [114402], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:24:28 TP0] Decode batch [114442], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:29 TP0] Decode batch [114482], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:24:30 TP0] Decode batch [114522], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:24:30 TP0] Decode batch [114562], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:31 TP0] Decode batch [114602], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:32 TP0] Decode batch [114642], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:24:33 TP0] Decode batch [114682], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:24:34 TP0] Decode batch [114722], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:35 TP0] Decode batch [114762], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:24:35 TP0] Decode batch [114802], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:36 TP0] Decode batch [114842], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:24:37 TP0] Decode batch [114882], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:24:38 TP0] Decode batch [114922], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:24:39 TP0] Decode batch [114962], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.24, #queue-req: 0, 
[2025-10-24 16:24:39 TP0] Decode batch [115002], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:24:40] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:24:40 TP0] Prefill batch [115024], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:24:40 TP0] Decode batch [115043], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-10-24 16:24:41 TP0] Decode batch [115083], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:24:42 TP0] Decode batch [115123], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:24:43 TP0] Decode batch [115163], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:24:44 TP0] Decode batch [115203], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:24:44 TP0] Decode batch [115243], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:24:45 TP0] Decode batch [115283], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:24:46 TP0] Decode batch [115323], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:24:47 TP0] Decode batch [115363], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:48 TP0] Decode batch [115403], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:24:49 TP0] Decode batch [115443], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:49 TP0] Decode batch [115483], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:50 TP0] Decode batch [115523], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:51 TP0] Decode batch [115563], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:24:52 TP0] Decode batch [115603], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:24:53 TP0] Decode batch [115643], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:24:53 TP0] Decode batch [115683], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:24:54 TP0] Decode batch [115723], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:24:55 TP0] Decode batch [115763], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:24:56 TP0] Decode batch [115803], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:24:56] INFO:     127.0.0.1:38900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:24:56 TP0] Prefill batch [115825], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:24:57 TP0] Decode batch [115844], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:24:58 TP0] Decode batch [115884], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-24 16:24:58 TP0] Decode batch [115924], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:24:59 TP0] Decode batch [115964], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:25:00 TP0] Decode batch [116004], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:25:01 TP0] Decode batch [116044], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:25:02 TP0] Decode batch [116084], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:25:03 TP0] Decode batch [116124], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:25:03 TP0] Decode batch [116164], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:25:04 TP0] Decode batch [116204], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:25:05 TP0] Decode batch [116244], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:25:06 TP0] Decode batch [116284], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:25:07 TP0] Decode batch [116324], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:25:08 TP0] Decode batch [116364], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:25:08 TP0] Decode batch [116404], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:25:09 TP0] Decode batch [116444], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:25:10 TP0] Decode batch [116484], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:25:11 TP0] Decode batch [116524], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:25:12 TP0] Decode batch [116564], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:25:12 TP0] Decode batch [116604], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:25:13] INFO:     127.0.0.1:53652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:25:13 TP0] Prefill batch [116626], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:25:13 TP0] Decode batch [116645], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-24 16:25:14 TP0] Decode batch [116685], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:25:15 TP0] Decode batch [116725], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:25:16 TP0] Decode batch [116765], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:25:17 TP0] Decode batch [116805], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:25:17 TP0] Decode batch [116845], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:25:18 TP0] Decode batch [116885], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:25:19 TP0] Decode batch [116925], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:25:20 TP0] Decode batch [116965], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:25:21 TP0] Decode batch [117005], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:25:22 TP0] Decode batch [117045], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:25:22 TP0] Decode batch [117085], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:25:23 TP0] Decode batch [117125], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:25:24 TP0] Decode batch [117165], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:25:25 TP0] Decode batch [117205], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:25:26 TP0] Decode batch [117245], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:25:26 TP0] Decode batch [117285], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:25:27 TP0] Decode batch [117325], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:25:28 TP0] Decode batch [117365], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:25:29 TP0] Decode batch [117405], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:25:29] INFO:     127.0.0.1:36058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:25:29 TP0] Prefill batch [117427], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:25:30 TP0] Decode batch [117446], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-24 16:25:31 TP0] Decode batch [117486], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:25:31 TP0] Decode batch [117526], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:25:32 TP0] Decode batch [117566], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:25:33 TP0] Decode batch [117606], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:25:34 TP0] Decode batch [117646], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:25:35 TP0] Decode batch [117686], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:25:36 TP0] Decode batch [117726], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:25:36 TP0] Decode batch [117766], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:25:37 TP0] Decode batch [117806], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:25:38 TP0] Decode batch [117846], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:25:39 TP0] Decode batch [117886], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:25:40 TP0] Decode batch [117926], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:25:41 TP0] Decode batch [117966], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:25:41 TP0] Decode batch [118006], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:25:42 TP0] Decode batch [118046], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:25:43 TP0] Decode batch [118086], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:25:44 TP0] Decode batch [118126], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:25:45 TP0] Decode batch [118166], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:25:45 TP0] Decode batch [118206], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:25:46] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:25:46 TP0] Prefill batch [118228], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:25:46 TP0] Decode batch [118247], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.16, #queue-req: 0, 
[2025-10-24 16:25:47 TP0] Decode batch [118287], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:25:48 TP0] Decode batch [118327], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:25:49 TP0] Decode batch [118367], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:25:50 TP0] Decode batch [118407], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:25:50 TP0] Decode batch [118447], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:25:51 TP0] Decode batch [118487], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:25:52 TP0] Decode batch [118527], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:25:53 TP0] Decode batch [118567], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:25:54 TP0] Decode batch [118607], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:25:55 TP0] Decode batch [118647], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:25:55 TP0] Decode batch [118687], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:25:56 TP0] Decode batch [118727], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:25:57 TP0] Decode batch [118767], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:25:58 TP0] Decode batch [118807], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:25:59 TP0] Decode batch [118847], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:25:59 TP0] Decode batch [118887], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:26:00 TP0] Decode batch [118927], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:26:01 TP0] Decode batch [118967], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:26:02 TP0] Decode batch [119007], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:26:02] INFO:     127.0.0.1:51588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:26:02 TP0] Prefill batch [119029], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:26:03 TP0] Decode batch [119048], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-24 16:26:04 TP0] Decode batch [119088], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:26:04 TP0] Decode batch [119128], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:26:05 TP0] Decode batch [119168], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:26:06 TP0] Decode batch [119208], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:26:07 TP0] Decode batch [119248], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:26:08 TP0] Decode batch [119288], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:26:09 TP0] Decode batch [119328], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:26:09 TP0] Decode batch [119368], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:26:10 TP0] Decode batch [119408], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:26:11 TP0] Decode batch [119448], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:26:12 TP0] Decode batch [119488], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:26:13 TP0] Decode batch [119528], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:26:14 TP0] Decode batch [119568], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:26:14 TP0] Decode batch [119608], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:26:15 TP0] Decode batch [119648], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:26:16 TP0] Decode batch [119688], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:26:17 TP0] Decode batch [119728], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:26:18 TP0] Decode batch [119768], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:26:18 TP0] Decode batch [119808], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:26:19] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:26:19 TP0] Prefill batch [119830], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:26:19 TP0] Decode batch [119849], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:26:20 TP0] Decode batch [119889], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:26:21 TP0] Decode batch [119929], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:26:22 TP0] Decode batch [119969], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:26:23 TP0] Decode batch [120009], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:26:23 TP0] Decode batch [120049], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:26:24 TP0] Decode batch [120089], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:26:25 TP0] Decode batch [120129], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:26:26 TP0] Decode batch [120169], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:26:27 TP0] Decode batch [120209], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:26:28 TP0] Decode batch [120249], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:26:28 TP0] Decode batch [120289], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.60, #queue-req: 0, 
[2025-10-24 16:26:29 TP0] Decode batch [120329], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.59, #queue-req: 0, 
[2025-10-24 16:26:30 TP0] Decode batch [120369], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.59, #queue-req: 0, 
[2025-10-24 16:26:31 TP0] Decode batch [120409], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-10-24 16:26:32 TP0] Decode batch [120449], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-10-24 16:26:33 TP0] Decode batch [120489], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.59, #queue-req: 0, 
[2025-10-24 16:26:33 TP0] Decode batch [120529], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-10-24 16:26:34 TP0] Decode batch [120569], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.57, #queue-req: 0, 
[2025-10-24 16:26:35 TP0] Decode batch [120609], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.59, #queue-req: 0, 
[2025-10-24 16:26:35] INFO:     127.0.0.1:48228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:26:35 TP0] Prefill batch [120631], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:26:36 TP0] Decode batch [120650], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-24 16:26:37 TP0] Decode batch [120690], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:26:38 TP0] Decode batch [120730], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:26:38 TP0] Decode batch [120770], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:26:39 TP0] Decode batch [120810], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:26:40 TP0] Decode batch [120850], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:26:41 TP0] Decode batch [120890], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:26:42 TP0] Decode batch [120930], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:26:42 TP0] Decode batch [120970], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:26:43 TP0] Decode batch [121010], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:26:44 TP0] Decode batch [121050], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:26:45 TP0] Decode batch [121090], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:26:46 TP0] Decode batch [121130], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:26:47 TP0] Decode batch [121170], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:26:47 TP0] Decode batch [121210], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:26:48 TP0] Decode batch [121250], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:26:49 TP0] Decode batch [121290], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:26:50 TP0] Decode batch [121330], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:26:51 TP0] Decode batch [121370], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:26:51 TP0] Decode batch [121410], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:26:52] INFO:     127.0.0.1:60814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:26:52 TP0] Prefill batch [121432], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:26:52 TP0] Decode batch [121451], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-24 16:26:53 TP0] Decode batch [121491], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:26:54 TP0] Decode batch [121531], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:26:55 TP0] Decode batch [121571], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:26:56 TP0] Decode batch [121611], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:26:56 TP0] Decode batch [121651], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:26:57 TP0] Decode batch [121691], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:26:58 TP0] Decode batch [121731], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:26:59 TP0] Decode batch [121771], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:27:00 TP0] Decode batch [121811], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:27:01 TP0] Decode batch [121851], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:27:01 TP0] Decode batch [121891], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:27:02 TP0] Decode batch [121931], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.17, #queue-req: 0, 
[2025-10-24 16:27:03 TP0] Decode batch [121971], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:27:04 TP0] Decode batch [122011], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:27:05 TP0] Decode batch [122051], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:27:06 TP0] Decode batch [122091], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:27:06 TP0] Decode batch [122131], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:27:07 TP0] Decode batch [122171], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:27:08 TP0] Decode batch [122211], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:27:08] INFO:     127.0.0.1:40318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:27:08 TP0] Prefill batch [122233], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:27:09 TP0] Decode batch [122252], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-24 16:27:10 TP0] Decode batch [122292], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:27:11 TP0] Decode batch [122332], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:27:11 TP0] Decode batch [122372], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:27:12 TP0] Decode batch [122412], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:27:13 TP0] Decode batch [122452], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:27:14 TP0] Decode batch [122492], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:27:15 TP0] Decode batch [122532], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:27:15 TP0] Decode batch [122572], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:27:16 TP0] Decode batch [122612], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:27:17 TP0] Decode batch [122652], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-10-24 16:27:18 TP0] Decode batch [122692], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.56, #queue-req: 0, 
[2025-10-24 16:27:19 TP0] Decode batch [122732], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.57, #queue-req: 0, 
[2025-10-24 16:27:20 TP0] Decode batch [122772], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-10-24 16:27:20 TP0] Decode batch [122812], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.56, #queue-req: 0, 
[2025-10-24 16:27:21 TP0] Decode batch [122852], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.56, #queue-req: 0, 
[2025-10-24 16:27:22 TP0] Decode batch [122892], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.56, #queue-req: 0, 
[2025-10-24 16:27:23 TP0] Decode batch [122932], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.57, #queue-req: 0, 
[2025-10-24 16:27:24 TP0] Decode batch [122972], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.55, #queue-req: 0, 
[2025-10-24 16:27:25 TP0] Decode batch [123012], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.56, #queue-req: 0, 
[2025-10-24 16:27:25] INFO:     127.0.0.1:42188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:27:25 TP0] Prefill batch [123034], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:27:25 TP0] Decode batch [123053], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-24 16:27:26 TP0] Decode batch [123093], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:27:27 TP0] Decode batch [123133], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:27:28 TP0] Decode batch [123173], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:27:29 TP0] Decode batch [123213], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:27:30 TP0] Decode batch [123253], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:27:30 TP0] Decode batch [123293], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:27:31 TP0] Decode batch [123333], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:27:32 TP0] Decode batch [123373], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:27:33 TP0] Decode batch [123413], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:27:34 TP0] Decode batch [123453], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:27:34 TP0] Decode batch [123493], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:27:35 TP0] Decode batch [123533], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:27:36 TP0] Decode batch [123573], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:27:37 TP0] Decode batch [123613], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:27:38 TP0] Decode batch [123653], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:27:39 TP0] Decode batch [123693], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:27:39 TP0] Decode batch [123733], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:27:40 TP0] Decode batch [123773], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:27:41 TP0] Decode batch [123813], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:27:41] INFO:     127.0.0.1:57144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:27:41 TP0] Prefill batch [123835], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:27:42 TP0] Decode batch [123854], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.04, #queue-req: 0, 
[2025-10-24 16:27:43 TP0] Decode batch [123894], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:27:44 TP0] Decode batch [123934], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:27:44 TP0] Decode batch [123974], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:27:45 TP0] Decode batch [124014], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:27:46 TP0] Decode batch [124054], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:27:47 TP0] Decode batch [124094], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:27:48 TP0] Decode batch [124134], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:27:48 TP0] Decode batch [124174], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:27:49 TP0] Decode batch [124214], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:27:50 TP0] Decode batch [124254], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:27:51 TP0] Decode batch [124294], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:27:52 TP0] Decode batch [124334], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:27:53 TP0] Decode batch [124374], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:27:53 TP0] Decode batch [124414], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:27:54 TP0] Decode batch [124454], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:27:55 TP0] Decode batch [124494], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:27:56 TP0] Decode batch [124534], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:27:57 TP0] Decode batch [124574], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:27:57 TP0] Decode batch [124614], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:27:58] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:27:58 TP0] Prefill batch [124636], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:27:58 TP0] Decode batch [124655], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-24 16:27:59 TP0] Decode batch [124695], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:28:00 TP0] Decode batch [124735], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:28:01 TP0] Decode batch [124775], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:28:02 TP0] Decode batch [124815], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:28:02 TP0] Decode batch [124855], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:28:03 TP0] Decode batch [124895], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:28:04 TP0] Decode batch [124935], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:28:05 TP0] Decode batch [124975], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:06 TP0] Decode batch [125015], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:28:07 TP0] Decode batch [125055], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:07 TP0] Decode batch [125095], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:28:08 TP0] Decode batch [125135], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:09 TP0] Decode batch [125175], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:28:10 TP0] Decode batch [125215], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:28:11 TP0] Decode batch [125255], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:12 TP0] Decode batch [125295], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:28:12 TP0] Decode batch [125335], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:13 TP0] Decode batch [125375], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:28:14 TP0] Decode batch [125415], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:28:14] INFO:     127.0.0.1:41848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:28:14 TP0] Prefill batch [125437], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:28:15 TP0] Decode batch [125456], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:28:16 TP0] Decode batch [125496], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-24 16:28:17 TP0] Decode batch [125536], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:28:17 TP0] Decode batch [125576], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:28:18 TP0] Decode batch [125616], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:28:19 TP0] Decode batch [125656], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:28:20 TP0] Decode batch [125696], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:28:21 TP0] Decode batch [125736], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:28:21 TP0] Decode batch [125776], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:28:22 TP0] Decode batch [125816], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:28:23 TP0] Decode batch [125856], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:28:24 TP0] Decode batch [125896], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:28:25 TP0] Decode batch [125936], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:28:26 TP0] Decode batch [125976], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:26 TP0] Decode batch [126016], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:28:27 TP0] Decode batch [126056], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:28 TP0] Decode batch [126096], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:29 TP0] Decode batch [126136], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:30 TP0] Decode batch [126176], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:28:30 TP0] Decode batch [126216], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:31] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:28:31 TP0] Prefill batch [126238], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:28:31 TP0] Decode batch [126257], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-10-24 16:28:32 TP0] Decode batch [126297], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:33 TP0] Decode batch [126337], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:28:34 TP0] Decode batch [126377], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:35 TP0] Decode batch [126417], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:35 TP0] Decode batch [126457], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:28:36 TP0] Decode batch [126497], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:28:37 TP0] Decode batch [126537], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:28:38 TP0] Decode batch [126577], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:28:39 TP0] Decode batch [126617], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:28:40 TP0] Decode batch [126657], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:28:40 TP0] Decode batch [126697], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:28:41 TP0] Decode batch [126737], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:28:42 TP0] Decode batch [126777], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:28:43 TP0] Decode batch [126817], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:28:44 TP0] Decode batch [126857], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:28:45 TP0] Decode batch [126897], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:28:45 TP0] Decode batch [126937], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:28:46 TP0] Decode batch [126977], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:28:47 TP0] Decode batch [127017], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:28:47] INFO:     127.0.0.1:36758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:28:47 TP0] Prefill batch [127039], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:28:48 TP0] Decode batch [127058], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-24 16:28:49 TP0] Decode batch [127098], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:28:50 TP0] Decode batch [127138], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:28:50 TP0] Decode batch [127178], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:28:51 TP0] Decode batch [127218], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:28:52 TP0] Decode batch [127258], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:28:53 TP0] Decode batch [127298], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:28:54 TP0] Decode batch [127338], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:28:54 TP0] Decode batch [127378], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:28:55 TP0] Decode batch [127418], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:28:56 TP0] Decode batch [127458], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:28:57 TP0] Decode batch [127498], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:28:58 TP0] Decode batch [127538], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:28:59 TP0] Decode batch [127578], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:28:59 TP0] Decode batch [127618], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:29:00 TP0] Decode batch [127658], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:29:01 TP0] Decode batch [127698], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:29:02 TP0] Decode batch [127738], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:29:03 TP0] Decode batch [127778], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:29:03 TP0] Decode batch [127818], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:29:04] INFO:     127.0.0.1:40698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:29:04 TP0] Prefill batch [127840], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:29:04 TP0] Decode batch [127859], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-24 16:29:05 TP0] Decode batch [127899], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:29:06 TP0] Decode batch [127939], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:29:07 TP0] Decode batch [127979], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:29:08 TP0] Decode batch [128019], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:29:08 TP0] Decode batch [128059], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:29:09 TP0] Decode batch [128099], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:29:10 TP0] Decode batch [128139], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:29:11 TP0] Decode batch [128179], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:29:12 TP0] Decode batch [128219], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:29:13 TP0] Decode batch [128259], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:29:13 TP0] Decode batch [128299], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:29:14 TP0] Decode batch [128339], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:29:15 TP0] Decode batch [128379], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:29:16 TP0] Decode batch [128419], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:29:17 TP0] Decode batch [128459], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:29:18 TP0] Decode batch [128499], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:29:18 TP0] Decode batch [128539], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:29:19 TP0] Decode batch [128579], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:29:20 TP0] Decode batch [128619], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:29:20] INFO:     127.0.0.1:42036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:29:20 TP0] Prefill batch [128641], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:29:21 TP0] Decode batch [128660], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-24 16:29:22 TP0] Decode batch [128700], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:29:23 TP0] Decode batch [128740], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:29:23 TP0] Decode batch [128780], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:29:24 TP0] Decode batch [128820], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:29:25 TP0] Decode batch [128860], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:29:26 TP0] Decode batch [128900], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:27 TP0] Decode batch [128940], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:27 TP0] Decode batch [128980], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:29:28 TP0] Decode batch [129020], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:29:29 TP0] Decode batch [129060], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:30 TP0] Decode batch [129100], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:29:31 TP0] Decode batch [129140], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:32 TP0] Decode batch [129180], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:32 TP0] Decode batch [129220], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:33 TP0] Decode batch [129260], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:34 TP0] Decode batch [129300], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:29:35 TP0] Decode batch [129340], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:36 TP0] Decode batch [129380], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:36 TP0] Decode batch [129420], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:29:37] INFO:     127.0.0.1:42746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:29:37 TP0] Prefill batch [129442], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:29:37 TP0] Decode batch [129461], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-24 16:29:38 TP0] Decode batch [129501], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:29:39 TP0] Decode batch [129541], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:29:40 TP0] Decode batch [129581], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:29:41 TP0] Decode batch [129621], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:29:41 TP0] Decode batch [129661], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:29:42 TP0] Decode batch [129701], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:29:43 TP0] Decode batch [129741], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:29:44 TP0] Decode batch [129781], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:45 TP0] Decode batch [129821], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:46 TP0] Decode batch [129861], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:46 TP0] Decode batch [129901], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:29:47 TP0] Decode batch [129941], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:48 TP0] Decode batch [129981], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:29:49 TP0] Decode batch [130021], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:29:50 TP0] Decode batch [130061], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:29:51 TP0] Decode batch [130101], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:29:51 TP0] Decode batch [130141], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:29:52 TP0] Decode batch [130181], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:29:53 TP0] Decode batch [130221], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:29:53] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:29:53 TP0] Prefill batch [130243], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:29:54 TP0] Decode batch [130262], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:29:55 TP0] Decode batch [130302], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:29:56 TP0] Decode batch [130342], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:29:56 TP0] Decode batch [130382], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:29:57 TP0] Decode batch [130422], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:29:58 TP0] Decode batch [130462], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:29:59 TP0] Decode batch [130502], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:30:00 TP0] Decode batch [130542], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:30:00 TP0] Decode batch [130582], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:30:01 TP0] Decode batch [130622], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:02 TP0] Decode batch [130662], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:30:03 TP0] Decode batch [130702], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:30:04 TP0] Decode batch [130742], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:30:05 TP0] Decode batch [130782], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:05 TP0] Decode batch [130822], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:06 TP0] Decode batch [130862], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:07 TP0] Decode batch [130902], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:08 TP0] Decode batch [130942], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:09 TP0] Decode batch [130982], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:09 TP0] Decode batch [131022], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:30:10] INFO:     127.0.0.1:40438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:30:10 TP0] Prefill batch [131044], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:30:10 TP0] Decode batch [131063], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-24 16:30:11 TP0] Decode batch [131103], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:30:12 TP0] Decode batch [131143], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:30:13 TP0] Decode batch [131183], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:30:14 TP0] Decode batch [131223], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:14 TP0] Decode batch [131263], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:30:15 TP0] Decode batch [131303], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:30:16 TP0] Decode batch [131343], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:30:17 TP0] Decode batch [131383], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:30:18 TP0] Decode batch [131423], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:30:19 TP0] Decode batch [131463], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:30:19 TP0] Decode batch [131503], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:30:20 TP0] Decode batch [131543], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:30:21 TP0] Decode batch [131583], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:30:22 TP0] Decode batch [131623], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:30:23 TP0] Decode batch [131663], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:30:24 TP0] Decode batch [131703], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:30:24 TP0] Decode batch [131743], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:30:25 TP0] Decode batch [131783], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:30:26 TP0] Decode batch [131823], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:30:26] INFO:     127.0.0.1:41092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:30:26 TP0] Prefill batch [131845], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:30:27 TP0] Decode batch [131864], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-24 16:30:28 TP0] Decode batch [131904], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:30:29 TP0] Decode batch [131944], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:30:29 TP0] Decode batch [131984], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:30:30 TP0] Decode batch [132024], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:30:31 TP0] Decode batch [132064], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:30:32 TP0] Decode batch [132104], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:30:33 TP0] Decode batch [132144], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:30:33 TP0] Decode batch [132184], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:30:34 TP0] Decode batch [132224], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:30:35 TP0] Decode batch [132264], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:30:36 TP0] Decode batch [132304], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:30:37 TP0] Decode batch [132344], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:38 TP0] Decode batch [132384], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:38 TP0] Decode batch [132424], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:30:39 TP0] Decode batch [132464], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:30:40 TP0] Decode batch [132504], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:30:41 TP0] Decode batch [132544], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:30:42 TP0] Decode batch [132584], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:42 TP0] Decode batch [132624], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:43] INFO:     127.0.0.1:46542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:30:43 TP0] Prefill batch [132646], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:30:43 TP0] Decode batch [132665], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-24 16:30:44 TP0] Decode batch [132705], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:30:45 TP0] Decode batch [132745], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:30:46 TP0] Decode batch [132785], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:30:47 TP0] Decode batch [132825], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:47 TP0] Decode batch [132865], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:30:48 TP0] Decode batch [132905], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:30:49 TP0] Decode batch [132945], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:30:50 TP0] Decode batch [132985], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:30:51 TP0] Decode batch [133025], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:30:52 TP0] Decode batch [133065], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:30:52 TP0] Decode batch [133105], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:30:53 TP0] Decode batch [133145], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:30:54 TP0] Decode batch [133185], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:30:55 TP0] Decode batch [133225], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:30:56 TP0] Decode batch [133265], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:30:57 TP0] Decode batch [133305], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:30:57 TP0] Decode batch [133345], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:30:58 TP0] Decode batch [133385], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:30:59 TP0] Decode batch [133425], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:30:59] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:30:59 TP0] Prefill batch [133447], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:31:00 TP0] Decode batch [133466], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:31:01 TP0] Decode batch [133506], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:31:02 TP0] Decode batch [133546], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:31:02 TP0] Decode batch [133586], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:31:03 TP0] Decode batch [133626], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:31:04 TP0] Decode batch [133666], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:31:05 TP0] Decode batch [133706], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:31:06 TP0] Decode batch [133746], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:31:06 TP0] Decode batch [133786], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:07 TP0] Decode batch [133826], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:08 TP0] Decode batch [133866], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:31:09 TP0] Decode batch [133906], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:10 TP0] Decode batch [133946], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:11 TP0] Decode batch [133986], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:11 TP0] Decode batch [134026], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:31:12 TP0] Decode batch [134066], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:31:13 TP0] Decode batch [134106], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:14 TP0] Decode batch [134146], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:15 TP0] Decode batch [134186], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:15 TP0] Decode batch [134226], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:31:16] INFO:     127.0.0.1:40538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:31:16 TP0] Prefill batch [134248], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:31:16 TP0] Decode batch [134267], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-24 16:31:17 TP0] Decode batch [134307], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:31:18 TP0] Decode batch [134347], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:31:19 TP0] Decode batch [134387], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:31:20 TP0] Decode batch [134427], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:20 TP0] Decode batch [134467], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:31:21 TP0] Decode batch [134507], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:22 TP0] Decode batch [134547], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:31:23 TP0] Decode batch [134587], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:24 TP0] Decode batch [134627], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:25 TP0] Decode batch [134667], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:25 TP0] Decode batch [134707], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:26 TP0] Decode batch [134747], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:27 TP0] Decode batch [134787], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:31:28 TP0] Decode batch [134827], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:29 TP0] Decode batch [134867], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:31:30 TP0] Decode batch [134907], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:31:30 TP0] Decode batch [134947], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:31:31 TP0] Decode batch [134987], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:32 TP0] Decode batch [135027], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:31:32] INFO:     127.0.0.1:42320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:31:32 TP0] Prefill batch [135049], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:31:33 TP0] Decode batch [135068], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-24 16:31:34 TP0] Decode batch [135108], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:31:35 TP0] Decode batch [135148], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:31:35 TP0] Decode batch [135188], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:31:36 TP0] Decode batch [135228], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:31:37 TP0] Decode batch [135268], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:31:38 TP0] Decode batch [135308], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:31:39 TP0] Decode batch [135348], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:39 TP0] Decode batch [135388], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:31:40 TP0] Decode batch [135428], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:41 TP0] Decode batch [135468], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:31:42 TP0] Decode batch [135508], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:31:43 TP0] Decode batch [135548], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:44 TP0] Decode batch [135588], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:44 TP0] Decode batch [135628], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:31:45 TP0] Decode batch [135668], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:31:46 TP0] Decode batch [135708], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:47 TP0] Decode batch [135748], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:31:48 TP0] Decode batch [135788], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:48 TP0] Decode batch [135828], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:31:49] INFO:     127.0.0.1:44276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:31:49 TP0] Prefill batch [135850], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:31:49 TP0] Decode batch [135869], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-24 16:31:50 TP0] Decode batch [135909], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:31:51 TP0] Decode batch [135949], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:31:52 TP0] Decode batch [135989], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:31:53 TP0] Decode batch [136029], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:31:53 TP0] Decode batch [136069], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:54 TP0] Decode batch [136109], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:31:55 TP0] Decode batch [136149], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:56 TP0] Decode batch [136189], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:31:57 TP0] Decode batch [136229], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:58 TP0] Decode batch [136269], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:58 TP0] Decode batch [136309], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:31:59 TP0] Decode batch [136349], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:32:00 TP0] Decode batch [136389], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:32:01 TP0] Decode batch [136429], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:32:02 TP0] Decode batch [136469], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:02 TP0] Decode batch [136509], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:32:03 TP0] Decode batch [136549], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:32:04 TP0] Decode batch [136589], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:05 TP0] Decode batch [136629], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:32:05] INFO:     127.0.0.1:49162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:32:05 TP0] Prefill batch [136651], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:32:06 TP0] Decode batch [136670], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-24 16:32:07 TP0] Decode batch [136710], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:32:08 TP0] Decode batch [136750], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:32:08 TP0] Decode batch [136790], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:32:09 TP0] Decode batch [136830], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:32:10 TP0] Decode batch [136870], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:32:11 TP0] Decode batch [136910], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:32:12 TP0] Decode batch [136950], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:32:12 TP0] Decode batch [136990], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:32:13 TP0] Decode batch [137030], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:32:14 TP0] Decode batch [137070], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:32:15 TP0] Decode batch [137110], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:32:16 TP0] Decode batch [137150], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:17 TP0] Decode batch [137190], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:17 TP0] Decode batch [137230], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:18 TP0] Decode batch [137270], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:32:19 TP0] Decode batch [137310], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:20 TP0] Decode batch [137350], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.12, #queue-req: 0, 
[2025-10-24 16:32:21 TP0] Decode batch [137390], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:32:21 TP0] Decode batch [137430], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:32:22] INFO:     127.0.0.1:45178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:32:22 TP0] Prefill batch [137452], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:32:22 TP0] Decode batch [137471], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:32:23 TP0] Decode batch [137511], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:32:24 TP0] Decode batch [137551], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:32:25 TP0] Decode batch [137591], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:32:26 TP0] Decode batch [137631], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:32:26 TP0] Decode batch [137671], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:32:27 TP0] Decode batch [137711], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:32:28 TP0] Decode batch [137751], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:32:29 TP0] Decode batch [137791], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:32:30 TP0] Decode batch [137831], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:32:31 TP0] Decode batch [137871], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:32:31 TP0] Decode batch [137911], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:32:32 TP0] Decode batch [137951], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:32:33 TP0] Decode batch [137991], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:32:34 TP0] Decode batch [138031], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:32:35 TP0] Decode batch [138071], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:36 TP0] Decode batch [138111], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:32:36 TP0] Decode batch [138151], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:32:37 TP0] Decode batch [138191], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:38 TP0] Decode batch [138231], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:32:38] INFO:     127.0.0.1:56284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:32:38 TP0] Prefill batch [138253], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:32:39 TP0] Decode batch [138272], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-24 16:32:40 TP0] Decode batch [138312], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:41 TP0] Decode batch [138352], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:32:41 TP0] Decode batch [138392], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:32:42 TP0] Decode batch [138432], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:43 TP0] Decode batch [138472], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:44 TP0] Decode batch [138512], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:45 TP0] Decode batch [138552], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:45 TP0] Decode batch [138592], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:46 TP0] Decode batch [138632], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:47 TP0] Decode batch [138672], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:32:48 TP0] Decode batch [138712], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:32:49 TP0] Decode batch [138752], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:32:50 TP0] Decode batch [138792], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:32:50 TP0] Decode batch [138832], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:32:51 TP0] Decode batch [138872], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:32:52 TP0] Decode batch [138912], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:32:53 TP0] Decode batch [138952], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:32:54 TP0] Decode batch [138992], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, 
[2025-10-24 16:32:54 TP0] Decode batch [139032], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:32:55] INFO:     127.0.0.1:42034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:32:55 TP0] Prefill batch [139054], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:32:55 TP0] Decode batch [139073], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-24 16:32:56 TP0] Decode batch [139113], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:32:57 TP0] Decode batch [139153], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:32:58 TP0] Decode batch [139193], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:32:59 TP0] Decode batch [139233], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:33:00 TP0] Decode batch [139273], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:33:00 TP0] Decode batch [139313], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:33:01 TP0] Decode batch [139353], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:02 TP0] Decode batch [139393], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:03 TP0] Decode batch [139433], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:04 TP0] Decode batch [139473], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:33:04 TP0] Decode batch [139513], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:33:05 TP0] Decode batch [139553], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:06 TP0] Decode batch [139593], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:07 TP0] Decode batch [139633], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:08 TP0] Decode batch [139673], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:33:09 TP0] Decode batch [139713], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:33:09 TP0] Decode batch [139753], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:10 TP0] Decode batch [139793], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:33:11 TP0] Decode batch [139833], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:33:11] INFO:     127.0.0.1:58958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:33:11 TP0] Prefill batch [139855], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:33:12 TP0] Decode batch [139874], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-24 16:33:13 TP0] Decode batch [139914], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:33:14 TP0] Decode batch [139954], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:33:14 TP0] Decode batch [139994], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:33:15 TP0] Decode batch [140034], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:33:16 TP0] Decode batch [140074], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:33:17 TP0] Decode batch [140114], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:33:18 TP0] Decode batch [140154], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:33:18 TP0] Decode batch [140194], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:33:19 TP0] Decode batch [140234], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:33:20 TP0] Decode batch [140274], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-24 16:33:21 TP0] Decode batch [140314], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-24 16:33:22 TP0] Decode batch [140354], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-24 16:33:23 TP0] Decode batch [140394], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-24 16:33:23 TP0] Decode batch [140434], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-24 16:33:24 TP0] Decode batch [140474], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:33:25 TP0] Decode batch [140514], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:33:26 TP0] Decode batch [140554], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-10-24 16:33:27 TP0] Decode batch [140594], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.59, #queue-req: 0, 
[2025-10-24 16:33:28 TP0] Decode batch [140634], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.59, #queue-req: 0, 
[2025-10-24 16:33:28] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:33:28 TP0] Prefill batch [140656], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:33:28 TP0] Decode batch [140675], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-24 16:33:29 TP0] Decode batch [140715], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:33:30 TP0] Decode batch [140755], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:31 TP0] Decode batch [140795], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:32 TP0] Decode batch [140835], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:33 TP0] Decode batch [140875], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:33:33 TP0] Decode batch [140915], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:34 TP0] Decode batch [140955], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:33:35 TP0] Decode batch [140995], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:33:36 TP0] Decode batch [141035], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:33:37 TP0] Decode batch [141075], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:33:37 TP0] Decode batch [141115], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:33:38 TP0] Decode batch [141155], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:33:39 TP0] Decode batch [141195], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:33:40 TP0] Decode batch [141235], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:33:41 TP0] Decode batch [141275], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:33:42 TP0] Decode batch [141315], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:33:42 TP0] Decode batch [141355], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:33:43 TP0] Decode batch [141395], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:33:44 TP0] Decode batch [141435], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:33:44] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:33:44 TP0] Prefill batch [141457], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:33:45 TP0] Decode batch [141476], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-24 16:33:46 TP0] Decode batch [141516], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:33:47 TP0] Decode batch [141556], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:33:47 TP0] Decode batch [141596], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:33:48 TP0] Decode batch [141636], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:33:49 TP0] Decode batch [141676], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:50 TP0] Decode batch [141716], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:51 TP0] Decode batch [141756], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:33:51 TP0] Decode batch [141796], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:33:52 TP0] Decode batch [141836], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:33:53 TP0] Decode batch [141876], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:33:54 TP0] Decode batch [141916], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:33:55 TP0] Decode batch [141956], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:33:56 TP0] Decode batch [141996], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:33:56 TP0] Decode batch [142036], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:33:57 TP0] Decode batch [142076], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:33:58 TP0] Decode batch [142116], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:33:59 TP0] Decode batch [142156], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:34:00 TP0] Decode batch [142196], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:34:01 TP0] Decode batch [142236], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-24 16:34:01] INFO:     127.0.0.1:34300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:34:01 TP0] Prefill batch [142258], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:34:01 TP0] Decode batch [142277], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-24 16:34:02 TP0] Decode batch [142317], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:34:03 TP0] Decode batch [142357], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:04 TP0] Decode batch [142397], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:05 TP0] Decode batch [142437], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:06 TP0] Decode batch [142477], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:06 TP0] Decode batch [142517], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:07 TP0] Decode batch [142557], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:34:08 TP0] Decode batch [142597], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:09 TP0] Decode batch [142637], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:10 TP0] Decode batch [142677], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:10 TP0] Decode batch [142717], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:34:11 TP0] Decode batch [142757], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:12 TP0] Decode batch [142797], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:34:13 TP0] Decode batch [142837], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:14 TP0] Decode batch [142877], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:34:15 TP0] Decode batch [142917], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:34:15 TP0] Decode batch [142957], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:34:16 TP0] Decode batch [142997], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:34:17 TP0] Decode batch [143037], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:34:17] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:34:17 TP0] Prefill batch [143059], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:34:18 TP0] Decode batch [143078], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-24 16:34:19 TP0] Decode batch [143118], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-24 16:34:20 TP0] Decode batch [143158], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:20 TP0] Decode batch [143198], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:34:21 TP0] Decode batch [143238], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:22 TP0] Decode batch [143278], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:34:23 TP0] Decode batch [143318], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:24 TP0] Decode batch [143358], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:34:24 TP0] Decode batch [143398], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:34:25 TP0] Decode batch [143438], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:34:26 TP0] Decode batch [143478], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:34:27 TP0] Decode batch [143518], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:34:28 TP0] Decode batch [143558], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:29 TP0] Decode batch [143598], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:29 TP0] Decode batch [143638], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:34:30 TP0] Decode batch [143678], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:34:31 TP0] Decode batch [143718], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:34:32 TP0] Decode batch [143758], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:33 TP0] Decode batch [143798], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:34 TP0] Decode batch [143838], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:34:34] INFO:     127.0.0.1:44002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:34:34 TP0] Prefill batch [143860], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:34:34 TP0] Decode batch [143879], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-24 16:34:35 TP0] Decode batch [143919], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:36 TP0] Decode batch [143959], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:37 TP0] Decode batch [143999], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:34:38 TP0] Decode batch [144039], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:39 TP0] Decode batch [144079], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:34:39 TP0] Decode batch [144119], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:40 TP0] Decode batch [144159], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:41 TP0] Decode batch [144199], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:34:42 TP0] Decode batch [144239], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:43 TP0] Decode batch [144279], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:34:43 TP0] Decode batch [144319], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:34:44 TP0] Decode batch [144359], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:45 TP0] Decode batch [144399], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:46 TP0] Decode batch [144439], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:47 TP0] Decode batch [144479], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:34:48 TP0] Decode batch [144519], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:48 TP0] Decode batch [144559], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:49 TP0] Decode batch [144599], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:34:50 TP0] Decode batch [144639], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:34:50] INFO:     127.0.0.1:51824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:34:50 TP0] Prefill batch [144661], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:34:51 TP0] Decode batch [144680], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-24 16:34:52 TP0] Decode batch [144720], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:34:53 TP0] Decode batch [144760], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:34:53 TP0] Decode batch [144800], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:34:54 TP0] Decode batch [144840], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:34:55 TP0] Decode batch [144880], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:56 TP0] Decode batch [144920], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:57 TP0] Decode batch [144960], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:34:57 TP0] Decode batch [145000], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:58 TP0] Decode batch [145040], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:34:59 TP0] Decode batch [145080], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:00 TP0] Decode batch [145120], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:01 TP0] Decode batch [145160], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:02 TP0] Decode batch [145200], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:02 TP0] Decode batch [145240], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:35:03 TP0] Decode batch [145280], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:35:04 TP0] Decode batch [145320], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:35:05 TP0] Decode batch [145360], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:35:06 TP0] Decode batch [145400], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:06 TP0] Decode batch [145440], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-24 16:35:07] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:35:07 TP0] Prefill batch [145462], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:35:07 TP0] Decode batch [145481], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-24 16:35:08 TP0] Decode batch [145521], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:35:09 TP0] Decode batch [145561], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-24 16:35:10 TP0] Decode batch [145601], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:35:11 TP0] Decode batch [145641], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:35:11 TP0] Decode batch [145681], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:35:12 TP0] Decode batch [145721], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-24 16:35:13 TP0] Decode batch [145761], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:35:14 TP0] Decode batch [145801], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:35:15 TP0] Decode batch [145841], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:16 TP0] Decode batch [145881], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:35:16 TP0] Decode batch [145921], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:35:17 TP0] Decode batch [145961], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:35:18 TP0] Decode batch [146001], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:19 TP0] Decode batch [146041], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:20 TP0] Decode batch [146081], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:21 TP0] Decode batch [146121], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:21 TP0] Decode batch [146161], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:22 TP0] Decode batch [146201], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:23 TP0] Decode batch [146241], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:23] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-24 16:35:23 TP0] Prefill batch [146263], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-24 16:35:24 TP0] Decode batch [146282], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-24 16:35:25 TP0] Decode batch [146322], #running-req: 1, #token: 3260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:35:26 TP0] Decode batch [146362], #running-req: 1, #token: 3300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:35:26 TP0] Decode batch [146402], #running-req: 1, #token: 3340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:27 TP0] Decode batch [146442], #running-req: 1, #token: 3380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-24 16:35:28 TP0] Decode batch [146482], #running-req: 1, #token: 3420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-24 16:35:29 TP0] Decode batch [146522], #running-req: 1, #token: 3460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:30 TP0] Decode batch [146562], #running-req: 1, #token: 3500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:30 TP0] Decode batch [146602], #running-req: 1, #token: 3540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:35:31 TP0] Decode batch [146642], #running-req: 1, #token: 3580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-24 16:35:32 TP0] Decode batch [146682], #running-req: 1, #token: 3620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:35:33 TP0] Decode batch [146722], #running-req: 1, #token: 3660, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:35:34 TP0] Decode batch [146762], #running-req: 1, #token: 3700, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:35:35 TP0] Decode batch [146802], #running-req: 1, #token: 3740, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-24 16:35:35 TP0] Decode batch [146842], #running-req: 1, #token: 3780, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:35:36 TP0] Decode batch [146882], #running-req: 1, #token: 3820, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:35:37 TP0] Decode batch [146922], #running-req: 1, #token: 3860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-24 16:35:38 TP0] Decode batch [146962], #running-req: 1, #token: 3900, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:35:39 TP0] Decode batch [147002], #running-req: 1, #token: 3940, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:35:39 TP0] Decode batch [147042], #running-req: 1, #token: 3980, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-24 16:35:40] INFO:     127.0.0.1:41954 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-24 16:35:48] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-24 16:35:48] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
