[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-15 10:00:39] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
INFO 12-15 10:00:40 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-15 10:00:41] WARNING server_args.py:1520: Attention backend not explicitly specified. Use aiter backend by default.
[2025-12-15 10:00:41] server_args=ServerArgs(model_path='/data/models/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/data/models/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.765, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=943936429, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/models/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2025-12-15 10:00:42] Using default HuggingFace chat template with detected content format: string
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-15 10:00:48] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-15 10:00:48] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-15 10:00:48] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-15 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-15 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-15 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
INFO 12-15 10:00:49 [__init__.py:241] Automatically detected platform rocm.
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-15 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-15 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-15 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
INFO 12-15 10:00:49 [__init__.py:241] Automatically detected platform rocm.
INFO 12-15 10:00:49 [__init__.py:241] Automatically detected platform rocm.
INFO 12-15 10:00:49 [__init__.py:241] Automatically detected platform rocm.
INFO 12-15 10:00:49 [__init__.py:241] Automatically detected platform rocm.
INFO 12-15 10:00:50 [__init__.py:241] Automatically detected platform rocm.
INFO 12-15 10:00:50 [__init__.py:241] Automatically detected platform rocm.
INFO 12-15 10:00:50 [__init__.py:241] Automatically detected platform rocm.
INFO 12-15 10:00:50 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-15 10:00:50 TP0] Process 218 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-15 10:00:50 TP0] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-12-15 10:00:50 TP2] Process 220 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-15 10:00:51 TP4] Process 222 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-12-15 10:00:51 TP6] Process 224 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-12-15 10:00:51 TP2] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-15 10:00:51 TP3] Process 221 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-12-15 10:00:51 TP4] Init torch distributed begin.
[2025-12-15 10:00:51 TP7] Process 225 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[2025-12-15 10:00:51 TP6] Init torch distributed begin.
[2025-12-15 10:00:51 TP5] Process 223 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-12-15 10:00:51 TP1] Process 219 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2025-12-15 10:00:51 TP3] Init torch distributed begin.
[2025-12-15 10:00:51 TP7] Init torch distributed begin.
[2025-12-15 10:00:51 TP5] Init torch distributed begin.
[2025-12-15 10:00:51 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-12-15 10:00:51 TP0] sglang is using nccl==2.26.6
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-15 10:00:59 TP3] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-15 10:00:59 TP0] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-15 10:00:59 TP6] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-15 10:00:59 TP1] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-15 10:00:59 TP2] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-15 10:00:59 TP5] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-15 10:00:59 TP4] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-15 10:00:59 TP7] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-15 10:00:59 TP3] Using AiterCustomAllreduce for ROCm.
[2025-12-15 10:00:59 TP1] Using AiterCustomAllreduce for ROCm.
[2025-12-15 10:00:59 TP0] Using AiterCustomAllreduce for ROCm.
[2025-12-15 10:00:59 TP6] Using AiterCustomAllreduce for ROCm.
[2025-12-15 10:00:59 TP2] Using AiterCustomAllreduce for ROCm.
[2025-12-15 10:00:59 TP5] Using AiterCustomAllreduce for ROCm.
[2025-12-15 10:00:59 TP4] Using AiterCustomAllreduce for ROCm.
[2025-12-15 10:00:59 TP7] Using AiterCustomAllreduce for ROCm.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-12-15 10:00:59 TP0] Init torch distributed ends. mem usage=2.95 GB
[2025-12-15 10:00:59 TP7] Init torch distributed ends. mem usage=3.23 GB
[2025-12-15 10:00:59 TP6] Init torch distributed ends. mem usage=3.24 GB
[2025-12-15 10:00:59 TP5] Init torch distributed ends. mem usage=3.22 GB
[2025-12-15 10:00:59 TP4] Init torch distributed ends. mem usage=3.31 GB
[2025-12-15 10:00:59 TP3] Init torch distributed ends. mem usage=3.36 GB
[2025-12-15 10:00:59 TP2] Init torch distributed ends. mem usage=3.37 GB
[2025-12-15 10:00:59 TP1] Init torch distributed ends. mem usage=3.37 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-12-15 10:01:00 TP5] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-15 10:01:00 TP7] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-15 10:01:00 TP4] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-15 10:01:00 TP3] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-15 10:01:00 TP6] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-15 10:01:00 TP1] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-15 10:01:00 TP2] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-15 10:01:00 TP0] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-15 10:01:01 TP5] Load weight begin. avail mem=188.21 GB
[2025-12-15 10:01:01 TP3] Load weight begin. avail mem=188.07 GB
[2025-12-15 10:01:01 TP4] Load weight begin. avail mem=188.12 GB
[2025-12-15 10:01:01 TP1] Load weight begin. avail mem=188.06 GB
[2025-12-15 10:01:01 TP7] Load weight begin. avail mem=188.20 GB
[2025-12-15 10:01:01 TP6] Load weight begin. avail mem=188.19 GB
[2025-12-15 10:01:01 TP2] Load weight begin. avail mem=188.06 GB
[2025-12-15 10:01:01 TP0] Load weight begin. avail mem=188.48 GB
[2025-12-15 10:01:01 TP0] Detected fp8 checkpoint.
[2025-12-15 10:01:01 TP0] Shared experts fusion optimization enabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:19,  8.35it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:19,  8.34it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:13, 11.40it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:00<00:10, 14.43it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:18,  8.45it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:01<00:21,  7.08it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:01<00:25,  5.82it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:02<00:30,  4.83it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:02<00:38,  3.86it/s]
Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:02<00:32,  4.55it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:03<00:40,  3.59it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:03<00:47,  3.09it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:04<01:04,  2.24it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:04<00:55,  2.58it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:04<00:44,  3.21it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:05<01:08,  2.05it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:06<01:18,  1.79it/s]
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:06<01:04,  2.14it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:07<01:06,  2.07it/s]
Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:07<00:54,  2.50it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:08<00:53,  2.54it/s]
Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:08<00:57,  2.33it/s]
Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:08<00:46,  2.89it/s]
Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:09<01:08,  1.93it/s]
Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:10<01:13,  1.79it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:11<01:39,  1.31it/s]
Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:12<01:46,  1.22it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:12<01:26,  1.48it/s]
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:13<01:18,  1.61it/s]
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:13<01:02,  2.01it/s]
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:13<00:55,  2.25it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:14<00:48,  2.57it/s]
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:14<00:37,  3.26it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:14<00:32,  3.71it/s]
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:14<00:26,  4.53it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:14<00:23,  5.22it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:14<00:18,  6.43it/s]
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:15<00:17,  6.61it/s]
Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:16<00:28,  4.02it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:16<00:21,  5.31it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:16<00:20,  5.53it/s]
Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:16<00:21,  5.12it/s]
Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:16<00:19,  5.65it/s]
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:17<00:30,  3.46it/s]
Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:17<00:21,  4.97it/s]
Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:18<00:29,  3.56it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:18<00:33,  3.06it/s]
Loading safetensors checkpoint shards:  37% Completed | 61/163 [00:19<00:32,  3.10it/s]
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:19<00:31,  3.24it/s]
Loading safetensors checkpoint shards:  39% Completed | 63/163 [00:19<00:33,  2.96it/s]
Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:19<00:28,  3.44it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:20<00:23,  4.11it/s]
Loading safetensors checkpoint shards:  41% Completed | 67/163 [00:20<00:18,  5.18it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:20<00:20,  4.70it/s]
Loading safetensors checkpoint shards:  43% Completed | 70/163 [00:20<00:17,  5.34it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:21<00:17,  5.31it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:21<00:12,  7.03it/s]
Loading safetensors checkpoint shards:  45% Completed | 74/163 [00:21<00:14,  6.13it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:22<00:23,  3.78it/s]
Loading safetensors checkpoint shards:  47% Completed | 76/163 [00:22<00:28,  3.02it/s]
Loading safetensors checkpoint shards:  47% Completed | 77/163 [00:22<00:25,  3.33it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:23<00:42,  2.01it/s]
Loading safetensors checkpoint shards:  48% Completed | 79/163 [00:24<00:41,  2.00it/s]
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:25<00:46,  1.78it/s]
Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:25<00:41,  1.95it/s]
Loading safetensors checkpoint shards:  50% Completed | 82/163 [00:26<00:44,  1.82it/s]
Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:27<00:59,  1.36it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:28<01:07,  1.17it/s]
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:29<01:13,  1.06it/s]
Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:29<00:53,  1.44it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:30<00:51,  1.48it/s]
Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:30<00:42,  1.78it/s]
Loading safetensors checkpoint shards:  55% Completed | 89/163 [00:31<00:41,  1.79it/s]
Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:31<00:34,  2.14it/s]
Loading safetensors checkpoint shards:  56% Completed | 91/163 [00:31<00:33,  2.18it/s]
Loading safetensors checkpoint shards:  56% Completed | 92/163 [00:32<00:28,  2.52it/s]
Loading safetensors checkpoint shards:  57% Completed | 93/163 [00:32<00:28,  2.49it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:33<00:38,  1.80it/s]
Loading safetensors checkpoint shards:  58% Completed | 95/163 [00:33<00:30,  2.24it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:33<00:23,  2.91it/s]
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:33<00:15,  4.29it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:33<00:12,  4.97it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:34<00:12,  5.19it/s]
Loading safetensors checkpoint shards:  62% Completed | 101/163 [00:34<00:12,  5.04it/s]
Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:34<00:08,  6.69it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:34<00:07,  7.93it/s]
Loading safetensors checkpoint shards:  65% Completed | 106/163 [00:34<00:07,  7.68it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:34<00:05,  9.40it/s]
Loading safetensors checkpoint shards:  67% Completed | 110/163 [00:35<00:04, 10.61it/s]
Loading safetensors checkpoint shards:  69% Completed | 112/163 [00:35<00:04, 10.33it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:35<00:04, 11.24it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:35<00:04, 11.52it/s]
Loading safetensors checkpoint shards:  72% Completed | 118/163 [00:36<00:05,  7.97it/s]
Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:36<00:04,  8.82it/s]
Loading safetensors checkpoint shards:  75% Completed | 122/163 [00:36<00:04, 10.18it/s]
Loading safetensors checkpoint shards:  76% Completed | 124/163 [00:36<00:03, 11.25it/s]
Loading safetensors checkpoint shards:  77% Completed | 126/163 [00:36<00:03, 11.29it/s]
Loading safetensors checkpoint shards:  79% Completed | 128/163 [00:37<00:05,  6.03it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:37<00:05,  6.39it/s]
Loading safetensors checkpoint shards:  80% Completed | 131/163 [00:37<00:04,  7.84it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:37<00:03,  7.97it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:38<00:04,  6.88it/s]
Loading safetensors checkpoint shards:  83% Completed | 136/163 [00:38<00:03,  6.86it/s]
Loading safetensors checkpoint shards:  84% Completed | 137/163 [00:38<00:03,  6.78it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:38<00:03,  6.82it/s]
Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:38<00:04,  5.85it/s]
Loading safetensors checkpoint shards:  86% Completed | 140/163 [00:39<00:04,  5.63it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:39<00:03,  6.12it/s]
Loading safetensors checkpoint shards:  87% Completed | 142/163 [00:39<00:03,  6.43it/s]
Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:39<00:03,  6.05it/s]
Loading safetensors checkpoint shards:  88% Completed | 144/163 [00:39<00:03,  5.85it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:40<00:04,  4.23it/s]
Loading safetensors checkpoint shards:  90% Completed | 146/163 [00:40<00:06,  2.80it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:41<00:06,  2.55it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:41<00:03,  3.82it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:41<00:02,  5.32it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:41<00:01,  6.12it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:42<00:01,  6.84it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:42<00:00,  8.38it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:42<00:00,  8.57it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:42<00:00, 10.23it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:42<00:00, 10.09it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:42<00:00,  3.81it/s]

[2025-12-15 10:02:21 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.44 GB, mem usage=79.63 GB.
[2025-12-15 10:02:21 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.57 GB, mem usage=79.63 GB.
[2025-12-15 10:02:21 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.56 GB, mem usage=79.63 GB.
[2025-12-15 10:02:21 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.85 GB, mem usage=79.63 GB.
[2025-12-15 10:02:21 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.43 GB, mem usage=79.63 GB.
[2025-12-15 10:02:21 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.49 GB, mem usage=79.63 GB.
[2025-12-15 10:02:21 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.58 GB, mem usage=79.63 GB.
[2025-12-15 10:02:21 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.44 GB, mem usage=79.63 GB.
[2025-12-15 10:02:21 TP0] Using KV cache dtype: torch.bfloat16
[2025-12-15 10:02:21 TP0] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-15 10:02:21 TP0] Memory pool end. avail mem=43.89 GB
[2025-12-15 10:02:21 TP2] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-15 10:02:21 TP2] Memory pool end. avail mem=43.47 GB
[2025-12-15 10:02:21 TP1] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-15 10:02:21 TP1] Memory pool end. avail mem=43.47 GB
[2025-12-15 10:02:21 TP6] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-15 10:02:21 TP6] Memory pool end. avail mem=43.60 GB
[2025-12-15 10:02:21 TP7] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-15 10:02:21 TP7] Memory pool end. avail mem=43.61 GB
[2025-12-15 10:02:21 TP3] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-15 10:02:21 TP3] Memory pool end. avail mem=43.48 GB
[2025-12-15 10:02:21 TP4] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-15 10:02:21 TP4] Memory pool end. avail mem=43.53 GB
[2025-12-15 10:02:21 TP5] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-15 10:02:21 TP5] Memory pool end. avail mem=43.61 GB
[2025-12-15 10:02:24 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.15 GB
[2025-12-15 10:02:24 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.15 GB
[2025-12-15 10:02:24 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.57 GB
[2025-12-15 10:02:24 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-12-15 10:02:24 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.21 GB
[2025-12-15 10:02:24 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.91 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-15 10:02:25 TP2] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-15 10:02:25 TP2] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-15 10:02:25 TP1] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-15 10:02:25 TP1] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-15 10:02:25 TP0] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-15 10:02:25 TP0] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-15 10:02:25 TP5] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-15 10:02:25 TP5] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-15 10:02:25 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-15 10:02:25 TP4] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-15 10:02:25 TP4] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-15 10:02:26 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.28 GB
[2025-12-15 10:02:26 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-15 10:02:26 TP7] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-15 10:02:26 TP7] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-15 10:02:26 TP6] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-15 10:02:26 TP6] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-15 10:02:27 TP3] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-15 10:02:27 TP3] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-15 10:02:33 TP2] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-15 10:02:33 TP2] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-15 10:02:34 TP0] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-15 10:02:34 TP0] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-15 10:02:36 TP3] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-15 10:02:36 TP3] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-15 10:02:36 TP2] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-15 10:02:37 TP2] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-15 10:02:37 TP2] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-15 10:02:37 TP2] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-15 10:02:37 TP6] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-15 10:02:37 TP6] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-15 10:02:38 TP2] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-15 10:02:38 TP2] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:38 TP2] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-15 10:02:38 TP2] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-15 10:02:38 TP2] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-15 10:02:38 TP1] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-15 10:02:38 TP1] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-15 10:02:39 TP0] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-15 10:02:39 TP0] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-15 10:02:39 TP0] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-15 10:02:39 TP0] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-15 10:02:40 TP5] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-15 10:02:40 TP5] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-15 10:02:40 TP0] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-15 10:02:40 TP0] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-15 10:02:40 TP3] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:40 TP0] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-15 10:02:40 TP0] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-15 10:02:40 TP0] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-15 10:02:41 TP3] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-15 10:02:41 TP3] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-15 10:02:41 TP3] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-15 10:02:41 TP7] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-15 10:02:41 TP7] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-15 10:02:42 TP3] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-15 10:02:42 TP6] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-15 10:02:42 TP3] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:42 TP3] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-15 10:02:42 TP3] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-15 10:02:42 TP3] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-15 10:02:42 TP6] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-15 10:02:42 TP6] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-15 10:02:42 TP6] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-15 10:02:42 TP4] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-15 10:02:42 TP4] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-15 10:02:43 TP6] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-15 10:02:44 TP1] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-15 10:02:44 TP6] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:44 TP6] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-15 10:02:44 TP6] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-15 10:02:44 TP6] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-15 10:02:44 TP1] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-15 10:02:44 TP1] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-15 10:02:44 TP1] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-15 10:02:45 TP5] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-15 10:02:45 TP5] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-15 10:02:45 TP5] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-15 10:02:45 TP5] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-15 10:02:45 TP1] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-15 10:02:45 TP1] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:45 TP1] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-15 10:02:45 TP1] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-15 10:02:45 TP1] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-15 10:02:46 TP5] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-15 10:02:46 TP7] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-15 10:02:46 TP5] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:46 TP5] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-15 10:02:46 TP5] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-15 10:02:46 TP5] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-15 10:02:46 TP7] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-15 10:02:46 TP7] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-15 10:02:46 TP7] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-15 10:02:47 TP4] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-15 10:02:47 TP4] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-15 10:02:47 TP4] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-15 10:02:47 TP4] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-15 10:02:48 TP7] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-15 10:02:48 TP7] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:48 TP7] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-15 10:02:48 TP7] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-15 10:02:48 TP7] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-15 10:02:49 TP4] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-15 10:02:49 TP4] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:49 TP4] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-15 10:02:49 TP4] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-15 10:02:49 TP4] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
Capturing batches (bs=512 avail_mem=42.91 GB):   2%|         | 1/52 [00:25<21:19, 25.08s/it]Capturing batches (bs=496 avail_mem=40.27 GB):   2%|         | 1/52 [00:25<21:19, 25.08s/it]Capturing batches (bs=496 avail_mem=40.27 GB):   4%|         | 2/52 [00:25<08:51, 10.63s/it]Capturing batches (bs=480 avail_mem=40.27 GB):   4%|         | 2/52 [00:25<08:51, 10.63s/it]Capturing batches (bs=480 avail_mem=40.27 GB):   6%|         | 3/52 [00:26<04:52,  5.97s/it]Capturing batches (bs=464 avail_mem=40.27 GB):   6%|         | 3/52 [00:26<04:52,  5.97s/it]Capturing batches (bs=464 avail_mem=40.27 GB):   8%|         | 4/52 [00:26<03:01,  3.77s/it]Capturing batches (bs=448 avail_mem=40.27 GB):   8%|         | 4/52 [00:26<03:01,  3.77s/it]Capturing batches (bs=448 avail_mem=40.27 GB):  10%|         | 5/52 [00:26<01:59,  2.55s/it]Capturing batches (bs=432 avail_mem=40.27 GB):  10%|         | 5/52 [00:26<01:59,  2.55s/it]Capturing batches (bs=432 avail_mem=40.27 GB):  12%|        | 6/52 [00:27<01:23,  1.81s/it]Capturing batches (bs=416 avail_mem=40.26 GB):  12%|        | 6/52 [00:27<01:23,  1.81s/it]Capturing batches (bs=416 avail_mem=40.26 GB):  13%|        | 7/52 [00:27<01:00,  1.34s/it]Capturing batches (bs=400 avail_mem=40.26 GB):  13%|        | 7/52 [00:27<01:00,  1.34s/it]Capturing batches (bs=400 avail_mem=40.26 GB):  15%|        | 8/52 [00:27<00:45,  1.03s/it]Capturing batches (bs=384 avail_mem=40.26 GB):  15%|        | 8/52 [00:27<00:45,  1.03s/it]Capturing batches (bs=384 avail_mem=40.26 GB):  17%|        | 9/52 [00:29<00:57,  1.33s/it]Capturing batches (bs=368 avail_mem=40.25 GB):  17%|        | 9/52 [00:29<00:57,  1.33s/it]Capturing batches (bs=368 avail_mem=40.25 GB):  19%|        | 10/52 [00:30<00:43,  1.03s/it]Capturing batches (bs=352 avail_mem=40.25 GB):  19%|        | 10/52 [00:30<00:43,  1.03s/it]Capturing batches (bs=352 avail_mem=40.25 GB):  21%|        | 11/52 [00:30<00:33,  1.21it/s]Capturing batches (bs=336 avail_mem=40.24 GB):  21%|        | 11/52 [00:30<00:33,  1.21it/s]Capturing batches (bs=336 avail_mem=40.24 GB):  23%|       | 12/52 [00:31<00:27,  1.45it/s]Capturing batches (bs=320 avail_mem=40.24 GB):  23%|       | 12/52 [00:31<00:27,  1.45it/s]Capturing batches (bs=320 avail_mem=40.24 GB):  25%|       | 13/52 [00:31<00:23,  1.69it/s]Capturing batches (bs=304 avail_mem=40.24 GB):  25%|       | 13/52 [00:31<00:23,  1.69it/s]Capturing batches (bs=304 avail_mem=40.24 GB):  27%|       | 14/52 [00:31<00:20,  1.89it/s]Capturing batches (bs=288 avail_mem=40.24 GB):  27%|       | 14/52 [00:31<00:20,  1.89it/s]Capturing batches (bs=288 avail_mem=40.24 GB):  29%|       | 15/52 [00:32<00:17,  2.07it/s]Capturing batches (bs=272 avail_mem=40.24 GB):  29%|       | 15/52 [00:32<00:17,  2.07it/s]Capturing batches (bs=272 avail_mem=40.24 GB):  31%|       | 16/52 [00:32<00:16,  2.24it/s]Capturing batches (bs=256 avail_mem=40.23 GB):  31%|       | 16/52 [00:32<00:16,  2.24it/s][aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:58 TP0] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:58 TP5] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:58 TP2] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:58 TP7] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:58 TP4] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:58 TP3] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:58 TP1] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:02:58 TP6] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=256 avail_mem=40.23 GB):  33%|      | 17/52 [00:34<00:34,  1.02it/s]Capturing batches (bs=248 avail_mem=40.22 GB):  33%|      | 17/52 [00:34<00:34,  1.02it/s]Capturing batches (bs=248 avail_mem=40.22 GB):  35%|      | 18/52 [00:36<00:43,  1.29s/it]Capturing batches (bs=240 avail_mem=40.21 GB):  35%|      | 18/52 [00:36<00:43,  1.29s/it]Capturing batches (bs=240 avail_mem=40.21 GB):  37%|      | 19/52 [00:37<00:33,  1.02s/it]Capturing batches (bs=232 avail_mem=40.21 GB):  37%|      | 19/52 [00:37<00:33,  1.02s/it]Capturing batches (bs=232 avail_mem=40.21 GB):  38%|      | 20/52 [00:37<00:26,  1.21it/s]Capturing batches (bs=224 avail_mem=40.21 GB):  38%|      | 20/52 [00:37<00:26,  1.21it/s]Capturing batches (bs=224 avail_mem=40.21 GB):  40%|      | 21/52 [00:37<00:21,  1.43it/s]Capturing batches (bs=216 avail_mem=40.20 GB):  40%|      | 21/52 [00:37<00:21,  1.43it/s]Capturing batches (bs=216 avail_mem=40.20 GB):  42%|     | 22/52 [00:38<00:17,  1.67it/s]Capturing batches (bs=208 avail_mem=40.20 GB):  42%|     | 22/52 [00:38<00:17,  1.67it/s]Capturing batches (bs=208 avail_mem=40.20 GB):  44%|     | 23/52 [00:38<00:15,  1.89it/s]Capturing batches (bs=200 avail_mem=40.20 GB):  44%|     | 23/52 [00:38<00:15,  1.89it/s]Capturing batches (bs=200 avail_mem=40.20 GB):  46%|     | 24/52 [00:39<00:13,  2.08it/s]Capturing batches (bs=192 avail_mem=40.20 GB):  46%|     | 24/52 [00:39<00:13,  2.08it/s]Capturing batches (bs=192 avail_mem=40.20 GB):  48%|     | 25/52 [00:39<00:12,  2.24it/s]Capturing batches (bs=184 avail_mem=40.20 GB):  48%|     | 25/52 [00:39<00:12,  2.24it/s]Capturing batches (bs=184 avail_mem=40.20 GB):  50%|     | 26/52 [00:39<00:11,  2.36it/s]Capturing batches (bs=176 avail_mem=40.19 GB):  50%|     | 26/52 [00:39<00:11,  2.36it/s]Capturing batches (bs=176 avail_mem=40.19 GB):  52%|    | 27/52 [00:40<00:10,  2.46it/s]Capturing batches (bs=168 avail_mem=40.19 GB):  52%|    | 27/52 [00:40<00:10,  2.46it/s]Capturing batches (bs=168 avail_mem=40.19 GB):  54%|    | 28/52 [00:40<00:09,  2.49it/s]Capturing batches (bs=160 avail_mem=40.19 GB):  54%|    | 28/52 [00:40<00:09,  2.49it/s]Capturing batches (bs=160 avail_mem=40.19 GB):  56%|    | 29/52 [00:40<00:09,  2.44it/s]Capturing batches (bs=152 avail_mem=40.19 GB):  56%|    | 29/52 [00:40<00:09,  2.44it/s]Capturing batches (bs=152 avail_mem=40.19 GB):  58%|    | 30/52 [00:41<00:09,  2.37it/s]Capturing batches (bs=144 avail_mem=40.19 GB):  58%|    | 30/52 [00:41<00:09,  2.37it/s]Capturing batches (bs=144 avail_mem=40.19 GB):  60%|    | 31/52 [00:41<00:08,  2.45it/s]Capturing batches (bs=136 avail_mem=40.18 GB):  60%|    | 31/52 [00:41<00:08,  2.45it/s]Capturing batches (bs=136 avail_mem=40.18 GB):  62%|   | 32/52 [00:42<00:07,  2.51it/s]Capturing batches (bs=128 avail_mem=40.18 GB):  62%|   | 32/52 [00:42<00:07,  2.51it/s][aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:09 TP2] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:09 TP3] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:09 TP0] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:09 TP7] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:09 TP6] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:09 TP4] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:09 TP1] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:09 TP5] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=128 avail_mem=40.18 GB):  63%|   | 33/52 [00:45<00:22,  1.20s/it]Capturing batches (bs=120 avail_mem=40.17 GB):  63%|   | 33/52 [00:45<00:22,  1.20s/it]Capturing batches (bs=120 avail_mem=40.17 GB):  65%|   | 34/52 [00:47<00:26,  1.47s/it]Capturing batches (bs=112 avail_mem=40.16 GB):  65%|   | 34/52 [00:47<00:26,  1.47s/it]Capturing batches (bs=112 avail_mem=40.16 GB):  67%|   | 35/52 [00:47<00:19,  1.17s/it]Capturing batches (bs=104 avail_mem=40.16 GB):  67%|   | 35/52 [00:47<00:19,  1.17s/it]Capturing batches (bs=104 avail_mem=40.16 GB):  69%|   | 36/52 [00:48<00:15,  1.06it/s]Capturing batches (bs=96 avail_mem=40.15 GB):  69%|   | 36/52 [00:48<00:15,  1.06it/s] Capturing batches (bs=96 avail_mem=40.15 GB):  71%|   | 37/52 [00:48<00:11,  1.30it/s]Capturing batches (bs=88 avail_mem=40.15 GB):  71%|   | 37/52 [00:48<00:11,  1.30it/s]Capturing batches (bs=88 avail_mem=40.15 GB):  73%|  | 38/52 [00:48<00:09,  1.54it/s]Capturing batches (bs=80 avail_mem=40.15 GB):  73%|  | 38/52 [00:48<00:09,  1.54it/s]Capturing batches (bs=80 avail_mem=40.15 GB):  75%|  | 39/52 [00:49<00:07,  1.77it/s]Capturing batches (bs=72 avail_mem=40.15 GB):  75%|  | 39/52 [00:49<00:07,  1.77it/s]Capturing batches (bs=72 avail_mem=40.15 GB):  77%|  | 40/52 [00:49<00:06,  1.98it/s]Capturing batches (bs=64 avail_mem=40.14 GB):  77%|  | 40/52 [00:49<00:06,  1.98it/s][aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:14 TP4] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:14 TP3] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:14 TP0] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:14 TP6] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:14 TP1] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:14 TP5] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:14 TP2] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:14 TP7] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=64 avail_mem=40.14 GB):  79%|  | 41/52 [00:50<00:05,  1.85it/s]Capturing batches (bs=56 avail_mem=40.14 GB):  79%|  | 41/52 [00:50<00:05,  1.85it/s]Capturing batches (bs=56 avail_mem=40.14 GB):  81%|  | 42/52 [00:50<00:05,  1.99it/s]Capturing batches (bs=48 avail_mem=40.14 GB):  81%|  | 42/52 [00:50<00:05,  1.99it/s]Capturing batches (bs=48 avail_mem=40.14 GB):  83%| | 43/52 [00:51<00:05,  1.70it/s]Capturing batches (bs=40 avail_mem=40.14 GB):  83%| | 43/52 [00:51<00:05,  1.70it/s]Capturing batches (bs=40 avail_mem=40.14 GB):  85%| | 44/52 [00:52<00:05,  1.37it/s]Capturing batches (bs=32 avail_mem=40.13 GB):  85%| | 44/52 [00:52<00:05,  1.37it/s][aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP6] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP1] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP5] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP7] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP2] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP3] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP4] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP0] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=32 avail_mem=40.13 GB):  87%| | 45/52 [00:52<00:04,  1.59it/s]Capturing batches (bs=24 avail_mem=40.13 GB):  87%| | 45/52 [00:52<00:04,  1.59it/s]Capturing batches (bs=24 avail_mem=40.13 GB):  88%| | 46/52 [00:53<00:03,  1.78it/s]Capturing batches (bs=16 avail_mem=40.13 GB):  88%| | 46/52 [00:53<00:03,  1.78it/s][aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP3] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP1] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP6] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP5] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP7] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP2] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP4] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:03:17 TP0] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=16 avail_mem=40.13 GB):  90%| | 47/52 [00:53<00:02,  1.99it/s]Capturing batches (bs=12 avail_mem=40.13 GB):  90%| | 47/52 [00:53<00:02,  1.99it/s]Capturing batches (bs=12 avail_mem=40.13 GB):  92%|| 48/52 [00:54<00:01,  2.15it/s]Capturing batches (bs=8 avail_mem=40.13 GB):  92%|| 48/52 [00:54<00:01,  2.15it/s] Capturing batches (bs=8 avail_mem=40.13 GB):  94%|| 49/52 [00:54<00:01,  2.30it/s]Capturing batches (bs=4 avail_mem=40.12 GB):  94%|| 49/52 [00:54<00:01,  2.30it/s]Capturing batches (bs=4 avail_mem=40.12 GB):  96%|| 50/52 [00:54<00:00,  2.23it/s]Capturing batches (bs=2 avail_mem=40.12 GB):  96%|| 50/52 [00:54<00:00,  2.23it/s]Capturing batches (bs=2 avail_mem=40.12 GB):  98%|| 51/52 [00:55<00:00,  2.37it/s]Capturing batches (bs=1 avail_mem=40.12 GB):  98%|| 51/52 [00:55<00:00,  2.37it/s]Capturing batches (bs=1 avail_mem=40.12 GB): 100%|| 52/52 [00:57<00:00,  1.04it/s]Capturing batches (bs=1 avail_mem=40.12 GB): 100%|| 52/52 [00:57<00:00,  1.11s/it]
[aiter] Registering 6396 cuda graph addresses
[2025-12-15 10:03:22 TP7] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-15 10:03:22 TP3] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-15 10:03:22 TP1] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-15 10:03:22 TP4] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-15 10:03:22 TP5] Registering 6396 cuda graph addresses
[2025-12-15 10:03:22 TP2] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-15 10:03:22 TP0] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-15 10:03:22 TP6] Registering 6396 cuda graph addresses
[2025-12-15 10:03:22 TP0] Capture cuda graph end. Time elapsed: 58.16 s. mem usage=3.46 GB. avail mem=40.11 GB.
[2025-12-15 10:03:22 TP4] Capture cuda graph end. Time elapsed: 58.04 s. mem usage=3.46 GB. avail mem=39.74 GB.
[2025-12-15 10:03:22 TP3] Capture cuda graph end. Time elapsed: 56.33 s. mem usage=3.46 GB. avail mem=39.70 GB.
[2025-12-15 10:03:22 TP1] Capture cuda graph end. Time elapsed: 58.20 s. mem usage=3.46 GB. avail mem=39.69 GB.
[2025-12-15 10:03:22 TP7] Capture cuda graph end. Time elapsed: 56.55 s. mem usage=3.46 GB. avail mem=39.82 GB.
[2025-12-15 10:03:22 TP2] Capture cuda graph end. Time elapsed: 58.32 s. mem usage=3.46 GB. avail mem=39.68 GB.
[2025-12-15 10:03:22 TP5] Capture cuda graph end. Time elapsed: 58.02 s. mem usage=3.46 GB. avail mem=39.83 GB.
[2025-12-15 10:03:22 TP6] Capture cuda graph end. Time elapsed: 56.49 s. mem usage=3.46 GB. avail mem=39.81 GB.
[2025-12-15 10:03:22 TP0] max_total_num_tokens=981350, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=40.11 GB
[2025-12-15 10:03:23] INFO:     Started server process [53]
[2025-12-15 10:03:23] INFO:     Waiting for application startup.
[2025-12-15 10:03:23] INFO:     Application startup complete.
[2025-12-15 10:03:23] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-12-15 10:03:23] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-15 10:03:23] INFO:     127.0.0.1:46120 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-12-15 10:03:24] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-15 10:03:24] INFO:     127.0.0.1:46132 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-12-15 10:03:24 TP0] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:03:25 TP2] start build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:03:25 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:03:25 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:03:25 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:03:25 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:03:25 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:03:25 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:03:25 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:03:28] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-15 10:03:28] INFO:     127.0.0.1:46144 - "GET /get_model_info HTTP/1.1" 200 OK
[aiter] [32mfinish build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale], cost 43.0s [0m
[2025-12-15 10:04:08 TP2] [32mfinish build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale], cost 43.0s [0m
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:04:08 TP2] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:04:08 TP7] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:04:08 TP6] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:04:08 TP5] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:04:08 TP4] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:04:08 TP1] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:04:08 TP3] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:04:08 TP0] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:04:12 TP0] Prefill batch, #new-seq: 1, #new-token: 667, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:04:14 TP2] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:04:14 TP4] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:04:14 TP6] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:04:14 TP3] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:04:14 TP0] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:04:14 TP5] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:04:14 TP7] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:04:14 TP1] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-15 10:04:15] INFO:     127.0.0.1:46150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:15 TP0] Prefill batch, #new-seq: 33, #new-token: 1943, #cached-token: 22011, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-12-15 10:04:18 TP0] Prefill batch, #new-seq: 263, #new-token: 16377, #cached-token: 175421, token usage: 0.00, #running-req: 34, #queue-req: 915, 
[2025-12-15 10:04:20 TP0] Prefill batch, #new-seq: 277, #new-token: 16364, #cached-token: 185373, token usage: 0.02, #running-req: 297, #queue-req: 746, 
[2025-12-15 10:04:22 TP0] Prefill batch, #new-seq: 271, #new-token: 16378, #cached-token: 181470, token usage: 0.04, #running-req: 574, #queue-req: 475, 
[2025-12-15 10:04:23 TP0] Prefill batch, #new-seq: 179, #new-token: 10913, #cached-token: 119907, token usage: 0.05, #running-req: 845, #queue-req: 296, 
[2025-12-15 10:04:30] INFO:     127.0.0.1:46142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:30] The server is fired up and ready to roll!
[2025-12-15 10:04:31 TP0] Prefill batch, #new-seq: 1, #new-token: 49, #cached-token: 669, token usage: 0.07, #running-req: 1023, #queue-req: 295, 
[2025-12-15 10:04:35] INFO:     127.0.0.1:37006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:35 TP0] Prefill batch, #new-seq: 1, #new-token: 74, #cached-token: 670, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-12-15 10:04:35] INFO:     127.0.0.1:38864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:36] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:36 TP0] Prefill batch, #new-seq: 1, #new-token: 69, #cached-token: 669, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-12-15 10:04:36 TP0] Prefill batch, #new-seq: 1, #new-token: 87, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-12-15 10:04:36] INFO:     127.0.0.1:37048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:36 TP0] Prefill batch, #new-seq: 1, #new-token: 60, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 291, 
[2025-12-15 10:04:37 TP0] Decode batch, #running-req: 1024, #token: 101502, token usage: 0.10, cuda graph: False, gen throughput (token/s): 521.41, #queue-req: 291, 
[2025-12-15 10:04:37] INFO:     127.0.0.1:34588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:37] INFO:     127.0.0.1:37690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:37] INFO:     127.0.0.1:38368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:37] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:37] INFO:     127.0.0.1:34078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:37] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:37] INFO:     127.0.0.1:35754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:37] INFO:     127.0.0.1:35800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:37] INFO:     127.0.0.1:38706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:37] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:37 TP0] Prefill batch, #new-seq: 4, #new-token: 359, #cached-token: 2679, token usage: 0.10, #running-req: 1020, #queue-req: 287, 
[2025-12-15 10:04:40] INFO:     127.0.0.1:35390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:40] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:40] INFO:     127.0.0.1:36868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:40] INFO:     127.0.0.1:42702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:40 TP0] Prefill batch, #new-seq: 10, #new-token: 651, #cached-token: 6703, token usage: 0.11, #running-req: 1014, #queue-req: 277, 
[2025-12-15 10:04:41] INFO:     127.0.0.1:35062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:41] INFO:     127.0.0.1:37018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:41] INFO:     127.0.0.1:39840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:41 TP0] Prefill batch, #new-seq: 3, #new-token: 170, #cached-token: 2011, token usage: 0.11, #running-req: 1021, #queue-req: 274, 
[2025-12-15 10:04:42] INFO:     127.0.0.1:34384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:42] INFO:     127.0.0.1:34694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:42] INFO:     127.0.0.1:34838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:42] INFO:     127.0.0.1:34888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:42] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:42] INFO:     127.0.0.1:38432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:42] INFO:     127.0.0.1:38884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:42] INFO:     127.0.0.1:38958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:42] INFO:     127.0.0.1:41844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:42 TP0] Prefill batch, #new-seq: 9, #new-token: 472, #cached-token: 6031, token usage: 0.11, #running-req: 1015, #queue-req: 265, 
[2025-12-15 10:04:44] INFO:     127.0.0.1:35706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:44] INFO:     127.0.0.1:37428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:44] INFO:     127.0.0.1:39048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:44] INFO:     127.0.0.1:39592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:44] INFO:     127.0.0.1:40692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:44] INFO:     127.0.0.1:42690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:44] INFO:     127.0.0.1:42798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:44 TP0] Prefill batch, #new-seq: 7, #new-token: 341, #cached-token: 4689, token usage: 0.11, #running-req: 1017, #queue-req: 258, 
[2025-12-15 10:04:45] INFO:     127.0.0.1:34626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:45] INFO:     127.0.0.1:37302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:45] INFO:     127.0.0.1:37846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:45] INFO:     127.0.0.1:39304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:45] INFO:     127.0.0.1:40866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:45] INFO:     127.0.0.1:42006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:45 TP0] Prefill batch, #new-seq: 6, #new-token: 295, #cached-token: 4021, token usage: 0.11, #running-req: 1018, #queue-req: 252, 
[2025-12-15 10:04:45] INFO:     127.0.0.1:38506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:45] INFO:     127.0.0.1:40086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:45] INFO:     127.0.0.1:41954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:45] INFO:     127.0.0.1:42248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:46 TP0] Prefill batch, #new-seq: 4, #new-token: 304, #cached-token: 2679, token usage: 0.11, #running-req: 1020, #queue-req: 248, 
[2025-12-15 10:04:46] INFO:     127.0.0.1:34442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:46] INFO:     127.0.0.1:35718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:46] INFO:     127.0.0.1:36194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:46] INFO:     127.0.0.1:37540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:46] INFO:     127.0.0.1:37686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:46] INFO:     127.0.0.1:41352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:46] INFO:     127.0.0.1:42152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:46] INFO:     127.0.0.1:42748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:46 TP0] Prefill batch, #new-seq: 8, #new-token: 594, #cached-token: 5360, token usage: 0.11, #running-req: 1016, #queue-req: 240, 
[2025-12-15 10:04:49] INFO:     127.0.0.1:34054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:49] INFO:     127.0.0.1:36180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:49] INFO:     127.0.0.1:36250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:49] INFO:     127.0.0.1:36994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:49] INFO:     127.0.0.1:40408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:49] INFO:     127.0.0.1:41418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:49] INFO:     127.0.0.1:41696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:49] INFO:     127.0.0.1:42554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:49] INFO:     127.0.0.1:43134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:49 TP0] Prefill batch, #new-seq: 9, #new-token: 516, #cached-token: 6032, token usage: 0.11, #running-req: 1015, #queue-req: 231, 
[2025-12-15 10:04:50] INFO:     127.0.0.1:35272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:50] INFO:     127.0.0.1:38180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:50] INFO:     127.0.0.1:39902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:50] INFO:     127.0.0.1:41198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:50] INFO:     127.0.0.1:41694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:50] INFO:     127.0.0.1:41792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:50] INFO:     127.0.0.1:42844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:50 TP0] Prefill batch, #new-seq: 7, #new-token: 480, #cached-token: 4690, token usage: 0.11, #running-req: 1017, #queue-req: 224, 
[2025-12-15 10:04:51] INFO:     127.0.0.1:34990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51] INFO:     127.0.0.1:35374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51] INFO:     127.0.0.1:38546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51] INFO:     127.0.0.1:38642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51] INFO:     127.0.0.1:38962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51] INFO:     127.0.0.1:39696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51] INFO:     127.0.0.1:41636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51] INFO:     127.0.0.1:42874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51 TP0] Prefill batch, #new-seq: 8, #new-token: 461, #cached-token: 5358, token usage: 0.11, #running-req: 1016, #queue-req: 216, 
[2025-12-15 10:04:51] INFO:     127.0.0.1:36684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51] INFO:     127.0.0.1:38246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51] INFO:     127.0.0.1:40382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51] INFO:     127.0.0.1:41894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51] INFO:     127.0.0.1:42014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:51 TP0] Prefill batch, #new-seq: 7, #new-token: 309, #cached-token: 4688, token usage: 0.11, #running-req: 1017, #queue-req: 209, 
[2025-12-15 10:04:52] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:35438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:36064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:37226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:38010 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:40508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:41966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52 TP0] Prefill batch, #new-seq: 7, #new-token: 353, #cached-token: 4688, token usage: 0.11, #running-req: 1017, #queue-req: 202, 
[2025-12-15 10:04:52] INFO:     127.0.0.1:34482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:34672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:35230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:35992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:37414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:38876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:39856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:39994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:40564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:52 TP0] Prefill batch, #new-seq: 11, #new-token: 700, #cached-token: 7370, token usage: 0.12, #running-req: 1013, #queue-req: 191, 
[2025-12-15 10:04:53] INFO:     127.0.0.1:34312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:35124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:38402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:38410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:39296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:39628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:40858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53 TP0] Prefill batch, #new-seq: 8, #new-token: 452, #cached-token: 5358, token usage: 0.12, #running-req: 1016, #queue-req: 183, 
[2025-12-15 10:04:53] INFO:     127.0.0.1:34040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:35050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:35892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:36618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:37484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:40268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:40448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:40952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:42324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53 TP0] Prefill batch, #new-seq: 9, #new-token: 495, #cached-token: 6030, token usage: 0.12, #running-req: 1015, #queue-req: 174, 
[2025-12-15 10:04:53] INFO:     127.0.0.1:34770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:35526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:35852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:36030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:36416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:36474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:36836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:40562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53] INFO:     127.0.0.1:41846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:53 TP0] Prefill batch, #new-seq: 9, #new-token: 469, #cached-token: 6028, token usage: 0.12, #running-req: 1015, #queue-req: 165, 
[2025-12-15 10:04:54] INFO:     127.0.0.1:35086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:35586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:36366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:37104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:37432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:38490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:39978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:40054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54 TP0] Prefill batch, #new-seq: 8, #new-token: 449, #cached-token: 5360, token usage: 0.12, #running-req: 1016, #queue-req: 157, 
[2025-12-15 10:04:54 TP0] Prefill batch, #new-seq: 8, #new-token: 442, #cached-token: 5358, token usage: 0.12, #running-req: 1016, #queue-req: 149, 
[2025-12-15 10:04:54] INFO:     127.0.0.1:39966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:40190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:40812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:41184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:41360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:41650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:42494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:36278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:36824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:37570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:38470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:38482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:38836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:39038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:40738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:42576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54] INFO:     127.0.0.1:43006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:54 TP0] Prefill batch, #new-seq: 9, #new-token: 636, #cached-token: 6031, token usage: 0.12, #running-req: 1015, #queue-req: 140, 
[2025-12-15 10:04:55] INFO:     127.0.0.1:35288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:55] INFO:     127.0.0.1:35868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:55] INFO:     127.0.0.1:36130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:55] INFO:     127.0.0.1:36360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:55] INFO:     127.0.0.1:36746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:55] INFO:     127.0.0.1:37126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:55] INFO:     127.0.0.1:38428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:55] INFO:     127.0.0.1:38532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:55] INFO:     127.0.0.1:39806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:55] INFO:     127.0.0.1:42272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:55 TP0] Prefill batch, #new-seq: 10, #new-token: 522, #cached-token: 6702, token usage: 0.12, #running-req: 1014, #queue-req: 130, 
[2025-12-15 10:04:56] INFO:     127.0.0.1:34240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:34870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:35126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:35588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:36262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:36722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:39650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:39724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:41434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:42188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56 TP0] Prefill batch, #new-seq: 10, #new-token: 763, #cached-token: 6699, token usage: 0.12, #running-req: 1014, #queue-req: 120, 
[2025-12-15 10:04:56] INFO:     127.0.0.1:34824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:35560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:37450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:39028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:39196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:41024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56] INFO:     127.0.0.1:41540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:56 TP0] Prefill batch, #new-seq: 7, #new-token: 417, #cached-token: 4690, token usage: 0.12, #running-req: 1017, #queue-req: 113, 
[2025-12-15 10:04:57] INFO:     127.0.0.1:34124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:35324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:36838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:36942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:37544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:39478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:40124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:41452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:42070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57 TP0] Prefill batch, #new-seq: 10, #new-token: 660, #cached-token: 6703, token usage: 0.12, #running-req: 1014, #queue-req: 103, 
[2025-12-15 10:04:57] INFO:     127.0.0.1:34348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:34992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:35684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:37142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:37172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:38126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:38618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:39400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:40882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:41918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57] INFO:     127.0.0.1:42094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:57 TP0] Prefill batch, #new-seq: 11, #new-token: 706, #cached-token: 7374, token usage: 0.12, #running-req: 1013, #queue-req: 92, 
[2025-12-15 10:04:58] INFO:     127.0.0.1:34658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:35668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:36090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:37000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:37464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:37826 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:38392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:38682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:39134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:40318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:41210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58 TP0] Prefill batch, #new-seq: 13, #new-token: 697, #cached-token: 8708, token usage: 0.12, #running-req: 1011, #queue-req: 79, 
[2025-12-15 10:04:58] INFO:     127.0.0.1:35492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:35510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:36124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:39486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:40052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:41970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58] INFO:     127.0.0.1:42098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:58 TP0] Prefill batch, #new-seq: 7, #new-token: 412, #cached-token: 4687, token usage: 0.12, #running-req: 1017, #queue-req: 72, 
[2025-12-15 10:04:59] INFO:     127.0.0.1:34306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:34412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:34634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:35152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:36498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:36788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:37148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:37320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:38430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:38940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:39884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:40432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:40956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:41466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:41854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:42080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59] INFO:     127.0.0.1:42230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:04:59 TP0] Prefill batch, #new-seq: 17, #new-token: 1260, #cached-token: 11386, token usage: 0.12, #running-req: 1007, #queue-req: 55, 
[2025-12-15 10:05:01] INFO:     127.0.0.1:34350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01] INFO:     127.0.0.1:35774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01] INFO:     127.0.0.1:36114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01] INFO:     127.0.0.1:37490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01] INFO:     127.0.0.1:37604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01] INFO:     127.0.0.1:39668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01] INFO:     127.0.0.1:39798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01] INFO:     127.0.0.1:39820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01] INFO:     127.0.0.1:40454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01] INFO:     127.0.0.1:40912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01] INFO:     127.0.0.1:41064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01] INFO:     127.0.0.1:41866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01] INFO:     127.0.0.1:41912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01] INFO:     127.0.0.1:43056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:01 TP0] Prefill batch, #new-seq: 14, #new-token: 849, #cached-token: 9378, token usage: 0.12, #running-req: 1010, #queue-req: 41, 
[2025-12-15 10:05:04] INFO:     127.0.0.1:35348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:04] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:04] INFO:     127.0.0.1:37994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:04] INFO:     127.0.0.1:38234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:04] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:04] INFO:     127.0.0.1:39708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:04] INFO:     127.0.0.1:39930 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:04] INFO:     127.0.0.1:40072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:04] INFO:     127.0.0.1:40152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:04 TP0] Prefill batch, #new-seq: 9, #new-token: 531, #cached-token: 6029, token usage: 0.12, #running-req: 1015, #queue-req: 32, 
[2025-12-15 10:05:05] INFO:     127.0.0.1:35334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:36436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:37822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:39092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:39432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:41326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:41406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05 TP0] Prefill batch, #new-seq: 8, #new-token: 431, #cached-token: 5363, token usage: 0.12, #running-req: 1016, #queue-req: 24, 
[2025-12-15 10:05:05] INFO:     127.0.0.1:34716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:35884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:37250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:37594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:38032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:38102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:38258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:38772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:40580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:40752 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:40838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:40948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:41298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:43058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05] INFO:     127.0.0.1:43104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:05 TP0] Prefill batch, #new-seq: 16, #new-token: 1045, #cached-token: 10716, token usage: 0.12, #running-req: 1008, #queue-req: 8, 
[2025-12-15 10:05:07] INFO:     127.0.0.1:34398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:34924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:35946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:36312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:37252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:37858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:40102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:40280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:40292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:40792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:41948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:42342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:42962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:07] INFO:     127.0.0.1:43026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08 TP0] Prefill batch, #new-seq: 8, #new-token: 411, #cached-token: 5365, token usage: 0.12, #running-req: 1008, #queue-req: 0, 
[2025-12-15 10:05:08] INFO:     127.0.0.1:35166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:35574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:41848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:42044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:42658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:34644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:34938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:35622 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:37608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:38360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:38744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:40698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:41224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:42682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:08] INFO:     127.0.0.1:42772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:34354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:35696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:36044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:37954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:38458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:38738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:39016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:40672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:42178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:42260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:42310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:42910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:34786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:35606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:36532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:36854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:37110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:37558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:38394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:39924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:41138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:41284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:41468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:34324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:35976 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:36810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:38288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:38760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:39912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:41782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:42120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:42374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:42408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:42562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09 TP0] Decode batch, #running-req: 973, #token: 119304, token usage: 0.12, cuda graph: False, gen throughput (token/s): 1262.63, #queue-req: 0, 
[2025-12-15 10:05:09] INFO:     127.0.0.1:35298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:35468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:36892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:37256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:38692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:38946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:39080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:40462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:41922 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:42218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:34884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:35080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:36660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:37386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:39372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:41442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:41660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:42786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:34376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:34690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:35804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:36216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:38972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:39124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:39236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:40446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:09] INFO:     127.0.0.1:42138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:35102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:35502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:36330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:41600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:35956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:36026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:38132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:38718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:39672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:40322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:40550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:41478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:41620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:41748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:42282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:34704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:36484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:36740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:38024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:40458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:41048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:34684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:36906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:37606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:37704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:38602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:39070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:39102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:39654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:40744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:42484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:42762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:34548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:34848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:36674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:37160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:37916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:38156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:38550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:40614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:41230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:41250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:41568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:41994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:42544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:43002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:43272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:36104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:36594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:37022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:37532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:37962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:38704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:39280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:39384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:39522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:40266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:41708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:41756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:42992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:43240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:34470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:35448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:35558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:36548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:37518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:37870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:40954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:10] INFO:     127.0.0.1:43196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:34498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:34966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:34976 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:37088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:37360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:37542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:37688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:38346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:39474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:40232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:40716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:40900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:41398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:42136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:42206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:42458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:35202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:41980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:42084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:42432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:42468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:42860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:35138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:35176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:35634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:36978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:38660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:40024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:40520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:34802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:34948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:35052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:40248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:40742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:11] INFO:     127.0.0.1:40850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:35212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:35370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:38576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:38776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:39172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:40216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:40594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:42612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:43028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:43640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:38674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:39886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:41010 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:41818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:42056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:42736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:43818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:37514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:39058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:39326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:39332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:34108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:34656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:35192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:37286 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:37892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:38036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:38566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:39412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:43294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:35008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:35978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:38188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:39740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:40090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:40540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:41510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:42348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:42624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:35478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:36308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:36612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:37204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:37806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:38650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:38672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:39550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:40128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:41130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:41222 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:42362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:42834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:13] INFO:     127.0.0.1:44298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:14] INFO:     127.0.0.1:34300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:14] INFO:     127.0.0.1:34590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:14] INFO:     127.0.0.1:38148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:14] INFO:     127.0.0.1:38302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:14] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:14] INFO:     127.0.0.1:39786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:14] INFO:     127.0.0.1:40554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:14] INFO:     127.0.0.1:43168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:14] INFO:     127.0.0.1:44290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:34760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:36236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:37610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:38964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:39252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:40532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:41604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:41934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:43012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:36232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:36914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:37500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:38414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:38820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:39952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:34960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:35028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:38078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:38790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:41988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:42190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:43164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:34194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:34224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:35076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:36336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:36712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:36738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:37314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:38218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:39760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:40676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:41424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:43764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:37032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:37282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:39426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:40350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:42636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:43356 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:43504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:43712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:43976 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:44190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:34564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:35044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:35292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:37626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:37876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:38048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:41088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:43148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:43918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:36210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:37556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:37770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:38316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:40424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:40710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:41590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:41628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:41834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:43040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:44440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:16] INFO:     127.0.0.1:45528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:35548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:35716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:37334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:38380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:39112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:39470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:39538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:40646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:40782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:41704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:42714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:43936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:44260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:44598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:45016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:34284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:34936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:35662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:36394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:36880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:37132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:37804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:38872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:41266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:41730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:42418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:34854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:36860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:37664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:37908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:39282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:39460 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:42894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:42942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:43966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:34978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:35538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:36158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:36188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:36772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:38378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:38444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:39404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:40114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:40770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:41618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:42526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:34270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:37978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:38586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:38610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:38716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:39608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:40058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:41654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:42944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:43284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:44396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:35426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:37346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:39012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:40002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:42450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:42512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:43118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:45646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:34718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:37010 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:37298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:37948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:39444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:40006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:41900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:42646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:42666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:42756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:42924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:17] INFO:     127.0.0.1:44820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:36754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:37474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:37530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:40250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:40386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:41126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:42294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:42978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:43874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:34234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:34610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:36584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:37238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:37650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:38632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:40666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:40996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:41030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:42700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:42926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:44058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:36052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:37190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:39950 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:41614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:43494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:44544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:34206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:35278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:37258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:37330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:37398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:38948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:39612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:40920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:41202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:41554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:41770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:43684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:44408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:34364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:35820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:38272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:38612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:39510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:39860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:43340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:43808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:43996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:45072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:45190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18 TP0] Decode batch, #running-req: 555, #token: 84658, token usage: 0.09, cuda graph: False, gen throughput (token/s): 3293.77, #queue-req: 0, 
[2025-12-15 10:05:18] INFO:     127.0.0.1:34254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:35636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:36856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:39656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:39944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:40390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:40732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:42100 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:42626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:42770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:44046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:44342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:44854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:44970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:34334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:37062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:38330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:39024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:39772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:40164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:40400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:41154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:41520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:41686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:42364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:43846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:18] INFO:     127.0.0.1:44176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:35936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:37182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:40176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:42276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:42730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:44018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:35360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:36406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:36964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:37960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:38182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:40360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:40482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:40492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:42712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:44274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:36284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:36564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:38196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:38594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:41506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:35836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:37738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:38806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:40138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:44142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:44340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:35974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:36608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:36770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:38162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:38264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:41750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:41808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:43464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:43568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:44014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:34180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:34664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:38184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:39894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:41440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:35758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:40158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:40766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:41074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:37748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:41642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:41724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:43182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:44642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:35244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:41860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:42030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:44080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:44568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:44960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:35938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:37376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:38988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:42672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:43704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:43734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:45532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:34910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:45290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:45374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:34830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:35018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:36296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:36968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:37270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:37582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:38120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:39500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:40336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:40658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:42236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:44668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:45780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:35598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:36164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:38848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:39158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:41732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:43030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:44072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:19] INFO:     127.0.0.1:44956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:37444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:37684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:40802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:35626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:38030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:38922 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:39868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:41492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:34370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:36346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:37546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:38834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:39312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:39748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:42168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:42706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:36808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:36948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:37758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:40970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:41044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:41314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:41862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:36022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:36934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:37862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:39496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:41178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:42336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:34326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:36008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:36526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:37642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:40308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:41244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:37836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:39828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:40830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:41514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:35886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:38280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:42568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:38290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:43672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:45758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:35970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:38924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:20] INFO:     127.0.0.1:44310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:41670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:44968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:36078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:36530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:39642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:44558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:35730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:38202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:34458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:38164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:39684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:40816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:37830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:38110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:44466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:44874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:36016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:37460 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:40374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:34136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:37268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:37782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:44162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:40338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:42828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:44314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:36144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:44754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:34606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:35646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:36324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:37680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:42436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21 TP0] Decode batch, #running-req: 258, #token: 49507, token usage: 0.05, cuda graph: True, gen throughput (token/s): 5112.63, #queue-req: 0, 
[2025-12-15 10:05:21] INFO:     127.0.0.1:34620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:37212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:44328 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:44794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:36136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:36640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:38926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:43624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:37046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:44640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:40630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:44572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:21] INFO:     127.0.0.1:45468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:37370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:38100 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:39268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:42940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:43220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:43480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:34338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:35866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:43520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:37760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:43612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:40040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:35742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:36316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:38008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:38064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:39716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:43252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:43526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:39494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:39844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:40864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:41282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45010 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:36510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:43582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:43692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44356 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:43324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:34796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:36624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:40644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:39580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:40600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:40894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:34528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:35458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:35486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:39180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:39936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:40486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:41290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:45280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:42788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:44118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:34814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:42308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:22] INFO:     127.0.0.1:43782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:38802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:39370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:41880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:43680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:43776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:44946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:45138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:45740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:43610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:43630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:43772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:44502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:45130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:35422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:38284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:39362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:42060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:43548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:45564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:36066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:41528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:43578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:44218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:44974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:37074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:43762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:37710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:45792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:34514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:39858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:45418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:45684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:40778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:44410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:45448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:42590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:45158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:43746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:35786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:39212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:41928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:43248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:44180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:44482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:44690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:38500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:40312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:43274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:42880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:23] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:43070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:44898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24 TP0] Decode batch, #running-req: 97, #token: 23609, token usage: 0.02, cuda graph: True, gen throughput (token/s): 2763.79, #queue-req: 0, 
[2025-12-15 10:05:24] INFO:     127.0.0.1:36814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:44742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:45688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:34152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:40848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:45082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:36698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:38998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:43484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:35186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:35482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:40016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:44932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:44694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:43682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:36792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:44894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:37116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:38608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:38092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:38108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:39342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:38838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:43882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:44488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:35066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:36382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:36926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:37722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:43440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:45302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:41366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:42838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:44422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:45176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:45318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:35664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:37912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:41232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:43948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:44506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:40470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:44152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:45126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:45764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:45166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:24] INFO:     127.0.0.1:45838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:45416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:35404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:45202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:45736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:45700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:44094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:45390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:37796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:45862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:39462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:42402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:45610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:43400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:44520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:44654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:40574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:44578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:41500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:45018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:42352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25 TP0] Decode batch, #running-req: 27, #token: 8022, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1299.94, #queue-req: 0, 
[2025-12-15 10:05:25] INFO:     127.0.0.1:44776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:25] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:44366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:45750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:43728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:41098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:43394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:44448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:43188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:44004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:44124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:43650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:41042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:44862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:42530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:44764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:45406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:26] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:27 TP0] Decode batch, #running-req: 7, #token: 2683, token usage: 0.00, cuda graph: True, gen throughput (token/s): 539.68, #queue-req: 0, 
[2025-12-15 10:05:27] INFO:     127.0.0.1:41382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:27] INFO:     127.0.0.1:43088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:28 TP0] Decode batch, #running-req: 4, #token: 1762, token usage: 0.00, cuda graph: True, gen throughput (token/s): 169.09, #queue-req: 0, 
[2025-12-15 10:05:28] INFO:     127.0.0.1:44592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:28] INFO:     127.0.0.1:43306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:28] INFO:     127.0.0.1:43228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:28] INFO:     127.0.0.1:43858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:34] INFO:     127.0.0.1:39128 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-15 10:05:46] INFO:     127.0.0.1:34514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:46 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:05:49 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2.74, #queue-req: 0, 
[2025-12-15 10:05:50] INFO:     127.0.0.1:34518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:05:50] INFO:     127.0.0.1:34532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34622 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:50] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34950 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:34988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35100 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35104 - "POST /generate HTTP/1.1" 200 OK
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:05:51 TP4] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:05:51] INFO:     127.0.0.1:35120 - "POST /generate HTTP/1.1" 200 OK
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:05:51 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:05:51 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:05:51] INFO:     127.0.0.1:35122 - "POST /generate HTTP/1.1" 200 OK
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:05:51 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:05:51 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:05:51 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:05:51 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:05:51 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-15 10:05:51] INFO:     127.0.0.1:35136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:05:51] INFO:     127.0.0.1:35684 - "POST /generate HTTP/1.1" 200 OK
[aiter] [32mfinish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale], cost 42.6s [0m
[2025-12-15 10:06:33 TP4] [32mfinish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale], cost 42.6s [0m
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:06:33 TP4] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:06:33 TP0] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:06:33 TP2] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:06:33 TP6] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:06:33 TP7] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:06:33 TP5] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:06:33 TP3] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:06:33 TP1] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-15 10:06:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 122, 
[2025-12-15 10:06:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.02, #running-req: 6, #queue-req: 117, 
[2025-12-15 10:06:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-12-15 10:06:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-12-15 10:06:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-12-15 10:06:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.08, #running-req: 26, #queue-req: 97, 
[2025-12-15 10:06:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-12-15 10:06:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-12-15 10:06:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-12-15 10:06:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-12-15 10:06:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-12-15 10:06:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-12-15 10:06:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.20, #running-req: 61, #queue-req: 62, 
[2025-12-15 10:06:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.22, #running-req: 66, #queue-req: 57, 
[2025-12-15 10:06:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.23, #running-req: 71, #queue-req: 52, 
[2025-12-15 10:06:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.25, #running-req: 76, #queue-req: 47, 
[2025-12-15 10:06:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.26, #running-req: 81, #queue-req: 42, 
[2025-12-15 10:06:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.28, #running-req: 86, #queue-req: 37, 
[2025-12-15 10:06:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.30, #running-req: 91, #queue-req: 32, 
[2025-12-15 10:06:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.31, #running-req: 96, #queue-req: 27, 
[2025-12-15 10:06:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.33, #running-req: 101, #queue-req: 22, 
[2025-12-15 10:06:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.35, #running-req: 106, #queue-req: 17, 
[2025-12-15 10:06:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.36, #running-req: 111, #queue-req: 12, 
[2025-12-15 10:06:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.38, #running-req: 116, #queue-req: 7, 
[2025-12-15 10:06:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.39, #running-req: 121, #queue-req: 2, 
[2025-12-15 10:06:59 TP0] Prefill batch, #new-seq: 2, #new-token: 6394, #cached-token: 8, token usage: 0.41, #running-req: 126, #queue-req: 0, 
[2025-12-15 10:07:03 TP0] Decode batch, #running-req: 128, #token: 411551, token usage: 0.42, cuda graph: True, gen throughput (token/s): 28.03, #queue-req: 0, 
[2025-12-15 10:07:05 TP0] Decode batch, #running-req: 128, #token: 416671, token usage: 0.42, cuda graph: True, gen throughput (token/s): 1990.85, #queue-req: 0, 
[2025-12-15 10:07:08 TP0] Decode batch, #running-req: 128, #token: 421791, token usage: 0.43, cuda graph: True, gen throughput (token/s): 1970.22, #queue-req: 0, 
[2025-12-15 10:07:10 TP0] Decode batch, #running-req: 128, #token: 426911, token usage: 0.44, cuda graph: True, gen throughput (token/s): 1951.98, #queue-req: 0, 
[2025-12-15 10:07:15 TP0] Decode batch, #running-req: 128, #token: 432031, token usage: 0.44, cuda graph: True, gen throughput (token/s): 1190.26, #queue-req: 0, 
[2025-12-15 10:07:17 TP0] Decode batch, #running-req: 128, #token: 437151, token usage: 0.45, cuda graph: True, gen throughput (token/s): 1942.24, #queue-req: 0, 
[2025-12-15 10:07:20 TP0] Decode batch, #running-req: 128, #token: 442271, token usage: 0.45, cuda graph: True, gen throughput (token/s): 1916.32, #queue-req: 0, 
[2025-12-15 10:07:23 TP0] Decode batch, #running-req: 128, #token: 447391, token usage: 0.46, cuda graph: True, gen throughput (token/s): 1930.19, #queue-req: 0, 
[2025-12-15 10:07:25 TP0] Decode batch, #running-req: 128, #token: 452511, token usage: 0.46, cuda graph: True, gen throughput (token/s): 1922.29, #queue-req: 0, 
[2025-12-15 10:07:28 TP0] Decode batch, #running-req: 128, #token: 457631, token usage: 0.47, cuda graph: True, gen throughput (token/s): 1911.91, #queue-req: 0, 
[2025-12-15 10:07:31 TP0] Decode batch, #running-req: 128, #token: 462751, token usage: 0.47, cuda graph: True, gen throughput (token/s): 1898.21, #queue-req: 0, 
[2025-12-15 10:07:33 TP0] Decode batch, #running-req: 128, #token: 467871, token usage: 0.48, cuda graph: True, gen throughput (token/s): 1904.74, #queue-req: 0, 
[2025-12-15 10:07:36 TP0] Decode batch, #running-req: 128, #token: 472991, token usage: 0.48, cuda graph: True, gen throughput (token/s): 1897.29, #queue-req: 0, 
[2025-12-15 10:07:39 TP0] Decode batch, #running-req: 128, #token: 478111, token usage: 0.49, cuda graph: True, gen throughput (token/s): 1896.85, #queue-req: 0, 
[2025-12-15 10:07:42 TP0] Decode batch, #running-req: 128, #token: 483231, token usage: 0.49, cuda graph: True, gen throughput (token/s): 1889.65, #queue-req: 0, 
[2025-12-15 10:07:44 TP0] Decode batch, #running-req: 128, #token: 488351, token usage: 0.50, cuda graph: True, gen throughput (token/s): 1879.15, #queue-req: 0, 
[2025-12-15 10:07:47 TP0] Decode batch, #running-req: 128, #token: 493471, token usage: 0.50, cuda graph: True, gen throughput (token/s): 1868.77, #queue-req: 0, 
[2025-12-15 10:07:50 TP0] Decode batch, #running-req: 128, #token: 498591, token usage: 0.51, cuda graph: True, gen throughput (token/s): 1865.86, #queue-req: 0, 
[2025-12-15 10:07:52 TP0] Decode batch, #running-req: 128, #token: 503711, token usage: 0.51, cuda graph: True, gen throughput (token/s): 1862.39, #queue-req: 0, 
[2025-12-15 10:07:58 TP0] Decode batch, #running-req: 128, #token: 508831, token usage: 0.52, cuda graph: True, gen throughput (token/s): 975.93, #queue-req: 0, 
[2025-12-15 10:08:01] INFO:     127.0.0.1:35926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:35932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:35936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01 TP0] Prefill batch, #new-seq: 2, #new-token: 6390, #cached-token: 12, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:08:01] INFO:     127.0.0.1:35952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:35956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:35962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:35978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:35984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:35986 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:35998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:36004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:36012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:36014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:36030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:01] INFO:     127.0.0.1:36042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.01, #running-req: 2, #queue-req: 31, 
[2025-12-15 10:08:02] INFO:     127.0.0.1:36274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36752 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:36994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:37004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:37016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:37018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:37026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:37034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:37040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:37050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:37054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:02] INFO:     127.0.0.1:37056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:08:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 7, #queue-req: 116, 
[2025-12-15 10:08:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 12, #queue-req: 111, 
[2025-12-15 10:08:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.06, #running-req: 17, #queue-req: 106, 
[2025-12-15 10:08:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 22, #queue-req: 101, 
[2025-12-15 10:08:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.09, #running-req: 27, #queue-req: 96, 
[2025-12-15 10:08:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.10, #running-req: 32, #queue-req: 91, 
[2025-12-15 10:08:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.12, #running-req: 37, #queue-req: 86, 
[2025-12-15 10:08:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.14, #running-req: 42, #queue-req: 81, 
[2025-12-15 10:08:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 47, #queue-req: 76, 
[2025-12-15 10:08:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.17, #running-req: 52, #queue-req: 71, 
[2025-12-15 10:08:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.19, #running-req: 57, #queue-req: 66, 
[2025-12-15 10:08:19 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.20, #running-req: 62, #queue-req: 60, 
[2025-12-15 10:08:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.22, #running-req: 68, #queue-req: 55, 
[2025-12-15 10:08:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15977, #cached-token: 28, token usage: 0.23, #running-req: 73, #queue-req: 50, 
[2025-12-15 10:08:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.25, #running-req: 78, #queue-req: 45, 
[2025-12-15 10:08:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.27, #running-req: 83, #queue-req: 40, 
[2025-12-15 10:08:29 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.29, #running-req: 88, #queue-req: 34, 
[2025-12-15 10:08:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.30, #running-req: 94, #queue-req: 29, 
[2025-12-15 10:08:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.32, #running-req: 99, #queue-req: 24, 
[2025-12-15 10:08:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.34, #running-req: 104, #queue-req: 19, 
[2025-12-15 10:08:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.35, #running-req: 109, #queue-req: 14, 
[2025-12-15 10:08:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.37, #running-req: 114, #queue-req: 9, 
[2025-12-15 10:08:40 TP0] Prefill batch, #new-seq: 6, #new-token: 15982, #cached-token: 3224, token usage: 0.38, #running-req: 119, #queue-req: 3, 
[2025-12-15 10:08:42 TP0] Prefill batch, #new-seq: 3, #new-token: 9592, #cached-token: 11, token usage: 0.40, #running-req: 125, #queue-req: 0, 
[2025-12-15 10:08:49 TP0] Decode batch, #running-req: 128, #token: 405149, token usage: 0.41, cuda graph: True, gen throughput (token/s): 100.59, #queue-req: 0, 
[2025-12-15 10:08:54 TP0] Decode batch, #running-req: 128, #token: 410269, token usage: 0.42, cuda graph: True, gen throughput (token/s): 881.29, #queue-req: 0, 
[2025-12-15 10:09:02 TP0] Decode batch, #running-req: 128, #token: 415389, token usage: 0.42, cuda graph: True, gen throughput (token/s): 689.66, #queue-req: 0, 
[2025-12-15 10:09:08 TP0] Decode batch, #running-req: 128, #token: 420509, token usage: 0.43, cuda graph: True, gen throughput (token/s): 876.03, #queue-req: 0, 
[2025-12-15 10:09:14 TP0] Decode batch, #running-req: 128, #token: 425629, token usage: 0.43, cuda graph: True, gen throughput (token/s): 872.68, #queue-req: 0, 
[2025-12-15 10:09:19 TP0] Decode batch, #running-req: 128, #token: 430749, token usage: 0.44, cuda graph: True, gen throughput (token/s): 864.15, #queue-req: 0, 
[2025-12-15 10:09:25 TP0] Decode batch, #running-req: 128, #token: 435869, token usage: 0.44, cuda graph: True, gen throughput (token/s): 863.89, #queue-req: 0, 
[2025-12-15 10:09:31 TP0] Decode batch, #running-req: 128, #token: 440989, token usage: 0.45, cuda graph: True, gen throughput (token/s): 863.49, #queue-req: 0, 
[2025-12-15 10:09:37 TP0] Decode batch, #running-req: 128, #token: 446109, token usage: 0.45, cuda graph: True, gen throughput (token/s): 863.23, #queue-req: 0, 
[2025-12-15 10:09:43 TP0] Decode batch, #running-req: 128, #token: 451229, token usage: 0.46, cuda graph: True, gen throughput (token/s): 863.20, #queue-req: 0, 
[2025-12-15 10:09:49 TP0] Decode batch, #running-req: 128, #token: 456349, token usage: 0.47, cuda graph: True, gen throughput (token/s): 859.02, #queue-req: 0, 
[2025-12-15 10:09:55 TP0] Decode batch, #running-req: 128, #token: 461469, token usage: 0.47, cuda graph: True, gen throughput (token/s): 851.24, #queue-req: 0, 
[2025-12-15 10:10:01 TP0] Decode batch, #running-req: 128, #token: 466589, token usage: 0.48, cuda graph: True, gen throughput (token/s): 851.14, #queue-req: 0, 
[2025-12-15 10:10:07 TP0] Decode batch, #running-req: 128, #token: 471709, token usage: 0.48, cuda graph: True, gen throughput (token/s): 850.95, #queue-req: 0, 
[2025-12-15 10:10:13 TP0] Decode batch, #running-req: 128, #token: 476829, token usage: 0.49, cuda graph: True, gen throughput (token/s): 850.93, #queue-req: 0, 
[2025-12-15 10:10:19 TP0] Decode batch, #running-req: 128, #token: 481949, token usage: 0.49, cuda graph: True, gen throughput (token/s): 850.67, #queue-req: 0, 
[2025-12-15 10:10:25 TP0] Decode batch, #running-req: 128, #token: 487069, token usage: 0.50, cuda graph: True, gen throughput (token/s): 846.87, #queue-req: 0, 
[2025-12-15 10:10:31 TP0] Decode batch, #running-req: 128, #token: 492189, token usage: 0.50, cuda graph: True, gen throughput (token/s): 839.27, #queue-req: 0, 
[2025-12-15 10:10:38 TP0] Decode batch, #running-req: 128, #token: 497309, token usage: 0.51, cuda graph: True, gen throughput (token/s): 839.77, #queue-req: 0, 
[2025-12-15 10:10:44 TP0] Decode batch, #running-req: 128, #token: 502429, token usage: 0.51, cuda graph: True, gen throughput (token/s): 839.65, #queue-req: 0, 
[2025-12-15 10:10:47] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47 TP0] Prefill batch, #new-seq: 2, #new-token: 6396, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:10:47] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:47] INFO:     127.0.0.1:58374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.01, #running-req: 2, #queue-req: 33, 
[2025-12-15 10:10:48] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58976 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:58990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59100 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:48] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:10:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.02, #running-req: 7, #queue-req: 116, 
[2025-12-15 10:10:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.04, #running-req: 12, #queue-req: 111, 
[2025-12-15 10:10:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.06, #running-req: 17, #queue-req: 106, 
[2025-12-15 10:10:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.07, #running-req: 22, #queue-req: 101, 
[2025-12-15 10:10:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.09, #running-req: 27, #queue-req: 96, 
[2025-12-15 10:10:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.10, #running-req: 32, #queue-req: 91, 
[2025-12-15 10:10:58 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.12, #running-req: 37, #queue-req: 85, 
[2025-12-15 10:10:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.14, #running-req: 43, #queue-req: 80, 
[2025-12-15 10:11:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.16, #running-req: 48, #queue-req: 75, 
[2025-12-15 10:11:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15978, #cached-token: 27, token usage: 0.17, #running-req: 53, #queue-req: 70, 
[2025-12-15 10:11:04 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.19, #running-req: 58, #queue-req: 64, 
[2025-12-15 10:11:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.21, #running-req: 64, #queue-req: 59, 
[2025-12-15 10:11:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.22, #running-req: 69, #queue-req: 54, 
[2025-12-15 10:11:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.24, #running-req: 74, #queue-req: 49, 
[2025-12-15 10:11:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.26, #running-req: 79, #queue-req: 44, 
[2025-12-15 10:11:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.27, #running-req: 84, #queue-req: 39, 
[2025-12-15 10:11:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.29, #running-req: 89, #queue-req: 34, 
[2025-12-15 10:11:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.31, #running-req: 94, #queue-req: 29, 
[2025-12-15 10:11:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.32, #running-req: 99, #queue-req: 24, 
[2025-12-15 10:11:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.34, #running-req: 104, #queue-req: 19, 
[2025-12-15 10:11:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.36, #running-req: 109, #queue-req: 14, 
[2025-12-15 10:11:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.37, #running-req: 114, #queue-req: 9, 
[2025-12-15 10:11:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.39, #running-req: 119, #queue-req: 4, 
[2025-12-15 10:11:23 TP0] Prefill batch, #new-seq: 4, #new-token: 12795, #cached-token: 9, token usage: 0.40, #running-req: 124, #queue-req: 0, 
[2025-12-15 10:11:31 TP0] Decode batch, #running-req: 128, #token: 411548, token usage: 0.42, cuda graph: True, gen throughput (token/s): 108.60, #queue-req: 0, 
[2025-12-15 10:11:36 TP0] Decode batch, #running-req: 128, #token: 416668, token usage: 0.42, cuda graph: True, gen throughput (token/s): 891.32, #queue-req: 0, 
[2025-12-15 10:11:42 TP0] Decode batch, #running-req: 128, #token: 421788, token usage: 0.43, cuda graph: True, gen throughput (token/s): 889.53, #queue-req: 0, 
[2025-12-15 10:11:48 TP0] Decode batch, #running-req: 128, #token: 426908, token usage: 0.44, cuda graph: True, gen throughput (token/s): 886.88, #queue-req: 0, 
[2025-12-15 10:11:54 TP0] Decode batch, #running-req: 128, #token: 432028, token usage: 0.44, cuda graph: True, gen throughput (token/s): 876.37, #queue-req: 0, 
[2025-12-15 10:12:00 TP0] Decode batch, #running-req: 128, #token: 437148, token usage: 0.45, cuda graph: True, gen throughput (token/s): 876.49, #queue-req: 0, 
[2025-12-15 10:12:06 TP0] Decode batch, #running-req: 128, #token: 442268, token usage: 0.45, cuda graph: True, gen throughput (token/s): 875.42, #queue-req: 0, 
[2025-12-15 10:12:11 TP0] Decode batch, #running-req: 128, #token: 447388, token usage: 0.46, cuda graph: True, gen throughput (token/s): 868.79, #queue-req: 0, 
[2025-12-15 10:12:17 TP0] Decode batch, #running-req: 128, #token: 452508, token usage: 0.46, cuda graph: True, gen throughput (token/s): 874.34, #queue-req: 0, 
[2025-12-15 10:12:23 TP0] Decode batch, #running-req: 128, #token: 457628, token usage: 0.47, cuda graph: True, gen throughput (token/s): 874.06, #queue-req: 0, 
[2025-12-15 10:12:29 TP0] Decode batch, #running-req: 128, #token: 462748, token usage: 0.47, cuda graph: True, gen throughput (token/s): 863.87, #queue-req: 0, 
[2025-12-15 10:12:35 TP0] Decode batch, #running-req: 128, #token: 467868, token usage: 0.48, cuda graph: True, gen throughput (token/s): 863.55, #queue-req: 0, 
[2025-12-15 10:12:41 TP0] Decode batch, #running-req: 128, #token: 472988, token usage: 0.48, cuda graph: True, gen throughput (token/s): 864.04, #queue-req: 0, 
[2025-12-15 10:12:47 TP0] Decode batch, #running-req: 128, #token: 478108, token usage: 0.49, cuda graph: True, gen throughput (token/s): 863.35, #queue-req: 0, 
[2025-12-15 10:12:53 TP0] Decode batch, #running-req: 128, #token: 483228, token usage: 0.49, cuda graph: True, gen throughput (token/s): 862.05, #queue-req: 0, 
[2025-12-15 10:12:59 TP0] Decode batch, #running-req: 128, #token: 488348, token usage: 0.50, cuda graph: True, gen throughput (token/s): 863.15, #queue-req: 0, 
[2025-12-15 10:13:05 TP0] Decode batch, #running-req: 128, #token: 493468, token usage: 0.50, cuda graph: True, gen throughput (token/s): 857.36, #queue-req: 0, 
[2025-12-15 10:13:11 TP0] Decode batch, #running-req: 128, #token: 498588, token usage: 0.51, cuda graph: True, gen throughput (token/s): 851.18, #queue-req: 0, 
[2025-12-15 10:13:17 TP0] Decode batch, #running-req: 128, #token: 503708, token usage: 0.51, cuda graph: True, gen throughput (token/s): 852.08, #queue-req: 0, 
[2025-12-15 10:13:23 TP0] Decode batch, #running-req: 128, #token: 508828, token usage: 0.52, cuda graph: True, gen throughput (token/s): 851.49, #queue-req: 0, 
[2025-12-15 10:13:26] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26 TP0] Prefill batch, #new-seq: 2, #new-token: 6394, #cached-token: 8, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:13:26] INFO:     127.0.0.1:56706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:26] INFO:     127.0.0.1:56900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:56914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:56936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:56944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:56960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.01, #running-req: 2, #queue-req: 25, 
[2025-12-15 10:13:27] INFO:     127.0.0.1:56992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:56998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57146 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:13:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 7, #queue-req: 104, 
[2025-12-15 10:13:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 12, #queue-req: 99, 
[2025-12-15 10:13:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.06, #running-req: 17, #queue-req: 94, 
[2025-12-15 10:13:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.07, #running-req: 22, #queue-req: 89, 
[2025-12-15 10:13:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 27, #queue-req: 84, 
[2025-12-15 10:13:35 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.10, #running-req: 32, #queue-req: 79, 
[2025-12-15 10:13:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.12, #running-req: 37, #queue-req: 74, 
[2025-12-15 10:13:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.14, #running-req: 42, #queue-req: 69, 
[2025-12-15 10:13:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 47, #queue-req: 64, 
[2025-12-15 10:13:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.17, #running-req: 52, #queue-req: 59, 
[2025-12-15 10:13:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.19, #running-req: 57, #queue-req: 54, 
[2025-12-15 10:13:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.20, #running-req: 62, #queue-req: 49, 
[2025-12-15 10:13:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.22, #running-req: 67, #queue-req: 44, 
[2025-12-15 10:13:47 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.24, #running-req: 72, #queue-req: 38, 
[2025-12-15 10:13:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.25, #running-req: 78, #queue-req: 33, 
[2025-12-15 10:13:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.27, #running-req: 83, #queue-req: 28, 
[2025-12-15 10:13:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.29, #running-req: 88, #queue-req: 23, 
[2025-12-15 10:13:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.30, #running-req: 93, #queue-req: 18, 
[2025-12-15 10:13:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.32, #running-req: 98, #queue-req: 13, 
[2025-12-15 10:13:56 TP0] Prefill batch, #new-seq: 6, #new-token: 15992, #cached-token: 3214, token usage: 0.34, #running-req: 103, #queue-req: 7, 
[2025-12-15 10:13:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.36, #running-req: 109, #queue-req: 2, 
[2025-12-15 10:13:59 TP0] Prefill batch, #new-seq: 2, #new-token: 6395, #cached-token: 7, token usage: 0.37, #running-req: 114, #queue-req: 0, 
[2025-12-15 10:14:03 TP0] Decode batch, #running-req: 116, #token: 372970, token usage: 0.38, cuda graph: True, gen throughput (token/s): 121.32, #queue-req: 0, 
[2025-12-15 10:14:09 TP0] Decode batch, #running-req: 116, #token: 377610, token usage: 0.38, cuda graph: True, gen throughput (token/s): 805.08, #queue-req: 0, 
[2025-12-15 10:14:15 TP0] Decode batch, #running-req: 116, #token: 382250, token usage: 0.39, cuda graph: True, gen throughput (token/s): 806.26, #queue-req: 0, 
[2025-12-15 10:14:21 TP0] Decode batch, #running-req: 116, #token: 386890, token usage: 0.39, cuda graph: True, gen throughput (token/s): 804.15, #queue-req: 0, 
[2025-12-15 10:14:26 TP0] Decode batch, #running-req: 116, #token: 391530, token usage: 0.40, cuda graph: True, gen throughput (token/s): 796.12, #queue-req: 0, 
[2025-12-15 10:14:32 TP0] Decode batch, #running-req: 116, #token: 396170, token usage: 0.40, cuda graph: True, gen throughput (token/s): 792.87, #queue-req: 0, 
[2025-12-15 10:14:38 TP0] Decode batch, #running-req: 116, #token: 400810, token usage: 0.41, cuda graph: True, gen throughput (token/s): 793.50, #queue-req: 0, 
[2025-12-15 10:14:44 TP0] Decode batch, #running-req: 116, #token: 405450, token usage: 0.41, cuda graph: True, gen throughput (token/s): 791.48, #queue-req: 0, 
[2025-12-15 10:14:50 TP0] Decode batch, #running-req: 116, #token: 410090, token usage: 0.42, cuda graph: True, gen throughput (token/s): 792.12, #queue-req: 0, 
[2025-12-15 10:14:56 TP0] Decode batch, #running-req: 116, #token: 414730, token usage: 0.42, cuda graph: True, gen throughput (token/s): 792.54, #queue-req: 0, 
[2025-12-15 10:15:02 TP0] Decode batch, #running-req: 116, #token: 419370, token usage: 0.43, cuda graph: True, gen throughput (token/s): 799.58, #queue-req: 0, 
[2025-12-15 10:15:08 TP0] Decode batch, #running-req: 116, #token: 424010, token usage: 0.43, cuda graph: True, gen throughput (token/s): 782.30, #queue-req: 0, 
[2025-12-15 10:15:13 TP0] Decode batch, #running-req: 116, #token: 428650, token usage: 0.44, cuda graph: True, gen throughput (token/s): 782.47, #queue-req: 0, 
[2025-12-15 10:15:19 TP0] Decode batch, #running-req: 116, #token: 433290, token usage: 0.44, cuda graph: True, gen throughput (token/s): 782.46, #queue-req: 0, 
[2025-12-15 10:15:25 TP0] Decode batch, #running-req: 116, #token: 437930, token usage: 0.45, cuda graph: True, gen throughput (token/s): 781.98, #queue-req: 0, 
[2025-12-15 10:15:31 TP0] Decode batch, #running-req: 116, #token: 442570, token usage: 0.45, cuda graph: True, gen throughput (token/s): 782.08, #queue-req: 0, 
[2025-12-15 10:15:37 TP0] Decode batch, #running-req: 116, #token: 447210, token usage: 0.46, cuda graph: True, gen throughput (token/s): 779.58, #queue-req: 0, 
[2025-12-15 10:15:43 TP0] Decode batch, #running-req: 116, #token: 451850, token usage: 0.46, cuda graph: True, gen throughput (token/s): 770.03, #queue-req: 0, 
[2025-12-15 10:15:49 TP0] Decode batch, #running-req: 116, #token: 456490, token usage: 0.47, cuda graph: True, gen throughput (token/s): 770.15, #queue-req: 0, 
[2025-12-15 10:15:55 TP0] Decode batch, #running-req: 116, #token: 461130, token usage: 0.47, cuda graph: True, gen throughput (token/s): 770.44, #queue-req: 0, 
[2025-12-15 10:15:59] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-15 10:15:59] INFO:     127.0.0.1:49976 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-15 10:16:00] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-15 10:16:00] INFO:     127.0.0.1:49988 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-15 10:16:10] INFO:     127.0.0.1:57700 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-15 10:16:17] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:17 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:16:20 TP0] Decode batch, #running-req: 1, #token: 3218, token usage: 0.00, cuda graph: True, gen throughput (token/s): 112.38, #queue-req: 0, 
[2025-12-15 10:16:22] INFO:     127.0.0.1:46068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:16:22] INFO:     127.0.0.1:46070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46286 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 23, 
[2025-12-15 10:16:22] INFO:     127.0.0.1:46330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22] INFO:     127.0.0.1:46650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:16:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.02, #running-req: 6, #queue-req: 53, 
[2025-12-15 10:16:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-12-15 10:16:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-12-15 10:16:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-12-15 10:16:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.08, #running-req: 26, #queue-req: 33, 
[2025-12-15 10:16:30 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.10, #running-req: 31, #queue-req: 27, 
[2025-12-15 10:16:30 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.12, #running-req: 37, #queue-req: 21, 
[2025-12-15 10:16:31 TP0] Prefill batch, #new-seq: 6, #new-token: 15986, #cached-token: 3220, token usage: 0.14, #running-req: 43, #queue-req: 15, 
[2025-12-15 10:16:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.16, #running-req: 49, #queue-req: 10, 
[2025-12-15 10:16:35 TP0] Prefill batch, #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.18, #running-req: 54, #queue-req: 4, 
[2025-12-15 10:16:35 TP0] Prefill batch, #new-seq: 4, #new-token: 12793, #cached-token: 11, token usage: 0.20, #running-req: 60, #queue-req: 0, 
[2025-12-15 10:16:38 TP0] Decode batch, #running-req: 64, #token: 206312, token usage: 0.21, cuda graph: True, gen throughput (token/s): 86.74, #queue-req: 0, 
[2025-12-15 10:16:40 TP0] Decode batch, #running-req: 64, #token: 208872, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1223.87, #queue-req: 0, 
[2025-12-15 10:16:42 TP0] Decode batch, #running-req: 64, #token: 211432, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1212.70, #queue-req: 0, 
[2025-12-15 10:16:46 TP0] Decode batch, #running-req: 64, #token: 213992, token usage: 0.22, cuda graph: True, gen throughput (token/s): 693.62, #queue-req: 0, 
[2025-12-15 10:16:48 TP0] Decode batch, #running-req: 64, #token: 216552, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1194.84, #queue-req: 0, 
[2025-12-15 10:16:50 TP0] Decode batch, #running-req: 64, #token: 219112, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1193.93, #queue-req: 0, 
[2025-12-15 10:16:52 TP0] Decode batch, #running-req: 64, #token: 221672, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1186.89, #queue-req: 0, 
[2025-12-15 10:16:55 TP0] Decode batch, #running-req: 64, #token: 224232, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1191.22, #queue-req: 0, 
[2025-12-15 10:16:57 TP0] Decode batch, #running-req: 64, #token: 226792, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1185.09, #queue-req: 0, 
[2025-12-15 10:16:59 TP0] Decode batch, #running-req: 64, #token: 229352, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1182.35, #queue-req: 0, 
[2025-12-15 10:17:01 TP0] Decode batch, #running-req: 64, #token: 231912, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1173.59, #queue-req: 0, 
[2025-12-15 10:17:03 TP0] Decode batch, #running-req: 64, #token: 234472, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1174.93, #queue-req: 0, 
[2025-12-15 10:17:05 TP0] Decode batch, #running-req: 64, #token: 237032, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1158.59, #queue-req: 0, 
[2025-12-15 10:17:08 TP0] Decode batch, #running-req: 64, #token: 239592, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1170.89, #queue-req: 0, 
[2025-12-15 10:17:11 TP0] Decode batch, #running-req: 64, #token: 242152, token usage: 0.25, cuda graph: True, gen throughput (token/s): 733.55, #queue-req: 0, 
[2025-12-15 10:17:16 TP0] Decode batch, #running-req: 64, #token: 244712, token usage: 0.25, cuda graph: True, gen throughput (token/s): 506.41, #queue-req: 0, 
[2025-12-15 10:17:21 TP0] Decode batch, #running-req: 64, #token: 247272, token usage: 0.25, cuda graph: True, gen throughput (token/s): 506.61, #queue-req: 0, 
[2025-12-15 10:17:26 TP0] Decode batch, #running-req: 64, #token: 249832, token usage: 0.25, cuda graph: True, gen throughput (token/s): 506.89, #queue-req: 0, 
[2025-12-15 10:17:36 TP0] Decode batch, #running-req: 64, #token: 252392, token usage: 0.26, cuda graph: True, gen throughput (token/s): 254.91, #queue-req: 0, 
[2025-12-15 10:17:41 TP0] Decode batch, #running-req: 64, #token: 254952, token usage: 0.26, cuda graph: True, gen throughput (token/s): 514.45, #queue-req: 0, 
[2025-12-15 10:17:43] INFO:     127.0.0.1:42346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 3, 
[2025-12-15 10:17:43] INFO:     127.0.0.1:42426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:43] INFO:     127.0.0.1:42740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44] INFO:     127.0.0.1:42910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:17:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 5, #queue-req: 54, 
[2025-12-15 10:17:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.03, #running-req: 10, #queue-req: 49, 
[2025-12-15 10:17:47 TP0] Prefill batch, #new-seq: 6, #new-token: 15984, #cached-token: 3222, token usage: 0.05, #running-req: 15, #queue-req: 43, 
[2025-12-15 10:17:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-12-15 10:17:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.08, #running-req: 26, #queue-req: 33, 
[2025-12-15 10:17:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-12-15 10:17:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-12-15 10:17:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-12-15 10:17:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-12-15 10:17:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-12-15 10:17:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-12-15 10:18:00 TP0] Prefill batch, #new-seq: 3, #new-token: 9591, #cached-token: 12, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-12-15 10:18:05 TP0] Decode batch, #running-req: 64, #token: 206295, token usage: 0.21, cuda graph: True, gen throughput (token/s): 106.42, #queue-req: 0, 
[2025-12-15 10:18:10 TP0] Decode batch, #running-req: 64, #token: 208855, token usage: 0.21, cuda graph: True, gen throughput (token/s): 533.25, #queue-req: 0, 
[2025-12-15 10:18:15 TP0] Decode batch, #running-req: 64, #token: 211415, token usage: 0.22, cuda graph: True, gen throughput (token/s): 532.91, #queue-req: 0, 
[2025-12-15 10:18:20 TP0] Decode batch, #running-req: 64, #token: 213975, token usage: 0.22, cuda graph: True, gen throughput (token/s): 530.82, #queue-req: 0, 
[2025-12-15 10:18:25 TP0] Decode batch, #running-req: 64, #token: 216535, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.71, #queue-req: 0, 
[2025-12-15 10:18:29 TP0] Decode batch, #running-req: 64, #token: 219095, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.16, #queue-req: 0, 
[2025-12-15 10:18:34 TP0] Decode batch, #running-req: 64, #token: 221655, token usage: 0.23, cuda graph: True, gen throughput (token/s): 527.06, #queue-req: 0, 
[2025-12-15 10:18:39 TP0] Decode batch, #running-req: 64, #token: 224215, token usage: 0.23, cuda graph: True, gen throughput (token/s): 527.00, #queue-req: 0, 
[2025-12-15 10:18:44 TP0] Decode batch, #running-req: 64, #token: 226775, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.93, #queue-req: 0, 
[2025-12-15 10:18:49 TP0] Decode batch, #running-req: 64, #token: 229335, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.41, #queue-req: 0, 
[2025-12-15 10:18:54 TP0] Decode batch, #running-req: 64, #token: 231895, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.72, #queue-req: 0, 
[2025-12-15 10:18:59 TP0] Decode batch, #running-req: 64, #token: 234455, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.74, #queue-req: 0, 
[2025-12-15 10:19:04 TP0] Decode batch, #running-req: 64, #token: 237015, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.82, #queue-req: 0, 
[2025-12-15 10:19:09 TP0] Decode batch, #running-req: 64, #token: 239575, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.73, #queue-req: 0, 
[2025-12-15 10:19:13 TP0] Decode batch, #running-req: 64, #token: 242135, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.55, #queue-req: 0, 
[2025-12-15 10:19:18 TP0] Decode batch, #running-req: 64, #token: 244695, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.87, #queue-req: 0, 
[2025-12-15 10:19:23 TP0] Decode batch, #running-req: 64, #token: 247255, token usage: 0.25, cuda graph: True, gen throughput (token/s): 518.45, #queue-req: 0, 
[2025-12-15 10:19:28 TP0] Decode batch, #running-req: 64, #token: 249815, token usage: 0.25, cuda graph: True, gen throughput (token/s): 516.52, #queue-req: 0, 
[2025-12-15 10:19:33 TP0] Decode batch, #running-req: 64, #token: 252375, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.81, #queue-req: 0, 
[2025-12-15 10:19:38 TP0] Decode batch, #running-req: 64, #token: 254935, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.99, #queue-req: 0, 
[2025-12-15 10:19:40] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 2, 
[2025-12-15 10:19:40] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:47994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48328 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:40] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:19:41 TP0] Prefill batch, #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.02, #running-req: 5, #queue-req: 53, 
[2025-12-15 10:19:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-12-15 10:19:44 TP0] Prefill batch, #new-seq: 7, #new-token: 15992, #cached-token: 6415, token usage: 0.06, #running-req: 16, #queue-req: 41, 
[2025-12-15 10:19:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 23, #queue-req: 36, 
[2025-12-15 10:19:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.09, #running-req: 28, #queue-req: 31, 
[2025-12-15 10:19:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.11, #running-req: 33, #queue-req: 26, 
[2025-12-15 10:19:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.12, #running-req: 38, #queue-req: 21, 
[2025-12-15 10:19:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.14, #running-req: 43, #queue-req: 16, 
[2025-12-15 10:19:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.16, #running-req: 48, #queue-req: 11, 
[2025-12-15 10:19:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.17, #running-req: 53, #queue-req: 6, 
[2025-12-15 10:19:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.19, #running-req: 58, #queue-req: 1, 
[2025-12-15 10:19:58 TP0] Prefill batch, #new-seq: 1, #new-token: 3198, #cached-token: 3, token usage: 0.21, #running-req: 63, #queue-req: 0, 
[2025-12-15 10:20:02 TP0] Decode batch, #running-req: 64, #token: 206309, token usage: 0.21, cuda graph: True, gen throughput (token/s): 107.70, #queue-req: 0, 
[2025-12-15 10:20:07 TP0] Decode batch, #running-req: 64, #token: 208869, token usage: 0.21, cuda graph: True, gen throughput (token/s): 534.07, #queue-req: 0, 
[2025-12-15 10:20:12 TP0] Decode batch, #running-req: 64, #token: 211429, token usage: 0.22, cuda graph: True, gen throughput (token/s): 533.11, #queue-req: 0, 
[2025-12-15 10:20:16 TP0] Decode batch, #running-req: 64, #token: 213989, token usage: 0.22, cuda graph: True, gen throughput (token/s): 530.73, #queue-req: 0, 
[2025-12-15 10:20:21 TP0] Decode batch, #running-req: 64, #token: 216549, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.08, #queue-req: 0, 
[2025-12-15 10:20:26 TP0] Decode batch, #running-req: 64, #token: 219109, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.65, #queue-req: 0, 
[2025-12-15 10:20:31 TP0] Decode batch, #running-req: 64, #token: 221669, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.33, #queue-req: 0, 
[2025-12-15 10:20:36 TP0] Decode batch, #running-req: 64, #token: 224229, token usage: 0.23, cuda graph: True, gen throughput (token/s): 525.98, #queue-req: 0, 
[2025-12-15 10:20:41 TP0] Decode batch, #running-req: 64, #token: 226789, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.12, #queue-req: 0, 
[2025-12-15 10:20:46 TP0] Decode batch, #running-req: 64, #token: 229349, token usage: 0.23, cuda graph: True, gen throughput (token/s): 525.92, #queue-req: 0, 
[2025-12-15 10:20:50 TP0] Decode batch, #running-req: 64, #token: 231909, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.35, #queue-req: 0, 
[2025-12-15 10:20:55 TP0] Decode batch, #running-req: 64, #token: 234469, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.53, #queue-req: 0, 
[2025-12-15 10:21:00 TP0] Decode batch, #running-req: 64, #token: 237029, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.22, #queue-req: 0, 
[2025-12-15 10:21:05 TP0] Decode batch, #running-req: 64, #token: 239589, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.41, #queue-req: 0, 
[2025-12-15 10:21:10 TP0] Decode batch, #running-req: 64, #token: 242149, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.44, #queue-req: 0, 
[2025-12-15 10:21:15 TP0] Decode batch, #running-req: 64, #token: 244709, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.38, #queue-req: 0, 
[2025-12-15 10:21:20 TP0] Decode batch, #running-req: 64, #token: 247269, token usage: 0.25, cuda graph: True, gen throughput (token/s): 518.11, #queue-req: 0, 
[2025-12-15 10:21:25 TP0] Decode batch, #running-req: 64, #token: 249829, token usage: 0.25, cuda graph: True, gen throughput (token/s): 516.39, #queue-req: 0, 
[2025-12-15 10:21:30 TP0] Decode batch, #running-req: 64, #token: 252389, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.24, #queue-req: 0, 
[2025-12-15 10:21:35 TP0] Decode batch, #running-req: 64, #token: 254949, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.45, #queue-req: 0, 
[2025-12-15 10:21:37] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.00, #running-req: 0, #queue-req: 1, 
[2025-12-15 10:21:37] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37] INFO:     127.0.0.1:53386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:21:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.02, #running-req: 6, #queue-req: 53, 
[2025-12-15 10:21:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-12-15 10:21:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-12-15 10:21:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-12-15 10:21:43 TP0] Prefill batch, #new-seq: 7, #new-token: 15992, #cached-token: 6415, token usage: 0.09, #running-req: 26, #queue-req: 31, 
[2025-12-15 10:21:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.11, #running-req: 33, #queue-req: 26, 
[2025-12-15 10:21:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 38, #queue-req: 21, 
[2025-12-15 10:21:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.14, #running-req: 43, #queue-req: 16, 
[2025-12-15 10:21:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.16, #running-req: 48, #queue-req: 11, 
[2025-12-15 10:21:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.17, #running-req: 53, #queue-req: 6, 
[2025-12-15 10:21:52 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.19, #running-req: 58, #queue-req: 0, 
[2025-12-15 10:21:58 TP0] Decode batch, #running-req: 64, #token: 203098, token usage: 0.21, cuda graph: True, gen throughput (token/s): 111.00, #queue-req: 0, 
[2025-12-15 10:22:03 TP0] Decode batch, #running-req: 64, #token: 205658, token usage: 0.21, cuda graph: True, gen throughput (token/s): 533.54, #queue-req: 0, 
[2025-12-15 10:22:07 TP0] Decode batch, #running-req: 64, #token: 208218, token usage: 0.21, cuda graph: True, gen throughput (token/s): 532.23, #queue-req: 0, 
[2025-12-15 10:22:12 TP0] Decode batch, #running-req: 64, #token: 210778, token usage: 0.21, cuda graph: True, gen throughput (token/s): 530.48, #queue-req: 0, 
[2025-12-15 10:22:17 TP0] Decode batch, #running-req: 64, #token: 213338, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.27, #queue-req: 0, 
[2025-12-15 10:22:22 TP0] Decode batch, #running-req: 64, #token: 215898, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.84, #queue-req: 0, 
[2025-12-15 10:22:27 TP0] Decode batch, #running-req: 64, #token: 218458, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.55, #queue-req: 0, 
[2025-12-15 10:22:32 TP0] Decode batch, #running-req: 64, #token: 221018, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.61, #queue-req: 0, 
[2025-12-15 10:22:37 TP0] Decode batch, #running-req: 64, #token: 223578, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.63, #queue-req: 0, 
[2025-12-15 10:22:41 TP0] Decode batch, #running-req: 64, #token: 226138, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.25, #queue-req: 0, 
[2025-12-15 10:22:46 TP0] Decode batch, #running-req: 64, #token: 228698, token usage: 0.23, cuda graph: True, gen throughput (token/s): 521.44, #queue-req: 0, 
[2025-12-15 10:22:51 TP0] Decode batch, #running-req: 64, #token: 231258, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.53, #queue-req: 0, 
[2025-12-15 10:22:56 TP0] Decode batch, #running-req: 64, #token: 233818, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.38, #queue-req: 0, 
[2025-12-15 10:23:01 TP0] Decode batch, #running-req: 64, #token: 236378, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.46, #queue-req: 0, 
[2025-12-15 10:23:06 TP0] Decode batch, #running-req: 64, #token: 238938, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.70, #queue-req: 0, 
[2025-12-15 10:23:11 TP0] Decode batch, #running-req: 64, #token: 241498, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.72, #queue-req: 0, 
[2025-12-15 10:23:16 TP0] Decode batch, #running-req: 64, #token: 244058, token usage: 0.25, cuda graph: True, gen throughput (token/s): 518.72, #queue-req: 0, 
[2025-12-15 10:23:21 TP0] Decode batch, #running-req: 64, #token: 246618, token usage: 0.25, cuda graph: True, gen throughput (token/s): 516.68, #queue-req: 0, 
[2025-12-15 10:23:26 TP0] Decode batch, #running-req: 64, #token: 249178, token usage: 0.25, cuda graph: True, gen throughput (token/s): 517.02, #queue-req: 0, 
[2025-12-15 10:23:31 TP0] Decode batch, #running-req: 64, #token: 251738, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.12, #queue-req: 0, 
[2025-12-15 10:23:33] INFO:     127.0.0.1:55666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.00, #running-req: 0, #queue-req: 4, 
[2025-12-15 10:23:33] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:55996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:23:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.02, #running-req: 5, #queue-req: 54, 
[2025-12-15 10:23:35 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.03, #running-req: 10, #queue-req: 49, 
[2025-12-15 10:23:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.05, #running-req: 15, #queue-req: 44, 
[2025-12-15 10:23:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 20, #queue-req: 39, 
[2025-12-15 10:23:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.08, #running-req: 25, #queue-req: 34, 
[2025-12-15 10:23:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.10, #running-req: 30, #queue-req: 29, 
[2025-12-15 10:23:42 TP0] Prefill batch, #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.12, #running-req: 35, #queue-req: 23, 
[2025-12-15 10:23:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-12-15 10:23:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-12-15 10:23:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-12-15 10:23:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-12-15 10:23:50 TP0] Prefill batch, #new-seq: 3, #new-token: 6391, #cached-token: 3212, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-12-15 10:23:54 TP0] Decode batch, #running-req: 64, #token: 206298, token usage: 0.21, cuda graph: True, gen throughput (token/s): 107.70, #queue-req: 0, 
[2025-12-15 10:23:59 TP0] Decode batch, #running-req: 64, #token: 208858, token usage: 0.21, cuda graph: True, gen throughput (token/s): 533.33, #queue-req: 0, 
[2025-12-15 10:24:04 TP0] Decode batch, #running-req: 64, #token: 211418, token usage: 0.22, cuda graph: True, gen throughput (token/s): 533.76, #queue-req: 0, 
[2025-12-15 10:24:09 TP0] Decode batch, #running-req: 64, #token: 213978, token usage: 0.22, cuda graph: True, gen throughput (token/s): 531.77, #queue-req: 0, 
[2025-12-15 10:24:14 TP0] Decode batch, #running-req: 64, #token: 216538, token usage: 0.22, cuda graph: True, gen throughput (token/s): 528.07, #queue-req: 0, 
[2025-12-15 10:24:19 TP0] Decode batch, #running-req: 64, #token: 219098, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.43, #queue-req: 0, 
[2025-12-15 10:24:23 TP0] Decode batch, #running-req: 64, #token: 221658, token usage: 0.23, cuda graph: True, gen throughput (token/s): 527.00, #queue-req: 0, 
[2025-12-15 10:24:28 TP0] Decode batch, #running-req: 64, #token: 224218, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.80, #queue-req: 0, 
[2025-12-15 10:24:33 TP0] Decode batch, #running-req: 64, #token: 226778, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.65, #queue-req: 0, 
[2025-12-15 10:24:38 TP0] Decode batch, #running-req: 64, #token: 229338, token usage: 0.23, cuda graph: True, gen throughput (token/s): 527.20, #queue-req: 0, 
[2025-12-15 10:24:43 TP0] Decode batch, #running-req: 64, #token: 231898, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.04, #queue-req: 0, 
[2025-12-15 10:24:48 TP0] Decode batch, #running-req: 64, #token: 234458, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.45, #queue-req: 0, 
[2025-12-15 10:24:53 TP0] Decode batch, #running-req: 64, #token: 237018, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.76, #queue-req: 0, 
[2025-12-15 10:24:58 TP0] Decode batch, #running-req: 64, #token: 239578, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.82, #queue-req: 0, 
[2025-12-15 10:25:03 TP0] Decode batch, #running-req: 64, #token: 242138, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.73, #queue-req: 0, 
[2025-12-15 10:25:07 TP0] Decode batch, #running-req: 64, #token: 244698, token usage: 0.25, cuda graph: True, gen throughput (token/s): 523.12, #queue-req: 0, 
[2025-12-15 10:25:12 TP0] Decode batch, #running-req: 64, #token: 247258, token usage: 0.25, cuda graph: True, gen throughput (token/s): 519.78, #queue-req: 0, 
[2025-12-15 10:25:17 TP0] Decode batch, #running-req: 64, #token: 249818, token usage: 0.25, cuda graph: True, gen throughput (token/s): 518.18, #queue-req: 0, 
[2025-12-15 10:25:22 TP0] Decode batch, #running-req: 64, #token: 252378, token usage: 0.26, cuda graph: True, gen throughput (token/s): 518.40, #queue-req: 0, 
[2025-12-15 10:25:27 TP0] Decode batch, #running-req: 64, #token: 254938, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.98, #queue-req: 0, 
[2025-12-15 10:25:29] INFO:     127.0.0.1:39072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.00, #running-req: 0, #queue-req: 3, 
[2025-12-15 10:25:29] INFO:     127.0.0.1:39156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39222 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39286 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39328 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:29] INFO:     127.0.0.1:39584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:25:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 5, #queue-req: 54, 
[2025-12-15 10:25:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.03, #running-req: 10, #queue-req: 49, 
[2025-12-15 10:25:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 15, #queue-req: 44, 
[2025-12-15 10:25:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.07, #running-req: 20, #queue-req: 39, 
[2025-12-15 10:25:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.08, #running-req: 25, #queue-req: 34, 
[2025-12-15 10:25:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.10, #running-req: 30, #queue-req: 29, 
[2025-12-15 10:25:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.11, #running-req: 35, #queue-req: 24, 
[2025-12-15 10:25:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.13, #running-req: 40, #queue-req: 19, 
[2025-12-15 10:25:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.15, #running-req: 45, #queue-req: 14, 
[2025-12-15 10:25:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.16, #running-req: 50, #queue-req: 9, 
[2025-12-15 10:25:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.18, #running-req: 55, #queue-req: 4, 
[2025-12-15 10:25:46 TP0] Prefill batch, #new-seq: 4, #new-token: 12795, #cached-token: 9, token usage: 0.20, #running-req: 60, #queue-req: 0, 
[2025-12-15 10:25:51 TP0] Decode batch, #running-req: 64, #token: 206312, token usage: 0.21, cuda graph: True, gen throughput (token/s): 105.72, #queue-req: 0, 
[2025-12-15 10:25:56 TP0] Decode batch, #running-req: 64, #token: 208872, token usage: 0.21, cuda graph: True, gen throughput (token/s): 532.66, #queue-req: 0, 
[2025-12-15 10:26:01 TP0] Decode batch, #running-req: 64, #token: 211432, token usage: 0.22, cuda graph: True, gen throughput (token/s): 532.27, #queue-req: 0, 
[2025-12-15 10:26:06 TP0] Decode batch, #running-req: 64, #token: 213992, token usage: 0.22, cuda graph: True, gen throughput (token/s): 530.23, #queue-req: 0, 
[2025-12-15 10:26:11 TP0] Decode batch, #running-req: 64, #token: 216552, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.83, #queue-req: 0, 
[2025-12-15 10:26:16 TP0] Decode batch, #running-req: 64, #token: 219112, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.94, #queue-req: 0, 
[2025-12-15 10:26:20 TP0] Decode batch, #running-req: 64, #token: 221672, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.91, #queue-req: 0, 
[2025-12-15 10:26:25 TP0] Decode batch, #running-req: 64, #token: 224232, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.73, #queue-req: 0, 
[2025-12-15 10:26:30 TP0] Decode batch, #running-req: 64, #token: 226792, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.89, #queue-req: 0, 
[2025-12-15 10:26:35 TP0] Decode batch, #running-req: 64, #token: 229352, token usage: 0.23, cuda graph: True, gen throughput (token/s): 527.02, #queue-req: 0, 
[2025-12-15 10:26:40 TP0] Decode batch, #running-req: 64, #token: 231912, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.62, #queue-req: 0, 
[2025-12-15 10:26:45 TP0] Decode batch, #running-req: 64, #token: 234472, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.61, #queue-req: 0, 
[2025-12-15 10:26:50 TP0] Decode batch, #running-req: 64, #token: 237032, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.24, #queue-req: 0, 
[2025-12-15 10:26:55 TP0] Decode batch, #running-req: 64, #token: 239592, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.54, #queue-req: 0, 
[2025-12-15 10:26:59 TP0] Decode batch, #running-req: 64, #token: 242152, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.06, #queue-req: 0, 
[2025-12-15 10:27:04 TP0] Decode batch, #running-req: 64, #token: 244712, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.19, #queue-req: 0, 
[2025-12-15 10:27:09 TP0] Decode batch, #running-req: 64, #token: 247272, token usage: 0.25, cuda graph: True, gen throughput (token/s): 518.57, #queue-req: 0, 
[2025-12-15 10:27:14 TP0] Decode batch, #running-req: 64, #token: 249832, token usage: 0.25, cuda graph: True, gen throughput (token/s): 516.84, #queue-req: 0, 
[2025-12-15 10:27:19 TP0] Decode batch, #running-req: 64, #token: 252392, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.73, #queue-req: 0, 
[2025-12-15 10:27:24 TP0] Decode batch, #running-req: 64, #token: 254952, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.63, #queue-req: 0, 
[2025-12-15 10:27:26] INFO:     127.0.0.1:53182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.00, #running-req: 0, #queue-req: 3, 
[2025-12-15 10:27:26] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:26] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:27] INFO:     127.0.0.1:53644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:27:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.02, #running-req: 5, #queue-req: 54, 
[2025-12-15 10:27:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.03, #running-req: 10, #queue-req: 49, 
[2025-12-15 10:27:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 15, #queue-req: 44, 
[2025-12-15 10:27:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 20, #queue-req: 39, 
[2025-12-15 10:27:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.08, #running-req: 25, #queue-req: 34, 
[2025-12-15 10:27:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.10, #running-req: 30, #queue-req: 29, 
[2025-12-15 10:27:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.11, #running-req: 35, #queue-req: 24, 
[2025-12-15 10:27:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.13, #running-req: 40, #queue-req: 19, 
[2025-12-15 10:27:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.15, #running-req: 45, #queue-req: 14, 
[2025-12-15 10:27:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.16, #running-req: 50, #queue-req: 9, 
[2025-12-15 10:27:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.18, #running-req: 55, #queue-req: 4, 
[2025-12-15 10:27:43 TP0] Prefill batch, #new-seq: 4, #new-token: 12791, #cached-token: 13, token usage: 0.20, #running-req: 60, #queue-req: 0, 
[2025-12-15 10:27:48 TP0] Decode batch, #running-req: 64, #token: 206302, token usage: 0.21, cuda graph: True, gen throughput (token/s): 105.60, #queue-req: 0, 
[2025-12-15 10:27:53 TP0] Decode batch, #running-req: 64, #token: 208862, token usage: 0.21, cuda graph: True, gen throughput (token/s): 533.42, #queue-req: 0, 
[2025-12-15 10:27:58 TP0] Decode batch, #running-req: 64, #token: 211422, token usage: 0.22, cuda graph: True, gen throughput (token/s): 533.44, #queue-req: 0, 
[2025-12-15 10:28:03 TP0] Decode batch, #running-req: 64, #token: 213982, token usage: 0.22, cuda graph: True, gen throughput (token/s): 531.56, #queue-req: 0, 
[2025-12-15 10:28:08 TP0] Decode batch, #running-req: 64, #token: 216542, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.73, #queue-req: 0, 
[2025-12-15 10:28:13 TP0] Decode batch, #running-req: 64, #token: 219102, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.16, #queue-req: 0, 
[2025-12-15 10:28:17 TP0] Decode batch, #running-req: 64, #token: 221662, token usage: 0.23, cuda graph: True, gen throughput (token/s): 527.10, #queue-req: 0, 
[2025-12-15 10:28:22 TP0] Decode batch, #running-req: 64, #token: 224222, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.91, #queue-req: 0, 
[2025-12-15 10:28:27 TP0] Decode batch, #running-req: 64, #token: 226782, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.88, #queue-req: 0, 
[2025-12-15 10:28:32 TP0] Decode batch, #running-req: 64, #token: 229342, token usage: 0.23, cuda graph: True, gen throughput (token/s): 527.27, #queue-req: 0, 
[2025-12-15 10:28:37 TP0] Decode batch, #running-req: 64, #token: 231902, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.63, #queue-req: 0, 
[2025-12-15 10:28:42 TP0] Decode batch, #running-req: 64, #token: 234462, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.62, #queue-req: 0, 
[2025-12-15 10:28:47 TP0] Decode batch, #running-req: 64, #token: 237022, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.48, #queue-req: 0, 
[2025-12-15 10:28:52 TP0] Decode batch, #running-req: 64, #token: 239582, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.67, #queue-req: 0, 
[2025-12-15 10:28:56 TP0] Decode batch, #running-req: 64, #token: 242142, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.51, #queue-req: 0, 
[2025-12-15 10:29:01 TP0] Decode batch, #running-req: 64, #token: 244702, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.32, #queue-req: 0, 
[2025-12-15 10:29:06 TP0] Decode batch, #running-req: 64, #token: 247262, token usage: 0.25, cuda graph: True, gen throughput (token/s): 519.12, #queue-req: 0, 
[2025-12-15 10:29:11 TP0] Decode batch, #running-req: 64, #token: 249822, token usage: 0.25, cuda graph: True, gen throughput (token/s): 517.30, #queue-req: 0, 
[2025-12-15 10:29:16 TP0] Decode batch, #running-req: 64, #token: 252382, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.28, #queue-req: 0, 
[2025-12-15 10:29:21 TP0] Decode batch, #running-req: 64, #token: 254942, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.61, #queue-req: 0, 
[2025-12-15 10:29:23] INFO:     127.0.0.1:42402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42460 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.00, #running-req: 0, #queue-req: 4, 
[2025-12-15 10:29:23] INFO:     127.0.0.1:42466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42826 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:23] INFO:     127.0.0.1:42878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:29:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 5, #queue-req: 42, 
[2025-12-15 10:29:25 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.04, #running-req: 10, #queue-req: 36, 
[2025-12-15 10:29:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.05, #running-req: 16, #queue-req: 31, 
[2025-12-15 10:29:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 21, #queue-req: 26, 
[2025-12-15 10:29:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.08, #running-req: 26, #queue-req: 21, 
[2025-12-15 10:29:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.10, #running-req: 31, #queue-req: 16, 
[2025-12-15 10:29:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 36, #queue-req: 11, 
[2025-12-15 10:29:34 TP0] Prefill batch, #new-seq: 6, #new-token: 15992, #cached-token: 3214, token usage: 0.14, #running-req: 41, #queue-req: 5, 
[2025-12-15 10:29:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15979, #cached-token: 26, token usage: 0.15, #running-req: 47, #queue-req: 0, 
[2025-12-15 10:29:41 TP0] Decode batch, #running-req: 52, #token: 167622, token usage: 0.17, cuda graph: True, gen throughput (token/s): 114.51, #queue-req: 0, 
[2025-12-15 10:29:46 TP0] Decode batch, #running-req: 52, #token: 169702, token usage: 0.17, cuda graph: True, gen throughput (token/s): 455.86, #queue-req: 0, 
[2025-12-15 10:29:50 TP0] Decode batch, #running-req: 52, #token: 171782, token usage: 0.18, cuda graph: True, gen throughput (token/s): 455.62, #queue-req: 0, 
[2025-12-15 10:29:55 TP0] Decode batch, #running-req: 52, #token: 173862, token usage: 0.18, cuda graph: True, gen throughput (token/s): 453.89, #queue-req: 0, 
[2025-12-15 10:29:59 TP0] Decode batch, #running-req: 52, #token: 175942, token usage: 0.18, cuda graph: True, gen throughput (token/s): 451.90, #queue-req: 0, 
[2025-12-15 10:30:04 TP0] Decode batch, #running-req: 52, #token: 178022, token usage: 0.18, cuda graph: True, gen throughput (token/s): 451.49, #queue-req: 0, 
[2025-12-15 10:30:09 TP0] Decode batch, #running-req: 52, #token: 180102, token usage: 0.18, cuda graph: True, gen throughput (token/s): 451.34, #queue-req: 0, 
[2025-12-15 10:30:13 TP0] Decode batch, #running-req: 52, #token: 182182, token usage: 0.19, cuda graph: True, gen throughput (token/s): 451.54, #queue-req: 0, 
[2025-12-15 10:30:18 TP0] Decode batch, #running-req: 52, #token: 184262, token usage: 0.19, cuda graph: True, gen throughput (token/s): 451.38, #queue-req: 0, 
[2025-12-15 10:30:22 TP0] Decode batch, #running-req: 52, #token: 186342, token usage: 0.19, cuda graph: True, gen throughput (token/s): 451.50, #queue-req: 0, 
[2025-12-15 10:30:27 TP0] Decode batch, #running-req: 52, #token: 188422, token usage: 0.19, cuda graph: True, gen throughput (token/s): 447.97, #queue-req: 0, 
[2025-12-15 10:30:32 TP0] Decode batch, #running-req: 52, #token: 190502, token usage: 0.19, cuda graph: True, gen throughput (token/s): 448.19, #queue-req: 0, 
[2025-12-15 10:30:36 TP0] Decode batch, #running-req: 52, #token: 192582, token usage: 0.20, cuda graph: True, gen throughput (token/s): 448.14, #queue-req: 0, 
[2025-12-15 10:30:41 TP0] Decode batch, #running-req: 52, #token: 194662, token usage: 0.20, cuda graph: True, gen throughput (token/s): 448.15, #queue-req: 0, 
[2025-12-15 10:30:46 TP0] Decode batch, #running-req: 52, #token: 196742, token usage: 0.20, cuda graph: True, gen throughput (token/s): 448.14, #queue-req: 0, 
[2025-12-15 10:30:50 TP0] Decode batch, #running-req: 52, #token: 198822, token usage: 0.20, cuda graph: True, gen throughput (token/s): 447.91, #queue-req: 0, 
[2025-12-15 10:30:55 TP0] Decode batch, #running-req: 52, #token: 200902, token usage: 0.20, cuda graph: True, gen throughput (token/s): 445.76, #queue-req: 0, 
[2025-12-15 10:31:00 TP0] Decode batch, #running-req: 52, #token: 202982, token usage: 0.21, cuda graph: True, gen throughput (token/s): 444.48, #queue-req: 0, 
[2025-12-15 10:31:04 TP0] Decode batch, #running-req: 52, #token: 205062, token usage: 0.21, cuda graph: True, gen throughput (token/s): 444.84, #queue-req: 0, 
[2025-12-15 10:31:09 TP0] Decode batch, #running-req: 52, #token: 207142, token usage: 0.21, cuda graph: True, gen throughput (token/s): 444.77, #queue-req: 0, 
[2025-12-15 10:31:11] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-15 10:31:11] INFO:     127.0.0.1:36662 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-15 10:31:12] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-15 10:31:12] INFO:     127.0.0.1:36674 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-15 10:31:22] INFO:     127.0.0.1:41726 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-15 10:31:29] INFO:     127.0.0.1:39688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:29 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:31:29 TP0] Decode batch, #running-req: 1, #token: 3226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 41.71, #queue-req: 0, 
[2025-12-15 10:31:31] INFO:     127.0.0.1:39690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:31:31] INFO:     127.0.0.1:39696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31] INFO:     127.0.0.1:39842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:31:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-12-15 10:31:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-12-15 10:31:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-12-15 10:31:34 TP0] Decode batch, #running-req: 16, #token: 51714, token usage: 0.05, cuda graph: True, gen throughput (token/s): 105.36, #queue-req: 0, 
[2025-12-15 10:31:37 TP0] Decode batch, #running-req: 16, #token: 52354, token usage: 0.05, cuda graph: True, gen throughput (token/s): 216.19, #queue-req: 0, 
[2025-12-15 10:31:39 TP0] Decode batch, #running-req: 16, #token: 52994, token usage: 0.05, cuda graph: True, gen throughput (token/s): 490.50, #queue-req: 0, 
[2025-12-15 10:31:40 TP0] Decode batch, #running-req: 16, #token: 53634, token usage: 0.05, cuda graph: True, gen throughput (token/s): 473.19, #queue-req: 0, 
[2025-12-15 10:31:41 TP0] Decode batch, #running-req: 16, #token: 54274, token usage: 0.06, cuda graph: True, gen throughput (token/s): 495.20, #queue-req: 0, 
[2025-12-15 10:31:43 TP0] Decode batch, #running-req: 16, #token: 54914, token usage: 0.06, cuda graph: True, gen throughput (token/s): 493.19, #queue-req: 0, 
[2025-12-15 10:31:44 TP0] Decode batch, #running-req: 16, #token: 55554, token usage: 0.06, cuda graph: True, gen throughput (token/s): 493.31, #queue-req: 0, 
[2025-12-15 10:31:46 TP0] Decode batch, #running-req: 16, #token: 56194, token usage: 0.06, cuda graph: True, gen throughput (token/s): 244.84, #queue-req: 0, 
[2025-12-15 10:31:48 TP0] Decode batch, #running-req: 16, #token: 56834, token usage: 0.06, cuda graph: True, gen throughput (token/s): 446.49, #queue-req: 0, 
[2025-12-15 10:31:49 TP0] Decode batch, #running-req: 16, #token: 57474, token usage: 0.06, cuda graph: True, gen throughput (token/s): 492.24, #queue-req: 0, 
[2025-12-15 10:31:50 TP0] Decode batch, #running-req: 16, #token: 58114, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.97, #queue-req: 0, 
[2025-12-15 10:31:52 TP0] Decode batch, #running-req: 16, #token: 58754, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.99, #queue-req: 0, 
[2025-12-15 10:31:53 TP0] Decode batch, #running-req: 16, #token: 59394, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.47, #queue-req: 0, 
[2025-12-15 10:31:54 TP0] Decode batch, #running-req: 16, #token: 60034, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.77, #queue-req: 0, 
[2025-12-15 10:31:56 TP0] Decode batch, #running-req: 16, #token: 60674, token usage: 0.06, cuda graph: True, gen throughput (token/s): 488.56, #queue-req: 0, 
[2025-12-15 10:31:57 TP0] Decode batch, #running-req: 16, #token: 61314, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.31, #queue-req: 0, 
[2025-12-15 10:31:58 TP0] Decode batch, #running-req: 16, #token: 61954, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.08, #queue-req: 0, 
[2025-12-15 10:32:00 TP0] Decode batch, #running-req: 16, #token: 62594, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.04, #queue-req: 0, 
[2025-12-15 10:32:01 TP0] Decode batch, #running-req: 16, #token: 63234, token usage: 0.06, cuda graph: True, gen throughput (token/s): 488.15, #queue-req: 0, 
[2025-12-15 10:32:02 TP0] Decode batch, #running-req: 16, #token: 63874, token usage: 0.07, cuda graph: True, gen throughput (token/s): 489.65, #queue-req: 0, 
[2025-12-15 10:32:03] INFO:     127.0.0.1:36122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:32:03] INFO:     127.0.0.1:36160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03] INFO:     127.0.0.1:36250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-12-15 10:32:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-12-15 10:32:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-12-15 10:32:06 TP0] Decode batch, #running-req: 16, #token: 51708, token usage: 0.05, cuda graph: True, gen throughput (token/s): 158.40, #queue-req: 0, 
[2025-12-15 10:32:08 TP0] Decode batch, #running-req: 16, #token: 52348, token usage: 0.05, cuda graph: True, gen throughput (token/s): 497.93, #queue-req: 0, 
[2025-12-15 10:32:09 TP0] Decode batch, #running-req: 16, #token: 52988, token usage: 0.05, cuda graph: True, gen throughput (token/s): 497.99, #queue-req: 0, 
[2025-12-15 10:32:10 TP0] Decode batch, #running-req: 16, #token: 53628, token usage: 0.05, cuda graph: True, gen throughput (token/s): 495.06, #queue-req: 0, 
[2025-12-15 10:32:11 TP0] Decode batch, #running-req: 16, #token: 54268, token usage: 0.06, cuda graph: True, gen throughput (token/s): 492.99, #queue-req: 0, 
[2025-12-15 10:32:13 TP0] Decode batch, #running-req: 16, #token: 54908, token usage: 0.06, cuda graph: True, gen throughput (token/s): 493.05, #queue-req: 0, 
[2025-12-15 10:32:14 TP0] Decode batch, #running-req: 16, #token: 55548, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.45, #queue-req: 0, 
[2025-12-15 10:32:15 TP0] Decode batch, #running-req: 16, #token: 56188, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.71, #queue-req: 0, 
[2025-12-15 10:32:17 TP0] Decode batch, #running-req: 16, #token: 56828, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.15, #queue-req: 0, 
[2025-12-15 10:32:18 TP0] Decode batch, #running-req: 16, #token: 57468, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.59, #queue-req: 0, 
[2025-12-15 10:32:19 TP0] Decode batch, #running-req: 16, #token: 58108, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.77, #queue-req: 0, 
[2025-12-15 10:32:21 TP0] Decode batch, #running-req: 16, #token: 58748, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.50, #queue-req: 0, 
[2025-12-15 10:32:22 TP0] Decode batch, #running-req: 16, #token: 59388, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.11, #queue-req: 0, 
[2025-12-15 10:32:23 TP0] Decode batch, #running-req: 16, #token: 60028, token usage: 0.06, cuda graph: True, gen throughput (token/s): 488.49, #queue-req: 0, 
[2025-12-15 10:32:25 TP0] Decode batch, #running-req: 16, #token: 60668, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.66, #queue-req: 0, 
[2025-12-15 10:32:26 TP0] Decode batch, #running-req: 16, #token: 61308, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.18, #queue-req: 0, 
[2025-12-15 10:32:27 TP0] Decode batch, #running-req: 16, #token: 61948, token usage: 0.06, cuda graph: True, gen throughput (token/s): 486.06, #queue-req: 0, 
[2025-12-15 10:32:28 TP0] Decode batch, #running-req: 16, #token: 62588, token usage: 0.06, cuda graph: True, gen throughput (token/s): 486.91, #queue-req: 0, 
[2025-12-15 10:32:30 TP0] Decode batch, #running-req: 16, #token: 63228, token usage: 0.06, cuda graph: True, gen throughput (token/s): 485.46, #queue-req: 0, 
[2025-12-15 10:32:31 TP0] Decode batch, #running-req: 16, #token: 63868, token usage: 0.07, cuda graph: True, gen throughput (token/s): 485.90, #queue-req: 0, 
[2025-12-15 10:32:31] INFO:     127.0.0.1:48154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31 TP0] Prefill batch, #new-seq: 2, #new-token: 6397, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:32:31] INFO:     127.0.0.1:48170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:31] INFO:     127.0.0.1:48264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:32:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.01, #running-req: 2, #queue-req: 9, 
[2025-12-15 10:32:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.02, #running-req: 7, #queue-req: 4, 
[2025-12-15 10:32:33 TP0] Prefill batch, #new-seq: 4, #new-token: 12786, #cached-token: 18, token usage: 0.04, #running-req: 12, #queue-req: 0, 
[2025-12-15 10:32:35 TP0] Decode batch, #running-req: 16, #token: 51710, token usage: 0.05, cuda graph: True, gen throughput (token/s): 157.60, #queue-req: 0, 
[2025-12-15 10:32:36 TP0] Decode batch, #running-req: 16, #token: 52350, token usage: 0.05, cuda graph: True, gen throughput (token/s): 501.03, #queue-req: 0, 
[2025-12-15 10:32:38 TP0] Decode batch, #running-req: 16, #token: 52990, token usage: 0.05, cuda graph: True, gen throughput (token/s): 498.47, #queue-req: 0, 
[2025-12-15 10:32:39 TP0] Decode batch, #running-req: 16, #token: 53630, token usage: 0.05, cuda graph: True, gen throughput (token/s): 498.07, #queue-req: 0, 
[2025-12-15 10:32:40 TP0] Decode batch, #running-req: 16, #token: 54270, token usage: 0.06, cuda graph: True, gen throughput (token/s): 495.41, #queue-req: 0, 
[2025-12-15 10:32:42 TP0] Decode batch, #running-req: 16, #token: 54910, token usage: 0.06, cuda graph: True, gen throughput (token/s): 494.57, #queue-req: 0, 
[2025-12-15 10:32:43 TP0] Decode batch, #running-req: 16, #token: 55550, token usage: 0.06, cuda graph: True, gen throughput (token/s): 492.92, #queue-req: 0, 
[2025-12-15 10:32:44 TP0] Decode batch, #running-req: 16, #token: 56190, token usage: 0.06, cuda graph: True, gen throughput (token/s): 493.44, #queue-req: 0, 
[2025-12-15 10:32:45 TP0] Decode batch, #running-req: 16, #token: 56830, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.70, #queue-req: 0, 
[2025-12-15 10:32:47 TP0] Decode batch, #running-req: 16, #token: 57470, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.51, #queue-req: 0, 
[2025-12-15 10:32:48 TP0] Decode batch, #running-req: 16, #token: 58110, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.94, #queue-req: 0, 
[2025-12-15 10:32:49 TP0] Decode batch, #running-req: 16, #token: 58750, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.40, #queue-req: 0, 
[2025-12-15 10:32:51 TP0] Decode batch, #running-req: 16, #token: 59390, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.86, #queue-req: 0, 
[2025-12-15 10:32:52 TP0] Decode batch, #running-req: 16, #token: 60030, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.74, #queue-req: 0, 
[2025-12-15 10:32:53 TP0] Decode batch, #running-req: 16, #token: 60670, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.29, #queue-req: 0, 
[2025-12-15 10:32:55 TP0] Decode batch, #running-req: 16, #token: 61310, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.72, #queue-req: 0, 
[2025-12-15 10:32:56 TP0] Decode batch, #running-req: 16, #token: 61950, token usage: 0.06, cuda graph: True, gen throughput (token/s): 488.58, #queue-req: 0, 
[2025-12-15 10:32:57 TP0] Decode batch, #running-req: 16, #token: 62590, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.81, #queue-req: 0, 
[2025-12-15 10:32:59 TP0] Decode batch, #running-req: 16, #token: 63230, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.36, #queue-req: 0, 
[2025-12-15 10:33:00 TP0] Decode batch, #running-req: 16, #token: 63870, token usage: 0.07, cuda graph: True, gen throughput (token/s): 487.56, #queue-req: 0, 
[2025-12-15 10:33:00] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:33:00] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00] INFO:     127.0.0.1:53598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:33:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-12-15 10:33:01 TP0] Prefill batch, #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.02, #running-req: 6, #queue-req: 4, 
[2025-12-15 10:33:01 TP0] Prefill batch, #new-seq: 4, #new-token: 12793, #cached-token: 11, token usage: 0.04, #running-req: 12, #queue-req: 0, 
[2025-12-15 10:33:06 TP0] Decode batch, #running-req: 16, #token: 51713, token usage: 0.05, cuda graph: True, gen throughput (token/s): 98.19, #queue-req: 0, 
[2025-12-15 10:33:10 TP0] Decode batch, #running-req: 16, #token: 52353, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.27, #queue-req: 0, 
[2025-12-15 10:33:14 TP0] Decode batch, #running-req: 16, #token: 52993, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.47, #queue-req: 0, 
[2025-12-15 10:33:18 TP0] Decode batch, #running-req: 16, #token: 53633, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.20, #queue-req: 0, 
[2025-12-15 10:33:22 TP0] Decode batch, #running-req: 16, #token: 54273, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.01, #queue-req: 0, 
[2025-12-15 10:33:26 TP0] Decode batch, #running-req: 16, #token: 54913, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.30, #queue-req: 0, 
[2025-12-15 10:33:30 TP0] Decode batch, #running-req: 16, #token: 55553, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.26, #queue-req: 0, 
[2025-12-15 10:33:33 TP0] Decode batch, #running-req: 16, #token: 56193, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.22, #queue-req: 0, 
[2025-12-15 10:33:37 TP0] Decode batch, #running-req: 16, #token: 56833, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.22, #queue-req: 0, 
[2025-12-15 10:33:41 TP0] Decode batch, #running-req: 16, #token: 57473, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.10, #queue-req: 0, 
[2025-12-15 10:33:45 TP0] Decode batch, #running-req: 16, #token: 58113, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.55, #queue-req: 0, 
[2025-12-15 10:33:49 TP0] Decode batch, #running-req: 16, #token: 58753, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.47, #queue-req: 0, 
[2025-12-15 10:33:53 TP0] Decode batch, #running-req: 16, #token: 59393, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.45, #queue-req: 0, 
[2025-12-15 10:33:57 TP0] Decode batch, #running-req: 16, #token: 60033, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.44, #queue-req: 0, 
[2025-12-15 10:34:01 TP0] Decode batch, #running-req: 16, #token: 60673, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.43, #queue-req: 0, 
[2025-12-15 10:34:05 TP0] Decode batch, #running-req: 16, #token: 61313, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.42, #queue-req: 0, 
[2025-12-15 10:34:08 TP0] Decode batch, #running-req: 16, #token: 61953, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.02, #queue-req: 0, 
[2025-12-15 10:34:12 TP0] Decode batch, #running-req: 16, #token: 62593, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.93, #queue-req: 0, 
[2025-12-15 10:34:16 TP0] Decode batch, #running-req: 16, #token: 63233, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.93, #queue-req: 0, 
[2025-12-15 10:34:20 TP0] Decode batch, #running-req: 16, #token: 63873, token usage: 0.07, cuda graph: True, gen throughput (token/s): 163.90, #queue-req: 0, 
[2025-12-15 10:34:21] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 7, 
[2025-12-15 10:34:21] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:34:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.02, #running-req: 5, #queue-req: 6, 
[2025-12-15 10:34:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.03, #running-req: 10, #queue-req: 1, 
[2025-12-15 10:34:24 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.05, #running-req: 15, #queue-req: 0, 
[2025-12-15 10:34:29 TP0] Decode batch, #running-req: 16, #token: 51708, token usage: 0.05, cuda graph: True, gen throughput (token/s): 72.36, #queue-req: 0, 
[2025-12-15 10:34:33 TP0] Decode batch, #running-req: 16, #token: 52348, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.76, #queue-req: 0, 
[2025-12-15 10:34:37 TP0] Decode batch, #running-req: 16, #token: 52988, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.73, #queue-req: 0, 
[2025-12-15 10:34:41 TP0] Decode batch, #running-req: 16, #token: 53628, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.56, #queue-req: 0, 
[2025-12-15 10:34:45 TP0] Decode batch, #running-req: 16, #token: 54268, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.21, #queue-req: 0, 
[2025-12-15 10:34:48 TP0] Decode batch, #running-req: 16, #token: 54908, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.20, #queue-req: 0, 
[2025-12-15 10:34:52 TP0] Decode batch, #running-req: 16, #token: 55548, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.07, #queue-req: 0, 
[2025-12-15 10:34:56 TP0] Decode batch, #running-req: 16, #token: 56188, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.24, #queue-req: 0, 
[2025-12-15 10:35:00 TP0] Decode batch, #running-req: 16, #token: 56828, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.15, #queue-req: 0, 
[2025-12-15 10:35:04 TP0] Decode batch, #running-req: 16, #token: 57468, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.08, #queue-req: 0, 
[2025-12-15 10:35:08 TP0] Decode batch, #running-req: 16, #token: 58108, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.64, #queue-req: 0, 
[2025-12-15 10:35:12 TP0] Decode batch, #running-req: 16, #token: 58748, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.65, #queue-req: 0, 
[2025-12-15 10:35:16 TP0] Decode batch, #running-req: 16, #token: 59388, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.66, #queue-req: 0, 
[2025-12-15 10:35:19 TP0] Decode batch, #running-req: 16, #token: 60028, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.63, #queue-req: 0, 
[2025-12-15 10:35:23 TP0] Decode batch, #running-req: 16, #token: 60668, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.68, #queue-req: 0, 
[2025-12-15 10:35:27 TP0] Decode batch, #running-req: 16, #token: 61308, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.66, #queue-req: 0, 
[2025-12-15 10:35:31 TP0] Decode batch, #running-req: 16, #token: 61948, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.23, #queue-req: 0, 
[2025-12-15 10:35:35 TP0] Decode batch, #running-req: 16, #token: 62588, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.14, #queue-req: 0, 
[2025-12-15 10:35:39 TP0] Decode batch, #running-req: 16, #token: 63228, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.13, #queue-req: 0, 
[2025-12-15 10:35:43 TP0] Decode batch, #running-req: 16, #token: 63868, token usage: 0.07, cuda graph: True, gen throughput (token/s): 164.12, #queue-req: 0, 
[2025-12-15 10:35:44] INFO:     127.0.0.1:59434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.00, #running-req: 0, #queue-req: 7, 
[2025-12-15 10:35:44] INFO:     127.0.0.1:59562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44] INFO:     127.0.0.1:59576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:35:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-12-15 10:35:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-12-15 10:35:51 TP0] Decode batch, #running-req: 16, #token: 51708, token usage: 0.05, cuda graph: True, gen throughput (token/s): 76.01, #queue-req: 0, 
[2025-12-15 10:35:55 TP0] Decode batch, #running-req: 16, #token: 52348, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.83, #queue-req: 0, 
[2025-12-15 10:35:59 TP0] Decode batch, #running-req: 16, #token: 52988, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.81, #queue-req: 0, 
[2025-12-15 10:36:03 TP0] Decode batch, #running-req: 16, #token: 53628, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.66, #queue-req: 0, 
[2025-12-15 10:36:07 TP0] Decode batch, #running-req: 16, #token: 54268, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.32, #queue-req: 0, 
[2025-12-15 10:36:11 TP0] Decode batch, #running-req: 16, #token: 54908, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.22, #queue-req: 0, 
[2025-12-15 10:36:14 TP0] Decode batch, #running-req: 16, #token: 55548, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.24, #queue-req: 0, 
[2025-12-15 10:36:18 TP0] Decode batch, #running-req: 16, #token: 56188, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.27, #queue-req: 0, 
[2025-12-15 10:36:22 TP0] Decode batch, #running-req: 16, #token: 56828, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.26, #queue-req: 0, 
[2025-12-15 10:36:26 TP0] Decode batch, #running-req: 16, #token: 57468, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.18, #queue-req: 0, 
[2025-12-15 10:36:30 TP0] Decode batch, #running-req: 16, #token: 58108, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.76, #queue-req: 0, 
[2025-12-15 10:36:34 TP0] Decode batch, #running-req: 16, #token: 58748, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.70, #queue-req: 0, 
[2025-12-15 10:36:38 TP0] Decode batch, #running-req: 16, #token: 59388, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.67, #queue-req: 0, 
[2025-12-15 10:36:42 TP0] Decode batch, #running-req: 16, #token: 60028, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.71, #queue-req: 0, 
[2025-12-15 10:36:45 TP0] Decode batch, #running-req: 16, #token: 60668, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.70, #queue-req: 0, 
[2025-12-15 10:36:49 TP0] Decode batch, #running-req: 16, #token: 61308, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.73, #queue-req: 0, 
[2025-12-15 10:36:53 TP0] Decode batch, #running-req: 16, #token: 61948, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.29, #queue-req: 0, 
[2025-12-15 10:36:57 TP0] Decode batch, #running-req: 16, #token: 62588, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.23, #queue-req: 0, 
[2025-12-15 10:37:01 TP0] Decode batch, #running-req: 16, #token: 63228, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.21, #queue-req: 0, 
[2025-12-15 10:37:05 TP0] Decode batch, #running-req: 16, #token: 63868, token usage: 0.07, cuda graph: True, gen throughput (token/s): 164.21, #queue-req: 0, 
[2025-12-15 10:37:06] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 0, #queue-req: 7, 
[2025-12-15 10:37:06] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06] INFO:     127.0.0.1:56542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:37:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.02, #running-req: 5, #queue-req: 6, 
[2025-12-15 10:37:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.03, #running-req: 10, #queue-req: 1, 
[2025-12-15 10:37:09 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.05, #running-req: 15, #queue-req: 0, 
[2025-12-15 10:37:14 TP0] Decode batch, #running-req: 16, #token: 51710, token usage: 0.05, cuda graph: True, gen throughput (token/s): 72.33, #queue-req: 0, 
[2025-12-15 10:37:18 TP0] Decode batch, #running-req: 16, #token: 52350, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.58, #queue-req: 0, 
[2025-12-15 10:37:22 TP0] Decode batch, #running-req: 16, #token: 52990, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.49, #queue-req: 0, 
[2025-12-15 10:37:25 TP0] Decode batch, #running-req: 16, #token: 53630, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.34, #queue-req: 0, 
[2025-12-15 10:37:29 TP0] Decode batch, #running-req: 16, #token: 54270, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.01, #queue-req: 0, 
[2025-12-15 10:37:33 TP0] Decode batch, #running-req: 16, #token: 54910, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.95, #queue-req: 0, 
[2025-12-15 10:37:37 TP0] Decode batch, #running-req: 16, #token: 55550, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.91, #queue-req: 0, 
[2025-12-15 10:37:41 TP0] Decode batch, #running-req: 16, #token: 56190, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.90, #queue-req: 0, 
[2025-12-15 10:37:45 TP0] Decode batch, #running-req: 16, #token: 56830, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.92, #queue-req: 0, 
[2025-12-15 10:37:49 TP0] Decode batch, #running-req: 16, #token: 57470, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.86, #queue-req: 0, 
[2025-12-15 10:37:53 TP0] Decode batch, #running-req: 16, #token: 58110, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.44, #queue-req: 0, 
[2025-12-15 10:37:56 TP0] Decode batch, #running-req: 16, #token: 58750, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.42, #queue-req: 0, 
[2025-12-15 10:38:00 TP0] Decode batch, #running-req: 16, #token: 59390, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.43, #queue-req: 0, 
[2025-12-15 10:38:04 TP0] Decode batch, #running-req: 16, #token: 60030, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.38, #queue-req: 0, 
[2025-12-15 10:38:08 TP0] Decode batch, #running-req: 16, #token: 60670, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.35, #queue-req: 0, 
[2025-12-15 10:38:12 TP0] Decode batch, #running-req: 16, #token: 61310, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.39, #queue-req: 0, 
[2025-12-15 10:38:16 TP0] Decode batch, #running-req: 16, #token: 61950, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.90, #queue-req: 0, 
[2025-12-15 10:38:20 TP0] Decode batch, #running-req: 16, #token: 62590, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.85, #queue-req: 0, 
[2025-12-15 10:38:24 TP0] Decode batch, #running-req: 16, #token: 63230, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.81, #queue-req: 0, 
[2025-12-15 10:38:28 TP0] Decode batch, #running-req: 16, #token: 63870, token usage: 0.07, cuda graph: True, gen throughput (token/s): 163.81, #queue-req: 0, 
[2025-12-15 10:38:28] INFO:     127.0.0.1:37814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 0, #queue-req: 6, 
[2025-12-15 10:38:28] INFO:     127.0.0.1:37924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:28] INFO:     127.0.0.1:37954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:38:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 5, #queue-req: 6, 
[2025-12-15 10:38:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.03, #running-req: 10, #queue-req: 1, 
[2025-12-15 10:38:32 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.05, #running-req: 15, #queue-req: 0, 
[2025-12-15 10:38:37 TP0] Decode batch, #running-req: 16, #token: 51711, token usage: 0.05, cuda graph: True, gen throughput (token/s): 72.38, #queue-req: 0, 
[2025-12-15 10:38:40 TP0] Decode batch, #running-req: 16, #token: 52351, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.41, #queue-req: 0, 
[2025-12-15 10:38:44 TP0] Decode batch, #running-req: 16, #token: 52991, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.40, #queue-req: 0, 
[2025-12-15 10:38:48 TP0] Decode batch, #running-req: 16, #token: 53631, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.17, #queue-req: 0, 
[2025-12-15 10:38:52 TP0] Decode batch, #running-req: 16, #token: 54271, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.87, #queue-req: 0, 
[2025-12-15 10:38:56 TP0] Decode batch, #running-req: 16, #token: 54911, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.82, #queue-req: 0, 
[2025-12-15 10:39:00 TP0] Decode batch, #running-req: 16, #token: 55551, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.82, #queue-req: 0, 
[2025-12-15 10:39:04 TP0] Decode batch, #running-req: 16, #token: 56191, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.84, #queue-req: 0, 
[2025-12-15 10:39:08 TP0] Decode batch, #running-req: 16, #token: 56831, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.87, #queue-req: 0, 
[2025-12-15 10:39:11 TP0] Decode batch, #running-req: 16, #token: 57471, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.80, #queue-req: 0, 
[2025-12-15 10:39:15 TP0] Decode batch, #running-req: 16, #token: 58111, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.39, #queue-req: 0, 
[2025-12-15 10:39:19 TP0] Decode batch, #running-req: 16, #token: 58751, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.36, #queue-req: 0, 
[2025-12-15 10:39:23 TP0] Decode batch, #running-req: 16, #token: 59391, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.33, #queue-req: 0, 
[2025-12-15 10:39:27 TP0] Decode batch, #running-req: 16, #token: 60031, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.31, #queue-req: 0, 
[2025-12-15 10:39:31 TP0] Decode batch, #running-req: 16, #token: 60671, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.31, #queue-req: 0, 
[2025-12-15 10:39:35 TP0] Decode batch, #running-req: 16, #token: 61311, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.30, #queue-req: 0, 
[2025-12-15 10:39:39 TP0] Decode batch, #running-req: 16, #token: 61951, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.85, #queue-req: 0, 
[2025-12-15 10:39:43 TP0] Decode batch, #running-req: 16, #token: 62591, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.74, #queue-req: 0, 
[2025-12-15 10:39:47 TP0] Decode batch, #running-req: 16, #token: 63231, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.72, #queue-req: 0, 
[2025-12-15 10:39:50 TP0] Decode batch, #running-req: 16, #token: 63871, token usage: 0.07, cuda graph: True, gen throughput (token/s): 163.73, #queue-req: 0, 
[2025-12-15 10:39:51] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-15 10:39:51] INFO:     127.0.0.1:39170 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-15 10:39:51] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-15 10:39:51] INFO:     127.0.0.1:39182 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-15 10:40:02] INFO:     127.0.0.1:46510 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-15 10:40:08] INFO:     127.0.0.1:41482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:40:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:40:09 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 8.47, #queue-req: 0, 
[2025-12-15 10:40:10] INFO:     127.0.0.1:41496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:40:10] INFO:     127.0.0.1:41506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:40:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:40:10] INFO:     127.0.0.1:41514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:40:10] INFO:     127.0.0.1:41530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:40:10 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-12-15 10:40:12 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 66.74, #queue-req: 0, 
[2025-12-15 10:40:13 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-15 10:40:14 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.02, #queue-req: 0, 
[2025-12-15 10:40:15 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.86, #queue-req: 0, 
[2025-12-15 10:40:16 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:40:17 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.75, #queue-req: 0, 
[2025-12-15 10:40:18 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-15 10:40:23 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 35.73, #queue-req: 0, 
[2025-12-15 10:40:24 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:40:25 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-15 10:40:26 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.33, #queue-req: 0, 
[2025-12-15 10:40:27 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.32, #queue-req: 0, 
[2025-12-15 10:40:28 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 105.48, #queue-req: 0, 
[2025-12-15 10:40:31 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 69.03, #queue-req: 0, 
[2025-12-15 10:40:32 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.67, #queue-req: 0, 
[2025-12-15 10:40:33 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:40:34 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.31, #queue-req: 0, 
[2025-12-15 10:40:35 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.31, #queue-req: 0, 
[2025-12-15 10:40:36 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 132.83, #queue-req: 0, 
[2025-12-15 10:40:37] INFO:     127.0.0.1:49880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:40:37] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:40:37] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:40:37] INFO:     127.0.0.1:49892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:40:37 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 149.05, #queue-req: 0, 
[2025-12-15 10:40:37 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:40:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:40:39 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 107.01, #queue-req: 0, 
[2025-12-15 10:40:40 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.02, #queue-req: 0, 
[2025-12-15 10:40:41 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.02, #queue-req: 0, 
[2025-12-15 10:40:42 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.84, #queue-req: 0, 
[2025-12-15 10:40:43 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:40:44 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:40:45 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-15 10:40:46 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:40:47 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:40:48 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-15 10:40:49 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.34, #queue-req: 0, 
[2025-12-15 10:40:50 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.33, #queue-req: 0, 
[2025-12-15 10:40:51 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.77, #queue-req: 0, 
[2025-12-15 10:40:52 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.28, #queue-req: 0, 
[2025-12-15 10:40:53 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.30, #queue-req: 0, 
[2025-12-15 10:40:54 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.30, #queue-req: 0, 
[2025-12-15 10:40:55 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.84, #queue-req: 0, 
[2025-12-15 10:40:57 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:40:58 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:40:59] INFO:     127.0.0.1:39976 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:40:59] INFO:     127.0.0.1:39984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:40:59] INFO:     127.0.0.1:40000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:40:59] INFO:     127.0.0.1:40016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:40:59 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.89, #queue-req: 0, 
[2025-12-15 10:40:59 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:40:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:41:00 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 111.69, #queue-req: 0, 
[2025-12-15 10:41:01 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.03, #queue-req: 0, 
[2025-12-15 10:41:02 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.07, #queue-req: 0, 
[2025-12-15 10:41:03 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.88, #queue-req: 0, 
[2025-12-15 10:41:04 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:41:05 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-15 10:41:06 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-15 10:41:07 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.76, #queue-req: 0, 
[2025-12-15 10:41:09 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:41:10 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-15 10:41:11 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:41:12 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.31, #queue-req: 0, 
[2025-12-15 10:41:13 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-15 10:41:14 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.27, #queue-req: 0, 
[2025-12-15 10:41:15 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.31, #queue-req: 0, 
[2025-12-15 10:41:16 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.31, #queue-req: 0, 
[2025-12-15 10:41:17 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.83, #queue-req: 0, 
[2025-12-15 10:41:18 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.84, #queue-req: 0, 
[2025-12-15 10:41:19 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.81, #queue-req: 0, 
[2025-12-15 10:41:20] INFO:     127.0.0.1:41722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:41:20] INFO:     127.0.0.1:41724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:41:20] INFO:     127.0.0.1:41730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:41:20] INFO:     127.0.0.1:41742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:41:20 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.87, #queue-req: 0, 
[2025-12-15 10:41:20 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:41:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:41:22 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.52, #queue-req: 0, 
[2025-12-15 10:41:23 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.05, #queue-req: 0, 
[2025-12-15 10:41:24 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.06, #queue-req: 0, 
[2025-12-15 10:41:25 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-15 10:41:26 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:41:27 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:41:28 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.49, #queue-req: 0, 
[2025-12-15 10:41:29 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.87, #queue-req: 0, 
[2025-12-15 10:41:30 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-15 10:41:31 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-15 10:41:32 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.31, #queue-req: 0, 
[2025-12-15 10:41:33 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.29, #queue-req: 0, 
[2025-12-15 10:41:34 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-15 10:41:35 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.25, #queue-req: 0, 
[2025-12-15 10:41:36 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.29, #queue-req: 0, 
[2025-12-15 10:41:37 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.32, #queue-req: 0, 
[2025-12-15 10:41:38 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.82, #queue-req: 0, 
[2025-12-15 10:41:40 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.82, #queue-req: 0, 
[2025-12-15 10:41:41 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.79, #queue-req: 0, 
[2025-12-15 10:41:42] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:41:42] INFO:     127.0.0.1:53652 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:41:42] INFO:     127.0.0.1:53666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:41:42] INFO:     127.0.0.1:53674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:41:42 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.85, #queue-req: 0, 
[2025-12-15 10:41:42 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:41:42 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:41:43 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.62, #queue-req: 0, 
[2025-12-15 10:41:44 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.04, #queue-req: 0, 
[2025-12-15 10:41:45 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.07, #queue-req: 0, 
[2025-12-15 10:41:46 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.85, #queue-req: 0, 
[2025-12-15 10:41:47 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:41:48 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.67, #queue-req: 0, 
[2025-12-15 10:41:49 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:41:50 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.67, #queue-req: 0, 
[2025-12-15 10:41:52 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:41:53 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-15 10:41:54 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:41:55 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:41:56 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-15 10:41:57 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.28, #queue-req: 0, 
[2025-12-15 10:41:58 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.27, #queue-req: 0, 
[2025-12-15 10:41:59 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.30, #queue-req: 0, 
[2025-12-15 10:42:00 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.85, #queue-req: 0, 
[2025-12-15 10:42:01 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:42:02 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:42:03] INFO:     127.0.0.1:38602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:42:03] INFO:     127.0.0.1:38606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:42:03] INFO:     127.0.0.1:38614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:42:03 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.91, #queue-req: 0, 
[2025-12-15 10:42:03] INFO:     127.0.0.1:38616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:42:03 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:42:03 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:42:05 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.81, #queue-req: 0, 
[2025-12-15 10:42:06 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.05, #queue-req: 0, 
[2025-12-15 10:42:07 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.07, #queue-req: 0, 
[2025-12-15 10:42:08 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.86, #queue-req: 0, 
[2025-12-15 10:42:09 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:42:10 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:42:11 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:42:12 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:42:13 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:42:14 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-15 10:42:15 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.37, #queue-req: 0, 
[2025-12-15 10:42:16 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.37, #queue-req: 0, 
[2025-12-15 10:42:17 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.36, #queue-req: 0, 
[2025-12-15 10:42:18 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:42:19 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.43, #queue-req: 0, 
[2025-12-15 10:42:20 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.48, #queue-req: 0, 
[2025-12-15 10:42:21 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.92, #queue-req: 0, 
[2025-12-15 10:42:23 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.97, #queue-req: 0, 
[2025-12-15 10:42:24 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.94, #queue-req: 0, 
[2025-12-15 10:42:25] INFO:     127.0.0.1:49218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:42:25] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:42:25] INFO:     127.0.0.1:49230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:42:25 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.96, #queue-req: 0, 
[2025-12-15 10:42:25] INFO:     127.0.0.1:49244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:42:25 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:42:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:42:26 TP0] Decode batch, #running-req: 4, #token: 12960, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.26, #queue-req: 0, 
[2025-12-15 10:42:27 TP0] Decode batch, #running-req: 4, #token: 13120, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.06, #queue-req: 0, 
[2025-12-15 10:42:28 TP0] Decode batch, #running-req: 4, #token: 13280, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.07, #queue-req: 0, 
[2025-12-15 10:42:29 TP0] Decode batch, #running-req: 4, #token: 13440, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.89, #queue-req: 0, 
[2025-12-15 10:42:30 TP0] Decode batch, #running-req: 4, #token: 13600, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:42:31 TP0] Decode batch, #running-req: 4, #token: 13760, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:42:32 TP0] Decode batch, #running-req: 4, #token: 13920, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:42:33 TP0] Decode batch, #running-req: 4, #token: 14080, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.67, #queue-req: 0, 
[2025-12-15 10:42:34 TP0] Decode batch, #running-req: 4, #token: 14240, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:42:36 TP0] Decode batch, #running-req: 4, #token: 14400, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-15 10:42:37 TP0] Decode batch, #running-req: 4, #token: 14560, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.34, #queue-req: 0, 
[2025-12-15 10:42:38 TP0] Decode batch, #running-req: 4, #token: 14720, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:42:39 TP0] Decode batch, #running-req: 4, #token: 14880, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.29, #queue-req: 0, 
[2025-12-15 10:42:40 TP0] Decode batch, #running-req: 4, #token: 15040, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.27, #queue-req: 0, 
[2025-12-15 10:42:41 TP0] Decode batch, #running-req: 4, #token: 15200, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.29, #queue-req: 0, 
[2025-12-15 10:42:42 TP0] Decode batch, #running-req: 4, #token: 15360, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:42:43 TP0] Decode batch, #running-req: 4, #token: 15520, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.87, #queue-req: 0, 
[2025-12-15 10:42:44 TP0] Decode batch, #running-req: 4, #token: 15680, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-15 10:42:45 TP0] Decode batch, #running-req: 4, #token: 15840, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:42:46] INFO:     127.0.0.1:49224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:42:46] INFO:     127.0.0.1:49232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:42:46] INFO:     127.0.0.1:49234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:42:46] INFO:     127.0.0.1:49246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:42:46 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.90, #queue-req: 0, 
[2025-12-15 10:42:46 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:42:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:42:48 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.33, #queue-req: 0, 
[2025-12-15 10:42:49 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.11, #queue-req: 0, 
[2025-12-15 10:42:50 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.10, #queue-req: 0, 
[2025-12-15 10:42:51 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.85, #queue-req: 0, 
[2025-12-15 10:42:52 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:42:53 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:42:54 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-15 10:42:55 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:42:56 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:42:57 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-15 10:42:58 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.33, #queue-req: 0, 
[2025-12-15 10:42:59 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.38, #queue-req: 0, 
[2025-12-15 10:43:00 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.27, #queue-req: 0, 
[2025-12-15 10:43:01 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-15 10:43:02 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.40, #queue-req: 0, 
[2025-12-15 10:43:03 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.28, #queue-req: 0, 
[2025-12-15 10:43:04 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.84, #queue-req: 0, 
[2025-12-15 10:43:05 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:43:07 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.83, #queue-req: 0, 
[2025-12-15 10:43:08] INFO:     127.0.0.1:59446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:43:08] INFO:     127.0.0.1:59462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:43:08] INFO:     127.0.0.1:59474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:43:08 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-15 10:43:08] INFO:     127.0.0.1:59480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:43:08 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:43:08 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:43:09 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.22, #queue-req: 0, 
[2025-12-15 10:43:10 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.12, #queue-req: 0, 
[2025-12-15 10:43:11 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.14, #queue-req: 0, 
[2025-12-15 10:43:12 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.93, #queue-req: 0, 
[2025-12-15 10:43:13 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:43:14 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-15 10:43:15 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:43:16 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:43:17 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-15 10:43:19 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-15 10:43:20 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.42, #queue-req: 0, 
[2025-12-15 10:43:21 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.42, #queue-req: 0, 
[2025-12-15 10:43:22 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.34, #queue-req: 0, 
[2025-12-15 10:43:23 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.36, #queue-req: 0, 
[2025-12-15 10:43:24 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.34, #queue-req: 0, 
[2025-12-15 10:43:25 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.38, #queue-req: 0, 
[2025-12-15 10:43:26 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:43:27 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:43:28 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:43:29] INFO:     127.0.0.1:37806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:43:29] INFO:     127.0.0.1:37816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:43:29] INFO:     127.0.0.1:37824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:43:29] INFO:     127.0.0.1:37826 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:43:29 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-15 10:43:29 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:43:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:43:31 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.23, #queue-req: 0, 
[2025-12-15 10:43:32 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.15, #queue-req: 0, 
[2025-12-15 10:43:33 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.14, #queue-req: 0, 
[2025-12-15 10:43:34 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.94, #queue-req: 0, 
[2025-12-15 10:43:35 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-15 10:43:36 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-15 10:43:37 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-15 10:43:38 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:43:39 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-15 10:43:40 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-15 10:43:41 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.47, #queue-req: 0, 
[2025-12-15 10:43:42 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.49, #queue-req: 0, 
[2025-12-15 10:43:43 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.43, #queue-req: 0, 
[2025-12-15 10:43:44 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.46, #queue-req: 0, 
[2025-12-15 10:43:45 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.46, #queue-req: 0, 
[2025-12-15 10:43:46 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.47, #queue-req: 0, 
[2025-12-15 10:43:47 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.97, #queue-req: 0, 
[2025-12-15 10:43:48 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.97, #queue-req: 0, 
[2025-12-15 10:43:50 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.97, #queue-req: 0, 
[2025-12-15 10:43:51] INFO:     127.0.0.1:46200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:43:51] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:43:51] INFO:     127.0.0.1:46208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:43:51] INFO:     127.0.0.1:46218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:43:51 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 151.00, #queue-req: 0, 
[2025-12-15 10:43:51 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:43:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:43:52 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.11, #queue-req: 0, 
[2025-12-15 10:43:53 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.07, #queue-req: 0, 
[2025-12-15 10:43:54 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.08, #queue-req: 0, 
[2025-12-15 10:43:55 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.90, #queue-req: 0, 
[2025-12-15 10:43:56 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-15 10:43:57 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-15 10:43:58 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:43:59 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:44:00 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:44:01 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-15 10:44:03 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.36, #queue-req: 0, 
[2025-12-15 10:44:04 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.33, #queue-req: 0, 
[2025-12-15 10:44:05 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.29, #queue-req: 0, 
[2025-12-15 10:44:06 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.30, #queue-req: 0, 
[2025-12-15 10:44:07 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.33, #queue-req: 0, 
[2025-12-15 10:44:08 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:44:09 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.85, #queue-req: 0, 
[2025-12-15 10:44:10 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-15 10:44:11 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:44:12] INFO:     127.0.0.1:46056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:44:12] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:44:12] INFO:     127.0.0.1:46080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:44:12] INFO:     127.0.0.1:46088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:44:12 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.93, #queue-req: 0, 
[2025-12-15 10:44:12 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:44:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:44:13 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.22, #queue-req: 0, 
[2025-12-15 10:44:15 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.13, #queue-req: 0, 
[2025-12-15 10:44:16 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.08, #queue-req: 0, 
[2025-12-15 10:44:17 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.90, #queue-req: 0, 
[2025-12-15 10:44:18 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:44:19 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:44:20 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-15 10:44:21 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.67, #queue-req: 0, 
[2025-12-15 10:44:22 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:44:23 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-15 10:44:24 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.31, #queue-req: 0, 
[2025-12-15 10:44:25 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.33, #queue-req: 0, 
[2025-12-15 10:44:26 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.33, #queue-req: 0, 
[2025-12-15 10:44:27 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.26, #queue-req: 0, 
[2025-12-15 10:44:28 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-15 10:44:29 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.29, #queue-req: 0, 
[2025-12-15 10:44:30 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:44:31 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.91, #queue-req: 0, 
[2025-12-15 10:44:32 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-15 10:44:34] INFO:     127.0.0.1:37924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:44:34] INFO:     127.0.0.1:37926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:44:34] INFO:     127.0.0.1:37938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:44:34 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.92, #queue-req: 0, 
[2025-12-15 10:44:34] INFO:     127.0.0.1:37952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:44:34 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:44:34 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:44:35 TP0] Decode batch, #running-req: 4, #token: 12961, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.49, #queue-req: 0, 
[2025-12-15 10:44:36 TP0] Decode batch, #running-req: 4, #token: 13121, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.08, #queue-req: 0, 
[2025-12-15 10:44:37 TP0] Decode batch, #running-req: 4, #token: 13281, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.09, #queue-req: 0, 
[2025-12-15 10:44:38 TP0] Decode batch, #running-req: 4, #token: 13441, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-15 10:44:39 TP0] Decode batch, #running-req: 4, #token: 13601, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:44:40 TP0] Decode batch, #running-req: 4, #token: 13761, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.75, #queue-req: 0, 
[2025-12-15 10:44:41 TP0] Decode batch, #running-req: 4, #token: 13921, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:44:42 TP0] Decode batch, #running-req: 4, #token: 14081, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:44:43 TP0] Decode batch, #running-req: 4, #token: 14241, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-15 10:44:44 TP0] Decode batch, #running-req: 4, #token: 14401, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-15 10:44:45 TP0] Decode batch, #running-req: 4, #token: 14561, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.43, #queue-req: 0, 
[2025-12-15 10:44:47 TP0] Decode batch, #running-req: 4, #token: 14721, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.48, #queue-req: 0, 
[2025-12-15 10:44:48 TP0] Decode batch, #running-req: 4, #token: 14881, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.46, #queue-req: 0, 
[2025-12-15 10:44:49 TP0] Decode batch, #running-req: 4, #token: 15041, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.44, #queue-req: 0, 
[2025-12-15 10:44:50 TP0] Decode batch, #running-req: 4, #token: 15201, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:44:51 TP0] Decode batch, #running-req: 4, #token: 15361, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.43, #queue-req: 0, 
[2025-12-15 10:44:52 TP0] Decode batch, #running-req: 4, #token: 15521, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.92, #queue-req: 0, 
[2025-12-15 10:44:53 TP0] Decode batch, #running-req: 4, #token: 15681, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.94, #queue-req: 0, 
[2025-12-15 10:44:54 TP0] Decode batch, #running-req: 4, #token: 15841, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.95, #queue-req: 0, 
[2025-12-15 10:44:55] INFO:     127.0.0.1:35548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:44:55] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:44:55] INFO:     127.0.0.1:35560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:44:55] INFO:     127.0.0.1:35572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:44:55 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.96, #queue-req: 0, 
[2025-12-15 10:44:55 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:44:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:44:56 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.30, #queue-req: 0, 
[2025-12-15 10:44:57 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.04, #queue-req: 0, 
[2025-12-15 10:44:59 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.07, #queue-req: 0, 
[2025-12-15 10:45:00 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.87, #queue-req: 0, 
[2025-12-15 10:45:01 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:45:02 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-15 10:45:03 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:45:04 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:45:05 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:45:06 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-15 10:45:07 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:45:08 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:45:09 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:45:10 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.29, #queue-req: 0, 
[2025-12-15 10:45:11 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.29, #queue-req: 0, 
[2025-12-15 10:45:12 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.29, #queue-req: 0, 
[2025-12-15 10:45:13 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.87, #queue-req: 0, 
[2025-12-15 10:45:14 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.90, #queue-req: 0, 
[2025-12-15 10:45:15 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-15 10:45:16] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:45:16] INFO:     127.0.0.1:59182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:45:17] INFO:     127.0.0.1:59192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:45:17 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.93, #queue-req: 0, 
[2025-12-15 10:45:17] INFO:     127.0.0.1:59196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:45:17 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:45:17 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:45:18 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.11, #queue-req: 0, 
[2025-12-15 10:45:19 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.04, #queue-req: 0, 
[2025-12-15 10:45:20 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.06, #queue-req: 0, 
[2025-12-15 10:45:21 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.87, #queue-req: 0, 
[2025-12-15 10:45:22 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:45:23 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.75, #queue-req: 0, 
[2025-12-15 10:45:24 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:45:25 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:45:26 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:45:27 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-15 10:45:28 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.45, #queue-req: 0, 
[2025-12-15 10:45:30 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.46, #queue-req: 0, 
[2025-12-15 10:45:31 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.44, #queue-req: 0, 
[2025-12-15 10:45:32 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.42, #queue-req: 0, 
[2025-12-15 10:45:33 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.40, #queue-req: 0, 
[2025-12-15 10:45:34 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.39, #queue-req: 0, 
[2025-12-15 10:45:35 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.92, #queue-req: 0, 
[2025-12-15 10:45:36 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.98, #queue-req: 0, 
[2025-12-15 10:45:37 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.92, #queue-req: 0, 
[2025-12-15 10:45:38] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:45:38] INFO:     127.0.0.1:59878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:45:38] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:45:38] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:45:38 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 151.02, #queue-req: 0, 
[2025-12-15 10:45:38 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:45:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:45:39 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.70, #queue-req: 0, 
[2025-12-15 10:45:40 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.94, #queue-req: 0, 
[2025-12-15 10:45:42 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.98, #queue-req: 0, 
[2025-12-15 10:45:43 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.79, #queue-req: 0, 
[2025-12-15 10:45:44 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-15 10:45:45 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-15 10:45:46 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-15 10:45:47 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-15 10:45:48 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-15 10:45:49 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.47, #queue-req: 0, 
[2025-12-15 10:45:50 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-15 10:45:51 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-15 10:45:52 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-15 10:45:53 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-15 10:45:54 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-15 10:45:55 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-15 10:45:56 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-15 10:45:57 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.72, #queue-req: 0, 
[2025-12-15 10:45:58 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.72, #queue-req: 0, 
[2025-12-15 10:45:59] INFO:     127.0.0.1:37458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:45:59] INFO:     127.0.0.1:37470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:45:59] INFO:     127.0.0.1:37472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:46:00] INFO:     127.0.0.1:37474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:46:00 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.78, #queue-req: 0, 
[2025-12-15 10:46:00 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:46:00 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:46:01 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.08, #queue-req: 0, 
[2025-12-15 10:46:02 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.97, #queue-req: 0, 
[2025-12-15 10:46:03 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.99, #queue-req: 0, 
[2025-12-15 10:46:04 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.82, #queue-req: 0, 
[2025-12-15 10:46:05 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-15 10:46:06 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:46:07 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.67, #queue-req: 0, 
[2025-12-15 10:46:08 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-15 10:46:09 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:46:10 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-15 10:46:11 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.28, #queue-req: 0, 
[2025-12-15 10:46:13 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.30, #queue-req: 0, 
[2025-12-15 10:46:14 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.27, #queue-req: 0, 
[2025-12-15 10:46:15 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-15 10:46:16 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-15 10:46:17 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.21, #queue-req: 0, 
[2025-12-15 10:46:18 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.80, #queue-req: 0, 
[2025-12-15 10:46:19 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.81, #queue-req: 0, 
[2025-12-15 10:46:20 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.83, #queue-req: 0, 
[2025-12-15 10:46:21] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:46:21] INFO:     127.0.0.1:48076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:46:21] INFO:     127.0.0.1:48088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:46:21 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-15 10:46:21] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:46:21 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:46:21 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:46:22 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.94, #queue-req: 0, 
[2025-12-15 10:46:23 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.18, #queue-req: 0, 
[2025-12-15 10:46:25 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.17, #queue-req: 0, 
[2025-12-15 10:46:26 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.94, #queue-req: 0, 
[2025-12-15 10:46:27 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.75, #queue-req: 0, 
[2025-12-15 10:46:28 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.77, #queue-req: 0, 
[2025-12-15 10:46:29 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-15 10:46:30 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:46:31 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.75, #queue-req: 0, 
[2025-12-15 10:46:32 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-15 10:46:33 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.51, #queue-req: 0, 
[2025-12-15 10:46:34 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.50, #queue-req: 0, 
[2025-12-15 10:46:35 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.49, #queue-req: 0, 
[2025-12-15 10:46:36 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.53, #queue-req: 0, 
[2025-12-15 10:46:37 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.47, #queue-req: 0, 
[2025-12-15 10:46:38 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.50, #queue-req: 0, 
[2025-12-15 10:46:39 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-15 10:46:40 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-15 10:46:41 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-15 10:46:42] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:46:42] INFO:     127.0.0.1:59630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:46:42] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:46:42 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 151.27, #queue-req: 0, 
[2025-12-15 10:46:42] INFO:     127.0.0.1:59634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:46:42 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:46:43 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:46:45 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 60.33, #queue-req: 0, 
[2025-12-15 10:46:47 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 111.22, #queue-req: 0, 
[2025-12-15 10:46:48 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.07, #queue-req: 0, 
[2025-12-15 10:46:49 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.89, #queue-req: 0, 
[2025-12-15 10:46:50 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-15 10:46:51 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-15 10:46:52 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-15 10:46:53 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-15 10:46:54 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-15 10:46:55 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-15 10:46:56 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.43, #queue-req: 0, 
[2025-12-15 10:46:57 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.38, #queue-req: 0, 
[2025-12-15 10:46:58 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.36, #queue-req: 0, 
[2025-12-15 10:47:00 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 103.03, #queue-req: 0, 
[2025-12-15 10:47:02 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 70.85, #queue-req: 0, 
[2025-12-15 10:47:03 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.38, #queue-req: 0, 
[2025-12-15 10:47:04 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.89, #queue-req: 0, 
[2025-12-15 10:47:05 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.89, #queue-req: 0, 
[2025-12-15 10:47:06 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.90, #queue-req: 0, 
[2025-12-15 10:47:07] INFO:     127.0.0.1:46618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:47:07] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:47:07] INFO:     127.0.0.1:46624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:47:07 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.92, #queue-req: 0, 
[2025-12-15 10:47:07] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:47:07 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:47:07 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:47:09 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.59, #queue-req: 0, 
[2025-12-15 10:47:10 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.14, #queue-req: 0, 
[2025-12-15 10:47:11 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.15, #queue-req: 0, 
[2025-12-15 10:47:12 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.93, #queue-req: 0, 
[2025-12-15 10:47:13 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-15 10:47:14 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.75, #queue-req: 0, 
[2025-12-15 10:47:15 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:47:16 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:47:17 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-15 10:47:18 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-15 10:47:19 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.40, #queue-req: 0, 
[2025-12-15 10:47:20 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.44, #queue-req: 0, 
[2025-12-15 10:47:21 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.42, #queue-req: 0, 
[2025-12-15 10:47:22 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.31, #queue-req: 0, 
[2025-12-15 10:47:23 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.32, #queue-req: 0, 
[2025-12-15 10:47:24 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:47:26 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:47:27 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.85, #queue-req: 0, 
[2025-12-15 10:47:28 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:47:29] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:47:29] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:47:29] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:47:29 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.89, #queue-req: 0, 
[2025-12-15 10:47:29] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:47:29 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:47:29 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:47:30 TP0] Decode batch, #running-req: 4, #token: 12961, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.34, #queue-req: 0, 
[2025-12-15 10:47:31 TP0] Decode batch, #running-req: 4, #token: 13121, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.15, #queue-req: 0, 
[2025-12-15 10:47:32 TP0] Decode batch, #running-req: 4, #token: 13281, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.19, #queue-req: 0, 
[2025-12-15 10:47:33 TP0] Decode batch, #running-req: 4, #token: 13441, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.97, #queue-req: 0, 
[2025-12-15 10:47:34 TP0] Decode batch, #running-req: 4, #token: 13601, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-15 10:47:35 TP0] Decode batch, #running-req: 4, #token: 13761, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.77, #queue-req: 0, 
[2025-12-15 10:47:36 TP0] Decode batch, #running-req: 4, #token: 13921, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-15 10:47:38 TP0] Decode batch, #running-req: 4, #token: 14081, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:47:39 TP0] Decode batch, #running-req: 4, #token: 14241, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-15 10:47:40 TP0] Decode batch, #running-req: 4, #token: 14401, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-15 10:47:41 TP0] Decode batch, #running-req: 4, #token: 14561, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.46, #queue-req: 0, 
[2025-12-15 10:47:42 TP0] Decode batch, #running-req: 4, #token: 14721, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.45, #queue-req: 0, 
[2025-12-15 10:47:43 TP0] Decode batch, #running-req: 4, #token: 14881, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.41, #queue-req: 0, 
[2025-12-15 10:47:44 TP0] Decode batch, #running-req: 4, #token: 15041, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.38, #queue-req: 0, 
[2025-12-15 10:47:45 TP0] Decode batch, #running-req: 4, #token: 15201, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.33, #queue-req: 0, 
[2025-12-15 10:47:46 TP0] Decode batch, #running-req: 4, #token: 15361, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.41, #queue-req: 0, 
[2025-12-15 10:47:47 TP0] Decode batch, #running-req: 4, #token: 15521, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.97, #queue-req: 0, 
[2025-12-15 10:47:48 TP0] Decode batch, #running-req: 4, #token: 15681, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.02, #queue-req: 0, 
[2025-12-15 10:47:49 TP0] Decode batch, #running-req: 4, #token: 15841, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.02, #queue-req: 0, 
[2025-12-15 10:47:50] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:47:50] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:47:50] INFO:     127.0.0.1:52718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:47:50 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-15 10:47:50] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:47:50 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:47:50 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:47:52 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.27, #queue-req: 0, 
[2025-12-15 10:47:53 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.11, #queue-req: 0, 
[2025-12-15 10:47:54 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.11, #queue-req: 0, 
[2025-12-15 10:47:55 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.90, #queue-req: 0, 
[2025-12-15 10:47:56 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:47:57 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-15 10:47:58 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:47:59 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:48:00 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:48:01 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-15 10:48:02 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.43, #queue-req: 0, 
[2025-12-15 10:48:03 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.42, #queue-req: 0, 
[2025-12-15 10:48:04 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.43, #queue-req: 0, 
[2025-12-15 10:48:05 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.42, #queue-req: 0, 
[2025-12-15 10:48:06 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.41, #queue-req: 0, 
[2025-12-15 10:48:07 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.44, #queue-req: 0, 
[2025-12-15 10:48:09 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.94, #queue-req: 0, 
[2025-12-15 10:48:10 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.96, #queue-req: 0, 
[2025-12-15 10:48:11 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.00, #queue-req: 0, 
[2025-12-15 10:48:12] INFO:     127.0.0.1:51542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:48:12] INFO:     127.0.0.1:51558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:48:12] INFO:     127.0.0.1:51570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:48:12] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:48:12 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 151.05, #queue-req: 0, 
[2025-12-15 10:48:12 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:48:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:48:13 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.70, #queue-req: 0, 
[2025-12-15 10:48:14 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.03, #queue-req: 0, 
[2025-12-15 10:48:15 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.07, #queue-req: 0, 
[2025-12-15 10:48:16 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.87, #queue-req: 0, 
[2025-12-15 10:48:17 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:48:18 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:48:19 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:48:20 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.67, #queue-req: 0, 
[2025-12-15 10:48:22 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:48:23 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-15 10:48:24 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.34, #queue-req: 0, 
[2025-12-15 10:48:25 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.37, #queue-req: 0, 
[2025-12-15 10:48:26 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.32, #queue-req: 0, 
[2025-12-15 10:48:27 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:48:28 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.28, #queue-req: 0, 
[2025-12-15 10:48:29 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.31, #queue-req: 0, 
[2025-12-15 10:48:30 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:48:31 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.90, #queue-req: 0, 
[2025-12-15 10:48:32 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-15 10:48:33] INFO:     127.0.0.1:37114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:48:33] INFO:     127.0.0.1:37124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:48:33] INFO:     127.0.0.1:37134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:48:33 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.95, #queue-req: 0, 
[2025-12-15 10:48:33] INFO:     127.0.0.1:37148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:48:33 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:48:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:48:35 TP0] Decode batch, #running-req: 4, #token: 12961, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.13, #queue-req: 0, 
[2025-12-15 10:48:36 TP0] Decode batch, #running-req: 4, #token: 13121, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.09, #queue-req: 0, 
[2025-12-15 10:48:37 TP0] Decode batch, #running-req: 4, #token: 13281, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.10, #queue-req: 0, 
[2025-12-15 10:48:38 TP0] Decode batch, #running-req: 4, #token: 13441, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.90, #queue-req: 0, 
[2025-12-15 10:48:39 TP0] Decode batch, #running-req: 4, #token: 13601, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:48:40 TP0] Decode batch, #running-req: 4, #token: 13761, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-15 10:48:41 TP0] Decode batch, #running-req: 4, #token: 13921, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:48:42 TP0] Decode batch, #running-req: 4, #token: 14081, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:48:43 TP0] Decode batch, #running-req: 4, #token: 14241, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:48:44 TP0] Decode batch, #running-req: 4, #token: 14401, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-15 10:48:45 TP0] Decode batch, #running-req: 4, #token: 14561, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:48:46 TP0] Decode batch, #running-req: 4, #token: 14721, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.37, #queue-req: 0, 
[2025-12-15 10:48:47 TP0] Decode batch, #running-req: 4, #token: 14881, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.32, #queue-req: 0, 
[2025-12-15 10:48:48 TP0] Decode batch, #running-req: 4, #token: 15041, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.32, #queue-req: 0, 
[2025-12-15 10:48:49 TP0] Decode batch, #running-req: 4, #token: 15201, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.27, #queue-req: 0, 
[2025-12-15 10:48:50 TP0] Decode batch, #running-req: 4, #token: 15361, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.25, #queue-req: 0, 
[2025-12-15 10:48:51 TP0] Decode batch, #running-req: 4, #token: 15521, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:48:53 TP0] Decode batch, #running-req: 4, #token: 15681, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-15 10:48:54 TP0] Decode batch, #running-req: 4, #token: 15841, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:48:55] INFO:     127.0.0.1:44026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:48:55] INFO:     127.0.0.1:44042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:48:55] INFO:     127.0.0.1:44052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:48:55 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.95, #queue-req: 0, 
[2025-12-15 10:48:55] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:48:55 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:48:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:48:56 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.65, #queue-req: 0, 
[2025-12-15 10:48:57 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.02, #queue-req: 0, 
[2025-12-15 10:48:58 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.07, #queue-req: 0, 
[2025-12-15 10:48:59 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.84, #queue-req: 0, 
[2025-12-15 10:49:00 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:49:01 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-15 10:49:02 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:49:03 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.67, #queue-req: 0, 
[2025-12-15 10:49:05 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:49:06 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-15 10:49:07 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.37, #queue-req: 0, 
[2025-12-15 10:49:08 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.44, #queue-req: 0, 
[2025-12-15 10:49:09 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.39, #queue-req: 0, 
[2025-12-15 10:49:10 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.43, #queue-req: 0, 
[2025-12-15 10:49:11 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.36, #queue-req: 0, 
[2025-12-15 10:49:12 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.34, #queue-req: 0, 
[2025-12-15 10:49:13 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.87, #queue-req: 0, 
[2025-12-15 10:49:14 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.89, #queue-req: 0, 
[2025-12-15 10:49:15 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.87, #queue-req: 0, 
[2025-12-15 10:49:16] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:49:16] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:49:16] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:49:16] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:49:16 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.96, #queue-req: 0, 
[2025-12-15 10:49:16 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:49:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:49:18 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.83, #queue-req: 0, 
[2025-12-15 10:49:19 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.06, #queue-req: 0, 
[2025-12-15 10:49:20 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.08, #queue-req: 0, 
[2025-12-15 10:49:21 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.86, #queue-req: 0, 
[2025-12-15 10:49:22 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:49:23 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-15 10:49:24 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-15 10:49:25 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:49:26 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:49:27 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-15 10:49:28 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.36, #queue-req: 0, 
[2025-12-15 10:49:29 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:49:30 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.36, #queue-req: 0, 
[2025-12-15 10:49:31 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-15 10:49:32 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.30, #queue-req: 0, 
[2025-12-15 10:49:33 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.34, #queue-req: 0, 
[2025-12-15 10:49:34 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-15 10:49:35 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.92, #queue-req: 0, 
[2025-12-15 10:49:37 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.89, #queue-req: 0, 
[2025-12-15 10:49:38] INFO:     127.0.0.1:53682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:49:38] INFO:     127.0.0.1:53684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:49:38] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:49:38 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.94, #queue-req: 0, 
[2025-12-15 10:49:38] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:49:38 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:49:38 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:49:41 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 55.15, #queue-req: 0, 
[2025-12-15 10:49:42 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.85, #queue-req: 0, 
[2025-12-15 10:49:43 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-15 10:49:44 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.78, #queue-req: 0, 
[2025-12-15 10:49:45 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-15 10:49:46 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-15 10:49:47 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-15 10:49:48 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-15 10:49:49 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-15 10:49:50 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.53, #queue-req: 0, 
[2025-12-15 10:49:51 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.15, #queue-req: 0, 
[2025-12-15 10:49:52 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-15 10:49:53 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-15 10:49:56 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 61.63, #queue-req: 0, 
[2025-12-15 10:49:57 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 131.20, #queue-req: 0, 
[2025-12-15 10:49:58 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-15 10:49:59 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.71, #queue-req: 0, 
[2025-12-15 10:50:00 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.72, #queue-req: 0, 
[2025-12-15 10:50:01 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-15 10:50:02] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:50:02] INFO:     127.0.0.1:59842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:50:02] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:50:02 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.75, #queue-req: 0, 
[2025-12-15 10:50:02] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:50:02 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:50:03 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:50:04 TP0] Decode batch, #running-req: 4, #token: 12961, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.85, #queue-req: 0, 
[2025-12-15 10:50:05 TP0] Decode batch, #running-req: 4, #token: 13121, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.06, #queue-req: 0, 
[2025-12-15 10:50:06 TP0] Decode batch, #running-req: 4, #token: 13281, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.09, #queue-req: 0, 
[2025-12-15 10:50:07 TP0] Decode batch, #running-req: 4, #token: 13441, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.86, #queue-req: 0, 
[2025-12-15 10:50:08 TP0] Decode batch, #running-req: 4, #token: 13601, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:50:09 TP0] Decode batch, #running-req: 4, #token: 13761, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-15 10:50:10 TP0] Decode batch, #running-req: 4, #token: 13921, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:50:11 TP0] Decode batch, #running-req: 4, #token: 14081, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:50:12 TP0] Decode batch, #running-req: 4, #token: 14241, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-15 10:50:13 TP0] Decode batch, #running-req: 4, #token: 14401, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-15 10:50:14 TP0] Decode batch, #running-req: 4, #token: 14561, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.32, #queue-req: 0, 
[2025-12-15 10:50:15 TP0] Decode batch, #running-req: 4, #token: 14721, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.31, #queue-req: 0, 
[2025-12-15 10:50:17 TP0] Decode batch, #running-req: 4, #token: 14881, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.29, #queue-req: 0, 
[2025-12-15 10:50:18 TP0] Decode batch, #running-req: 4, #token: 15041, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-15 10:50:19 TP0] Decode batch, #running-req: 4, #token: 15201, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-15 10:50:20 TP0] Decode batch, #running-req: 4, #token: 15361, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.28, #queue-req: 0, 
[2025-12-15 10:50:21 TP0] Decode batch, #running-req: 4, #token: 15521, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:50:22 TP0] Decode batch, #running-req: 4, #token: 15681, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.87, #queue-req: 0, 
[2025-12-15 10:50:23 TP0] Decode batch, #running-req: 4, #token: 15841, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.87, #queue-req: 0, 
[2025-12-15 10:50:24] INFO:     127.0.0.1:51124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:50:24] INFO:     127.0.0.1:51140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:50:24] INFO:     127.0.0.1:51152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:50:24 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.91, #queue-req: 0, 
[2025-12-15 10:50:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:50:24] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:50:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:50:25 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.88, #queue-req: 0, 
[2025-12-15 10:50:26 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.97, #queue-req: 0, 
[2025-12-15 10:50:27 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.03, #queue-req: 0, 
[2025-12-15 10:50:29 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.84, #queue-req: 0, 
[2025-12-15 10:50:30 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:50:31 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-15 10:50:32 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.67, #queue-req: 0, 
[2025-12-15 10:50:33 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:50:34 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:50:35 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-15 10:50:36 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.37, #queue-req: 0, 
[2025-12-15 10:50:37 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.37, #queue-req: 0, 
[2025-12-15 10:50:38 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.31, #queue-req: 0, 
[2025-12-15 10:50:39 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.28, #queue-req: 0, 
[2025-12-15 10:50:40 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.27, #queue-req: 0, 
[2025-12-15 10:50:41 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.29, #queue-req: 0, 
[2025-12-15 10:50:42 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.82, #queue-req: 0, 
[2025-12-15 10:50:43 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.84, #queue-req: 0, 
[2025-12-15 10:50:44 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-15 10:50:45] INFO:     127.0.0.1:42182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:50:45] INFO:     127.0.0.1:42190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:50:45] INFO:     127.0.0.1:42206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:50:45 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-15 10:50:45] INFO:     127.0.0.1:42216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:50:45 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:50:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-15 10:50:47 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.92, #queue-req: 0, 
[2025-12-15 10:50:48 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.02, #queue-req: 0, 
[2025-12-15 10:50:49 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.03, #queue-req: 0, 
[2025-12-15 10:50:50 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.83, #queue-req: 0, 
[2025-12-15 10:50:51 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:50:52 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-15 10:50:53 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:50:54 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-15 10:50:55 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-15 10:50:56 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-15 10:50:57 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.33, #queue-req: 0, 
[2025-12-15 10:50:58 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.39, #queue-req: 0, 
[2025-12-15 10:50:59 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.36, #queue-req: 0, 
[2025-12-15 10:51:01 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.32, #queue-req: 0, 
[2025-12-15 10:51:02 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.30, #queue-req: 0, 
[2025-12-15 10:51:03 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.32, #queue-req: 0, 
[2025-12-15 10:51:04 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-15 10:51:05 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-15 10:51:06 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.90, #queue-req: 0, 
[2025-12-15 10:51:07] INFO:     127.0.0.1:60718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:51:07] INFO:     127.0.0.1:60734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:51:07] INFO:     127.0.0.1:60746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:51:07 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.96, #queue-req: 0, 
[2025-12-15 10:51:07] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:51:07 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:51:07 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:51:08 TP0] Decode batch, #running-req: 4, #token: 12962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.13, #queue-req: 0, 
[2025-12-15 10:51:09 TP0] Decode batch, #running-req: 4, #token: 13122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.97, #queue-req: 0, 
[2025-12-15 10:51:10 TP0] Decode batch, #running-req: 4, #token: 13282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.02, #queue-req: 0, 
[2025-12-15 10:51:11 TP0] Decode batch, #running-req: 4, #token: 13442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.84, #queue-req: 0, 
[2025-12-15 10:51:13 TP0] Decode batch, #running-req: 4, #token: 13602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-15 10:51:14 TP0] Decode batch, #running-req: 4, #token: 13762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-15 10:51:15 TP0] Decode batch, #running-req: 4, #token: 13922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-15 10:51:16 TP0] Decode batch, #running-req: 4, #token: 14082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-15 10:51:17 TP0] Decode batch, #running-req: 4, #token: 14242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-15 10:51:18 TP0] Decode batch, #running-req: 4, #token: 14402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-15 10:51:19 TP0] Decode batch, #running-req: 4, #token: 14562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.26, #queue-req: 0, 
[2025-12-15 10:51:20 TP0] Decode batch, #running-req: 4, #token: 14722, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.25, #queue-req: 0, 
[2025-12-15 10:51:21 TP0] Decode batch, #running-req: 4, #token: 14882, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.26, #queue-req: 0, 
[2025-12-15 10:51:22 TP0] Decode batch, #running-req: 4, #token: 15042, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-15 10:51:23 TP0] Decode batch, #running-req: 4, #token: 15202, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.21, #queue-req: 0, 
[2025-12-15 10:51:24 TP0] Decode batch, #running-req: 4, #token: 15362, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-15 10:51:25 TP0] Decode batch, #running-req: 4, #token: 15522, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.78, #queue-req: 0, 
[2025-12-15 10:51:26 TP0] Decode batch, #running-req: 4, #token: 15682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.77, #queue-req: 0, 
[2025-12-15 10:51:27 TP0] Decode batch, #running-req: 4, #token: 15842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.77, #queue-req: 0, 
[2025-12-15 10:51:28] INFO:     127.0.0.1:54362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:51:28] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:51:28] INFO:     127.0.0.1:54384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:51:28 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.85, #queue-req: 0, 
[2025-12-15 10:51:28] INFO:     127.0.0.1:54388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:51:28 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:51:29 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-15 10:51:30 TP0] Decode batch, #running-req: 4, #token: 12960, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.82, #queue-req: 0, 
[2025-12-15 10:51:31 TP0] Decode batch, #running-req: 4, #token: 13120, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.83, #queue-req: 0, 
[2025-12-15 10:51:32 TP0] Decode batch, #running-req: 4, #token: 13280, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.86, #queue-req: 0, 
[2025-12-15 10:51:33 TP0] Decode batch, #running-req: 4, #token: 13440, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-15 10:51:34 TP0] Decode batch, #running-req: 4, #token: 13600, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-15 10:51:35 TP0] Decode batch, #running-req: 4, #token: 13760, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-15 10:51:36 TP0] Decode batch, #running-req: 4, #token: 13920, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.52, #queue-req: 0, 
[2025-12-15 10:51:37 TP0] Decode batch, #running-req: 4, #token: 14080, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.48, #queue-req: 0, 
[2025-12-15 10:51:38 TP0] Decode batch, #running-req: 4, #token: 14240, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.50, #queue-req: 0, 
[2025-12-15 10:51:39 TP0] Decode batch, #running-req: 4, #token: 14400, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.39, #queue-req: 0, 
[2025-12-15 10:51:40 TP0] Decode batch, #running-req: 4, #token: 14560, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.05, #queue-req: 0, 
[2025-12-15 10:51:41 TP0] Decode batch, #running-req: 4, #token: 14720, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.05, #queue-req: 0, 
[2025-12-15 10:51:42 TP0] Decode batch, #running-req: 4, #token: 14880, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.04, #queue-req: 0, 
[2025-12-15 10:51:44 TP0] Decode batch, #running-req: 4, #token: 15040, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.05, #queue-req: 0, 
[2025-12-15 10:51:45 TP0] Decode batch, #running-req: 4, #token: 15200, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.97, #queue-req: 0, 
[2025-12-15 10:51:46 TP0] Decode batch, #running-req: 4, #token: 15360, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.98, #queue-req: 0, 
[2025-12-15 10:51:47 TP0] Decode batch, #running-req: 4, #token: 15520, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.61, #queue-req: 0, 
[2025-12-15 10:51:48 TP0] Decode batch, #running-req: 4, #token: 15680, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.60, #queue-req: 0, 
[2025-12-15 10:51:49 TP0] Decode batch, #running-req: 4, #token: 15840, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.61, #queue-req: 0, 
[2025-12-15 10:51:50] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-15 10:51:50 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.69, #queue-req: 0, 
[2025-12-15 10:51:50] INFO:     127.0.0.1:42508 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-15 10:51:50] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-15 10:51:50] INFO:     127.0.0.1:42524 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-15 10:52:00] INFO:     127.0.0.1:47844 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-15 10:52:07] INFO:     127.0.0.1:44106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:52:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:52:09] INFO:     127.0.0.1:44112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:52:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:52:09 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2.05, #queue-req: 0, 
[2025-12-15 10:52:10 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-15 10:52:11 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:52:12 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-15 10:52:13 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:52:16 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 15.24, #queue-req: 0, 
[2025-12-15 10:52:17 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 36.55, #queue-req: 0, 
[2025-12-15 10:52:18 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 37.90, #queue-req: 0, 
[2025-12-15 10:52:19 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 10:52:20 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 10:52:21 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 10:52:22 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:52:23 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:52:25 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 24.32, #queue-req: 0, 
[2025-12-15 10:52:27 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 22.53, #queue-req: 0, 
[2025-12-15 10:52:28 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:52:29 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 10:52:30 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:52:31 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:52:32 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:52:32] INFO:     127.0.0.1:42068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:52:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:52:33 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.15, #queue-req: 0, 
[2025-12-15 10:52:34 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:52:35 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:52:36 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:52:37 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:52:38 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:52:39 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:52:40 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 10:52:41 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:52:42 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:52:43 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 10:52:44 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:52:45 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:52:46 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:52:47 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 32.93, #queue-req: 0, 
[2025-12-15 10:52:48 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.04, #queue-req: 0, 
[2025-12-15 10:52:49 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.16, #queue-req: 0, 
[2025-12-15 10:52:50 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 10:52:51 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:52:52 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:52:53] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:52:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:52:53 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.25, #queue-req: 0, 
[2025-12-15 10:52:54 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:52:55 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 10:52:56 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 10:52:57 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 10:52:58 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:52:59 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:53:00 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:53:01 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:53:02 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:53:03 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 10:53:04 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:53:05 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:53:06 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:53:07 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:53:08 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:53:09 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-15 10:53:10 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 10:53:11 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:53:12 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:53:12] INFO:     127.0.0.1:50254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:53:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:53:13 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 10:53:14 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:53:15 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 10:53:16 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:53:17 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:53:18 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:53:19 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:53:20 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:53:21 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:53:22 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:53:23 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 10:53:24 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:53:25 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:53:26 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:53:27 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:53:28 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:53:29 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 10:53:30 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 10:53:31 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:53:32 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:53:32] INFO:     127.0.0.1:53890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:53:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:53:33 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-15 10:53:34 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:53:35 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:53:36 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:53:37 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:53:38 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:53:39 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 10:53:40 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:53:41 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:53:42 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:53:43 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 10:53:44 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:53:45 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:53:46 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:53:47 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:53:48 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:53:49 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 10:53:50 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:53:51 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:53:52 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 10:53:52] INFO:     127.0.0.1:42440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:53:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:53:53 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-15 10:53:54 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:53:55 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:53:56 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:53:57 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 10:53:58 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:53:59 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:54:00 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:54:01 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:54:02 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:54:03 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 10:54:04 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:54:05 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:54:06 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:54:07 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:54:08 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:54:09 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 10:54:10 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:54:11 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 10:54:12 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:54:12] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:54:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:54:13 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-15 10:54:14 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:54:15 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:54:16 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:54:17 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:54:18 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:54:19 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:54:20 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:54:21 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:54:22 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:54:23 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 10:54:24 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:54:25 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:54:26 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:54:27 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:54:28 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:54:29 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 10:54:30 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:54:31 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:54:32 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 10:54:32] INFO:     127.0.0.1:41510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:54:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:54:33 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 10:54:34 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:54:35 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:54:36 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:54:37 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:54:38 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:54:39 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:54:40 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:54:41 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:54:42 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:54:43 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 10:54:44 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:54:45 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:54:46 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:54:47 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:54:48 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:54:49 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 10:54:50 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 10:54:51 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:54:52 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:54:52] INFO:     127.0.0.1:54890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:54:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:54:53 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 10:54:54 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:54:55 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:54:56 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:54:57 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:54:58 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:54:59 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:55:00 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:55:01 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:55:02 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:55:03 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 10:55:04 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:55:05 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:55:06 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:55:07 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:55:08 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:55:09 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 10:55:10 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 10:55:11 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:55:11 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:55:12] INFO:     127.0.0.1:42682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:55:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:55:13 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 32.46, #queue-req: 0, 
[2025-12-15 10:55:14 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:55:15 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:55:16 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:55:17 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 10:55:18 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:55:19 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:55:20 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:55:21 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:55:22 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:55:23 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 10:55:24 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:55:25 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:55:26 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:55:27 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:55:28 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:55:29 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 10:55:30 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 10:55:31 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:55:32 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:55:32] INFO:     127.0.0.1:55776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:55:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:55:33 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.14, #queue-req: 0, 
[2025-12-15 10:55:34 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:55:35 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:55:36 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:55:37 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 10:55:38 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:55:39 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:55:40 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:55:41 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:55:42 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:55:43 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 10:55:44 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:55:45 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:55:46 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:55:47 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:55:48 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:55:49 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 10:55:50 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:55:50 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 10:55:51 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:55:52] INFO:     127.0.0.1:36542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:55:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:55:53 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 10:55:54 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:55:55 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:55:56 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:55:57 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:55:58 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:55:59 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:56:00 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:56:01 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:56:02 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:56:03 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 10:56:04 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:56:05 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:56:06 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:56:06 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:56:07 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:56:08 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-15 10:56:09 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 10:56:10 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:56:11 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:56:12] INFO:     127.0.0.1:48108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:56:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:56:13 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 10:56:14 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:56:15 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:56:16 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:56:17 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:56:18 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:56:19 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:56:20 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:56:21 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:56:22 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:56:22 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 10:56:23 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:56:24 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:56:25 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:56:26 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:56:27 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:56:28 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 10:56:29 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:56:30 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:56:31 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:56:32] INFO:     127.0.0.1:46930 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:56:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:56:33 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 10:56:34 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:56:35 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:56:36 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:56:37 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:56:38 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:56:39 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:56:39 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:56:40 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:56:41 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:56:42 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 10:56:43 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:56:44 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:56:45 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:56:46 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:56:47 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:56:48 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 10:56:49 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:56:50 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:56:51 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:56:52] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:56:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:56:53 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-15 10:56:54 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:56:55 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:56:56 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:56:57 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:56:57 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:56:58 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:56:59 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:57:00 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 10:57:01 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 10:57:02 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 10:57:03 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:57:04 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:57:05 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:57:06 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:57:07 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:57:08 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 10:57:09 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:57:10 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:57:11 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:57:12] INFO:     127.0.0.1:45022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:57:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:57:13 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-15 10:57:14 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:57:14 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:57:15 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:57:16 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:57:17 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:57:18 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:57:19 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:57:20 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:57:21 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:57:22 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 10:57:23 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:57:24 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:57:25 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:57:26 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:57:27 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:57:28 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 10:57:29 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 10:57:30 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 10:57:31 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:57:32] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:57:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:57:32 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 10:57:33 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:57:34 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 10:57:35 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:57:36 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:57:37 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 10:57:38 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:57:39 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:57:40 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:57:41 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:57:42 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 10:57:43 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:57:44 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:57:45 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:57:46 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:57:47 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:57:48 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 10:57:49 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:57:50 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 10:57:51 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 10:57:52] INFO:     127.0.0.1:34750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:57:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:57:52 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 10:57:53 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:57:54 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 10:57:55 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:57:56 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 10:57:57 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:57:58 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:57:59 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:58:00 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:58:01 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:58:02 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 10:58:03 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:58:04 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:58:05 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:58:06 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:58:07 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:58:08 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 10:58:09 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:58:10 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:58:11 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:58:12] INFO:     127.0.0.1:60200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:58:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:58:12 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.14, #queue-req: 0, 
[2025-12-15 10:58:13 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:58:14 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:58:15 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:58:16 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 10:58:17 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:58:18 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:58:19 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:58:20 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:58:21 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:58:22 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 10:58:23 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:58:24 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:58:25 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:58:26 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:58:27 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 10:58:28 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 10:58:29 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:58:30 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:58:31 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:58:32] INFO:     127.0.0.1:57474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:58:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:58:32 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 10:58:33 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:58:34 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:58:35 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:58:36 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 10:58:37 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:58:38 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:58:39 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:58:40 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:58:41 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 10:58:42 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 10:58:43 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:58:44 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:58:45 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:58:46 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 10:58:47 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:58:48 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 10:58:49 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:58:50 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:58:51 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:58:52] INFO:     127.0.0.1:60978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:58:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:58:52 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-15 10:58:53 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:58:54 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 10:58:55 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 10:58:56 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:58:57 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:58:58 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:58:59 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:59:00 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:59:01 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:59:02 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 10:59:03 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:59:04 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:59:05 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:59:06 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:59:07 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:59:08 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 10:59:09 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:59:10 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:59:11 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 10:59:12] INFO:     127.0.0.1:54968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:59:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:59:12 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 10:59:13 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 10:59:14 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 10:59:15 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 10:59:16 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 10:59:17 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:59:18 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:59:19 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 10:59:20 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 10:59:21 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:59:22 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 10:59:23 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:59:24 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:59:25 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 10:59:26 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:59:27 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 10:59:28 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 10:59:29 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 10:59:30 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:59:31 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 10:59:32] INFO:     127.0.0.1:49536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:59:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:59:32 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-15 10:59:33 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 10:59:34 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 10:59:35 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 10:59:36 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 10:59:37 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 10:59:38 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 10:59:39 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 10:59:40 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 10:59:41 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 10:59:42 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 10:59:43 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 10:59:44 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 10:59:45 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 10:59:46 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 10:59:47 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 10:59:48 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 10:59:49 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 10:59:50 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 10:59:51 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 10:59:52] INFO:     127.0.0.1:59256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 10:59:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 10:59:52 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 10:59:53 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 10:59:54 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 10:59:55 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 10:59:56 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 10:59:57 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 10:59:58 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 10:59:59 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:00:00 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:00:01 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:00:02 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:00:03 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:00:04 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:00:05 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:00:06 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:00:07 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:00:08 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:00:09 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:00:10 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:00:11 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:00:12] INFO:     127.0.0.1:59004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:00:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:00:12 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-15 11:00:13 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:00:14 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:00:15 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:00:16 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:00:17 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:00:18 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:00:19 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:00:20 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:00:21 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:00:22 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:00:23 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:00:24 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 11:00:25 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:00:26 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:00:27 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:00:28 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:00:29 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:00:30 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:00:31 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:00:32] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:00:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:00:32 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:00:33 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:00:34 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:00:35 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:00:36 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:00:37 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:00:38 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:00:39 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:00:40 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:00:41 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:00:42 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:00:43 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:00:44 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:00:45 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:00:46 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:00:47 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:00:48 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:00:49 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:00:50 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:00:51 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:00:52] INFO:     127.0.0.1:38382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:00:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:00:52 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 33.34, #queue-req: 0, 
[2025-12-15 11:00:53 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:00:54 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:00:55 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:00:56 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:00:57 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:00:58 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:00:59 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:01:00 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:01:01 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:01:02 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:01:03 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:01:04 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:01:05 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:01:06 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:01:07 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:01:08 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:01:09 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:01:10 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:01:11 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:01:12] INFO:     127.0.0.1:54662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:01:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:01:12 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.09, #queue-req: 0, 
[2025-12-15 11:01:13 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:01:14 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:01:15 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:01:16 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:01:17 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:01:18 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:01:19 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:01:20 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:01:21 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:01:22 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:01:23 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:01:24 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:01:25 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:01:26 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:01:27 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:01:28 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:01:29 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:01:30 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:01:31 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:01:32] INFO:     127.0.0.1:46688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:01:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:01:32 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-15 11:01:33 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:01:34 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:01:35 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:01:36 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:01:37 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:01:38 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:01:39 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:01:40 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:01:41 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:01:42 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:01:43 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:01:44 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:01:45 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:01:46 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:01:47 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:01:48 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:01:49 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:01:50 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:01:51 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:01:52] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:01:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:01:52 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:01:53 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:01:54 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:01:55 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:01:56 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:01:57 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:01:58 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:01:59 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:02:00 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:02:01 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:02:02 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:02:03 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:02:04 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:02:05 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:02:06 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:02:07 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:02:08 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:02:09 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:02:10 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:02:11 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:02:12] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:02:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:02:12 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:02:13 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:02:14 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:02:15 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:02:16 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:02:17 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:02:18 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:02:19 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:02:20 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:02:21 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:02:22 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:02:23 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:02:24 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:02:25 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:02:26 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:02:27 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:02:28 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:02:29 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:02:30 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:02:31 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:02:32] INFO:     127.0.0.1:60858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:02:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:02:32 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-15 11:02:33 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:02:34 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:02:35 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:02:36 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:02:37 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:02:38 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:02:39 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:02:40 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:02:41 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:02:42 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:02:43 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:02:44 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:02:45 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:02:46 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:02:47 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:02:48 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:02:49 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:02:50 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:02:51 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:02:51] INFO:     127.0.0.1:37468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:02:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:02:52 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 11:02:53 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:02:54 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:02:55 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:02:56 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:02:57 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:02:58 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:02:59 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:03:00 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:03:01 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:03:02 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:03:03 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:03:04 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:03:05 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:03:06 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:03:07 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:03:08 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:03:09 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:03:10 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:03:11 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:03:11] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:03:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:03:12 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:03:13 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:03:14 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:03:15 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:03:16 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:03:17 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:03:18 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:03:19 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:03:20 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:03:21 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:03:22 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:03:23 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:03:24 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:03:25 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:03:26 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:03:27 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:03:28 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:03:29 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:03:30 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:03:31 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:03:31] INFO:     127.0.0.1:44370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:03:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:03:32 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:03:33 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:03:34 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.63, #queue-req: 0, 
[2025-12-15 11:03:35 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:03:36 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:03:37 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:03:38 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:03:39 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:03:40 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:03:41 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:03:42 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:03:43 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:03:44 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:03:45 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:03:46 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:03:47 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:03:48 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:03:49 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:03:50 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:03:51 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:03:51] INFO:     127.0.0.1:47120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:03:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:03:52 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-15 11:03:53 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:03:54 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:03:55 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:03:56 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:03:57 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:03:58 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:03:59 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:04:00 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:04:01 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:04:02 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:04:03 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:04:04 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:04:05 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:04:06 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:04:07 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:04:08 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:04:09 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:04:10 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:04:11 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:04:11] INFO:     127.0.0.1:42610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:04:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:04:12 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:04:13 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:04:14 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:04:15 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:04:16 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:04:17 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:04:18 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:04:19 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:04:20 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:04:21 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:04:22 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:04:23 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:04:24 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:04:25 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:04:26 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:04:26 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:04:27 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:04:28 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:04:29 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:04:30 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:04:31] INFO:     127.0.0.1:34814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:04:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:04:32 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 11:04:33 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:04:34 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:04:35 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:04:36 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:04:37 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:04:38 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:04:39 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:04:40 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:04:41 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:04:41 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:04:42 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:04:43 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:04:44 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:04:45 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:04:46 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:04:47 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:04:48 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:04:49 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:04:50 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:04:51] INFO:     127.0.0.1:57526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:04:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:04:52 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:04:53 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:04:54 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:04:55 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:04:56 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:04:57 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:04:57 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:04:58 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:04:59 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:05:00 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:05:01 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:05:02 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:05:03 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:05:04 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:05:05 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:05:06 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:05:07 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:05:08 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:05:09 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:05:10 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:05:11] INFO:     127.0.0.1:47764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:05:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:05:12 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-15 11:05:13 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:05:14 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:05:14 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:05:15 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:05:16 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:05:17 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:05:18 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:05:19 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:05:20 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:05:21 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:05:22 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:05:23 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:05:24 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:05:25 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:05:26 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:05:27 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:05:28 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:05:29 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:05:30 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:05:31] INFO:     127.0.0.1:42074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:05:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:05:31 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 11:05:32 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:05:33 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:05:34 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:05:35 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:05:36 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:05:37 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:05:38 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:05:39 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:05:40 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:05:41 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:05:42 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:05:43 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:05:44 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:05:45 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:05:46 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:05:47 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:05:48 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:05:49 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:05:50 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:05:51] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:05:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:05:51 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-15 11:05:52 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:05:53 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:05:54 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:05:55 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:05:56 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:05:57 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:05:58 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:05:59 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:06:00 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:06:01 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:06:02 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:06:03 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:06:04 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:06:05 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:06:06 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:06:07 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:06:08 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:06:09 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:06:10 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:06:11] INFO:     127.0.0.1:33660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:06:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:06:11 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:06:12 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:06:13 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:06:14 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:06:15 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:06:16 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:06:17 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:06:18 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:06:19 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:06:20 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:06:21 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:06:22 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:06:23 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:06:24 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:06:25 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:06:26 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:06:27 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:06:28 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:06:29 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:06:30 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:06:31] INFO:     127.0.0.1:39190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:06:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:06:31 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 32.67, #queue-req: 0, 
[2025-12-15 11:06:32 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:06:33 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:06:34 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:06:35 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:06:36 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:06:37 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:06:38 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:06:39 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:06:40 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:06:41 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:06:42 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:06:43 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:06:44 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:06:45 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:06:46 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:06:47 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:06:48 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:06:49 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:06:50 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:06:51] INFO:     127.0.0.1:46670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:06:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:06:51 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.09, #queue-req: 0, 
[2025-12-15 11:06:52 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:06:53 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:06:54 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:06:55 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:06:56 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:06:57 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:06:58 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:06:59 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:07:00 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:07:01 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:07:02 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:07:03 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:07:04 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:07:05 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:07:06 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:07:07 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:07:08 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:07:09 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:07:10 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:07:11] INFO:     127.0.0.1:44508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:07:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:07:11 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:07:12 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:07:13 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:07:14 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:07:15 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:07:16 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:07:17 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:07:18 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:07:19 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:07:20 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:07:21 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:07:22 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:07:23 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:07:24 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:07:25 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:07:26 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:07:27 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:07:28 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:07:29 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:07:30 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:07:31] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:07:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:07:31 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:07:32 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:07:33 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:07:34 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:07:35 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:07:36 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:07:37 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:07:38 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:07:39 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:07:40 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:07:41 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:07:42 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:07:43 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:07:44 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:07:45 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:07:46 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:07:47 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:07:48 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:07:49 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:07:50 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:07:51] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:07:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:07:51 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:07:52 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:07:53 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:07:54 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:07:55 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:07:56 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:07:57 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:07:58 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:07:59 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:08:00 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:08:01 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:08:02 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:08:03 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:08:04 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:08:05 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:08:06 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:08:07 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:08:08 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:08:09 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:08:10 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:08:11] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:08:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:08:11 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-15 11:08:12 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:08:13 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:08:14 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:08:15 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:08:16 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:08:17 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:08:18 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:08:19 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:08:20 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:08:21 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:08:22 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:08:23 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:08:24 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:08:25 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:08:26 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:08:27 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:08:28 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:08:29 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:08:30 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:08:31] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:08:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:08:31 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:08:32 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:08:33 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:08:34 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:08:35 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:08:36 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:08:37 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:08:38 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:08:39 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:08:40 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:08:41 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:08:42 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:08:43 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:08:44 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:08:45 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:08:46 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:08:47 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:08:48 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:08:49 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:08:50 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:08:51] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:08:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:08:51 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 11:08:52 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:08:53 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:08:54 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:08:55 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:08:56 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:08:57 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:08:58 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:08:59 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:09:00 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:09:01 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:09:02 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:09:03 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:09:04 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:09:05 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:09:06 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:09:07 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:09:08 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:09:09 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:09:10 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:09:11] INFO:     127.0.0.1:48642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:09:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:09:11 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:09:12 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:09:13 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:09:14 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:09:15 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:09:16 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:09:17 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:09:18 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:09:19 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:09:20 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:09:21 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:09:22 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:09:23 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:09:24 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:09:25 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:09:26 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:09:27 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:09:28 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:09:29 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:09:30 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:09:31] INFO:     127.0.0.1:35304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:09:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:09:31 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-15 11:09:32 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:09:33 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:09:34 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:09:35 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:09:36 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:09:37 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:09:38 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:09:39 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:09:40 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:09:41 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:09:42 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:09:43 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:09:44 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:09:45 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:09:46 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:09:47 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:09:48 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:09:49 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:09:50 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:09:51] INFO:     127.0.0.1:59338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:09:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:09:51 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 11:09:52 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:09:53 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:09:54 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:09:55 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:09:56 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:09:57 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:09:58 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:09:59 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:10:00 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:10:01 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 11:10:02 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:10:03 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:10:04 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:10:05 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:10:06 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:10:07 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:10:08 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:10:09 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:10:10 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:10:11] INFO:     127.0.0.1:33224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:10:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:10:11 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:10:12 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:10:13 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:10:14 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:10:15 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:10:16 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:10:17 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:10:18 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:10:19 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:10:20 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:10:21 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:10:22 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:10:23 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:10:24 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:10:25 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:10:26 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:10:27 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:10:28 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:10:29 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:10:30 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:10:30] INFO:     127.0.0.1:39238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:10:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:10:31 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:10:32 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:10:33 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:10:34 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:10:35 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:10:36 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:10:37 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:10:38 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:10:39 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:10:40 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:10:41 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:10:42 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:10:43 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:10:44 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:10:45 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:10:46 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:10:47 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:10:48 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:10:49 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:10:50 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:10:50] INFO:     127.0.0.1:46986 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:10:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:10:51 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.06, #queue-req: 0, 
[2025-12-15 11:10:52 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:10:53 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:10:54 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:10:55 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:10:56 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:10:57 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:10:58 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:10:59 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:11:00 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:11:01 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:11:02 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:11:03 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:11:04 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:11:05 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:11:06 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:11:07 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:11:08 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:11:09 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:11:10 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:11:10] INFO:     127.0.0.1:45166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:11:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:11:11 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:11:12 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:11:13 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:11:14 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:11:15 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:11:16 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:11:17 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:11:18 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:11:19 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:11:20 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:11:21 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:11:22 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:11:23 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:11:24 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:11:25 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:11:26 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:11:27 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:11:28 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:11:29 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:11:30 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:11:30] INFO:     127.0.0.1:52536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:11:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:11:31 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:11:32 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:11:33 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:11:34 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:11:35 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:11:36 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:11:37 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:11:38 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:11:39 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:11:40 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:11:41 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:11:42 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:11:43 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:11:44 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:11:45 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:11:46 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:11:47 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:11:48 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:11:49 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-15 11:11:50 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-15 11:11:50] INFO:     127.0.0.1:44748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:11:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:11:51 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:11:52 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:11:53 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:11:54 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:11:55 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:11:56 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:11:57 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:11:58 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:11:59 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:12:00 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:12:01 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:12:02 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:12:03 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:12:04 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:12:05 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:12:06 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:12:07 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:12:07 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-15 11:12:08 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:12:09 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:12:10] INFO:     127.0.0.1:47110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:12:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:12:11 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.02, #queue-req: 0, 
[2025-12-15 11:12:12 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:12:13 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:12:14 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:12:15 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:12:16 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:12:17 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:12:18 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:12:19 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:12:20 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:12:21 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:12:22 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:12:23 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:12:24 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:12:25 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:12:25 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:12:26 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-15 11:12:27 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-15 11:12:28 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:12:29 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-15 11:12:30] INFO:     127.0.0.1:47960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:12:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:12:31 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.12, #queue-req: 0, 
[2025-12-15 11:12:32 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:12:33 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:12:34 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:12:35 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:12:36 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:12:37 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:12:38 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:12:39 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:12:40 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:12:41 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:12:41 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:12:42 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:12:43 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:12:44 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:12:45 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:12:46 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:12:47 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:12:48 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:12:49 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:12:50] INFO:     127.0.0.1:47532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:12:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:12:51 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:12:52 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:12:53 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:12:54 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:12:55 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:12:56 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:12:57 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:12:57 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:12:58 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:12:59 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:13:00 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:13:01 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:13:02 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:13:03 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:13:04 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:13:05 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:13:06 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:13:07 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:13:08 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:13:09 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:13:10] INFO:     127.0.0.1:36444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:13:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:13:11 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-15 11:13:12 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:13:13 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:13:14 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:13:14 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:13:15 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:13:16 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:13:17 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:13:18 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:13:19 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:13:20 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:13:21 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:13:22 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:13:23 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:13:24 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:13:25 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:13:26 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:13:27 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:13:28 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:13:29 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:13:30] INFO:     127.0.0.1:43340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:13:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:13:31 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-15 11:13:31 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:13:32 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:13:33 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:13:34 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:13:35 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:13:36 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:13:37 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:13:38 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:13:39 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:13:40 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:13:41 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:13:42 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:13:43 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:13:44 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:13:45 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:13:46 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:13:47 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:13:48 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:13:49 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:13:50] INFO:     127.0.0.1:46246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:13:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:13:50 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-15 11:13:51 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-15 11:13:52 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:13:53 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-15 11:13:54 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:13:55 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:13:56 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:13:57 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:13:58 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:13:59 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:14:00 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:14:01 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:14:02 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:14:03 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:14:04 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:14:05 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:14:06 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-15 11:14:07 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:14:08 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:14:09 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:14:10] INFO:     127.0.0.1:33784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:14:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:14:10 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-15 11:14:11 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-15 11:14:12 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-15 11:14:13 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-15 11:14:14 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:14:15 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:14:16 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:14:17 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:14:18 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:14:19 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:14:20 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:14:21 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:14:22 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:14:23 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:14:24 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:14:25 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:14:26 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-15 11:14:27 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-15 11:14:28 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:14:29 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:14:30] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:14:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:14:30 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-15 11:14:31 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-15 11:14:32 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-15 11:14:33 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-15 11:14:34 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:14:35 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:14:36 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:14:37 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:14:38 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:14:39 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:14:40 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:14:41 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:14:42 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:14:43 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:14:44 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:14:45 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:14:46 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-15 11:14:47 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:14:48 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-15 11:14:49 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-15 11:14:50] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:14:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:14:50 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 11:14:51 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:14:52 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:14:53 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-15 11:14:54 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:14:55 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:14:56 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:14:57 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:14:58 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:14:59 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:15:00 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 11:15:01 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:15:02 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:15:03 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:15:04 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:15:05 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:15:06 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-15 11:15:07 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:15:08 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:15:09 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:15:10] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:15:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:15:10 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-15 11:15:11 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:15:12 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:15:13 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:15:14 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:15:15 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:15:16 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:15:17 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:15:18 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:15:19 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:15:20 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:15:21 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:15:22 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:15:23 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:15:24 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:15:25 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:15:26 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:15:27 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:15:28 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:15:29 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:15:30] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:15:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:15:30 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 11:15:31 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:15:32 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:15:33 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:15:34 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:15:35 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:15:36 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:15:37 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:15:38 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:15:39 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:15:40 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:15:41 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:15:42 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:15:43 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:15:44 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:15:45 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:15:46 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:15:47 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:15:48 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:15:49 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:15:50] INFO:     127.0.0.1:49202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:15:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:15:50 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-15 11:15:51 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:15:52 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:15:53 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:15:54 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:15:55 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:15:56 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:15:57 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:15:58 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:15:59 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:16:00 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:16:01 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:16:02 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:16:03 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:16:04 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:16:05 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:16:06 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:16:07 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:16:08 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:16:09 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:16:10] INFO:     127.0.0.1:38640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:16:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:16:10 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:16:11 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:16:12 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:16:13 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:16:14 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:16:15 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:16:16 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:16:17 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:16:18 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:16:19 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:16:20 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:16:21 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:16:22 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:16:23 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:16:24 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:16:25 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:16:26 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:16:27 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:16:28 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:16:29 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:16:30] INFO:     127.0.0.1:36770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:16:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:16:30 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.00, #queue-req: 0, 
[2025-12-15 11:16:31 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:16:32 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:16:33 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:16:34 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:16:35 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:16:36 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:16:37 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:16:38 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:16:39 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:16:40 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:16:41 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:16:42 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:16:43 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:16:44 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:16:45 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:16:46 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:16:47 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:16:48 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:16:49 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:16:50] INFO:     127.0.0.1:41210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:16:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:16:50 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 11:16:51 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:16:52 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:16:53 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:16:54 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:16:55 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:16:56 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:16:57 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:16:58 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:16:59 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:17:00 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:17:01 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:17:02 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:17:03 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:17:04 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:17:05 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:17:06 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:17:07 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:17:08 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:17:09 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:17:10] INFO:     127.0.0.1:53168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:17:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:17:10 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:17:11 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:17:12 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:17:13 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:17:14 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:17:15 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:17:16 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:17:17 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:17:18 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:17:19 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:17:20 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:17:21 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:17:22 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:17:23 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:17:24 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:17:25 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:17:26 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:17:27 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:17:28 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:17:29 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:17:30] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:17:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:17:30 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-15 11:17:31 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:17:32 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:17:33 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:17:34 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:17:35 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:17:36 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:17:37 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:17:38 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:17:39 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:17:40 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:17:41 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:17:42 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:17:43 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:17:44 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:17:45 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:17:46 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:17:47 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:17:48 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:17:49 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:17:50] INFO:     127.0.0.1:46570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:17:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:17:50 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 33.41, #queue-req: 0, 
[2025-12-15 11:17:51 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:17:52 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:17:53 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:17:54 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:17:55 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:17:56 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:17:57 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:17:58 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:17:59 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:18:00 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:18:01 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:18:02 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:18:03 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:18:04 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:18:05 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:18:06 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:18:07 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:18:08 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:18:09 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:18:10] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:18:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:18:10 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.16, #queue-req: 0, 
[2025-12-15 11:18:11 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:18:12 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:18:13 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:18:14 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:18:15 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:18:16 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:18:17 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:18:18 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:18:19 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:18:20 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:18:21 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:18:22 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:18:23 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:18:24 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:18:25 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:18:26 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:18:27 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:18:28 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:18:29 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:18:30] INFO:     127.0.0.1:38756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:18:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:18:30 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.25, #queue-req: 0, 
[2025-12-15 11:18:31 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:18:32 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:18:33 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:18:34 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:18:35 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:18:36 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:18:37 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:18:38 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:18:39 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:18:40 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:18:41 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:18:42 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:18:43 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:18:44 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:18:45 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:18:46 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:18:47 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:18:48 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:18:49 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:18:50] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:18:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:18:50 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 11:18:51 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:18:52 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:18:53 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:18:54 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:18:55 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:18:56 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:18:57 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:18:58 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:18:59 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:19:00 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:19:01 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:19:02 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:19:03 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:19:04 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:19:05 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:19:06 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:19:07 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:19:08 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:19:09 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:19:10] INFO:     127.0.0.1:60294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:19:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:19:10 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:19:11 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:19:12 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:19:13 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:19:14 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:19:15 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:19:16 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:19:17 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:19:18 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:19:19 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:19:20 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:19:21 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:19:22 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:19:23 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:19:24 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:19:25 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:19:26 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:19:27 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-15 11:19:28 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:19:29 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:19:29] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:19:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:19:30 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-15 11:19:31 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:19:32 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:19:33 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:19:34 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:19:35 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:19:36 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:19:37 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:19:38 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:19:39 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:19:40 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:19:41 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:19:42 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:19:43 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:19:44 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:19:45 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:19:46 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:19:47 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:19:48 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:19:49 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:19:49] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:19:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:19:50 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:19:51 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:19:52 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:19:53 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:19:54 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:19:55 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:19:56 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:19:57 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:19:58 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:19:59 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:20:00 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:20:01 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:20:02 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:20:03 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:20:04 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:20:05 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:20:06 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:20:07 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:20:08 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:20:09 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:20:09] INFO:     127.0.0.1:37434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:20:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:20:10 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:20:11 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:20:12 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:20:13 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:20:14 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:20:15 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:20:16 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:20:17 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:20:18 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:20:19 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:20:20 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:20:21 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:20:22 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:20:23 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:20:24 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:20:25 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:20:26 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:20:27 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:20:28 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:20:29 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:20:29] INFO:     127.0.0.1:41106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:20:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:20:30 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-15 11:20:31 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:20:32 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:20:33 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:20:34 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:20:35 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:20:36 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:20:37 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:20:38 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:20:39 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:20:40 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:20:41 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:20:42 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:20:43 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:20:44 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:20:45 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:20:46 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:20:47 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:20:48 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:20:49 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:20:49] INFO:     127.0.0.1:59146 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:20:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:20:50 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-15 11:20:51 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:20:52 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:20:53 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:20:54 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:20:55 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:20:56 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:20:57 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:20:58 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:20:59 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:21:00 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:21:01 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:21:02 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:21:03 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:21:04 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:21:04 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:21:05 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:21:06 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:21:07 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:21:08 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:21:09] INFO:     127.0.0.1:41666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:21:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:21:10 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:21:11 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:21:12 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:21:13 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:21:14 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:21:15 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:21:16 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:21:17 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:21:18 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:21:19 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:21:19 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:21:20 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:21:21 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:21:22 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:21:23 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:21:24 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:21:25 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:21:26 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:21:27 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:21:28 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:21:29] INFO:     127.0.0.1:50592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:21:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:21:30 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:21:31 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:21:32 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:21:33 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:21:34 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:21:35 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:21:36 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:21:36 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:21:37 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:21:38 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:21:39 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:21:40 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:21:41 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:21:42 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:21:43 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:21:44 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:21:45 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:21:46 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:21:47 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:21:48 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:21:49] INFO:     127.0.0.1:47942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:21:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:21:50 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.25, #queue-req: 0, 
[2025-12-15 11:21:51 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:21:52 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:21:52 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:21:53 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:21:54 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:21:55 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:21:56 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:21:57 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:21:58 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:21:59 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:22:00 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:22:01 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:22:02 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:22:03 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:22:04 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:22:05 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:22:06 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:22:07 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:22:08 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:22:09] INFO:     127.0.0.1:44510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:22:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:22:10 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.07, #queue-req: 0, 
[2025-12-15 11:22:10 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:22:11 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:22:12 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:22:13 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:22:14 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:22:15 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:22:16 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:22:17 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:22:18 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:22:19 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:22:20 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:22:21 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:22:22 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:22:23 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:22:24 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:22:25 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:22:26 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:22:27 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:22:28 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:22:29] INFO:     127.0.0.1:39526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:22:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:22:29 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:22:30 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:22:31 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:22:32 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:22:33 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:22:34 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:22:35 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:22:36 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:22:37 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:22:38 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:22:39 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:22:40 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:22:41 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:22:42 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:22:43 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:22:44 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:22:45 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:22:46 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:22:47 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:22:48 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:22:49] INFO:     127.0.0.1:54314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:22:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:22:49 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-15 11:22:50 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:22:51 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:22:52 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:22:53 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:22:54 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:22:55 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:22:56 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:22:57 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:22:58 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:22:59 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:23:00 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:23:01 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:23:02 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:23:03 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:23:04 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:23:05 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:23:06 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:23:07 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:23:08 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:23:09] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:23:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:23:09 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:23:10 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:23:11 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:23:12 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:23:13 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:23:14 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:23:15 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:23:16 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:23:17 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:23:18 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:23:19 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:23:20 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:23:21 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:23:22 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:23:23 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:23:24 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:23:25 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:23:26 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:23:27 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:23:28 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:23:29] INFO:     127.0.0.1:34028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:23:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:23:29 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.03, #queue-req: 0, 
[2025-12-15 11:23:30 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:23:31 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:23:32 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:23:33 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:23:34 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:23:35 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:23:36 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:23:37 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:23:38 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:23:39 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:23:40 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:23:41 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:23:42 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:23:43 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:23:44 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:23:45 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:23:46 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:23:47 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:23:48 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:23:49] INFO:     127.0.0.1:33132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:23:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:23:49 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-15 11:23:50 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:23:51 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:23:52 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:23:53 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:23:54 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:23:55 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:23:56 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:23:57 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:23:58 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:23:59 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:24:00 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:24:01 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:24:02 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:24:03 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:24:04 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:24:05 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:24:06 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:24:07 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:24:08 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:24:09] INFO:     127.0.0.1:54478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:24:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:24:09 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:24:10 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:24:11 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:24:12 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:24:13 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:24:14 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:24:15 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:24:16 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:24:17 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:24:18 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:24:19 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 11:24:20 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:24:21 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:24:22 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:24:23 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:24:24 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:24:25 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:24:26 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:24:27 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:24:28 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:24:29] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:24:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:24:29 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:24:30 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:24:31 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:24:32 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:24:33 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:24:34 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:24:35 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:24:36 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:24:37 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:24:38 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:24:39 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:24:40 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:24:41 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:24:42 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:24:43 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:24:44 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:24:45 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:24:46 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:24:47 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:24:48 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:24:49] INFO:     127.0.0.1:40058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:24:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:24:49 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:24:50 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:24:51 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:24:52 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:24:53 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:24:54 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:24:55 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:24:56 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:24:57 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:24:58 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:24:59 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 11:25:00 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:25:01 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:25:02 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:25:03 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:25:04 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:25:05 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:25:06 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:25:07 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:25:08 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:25:09] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:25:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:25:09 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-15 11:25:10 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:25:11 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:25:12 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:25:13 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:25:14 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:25:15 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:25:16 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:25:17 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:25:18 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:25:19 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 11:25:20 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:25:21 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:25:22 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:25:23 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:25:24 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:25:25 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:25:26 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:25:27 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:25:28 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:25:29] INFO:     127.0.0.1:48032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:25:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:25:29 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:25:30 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:25:31 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:25:32 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:25:33 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:25:34 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:25:35 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:25:36 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:25:37 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:25:38 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:25:39 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 11:25:40 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:25:41 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:25:42 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:25:43 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:25:44 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:25:45 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:25:46 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:25:47 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:25:48 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:25:49] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:25:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:25:49 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-15 11:25:50 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:25:51 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:25:52 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:25:53 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:25:54 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:25:55 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:25:56 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:25:57 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:25:58 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:25:59 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 11:26:00 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:26:01 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:26:02 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:26:03 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:26:04 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:26:05 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:26:06 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:26:07 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:26:08 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:26:09] INFO:     127.0.0.1:52598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:26:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:26:09 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 11:26:10 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:26:11 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:26:12 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:26:13 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:26:14 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:26:15 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:26:16 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:26:17 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:26:18 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:26:19 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:26:20 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:26:21 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:26:22 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:26:23 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:26:24 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:26:25 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:26:26 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:26:27 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-15 11:26:28 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:26:29] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:26:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:26:29 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-15 11:26:30 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:26:31 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:26:32 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.62, #queue-req: 0, 
[2025-12-15 11:26:33 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:26:34 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:26:35 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:26:36 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:26:37 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:26:38 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:26:39 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:26:40 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:26:41 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:26:42 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:26:43 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:26:44 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:26:45 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:26:46 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:26:47 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:26:48 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:26:49] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:26:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:26:49 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:26:50 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-15 11:26:51 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:26:52 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:26:53 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:26:54 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:26:55 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:26:56 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:26:57 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:26:58 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:26:59 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:27:00 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:27:01 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:27:02 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:27:03 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:27:04 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:27:05 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-15 11:27:06 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-15 11:27:07 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:27:08 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:27:09] INFO:     127.0.0.1:44130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:27:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:27:09 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:27:10 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-15 11:27:11 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-15 11:27:12 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-15 11:27:13 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:27:14 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:27:15 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:27:16 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:27:17 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:27:18 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:27:19 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:27:20 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-15 11:27:21 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-15 11:27:22 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:27:23 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:27:24 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-15 11:27:25 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-15 11:27:26 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-15 11:27:27 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-15 11:27:28 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-15 11:27:28] INFO:     127.0.0.1:51686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:27:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:27:29 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-15 11:27:30 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:27:31 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:27:32 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:27:33 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:27:34 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:27:35 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:27:36 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:27:37 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:27:38 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:27:39 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 11:27:40 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:27:41 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:27:42 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:27:43 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:27:44 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:27:45 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:27:46 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:27:47 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:27:48 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:27:48] INFO:     127.0.0.1:45454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:27:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:27:49 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.04, #queue-req: 0, 
[2025-12-15 11:27:50 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:27:51 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:27:52 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:27:53 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:27:54 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:27:55 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:27:56 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:27:57 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:27:58 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:27:59 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 11:28:00 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:28:01 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:28:02 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:28:03 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:28:04 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:28:05 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:28:06 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:28:07 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:28:08 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:28:08] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:28:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:28:09 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-15 11:28:10 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:28:11 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:28:12 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:28:13 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:28:14 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:28:15 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:28:16 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:28:17 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:28:18 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:28:19 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 11:28:20 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:28:21 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:28:22 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:28:23 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:28:24 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:28:25 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:28:26 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:28:27 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:28:28 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:28:28] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:28:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:28:29 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:28:30 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:28:31 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:28:32 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:28:33 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:28:34 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:28:35 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:28:36 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:28:37 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:28:38 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:28:39 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:28:40 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:28:41 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:28:42 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:28:43 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:28:44 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:28:45 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:28:46 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:28:47 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:28:48 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:28:48] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:28:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:28:49 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:28:50 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:28:51 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:28:52 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:28:53 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:28:54 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:28:55 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:28:56 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:28:57 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:28:58 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:28:59 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 11:29:00 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:29:01 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:29:02 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:29:03 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:29:04 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:29:05 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:29:06 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:29:07 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:29:08 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:29:08] INFO:     127.0.0.1:41216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:29:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:29:09 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.25, #queue-req: 0, 
[2025-12-15 11:29:10 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:29:11 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:29:12 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:29:13 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:29:14 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:29:15 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:29:16 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:29:17 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:29:18 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:29:19 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-15 11:29:20 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:29:21 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:29:22 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:29:23 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:29:24 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:29:25 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:29:26 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:29:27 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:29:28 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:29:28] INFO:     127.0.0.1:51806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:29:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:29:29 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-15 11:29:30 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:29:31 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:29:32 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:29:33 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:29:34 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:29:35 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:29:36 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:29:37 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:29:38 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:29:39 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 11:29:40 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:29:41 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:29:42 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:29:43 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:29:44 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:29:45 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:29:46 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:29:46 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:29:47 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:29:48] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:29:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:29:49 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-15 11:29:50 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:29:51 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:29:52 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:29:53 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:29:54 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:29:55 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:29:56 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:29:57 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:29:58 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:29:59 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:30:00 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:30:01 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:30:02 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:30:02 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:30:03 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:30:04 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-15 11:30:05 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:30:06 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:30:07 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:30:08] INFO:     127.0.0.1:54288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:30:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:30:09 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-15 11:30:10 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:30:11 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:30:12 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:30:13 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:30:14 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:30:15 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:30:16 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:30:17 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:30:18 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:30:19 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 11:30:19 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:30:20 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:30:21 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:30:22 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:30:23 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:30:24 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:30:25 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:30:26 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:30:27 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:30:28] INFO:     127.0.0.1:53398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:30:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:30:29 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:30:30 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:30:31 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:30:32 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:30:33 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:30:34 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:30:35 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:30:36 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:30:36 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:30:37 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:30:38 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 11:30:39 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:30:40 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:30:41 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:30:42 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:30:43 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:30:44 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:30:45 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:30:46 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:30:47 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:30:48] INFO:     127.0.0.1:42810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:30:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:30:49 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-15 11:30:50 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:30:51 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:30:52 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:30:53 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:30:54 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:30:54 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:30:55 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:30:56 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:30:57 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:30:58 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 11:30:59 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:31:00 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:31:01 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:31:02 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:31:03 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:31:04 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:31:05 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:31:06 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:31:07 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:31:08] INFO:     127.0.0.1:50114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:31:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:31:09 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:31:10 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:31:11 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:31:11 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:31:12 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:31:13 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:31:14 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:31:15 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:31:16 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:31:17 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:31:18 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 11:31:19 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:31:20 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:31:21 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:31:22 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:31:23 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:31:24 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-15 11:31:25 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:31:26 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:31:27 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:31:28] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:31:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:31:29 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-15 11:31:29 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:31:30 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:31:31 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:31:32 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:31:33 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:31:34 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:31:35 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:31:36 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:31:37 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:31:38 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 11:31:39 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:31:40 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:31:41 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:31:42 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:31:43 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:31:44 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:31:45 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:31:46 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:31:47 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:31:48] INFO:     127.0.0.1:38302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:31:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:31:48 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-15 11:31:49 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:31:50 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:31:51 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:31:52 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:31:53 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:31:54 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:31:55 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:31:56 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:31:57 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:31:58 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 11:31:59 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:32:00 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:32:01 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:32:02 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:32:03 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:32:04 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-15 11:32:05 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:32:06 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:32:07 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:32:08] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:32:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:32:08 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.25, #queue-req: 0, 
[2025-12-15 11:32:09 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:32:10 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:32:11 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:32:12 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:32:13 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:32:14 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:32:15 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:32:16 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:32:17 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:32:18 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:32:19 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:32:20 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:32:21 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:32:22 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:32:23 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:32:24 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:32:25 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:32:26 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:32:27 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:32:28] INFO:     127.0.0.1:49330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:32:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:32:28 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-15 11:32:29 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:32:30 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:32:31 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:32:32 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:32:33 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:32:34 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:32:35 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:32:36 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:32:37 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:32:38 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:32:39 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:32:40 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:32:41 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:32:42 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:32:43 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:32:44 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:32:45 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:32:46 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:32:47 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:32:48] INFO:     127.0.0.1:33596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:32:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:32:48 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-15 11:32:49 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:32:50 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:32:51 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:32:52 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:32:53 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:32:54 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:32:55 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:32:56 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:32:57 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:32:58 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:32:59 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:33:00 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:33:01 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:33:02 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:33:03 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:33:04 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:33:05 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:33:06 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:33:07 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:33:08] INFO:     127.0.0.1:42590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:33:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:33:08 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-15 11:33:09 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:33:10 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:33:11 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:33:12 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:33:13 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-15 11:33:14 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:33:15 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-15 11:33:16 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:33:17 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:33:18 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:33:19 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:33:20 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:33:21 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:33:22 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:33:23 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:33:24 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:33:25 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:33:26 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-15 11:33:27 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-15 11:33:28] INFO:     127.0.0.1:57076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:33:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:33:28 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-15 11:33:29 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:33:30 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:33:31 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-15 11:33:32 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-15 11:33:33 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:33:34 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:33:35 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:33:36 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:33:37 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-15 11:33:38 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:33:39 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-15 11:33:40 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:33:41 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:33:42 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:33:43 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:33:44 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:33:45 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-15 11:33:46 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-15 11:33:47 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-15 11:33:48] INFO:     127.0.0.1:39432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:33:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:33:48 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.11, #queue-req: 0, 
[2025-12-15 11:33:49 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.60, #queue-req: 0, 
[2025-12-15 11:33:50 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:33:51 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-15 11:33:52 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-15 11:33:53 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:33:54 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:33:55 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:33:56 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:33:57 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:33:58 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-15 11:33:59 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:34:00 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-15 11:34:01 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:34:02 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:34:03 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-15 11:34:04 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:34:05 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:34:06 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-15 11:34:07 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-15 11:34:08] INFO:     127.0.0.1:44400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:34:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:34:08 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-15 11:34:09 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:34:10 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:34:11 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-15 11:34:12 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-15 11:34:13 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:34:14 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:34:15 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-15 11:34:16 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:34:17 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-15 11:34:18 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-15 11:34:19 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:34:20 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:34:21 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:34:22 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-15 11:34:23 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:34:24 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:34:25 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:34:26 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:34:27 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:34:28] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 11:34:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 11:34:28 TP0] Decode batch, #running-req: 1, #token: 3210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-15 11:34:29 TP0] Decode batch, #running-req: 1, #token: 3250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:34:30 TP0] Decode batch, #running-req: 1, #token: 3290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:34:31 TP0] Decode batch, #running-req: 1, #token: 3330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-15 11:34:32 TP0] Decode batch, #running-req: 1, #token: 3370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-15 11:34:33 TP0] Decode batch, #running-req: 1, #token: 3410, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:34:34 TP0] Decode batch, #running-req: 1, #token: 3450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-15 11:34:35 TP0] Decode batch, #running-req: 1, #token: 3490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:34:36 TP0] Decode batch, #running-req: 1, #token: 3530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:34:37 TP0] Decode batch, #running-req: 1, #token: 3570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-15 11:34:38 TP0] Decode batch, #running-req: 1, #token: 3610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-15 11:34:39 TP0] Decode batch, #running-req: 1, #token: 3650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:34:40 TP0] Decode batch, #running-req: 1, #token: 3690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:34:41 TP0] Decode batch, #running-req: 1, #token: 3730, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-15 11:34:42 TP0] Decode batch, #running-req: 1, #token: 3770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:34:43 TP0] Decode batch, #running-req: 1, #token: 3810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-15 11:34:44 TP0] Decode batch, #running-req: 1, #token: 3850, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-15 11:34:45 TP0] Decode batch, #running-req: 1, #token: 3890, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:34:46 TP0] Decode batch, #running-req: 1, #token: 3930, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-15 11:34:47 TP0] Decode batch, #running-req: 1, #token: 3970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-15 11:34:48] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-15 11:34:48] INFO:     127.0.0.1:50686 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-15 11:34:48] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-15 11:34:48] INFO:     127.0.0.1:50694 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-15 11:34:51] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-12-15 11:34:55] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
