[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 10:00:50] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
INFO 12-16 10:00:51 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-16 10:00:52] WARNING server_args.py:1536: Attention backend not explicitly specified. Use aiter backend by default.
[2025-12-16 10:00:52] server_args=ServerArgs(model_path='/data/models/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/data/models/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.765, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=519758649, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/models/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2025-12-16 10:00:52] Using default HuggingFace chat template with detected content format: string
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 10:00:59] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 10:00:59] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 10:01:00] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 10:01:00] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 10:01:00] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 10:01:00] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 10:01:00] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 10:01:00] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 10:01:00] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
INFO 12-16 10:01:00 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 10:01:00 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 10:01:00 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 10:01:00 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 10:01:00 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 10:01:00 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 10:01:01 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 10:01:01 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 10:01:01 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-16 10:01:02 TP3] Process 221 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-16 10:01:02 TP5] Process 223 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-16 10:01:02 TP7] Process 225 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[2025-12-16 10:01:02 TP4] Process 222 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[2025-12-16 10:01:02 TP1] Process 219 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2025-12-16 10:01:02 TP6] Process 224 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
[2025-12-16 10:01:02 TP0] Process 218 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2025-12-16 10:01:02 TP2] Process 220 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
[2025-12-16 10:01:02 TP5] Init torch distributed begin.
[2025-12-16 10:01:02 TP3] Init torch distributed begin.
[2025-12-16 10:01:02 TP4] Init torch distributed begin.
[2025-12-16 10:01:02 TP7] Init torch distributed begin.
[2025-12-16 10:01:02 TP1] Init torch distributed begin.
[2025-12-16 10:01:02 TP6] Init torch distributed begin.
[2025-12-16 10:01:02 TP2] Init torch distributed begin.
[2025-12-16 10:01:02 TP0] Init torch distributed begin.
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-12-16 10:01:03 TP0] sglang is using nccl==2.26.6
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 10:01:10 TP2] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 10:01:10 TP0] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 10:01:10 TP3] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 10:01:10 TP6] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 10:01:10 TP1] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 10:01:10 TP7] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 10:01:10 TP5] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 10:01:10 TP4] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 10:01:10 TP2] Using AiterCustomAllreduce for ROCm.
[2025-12-16 10:01:10 TP0] Using AiterCustomAllreduce for ROCm.
[2025-12-16 10:01:10 TP3] Using AiterCustomAllreduce for ROCm.
[2025-12-16 10:01:10 TP5] Using AiterCustomAllreduce for ROCm.
[2025-12-16 10:01:10 TP6] Using AiterCustomAllreduce for ROCm.
[2025-12-16 10:01:10 TP7] Using AiterCustomAllreduce for ROCm.
[2025-12-16 10:01:10 TP1] Using AiterCustomAllreduce for ROCm.
[2025-12-16 10:01:10 TP4] Using AiterCustomAllreduce for ROCm.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-12-16 10:01:10 TP7] Init torch distributed ends. mem usage=3.23 GB
[2025-12-16 10:01:10 TP0] Init torch distributed ends. mem usage=2.95 GB
[2025-12-16 10:01:10 TP6] Init torch distributed ends. mem usage=3.24 GB
[2025-12-16 10:01:10 TP5] Init torch distributed ends. mem usage=3.22 GB
[2025-12-16 10:01:10 TP4] Init torch distributed ends. mem usage=3.31 GB
[2025-12-16 10:01:10 TP3] Init torch distributed ends. mem usage=3.36 GB
[2025-12-16 10:01:10 TP2] Init torch distributed ends. mem usage=3.37 GB
[2025-12-16 10:01:10 TP1] Init torch distributed ends. mem usage=3.37 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-12-16 10:01:11 TP2] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 10:01:11 TP5] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 10:01:11 TP1] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 10:01:11 TP3] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 10:01:11 TP6] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 10:01:11 TP0] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 10:01:11 TP7] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 10:01:11 TP4] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 10:01:12 TP5] Load weight begin. avail mem=188.21 GB
[2025-12-16 10:01:12 TP2] Load weight begin. avail mem=188.06 GB
[2025-12-16 10:01:12 TP6] Load weight begin. avail mem=188.19 GB
[2025-12-16 10:01:12 TP1] Load weight begin. avail mem=188.06 GB
[2025-12-16 10:01:12 TP3] Load weight begin. avail mem=188.07 GB
[2025-12-16 10:01:12 TP7] Load weight begin. avail mem=188.20 GB
[2025-12-16 10:01:12 TP0] Load weight begin. avail mem=188.48 GB
[2025-12-16 10:01:12 TP4] Load weight begin. avail mem=188.12 GB
[2025-12-16 10:01:12 TP0] Detected fp8 checkpoint.
[2025-12-16 10:01:12 TP0] Shared experts fusion optimization enabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:18,  8.61it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:22,  7.23it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:21,  7.41it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:00<00:13, 11.87it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:00<00:14, 10.73it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:01<00:24,  6.12it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:01<00:29,  5.19it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:02<00:31,  4.72it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:02<00:29,  5.06it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:02<00:29,  4.95it/s]
Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:02<00:29,  4.97it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:02<00:25,  5.64it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:02<00:19,  7.28it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:03<00:19,  7.25it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:03<00:31,  4.45it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:03<00:23,  5.85it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:04<00:20,  6.79it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:04<00:16,  8.14it/s]
Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:04<00:13,  9.77it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:04<00:10, 12.43it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:04<00:10, 11.92it/s]
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:04<00:09, 13.02it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:04<00:08, 14.12it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:05<00:08, 14.78it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:05<00:07, 15.33it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:05<00:08, 13.58it/s]
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:05<00:08, 14.48it/s]
Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:06<00:16,  7.11it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:06<00:14,  7.94it/s]
Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:06<00:12,  9.07it/s]
Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:06<00:11,  9.05it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:06<00:10, 10.00it/s]
Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:07<00:09, 10.41it/s]
Loading safetensors checkpoint shards:  37% Completed | 61/163 [00:07<00:08, 11.73it/s]
Loading safetensors checkpoint shards:  39% Completed | 63/163 [00:07<00:08, 11.28it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:07<00:07, 12.38it/s]
Loading safetensors checkpoint shards:  41% Completed | 67/163 [00:07<00:07, 12.61it/s]
Loading safetensors checkpoint shards:  42% Completed | 69/163 [00:07<00:08, 11.33it/s]
Loading safetensors checkpoint shards:  44% Completed | 72/163 [00:07<00:06, 13.57it/s]
Loading safetensors checkpoint shards:  45% Completed | 74/163 [00:08<00:07, 12.67it/s]
Loading safetensors checkpoint shards:  47% Completed | 76/163 [00:08<00:06, 12.75it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:08<00:06, 12.52it/s]
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:08<00:06, 13.06it/s]
Loading safetensors checkpoint shards:  50% Completed | 82/163 [00:08<00:05, 14.04it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:08<00:05, 13.98it/s]
Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:09<00:13,  5.68it/s]
Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:09<00:11,  6.34it/s]
Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:10<00:09,  7.47it/s]
Loading safetensors checkpoint shards:  56% Completed | 92/163 [00:10<00:09,  7.84it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:10<00:08,  8.00it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:10<00:07,  8.86it/s]
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:11<00:07,  8.47it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:11<00:08,  7.86it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:11<00:08,  7.86it/s]
Loading safetensors checkpoint shards:  62% Completed | 101/163 [00:11<00:08,  6.90it/s]
Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:11<00:07,  8.17it/s]
Loading safetensors checkpoint shards:  64% Completed | 104/163 [00:11<00:07,  7.46it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:11<00:07,  7.62it/s]
Loading safetensors checkpoint shards:  65% Completed | 106/163 [00:12<00:08,  6.93it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:12<00:07,  7.33it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:12<00:10,  4.95it/s]
Loading safetensors checkpoint shards:  67% Completed | 110/163 [00:13<00:10,  5.07it/s]
Loading safetensors checkpoint shards:  68% Completed | 111/163 [00:13<00:10,  5.01it/s]
Loading safetensors checkpoint shards:  69% Completed | 112/163 [00:13<00:11,  4.62it/s]
Loading safetensors checkpoint shards:  69% Completed | 113/163 [00:13<00:13,  3.82it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:14<00:11,  4.39it/s]
Loading safetensors checkpoint shards:  71% Completed | 115/163 [00:14<00:09,  5.03it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:14<00:09,  5.07it/s]
Loading safetensors checkpoint shards:  72% Completed | 118/163 [00:14<00:06,  7.18it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:14<00:06,  7.21it/s]
Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:14<00:07,  5.76it/s]
Loading safetensors checkpoint shards:  74% Completed | 121/163 [00:15<00:07,  6.00it/s]
Loading safetensors checkpoint shards:  75% Completed | 122/163 [00:15<00:06,  6.75it/s]
Loading safetensors checkpoint shards:  76% Completed | 124/163 [00:15<00:04,  9.35it/s]
Loading safetensors checkpoint shards:  77% Completed | 126/163 [00:15<00:03, 10.24it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:15<00:02, 12.07it/s]
Loading safetensors checkpoint shards:  80% Completed | 131/163 [00:16<00:05,  5.40it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:16<00:04,  6.08it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:16<00:03,  7.31it/s]
Loading safetensors checkpoint shards:  84% Completed | 137/163 [00:17<00:03,  7.91it/s]
Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:17<00:02,  9.06it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:17<00:02,  9.24it/s]
Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:17<00:01, 10.88it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:17<00:01, 11.54it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:17<00:01, 11.06it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:17<00:01, 11.72it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:18<00:01, 11.28it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:18<00:00, 10.04it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:18<00:00, 10.70it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:18<00:00, 11.17it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:18<00:00, 12.10it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:19<00:00, 12.45it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:19<00:00, 12.71it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:19<00:00,  8.50it/s]

[2025-12-16 10:02:24 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.85 GB, mem usage=79.63 GB.
[2025-12-16 10:02:25 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.44 GB, mem usage=79.63 GB.
[2025-12-16 10:02:25 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.43 GB, mem usage=79.63 GB.
[2025-12-16 10:02:25 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.57 GB, mem usage=79.63 GB.
[2025-12-16 10:02:25 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.56 GB, mem usage=79.63 GB.
[2025-12-16 10:02:25 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.44 GB, mem usage=79.63 GB.
[2025-12-16 10:02:25 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.49 GB, mem usage=79.63 GB.
[2025-12-16 10:02:26 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.58 GB, mem usage=79.63 GB.
[2025-12-16 10:02:26 TP0] Using KV cache dtype: torch.bfloat16
[2025-12-16 10:02:26 TP1] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-16 10:02:26 TP1] Memory pool end. avail mem=43.47 GB
[2025-12-16 10:02:26 TP6] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-16 10:02:26 TP6] Memory pool end. avail mem=43.60 GB
[2025-12-16 10:02:26 TP5] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-16 10:02:26 TP0] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-16 10:02:26 TP5] Memory pool end. avail mem=43.61 GB
[2025-12-16 10:02:26 TP0] Memory pool end. avail mem=43.89 GB
[2025-12-16 10:02:26 TP3] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-16 10:02:26 TP3] Memory pool end. avail mem=43.48 GB
[2025-12-16 10:02:26 TP2] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-16 10:02:26 TP2] Memory pool end. avail mem=43.47 GB
[2025-12-16 10:02:26 TP4] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-16 10:02:26 TP4] Memory pool end. avail mem=43.53 GB
[2025-12-16 10:02:26 TP7] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-16 10:02:26 TP7] Memory pool end. avail mem=43.61 GB
[2025-12-16 10:02:28 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-12-16 10:02:28 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.15 GB
[2025-12-16 10:02:28 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-12-16 10:02:29 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.28 GB
[2025-12-16 10:02:29 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.15 GB
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-16 10:02:29 TP7] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-16 10:02:29 TP7] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-16 10:02:29 TP1] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-16 10:02:29 TP1] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-16 10:02:29 TP5] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-16 10:02:29 TP5] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-16 10:02:30 TP2] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-16 10:02:30 TP2] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-16 10:02:30 TP6] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-16 10:02:30 TP6] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-16 10:02:30 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.57 GB
[2025-12-16 10:02:30 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-12-16 10:02:30 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.21 GB
[2025-12-16 10:02:30 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.91 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-16 10:02:31 TP4] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-16 10:02:31 TP0] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-16 10:02:31 TP4] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-16 10:02:31 TP0] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-16 10:02:31 TP3] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-16 10:02:31 TP3] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-16 10:02:36 TP3] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 10:02:36 TP3] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-16 10:02:38 TP0] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 10:02:38 TP0] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-16 10:02:39 TP6] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 10:02:39 TP6] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-16 10:02:41 TP1] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 10:02:41 TP1] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-16 10:02:41 TP3] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-16 10:02:42 TP3] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-16 10:02:42 TP3] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-16 10:02:42 TP3] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-16 10:02:42 TP4] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 10:02:42 TP4] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-16 10:02:43 TP3] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-16 10:02:43 TP0] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 10:02:43 TP3] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:02:43 TP3] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 10:02:43 TP3] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-16 10:02:43 TP3] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-16 10:02:43 TP0] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-16 10:02:43 TP0] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-16 10:02:43 TP0] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-16 10:02:44 TP2] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 10:02:44 TP2] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-16 10:02:44 TP6] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-16 10:02:45 TP6] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-16 10:02:45 TP6] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-16 10:02:45 TP6] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-16 10:02:45 TP0] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-16 10:02:45 TP1] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 10:02:45 TP0] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:02:45 TP0] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 10:02:45 TP0] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-16 10:02:45 TP0] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-16 10:02:45 TP5] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 10:02:45 TP5] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-16 10:02:45 TP1] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-16 10:02:45 TP1] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-16 10:02:45 TP1] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-16 10:02:46 TP6] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 10:02:46 TP6] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:02:46 TP6] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 10:02:46 TP6] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-16 10:02:46 TP6] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-16 10:02:46 TP1] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-16 10:02:46 TP7] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 10:02:46 TP7] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 10:02:47 TP1] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:02:47 TP1] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 10:02:47 TP1] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-16 10:02:47 TP1] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-16 10:02:47 TP4] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-16 10:02:47 TP4] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-16 10:02:47 TP4] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-16 10:02:47 TP4] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-16 10:02:48 TP2] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-16 10:02:48 TP2] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-16 10:02:48 TP2] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-16 10:02:48 TP2] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-16 10:02:48 TP4] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 10:02:49 TP4] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:02:49 TP4] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 10:02:49 TP4] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-16 10:02:49 TP4] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-16 10:02:49 TP2] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 10:02:49 TP2] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:02:49 TP2] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 10:02:49 TP2] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-16 10:02:49 TP2] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-16 10:02:50 TP5] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-16 10:02:50 TP5] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-16 10:02:50 TP5] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-16 10:02:50 TP5] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-16 10:02:51 TP7] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-16 10:02:51 TP7] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-16 10:02:51 TP7] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-16 10:02:51 TP7] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-16 10:02:52 TP5] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 10:02:52 TP5] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:02:52 TP5] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 10:02:52 TP5] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-16 10:02:52 TP5] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-16 10:02:52 TP7] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 10:02:53 TP7] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:02:53 TP7] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 10:02:53 TP7] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-16 10:02:53 TP7] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
Capturing batches (bs=512 avail_mem=42.91 GB):   2%|         | 1/52 [00:23<19:39, 23.14s/it]Capturing batches (bs=496 avail_mem=40.27 GB):   2%|         | 1/52 [00:23<19:39, 23.14s/it]Capturing batches (bs=496 avail_mem=40.27 GB):   4%|         | 2/52 [00:23<08:19,  9.99s/it]Capturing batches (bs=480 avail_mem=40.27 GB):   4%|         | 2/52 [00:23<08:19,  9.99s/it]Capturing batches (bs=480 avail_mem=40.27 GB):   6%|         | 3/52 [00:24<04:36,  5.63s/it]Capturing batches (bs=464 avail_mem=40.27 GB):   6%|         | 3/52 [00:24<04:36,  5.63s/it]Capturing batches (bs=464 avail_mem=40.27 GB):   8%|         | 4/52 [00:24<02:50,  3.56s/it]Capturing batches (bs=448 avail_mem=40.27 GB):   8%|         | 4/52 [00:24<02:50,  3.56s/it]Capturing batches (bs=448 avail_mem=40.27 GB):  10%|         | 5/52 [00:25<01:53,  2.41s/it]Capturing batches (bs=432 avail_mem=40.27 GB):  10%|         | 5/52 [00:25<01:53,  2.41s/it]Capturing batches (bs=432 avail_mem=40.27 GB):  12%|        | 6/52 [00:25<01:18,  1.72s/it]Capturing batches (bs=416 avail_mem=40.26 GB):  12%|        | 6/52 [00:25<01:18,  1.72s/it]Capturing batches (bs=416 avail_mem=40.26 GB):  13%|        | 7/52 [00:25<00:57,  1.28s/it]Capturing batches (bs=400 avail_mem=40.26 GB):  13%|        | 7/52 [00:25<00:57,  1.28s/it]Capturing batches (bs=400 avail_mem=40.26 GB):  15%|        | 8/52 [00:26<00:43,  1.01it/s]Capturing batches (bs=384 avail_mem=40.26 GB):  15%|        | 8/52 [00:26<00:43,  1.01it/s]Capturing batches (bs=384 avail_mem=40.26 GB):  17%|        | 9/52 [00:28<00:55,  1.29s/it]Capturing batches (bs=368 avail_mem=40.25 GB):  17%|        | 9/52 [00:28<00:55,  1.29s/it]Capturing batches (bs=368 avail_mem=40.25 GB):  19%|        | 10/52 [00:28<00:42,  1.01s/it]Capturing batches (bs=352 avail_mem=40.25 GB):  19%|        | 10/52 [00:28<00:42,  1.01s/it]Capturing batches (bs=352 avail_mem=40.25 GB):  21%|        | 11/52 [00:29<00:34,  1.20it/s]Capturing batches (bs=336 avail_mem=40.24 GB):  21%|        | 11/52 [00:29<00:34,  1.20it/s]Capturing batches (bs=336 avail_mem=40.24 GB):  23%|       | 12/52 [00:29<00:28,  1.42it/s]Capturing batches (bs=320 avail_mem=40.24 GB):  23%|       | 12/52 [00:29<00:28,  1.42it/s]Capturing batches (bs=320 avail_mem=40.24 GB):  25%|       | 13/52 [00:29<00:23,  1.66it/s]Capturing batches (bs=304 avail_mem=40.24 GB):  25%|       | 13/52 [00:29<00:23,  1.66it/s]Capturing batches (bs=304 avail_mem=40.24 GB):  27%|       | 14/52 [00:30<00:20,  1.88it/s]Capturing batches (bs=288 avail_mem=40.24 GB):  27%|       | 14/52 [00:30<00:20,  1.88it/s]Capturing batches (bs=288 avail_mem=40.24 GB):  29%|       | 15/52 [00:30<00:17,  2.07it/s]Capturing batches (bs=272 avail_mem=40.24 GB):  29%|       | 15/52 [00:30<00:17,  2.07it/s]Capturing batches (bs=272 avail_mem=40.24 GB):  31%|       | 16/52 [00:30<00:16,  2.23it/s]Capturing batches (bs=256 avail_mem=40.23 GB):  31%|       | 16/52 [00:30<00:16,  2.23it/s][aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:02 TP1] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:02 TP2] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:02 TP3] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:02 TP5] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:02 TP6] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:02 TP0] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:02 TP4] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:02 TP7] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=256 avail_mem=40.23 GB):  33%|      | 17/52 [00:32<00:31,  1.10it/s]Capturing batches (bs=248 avail_mem=40.22 GB):  33%|      | 17/52 [00:32<00:31,  1.10it/s]Capturing batches (bs=248 avail_mem=40.22 GB):  35%|      | 18/52 [00:34<00:41,  1.23s/it]Capturing batches (bs=240 avail_mem=40.21 GB):  35%|      | 18/52 [00:34<00:41,  1.23s/it]Capturing batches (bs=240 avail_mem=40.21 GB):  37%|      | 19/52 [00:35<00:32,  1.03it/s]Capturing batches (bs=232 avail_mem=40.21 GB):  37%|      | 19/52 [00:35<00:32,  1.03it/s]Capturing batches (bs=232 avail_mem=40.21 GB):  38%|      | 20/52 [00:35<00:25,  1.26it/s]Capturing batches (bs=224 avail_mem=40.21 GB):  38%|      | 20/52 [00:35<00:25,  1.26it/s]Capturing batches (bs=224 avail_mem=40.21 GB):  40%|      | 21/52 [00:35<00:20,  1.50it/s]Capturing batches (bs=216 avail_mem=40.20 GB):  40%|      | 21/52 [00:35<00:20,  1.50it/s]Capturing batches (bs=216 avail_mem=40.20 GB):  42%|     | 22/52 [00:36<00:17,  1.73it/s]Capturing batches (bs=208 avail_mem=40.20 GB):  42%|     | 22/52 [00:36<00:17,  1.73it/s]Capturing batches (bs=208 avail_mem=40.20 GB):  44%|     | 23/52 [00:37<00:25,  1.15it/s]Capturing batches (bs=200 avail_mem=40.20 GB):  44%|     | 23/52 [00:37<00:25,  1.15it/s]Capturing batches (bs=200 avail_mem=40.20 GB):  46%|     | 24/52 [00:38<00:20,  1.39it/s]Capturing batches (bs=192 avail_mem=40.20 GB):  46%|     | 24/52 [00:38<00:20,  1.39it/s]Capturing batches (bs=192 avail_mem=40.20 GB):  48%|     | 25/52 [00:38<00:16,  1.63it/s]Capturing batches (bs=184 avail_mem=40.20 GB):  48%|     | 25/52 [00:38<00:16,  1.63it/s]Capturing batches (bs=184 avail_mem=40.20 GB):  50%|     | 26/52 [00:39<00:14,  1.85it/s]Capturing batches (bs=176 avail_mem=40.19 GB):  50%|     | 26/52 [00:39<00:14,  1.85it/s]Capturing batches (bs=176 avail_mem=40.19 GB):  52%|    | 27/52 [00:39<00:12,  2.04it/s]Capturing batches (bs=168 avail_mem=40.19 GB):  52%|    | 27/52 [00:39<00:12,  2.04it/s]Capturing batches (bs=168 avail_mem=40.19 GB):  54%|    | 28/52 [00:39<00:10,  2.20it/s]Capturing batches (bs=160 avail_mem=40.19 GB):  54%|    | 28/52 [00:39<00:10,  2.20it/s]Capturing batches (bs=160 avail_mem=40.19 GB):  56%|    | 29/52 [00:40<00:09,  2.33it/s]Capturing batches (bs=152 avail_mem=40.19 GB):  56%|    | 29/52 [00:40<00:09,  2.33it/s]Capturing batches (bs=152 avail_mem=40.19 GB):  58%|    | 30/52 [00:40<00:09,  2.43it/s]Capturing batches (bs=144 avail_mem=40.19 GB):  58%|    | 30/52 [00:40<00:09,  2.43it/s]Capturing batches (bs=144 avail_mem=40.19 GB):  60%|    | 31/52 [00:40<00:08,  2.50it/s]Capturing batches (bs=136 avail_mem=40.18 GB):  60%|    | 31/52 [00:40<00:08,  2.50it/s]Capturing batches (bs=136 avail_mem=40.18 GB):  62%|   | 32/52 [00:41<00:07,  2.56it/s]Capturing batches (bs=128 avail_mem=40.18 GB):  62%|   | 32/52 [00:41<00:07,  2.56it/s][aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:13 TP6] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:13 TP1] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:13 TP4] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:13 TP2] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:13 TP3] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:13 TP7] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:13 TP0] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:13 TP5] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=128 avail_mem=40.18 GB):  63%|   | 33/52 [00:43<00:16,  1.15it/s]Capturing batches (bs=120 avail_mem=40.17 GB):  63%|   | 33/52 [00:43<00:16,  1.15it/s]Capturing batches (bs=120 avail_mem=40.17 GB):  65%|   | 34/52 [00:45<00:22,  1.23s/it]Capturing batches (bs=112 avail_mem=40.16 GB):  65%|   | 34/52 [00:45<00:22,  1.23s/it]Capturing batches (bs=112 avail_mem=40.16 GB):  67%|   | 35/52 [00:45<00:16,  1.03it/s]Capturing batches (bs=104 avail_mem=40.16 GB):  67%|   | 35/52 [00:45<00:16,  1.03it/s]Capturing batches (bs=104 avail_mem=40.16 GB):  69%|   | 36/52 [00:46<00:12,  1.26it/s]Capturing batches (bs=96 avail_mem=40.15 GB):  69%|   | 36/52 [00:46<00:12,  1.26it/s] Capturing batches (bs=96 avail_mem=40.15 GB):  71%|   | 37/52 [00:46<00:09,  1.50it/s]Capturing batches (bs=88 avail_mem=40.15 GB):  71%|   | 37/52 [00:46<00:09,  1.50it/s]Capturing batches (bs=88 avail_mem=40.15 GB):  73%|  | 38/52 [00:46<00:08,  1.73it/s]Capturing batches (bs=80 avail_mem=40.15 GB):  73%|  | 38/52 [00:46<00:08,  1.73it/s]Capturing batches (bs=80 avail_mem=40.15 GB):  75%|  | 39/52 [00:47<00:06,  1.94it/s]Capturing batches (bs=72 avail_mem=40.15 GB):  75%|  | 39/52 [00:47<00:06,  1.94it/s]Capturing batches (bs=72 avail_mem=40.15 GB):  77%|  | 40/52 [00:47<00:05,  2.11it/s]Capturing batches (bs=64 avail_mem=40.14 GB):  77%|  | 40/52 [00:47<00:05,  2.11it/s][aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:17 TP0] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:17 TP3] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:17 TP5] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:17 TP2] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:17 TP6] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:17 TP1] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:17 TP7] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:17 TP4] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=64 avail_mem=40.14 GB):  79%|  | 41/52 [00:47<00:04,  2.24it/s]Capturing batches (bs=56 avail_mem=40.14 GB):  79%|  | 41/52 [00:47<00:04,  2.24it/s]Capturing batches (bs=56 avail_mem=40.14 GB):  81%|  | 42/52 [00:48<00:04,  2.36it/s]Capturing batches (bs=48 avail_mem=40.14 GB):  81%|  | 42/52 [00:48<00:04,  2.36it/s]Capturing batches (bs=48 avail_mem=40.14 GB):  83%| | 43/52 [00:48<00:03,  2.45it/s]Capturing batches (bs=40 avail_mem=40.14 GB):  83%| | 43/52 [00:48<00:03,  2.45it/s]Capturing batches (bs=40 avail_mem=40.14 GB):  85%| | 44/52 [00:49<00:03,  2.52it/s]Capturing batches (bs=32 avail_mem=40.13 GB):  85%| | 44/52 [00:49<00:03,  2.52it/s][aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:19 TP5] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:19 TP3] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:19 TP0] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:19 TP6] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:19 TP1] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:19 TP2] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:19 TP7] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:19 TP4] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=32 avail_mem=40.13 GB):  87%| | 45/52 [00:49<00:02,  2.57it/s]Capturing batches (bs=24 avail_mem=40.13 GB):  87%| | 45/52 [00:49<00:02,  2.57it/s]Capturing batches (bs=24 avail_mem=40.13 GB):  88%| | 46/52 [00:49<00:02,  2.62it/s]Capturing batches (bs=16 avail_mem=40.13 GB):  88%| | 46/52 [00:49<00:02,  2.62it/s][aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:20 TP0] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:20 TP1] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:20 TP5] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:20 TP2] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:20 TP3] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:20 TP7] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:20 TP6] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:03:20 TP4] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=16 avail_mem=40.13 GB):  90%| | 47/52 [00:50<00:01,  2.65it/s]Capturing batches (bs=12 avail_mem=40.13 GB):  90%| | 47/52 [00:50<00:01,  2.65it/s]Capturing batches (bs=12 avail_mem=40.13 GB):  92%|| 48/52 [00:50<00:01,  2.68it/s]Capturing batches (bs=8 avail_mem=40.13 GB):  92%|| 48/52 [00:50<00:01,  2.68it/s] Capturing batches (bs=8 avail_mem=40.13 GB):  94%|| 49/52 [00:50<00:01,  2.69it/s]Capturing batches (bs=4 avail_mem=40.12 GB):  94%|| 49/52 [00:50<00:01,  2.69it/s]Capturing batches (bs=4 avail_mem=40.12 GB):  96%|| 50/52 [00:51<00:00,  2.70it/s]Capturing batches (bs=2 avail_mem=40.12 GB):  96%|| 50/52 [00:51<00:00,  2.70it/s]Capturing batches (bs=2 avail_mem=40.12 GB):  98%|| 51/52 [00:51<00:00,  2.71it/s]Capturing batches (bs=1 avail_mem=40.12 GB):  98%|| 51/52 [00:51<00:00,  2.71it/s]Capturing batches (bs=1 avail_mem=40.12 GB): 100%|| 52/52 [00:53<00:00,  1.09it/s]Capturing batches (bs=1 avail_mem=40.12 GB): 100%|| 52/52 [00:53<00:00,  1.03s/it]
[aiter] Registering 6396 cuda graph addresses
[2025-12-16 10:03:24 TP7] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-16 10:03:24 TP3] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-16 10:03:24 TP1] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-16 10:03:24 TP2] Registering 6396 cuda graph addresses
[2025-12-16 10:03:24 TP5] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-16 10:03:24 TP4] Registering 6396 cuda graph addresses
[2025-12-16 10:03:24 TP0] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-16 10:03:24 TP6] Registering 6396 cuda graph addresses
[2025-12-16 10:03:24 TP3] Capture cuda graph end. Time elapsed: 54.46 s. mem usage=3.46 GB. avail mem=39.70 GB.
[2025-12-16 10:03:24 TP7] Capture cuda graph end. Time elapsed: 56.26 s. mem usage=3.46 GB. avail mem=39.82 GB.
[2025-12-16 10:03:24 TP6] Capture cuda graph end. Time elapsed: 55.48 s. mem usage=3.46 GB. avail mem=39.81 GB.
[2025-12-16 10:03:24 TP2] Capture cuda graph end. Time elapsed: 55.46 s. mem usage=3.46 GB. avail mem=39.68 GB.
[2025-12-16 10:03:24 TP1] Capture cuda graph end. Time elapsed: 55.71 s. mem usage=3.46 GB. avail mem=39.69 GB.
[2025-12-16 10:03:24 TP0] Capture cuda graph end. Time elapsed: 54.49 s. mem usage=3.46 GB. avail mem=40.11 GB.
[2025-12-16 10:03:24 TP4] Capture cuda graph end. Time elapsed: 54.47 s. mem usage=3.46 GB. avail mem=39.74 GB.
[2025-12-16 10:03:24 TP5] Capture cuda graph end. Time elapsed: 55.66 s. mem usage=3.46 GB. avail mem=39.83 GB.
[2025-12-16 10:03:24 TP0] max_total_num_tokens=981350, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=40.11 GB
[2025-12-16 10:03:25] INFO:     Started server process [53]
[2025-12-16 10:03:25] INFO:     Waiting for application startup.
[2025-12-16 10:03:25] INFO:     Application startup complete.
[2025-12-16 10:03:25] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-12-16 10:03:26] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-16 10:03:26] INFO:     127.0.0.1:51068 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-12-16 10:03:26 TP0] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:03:27 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:03:27 TP2] start build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:03:27 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:03:27 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:03:27 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:03:27 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:03:27 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:03:27 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:03:29] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-16 10:03:29] INFO:     127.0.0.1:51078 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-12-16 10:03:34] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-16 10:03:34] INFO:     127.0.0.1:53910 - "GET /get_model_info HTTP/1.1" 200 OK
[aiter] [32mfinish build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale], cost 42.7s [0m
[2025-12-16 10:04:09 TP2] [32mfinish build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale], cost 42.7s [0m
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:04:09 TP2] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:04:10 TP1] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:04:10 TP0] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:04:10 TP3] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:04:10 TP6] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:04:10 TP5] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:04:10 TP7] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:04:10 TP4] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:04:12 TP0] Prefill batch, #new-seq: 1, #new-token: 667, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:04:14 TP7] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:04:14 TP3] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:04:14 TP1] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:04:14 TP5] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:04:14 TP6] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:04:14 TP2] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:04:14 TP0] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:04:14 TP4] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-16 10:04:16] INFO:     127.0.0.1:53916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:17 TP0] Prefill batch, #new-seq: 265, #new-token: 16362, #cached-token: 176755, token usage: 0.00, #running-req: 1, #queue-req: 79, 
[2025-12-16 10:04:21 TP0] Prefill batch, #new-seq: 264, #new-token: 16373, #cached-token: 176088, token usage: 0.02, #running-req: 266, #queue-req: 790, 
[2025-12-16 10:04:22 TP0] Prefill batch, #new-seq: 277, #new-token: 16373, #cached-token: 185489, token usage: 0.03, #running-req: 530, #queue-req: 513, 
[2025-12-16 10:04:25 TP0] Prefill batch, #new-seq: 217, #new-token: 13369, #cached-token: 145350, token usage: 0.05, #running-req: 807, #queue-req: 296, 
[2025-12-16 10:04:32] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:32] The server is fired up and ready to roll!
[2025-12-16 10:04:35 TP0] Prefill batch, #new-seq: 1, #new-token: 63, #cached-token: 671, token usage: 0.07, #running-req: 1023, #queue-req: 295, 
[2025-12-16 10:04:39] INFO:     127.0.0.1:41664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:39] INFO:     127.0.0.1:33580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:39 TP0] Prefill batch, #new-seq: 1, #new-token: 121, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 294, 
[2025-12-16 10:04:40 TP0] Prefill batch, #new-seq: 1, #new-token: 40, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-12-16 10:04:40] INFO:     127.0.0.1:36076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:40 TP0] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 669, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-12-16 10:04:41 TP0] Decode batch, #running-req: 1024, #token: 101568, token usage: 0.10, cuda graph: False, gen throughput (token/s): 507.78, #queue-req: 292, 
[2025-12-16 10:04:41] INFO:     127.0.0.1:36826 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:41] INFO:     127.0.0.1:37788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:41] INFO:     127.0.0.1:33266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:41] INFO:     127.0.0.1:33914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:41] INFO:     127.0.0.1:34362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:41] INFO:     127.0.0.1:37802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:41] INFO:     127.0.0.1:41656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:41 TP0] Prefill batch, #new-seq: 2, #new-token: 134, #cached-token: 1340, token usage: 0.11, #running-req: 1022, #queue-req: 290, 
[2025-12-16 10:04:42] INFO:     127.0.0.1:34330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:42] INFO:     127.0.0.1:35578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:42] INFO:     127.0.0.1:35934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:42] INFO:     127.0.0.1:38920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:42] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:42 TP0] Prefill batch, #new-seq: 10, #new-token: 733, #cached-token: 6701, token usage: 0.11, #running-req: 1014, #queue-req: 280, 
[2025-12-16 10:04:43] INFO:     127.0.0.1:34142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:43] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:43] INFO:     127.0.0.1:38082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:43] INFO:     127.0.0.1:38942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:43 TP0] Prefill batch, #new-seq: 4, #new-token: 239, #cached-token: 2682, token usage: 0.11, #running-req: 1020, #queue-req: 276, 
[2025-12-16 10:04:43] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:43] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:43] INFO:     127.0.0.1:33970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:43] INFO:     127.0.0.1:35212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:43] INFO:     127.0.0.1:37500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:43] INFO:     127.0.0.1:37600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:43] INFO:     127.0.0.1:38092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:43] INFO:     127.0.0.1:38160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:44 TP0] Prefill batch, #new-seq: 8, #new-token: 416, #cached-token: 5364, token usage: 0.11, #running-req: 1016, #queue-req: 268, 
[2025-12-16 10:04:44] INFO:     127.0.0.1:34840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:44] INFO:     127.0.0.1:38294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:44] INFO:     127.0.0.1:41634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:44] INFO:     127.0.0.1:41784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:44] INFO:     127.0.0.1:41926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:45 TP0] Prefill batch, #new-seq: 5, #new-token: 290, #cached-token: 3348, token usage: 0.11, #running-req: 1019, #queue-req: 263, 
[2025-12-16 10:04:47] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:47] INFO:     127.0.0.1:35342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:47] INFO:     127.0.0.1:36380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:47] INFO:     127.0.0.1:36982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:47] INFO:     127.0.0.1:38462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:47] INFO:     127.0.0.1:40954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:47] INFO:     127.0.0.1:41148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:47 TP0] Prefill batch, #new-seq: 7, #new-token: 361, #cached-token: 4687, token usage: 0.11, #running-req: 1017, #queue-req: 256, 
[2025-12-16 10:04:48] INFO:     127.0.0.1:39184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48] INFO:     127.0.0.1:41354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48 TP0] Prefill batch, #new-seq: 2, #new-token: 108, #cached-token: 1342, token usage: 0.11, #running-req: 1022, #queue-req: 254, 
[2025-12-16 10:04:48] INFO:     127.0.0.1:33604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48] INFO:     127.0.0.1:33756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48] INFO:     127.0.0.1:36612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48] INFO:     127.0.0.1:36790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48] INFO:     127.0.0.1:37664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48] INFO:     127.0.0.1:38076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48] INFO:     127.0.0.1:39080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48] INFO:     127.0.0.1:39986 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48] INFO:     127.0.0.1:40452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48] INFO:     127.0.0.1:40756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48] INFO:     127.0.0.1:40990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48] INFO:     127.0.0.1:41268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48] INFO:     127.0.0.1:41884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:48 TP0] Prefill batch, #new-seq: 13, #new-token: 882, #cached-token: 8709, token usage: 0.11, #running-req: 1011, #queue-req: 241, 
[2025-12-16 10:04:50] INFO:     127.0.0.1:33248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:50] INFO:     127.0.0.1:35282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:50] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:50] INFO:     127.0.0.1:38402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:50] INFO:     127.0.0.1:39460 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:50] INFO:     127.0.0.1:40548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:50] INFO:     127.0.0.1:40862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:50] INFO:     127.0.0.1:41108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:50] INFO:     127.0.0.1:42298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:50 TP0] Prefill batch, #new-seq: 9, #new-token: 533, #cached-token: 6030, token usage: 0.11, #running-req: 1015, #queue-req: 232, 
[2025-12-16 10:04:53] INFO:     127.0.0.1:33774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:53] INFO:     127.0.0.1:34394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:53] INFO:     127.0.0.1:35174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:53] INFO:     127.0.0.1:36470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:53] INFO:     127.0.0.1:40830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:53 TP0] Prefill batch, #new-seq: 5, #new-token: 318, #cached-token: 3353, token usage: 0.11, #running-req: 1019, #queue-req: 227, 
[2025-12-16 10:04:53] INFO:     127.0.0.1:34062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:53] INFO:     127.0.0.1:34410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:53] INFO:     127.0.0.1:37114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:53] INFO:     127.0.0.1:37720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:53] INFO:     127.0.0.1:37964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:53] INFO:     127.0.0.1:38186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:53] INFO:     127.0.0.1:38842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:53] INFO:     127.0.0.1:42006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:53 TP0] Prefill batch, #new-seq: 8, #new-token: 520, #cached-token: 5360, token usage: 0.11, #running-req: 1016, #queue-req: 219, 
[2025-12-16 10:04:54] INFO:     127.0.0.1:35778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:54] INFO:     127.0.0.1:37372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:54] INFO:     127.0.0.1:37538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:54] INFO:     127.0.0.1:38700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:54] INFO:     127.0.0.1:39414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:54] INFO:     127.0.0.1:41044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:54] INFO:     127.0.0.1:41158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:54] INFO:     127.0.0.1:42344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:54 TP0] Prefill batch, #new-seq: 8, #new-token: 410, #cached-token: 5356, token usage: 0.11, #running-req: 1016, #queue-req: 211, 
[2025-12-16 10:04:56] INFO:     127.0.0.1:34272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:56] INFO:     127.0.0.1:34538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:56] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:56] INFO:     127.0.0.1:39072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:56] INFO:     127.0.0.1:39598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:56] INFO:     127.0.0.1:41116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:56 TP0] Prefill batch, #new-seq: 6, #new-token: 284, #cached-token: 4018, token usage: 0.12, #running-req: 1018, #queue-req: 205, 
[2025-12-16 10:04:57] INFO:     127.0.0.1:33626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57] INFO:     127.0.0.1:34230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57] INFO:     127.0.0.1:35106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57] INFO:     127.0.0.1:35150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57] INFO:     127.0.0.1:36480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57] INFO:     127.0.0.1:37318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57] INFO:     127.0.0.1:39010 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57] INFO:     127.0.0.1:41574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57 TP0] Prefill batch, #new-seq: 8, #new-token: 506, #cached-token: 5359, token usage: 0.12, #running-req: 1016, #queue-req: 197, 
[2025-12-16 10:04:57] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57] INFO:     127.0.0.1:34892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57] INFO:     127.0.0.1:37564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57] INFO:     127.0.0.1:38480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57] INFO:     127.0.0.1:39314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57] INFO:     127.0.0.1:39514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57] INFO:     127.0.0.1:39972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:57 TP0] Prefill batch, #new-seq: 7, #new-token: 398, #cached-token: 4689, token usage: 0.12, #running-req: 1017, #queue-req: 190, 
[2025-12-16 10:04:57 TP0] Prefill batch, #new-seq: 9, #new-token: 521, #cached-token: 6028, token usage: 0.12, #running-req: 1015, #queue-req: 181, 
[2025-12-16 10:04:58] INFO:     127.0.0.1:40144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:41404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:41988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:34106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:34920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:34966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:35030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:36558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:39664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:34194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:34638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:35538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:35786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:35898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:38768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:39658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:41004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58 TP0] Prefill batch, #new-seq: 9, #new-token: 494, #cached-token: 6030, token usage: 0.12, #running-req: 1015, #queue-req: 172, 
[2025-12-16 10:04:58] INFO:     127.0.0.1:34156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:34696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:35242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:35402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:35464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:37650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:39138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58] INFO:     127.0.0.1:41216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:58 TP0] Prefill batch, #new-seq: 10, #new-token: 479, #cached-token: 6699, token usage: 0.12, #running-req: 1014, #queue-req: 162, 
[2025-12-16 10:04:59] INFO:     127.0.0.1:33728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:35354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:39068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:39432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:40312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:40472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:41276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:41770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59 TP0] Prefill batch, #new-seq: 8, #new-token: 439, #cached-token: 5358, token usage: 0.12, #running-req: 1016, #queue-req: 154, 
[2025-12-16 10:04:59] INFO:     127.0.0.1:36658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:37638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:37706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:38034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:39526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:39786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:39830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59] INFO:     127.0.0.1:42172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:04:59 TP0] Prefill batch, #new-seq: 8, #new-token: 499, #cached-token: 5360, token usage: 0.12, #running-req: 1016, #queue-req: 146, 
[2025-12-16 10:05:00] INFO:     127.0.0.1:34992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:35450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:37586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:38230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:39266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:39908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:40290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:40580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:41122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:41350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00 TP0] Prefill batch, #new-seq: 10, #new-token: 723, #cached-token: 6704, token usage: 0.12, #running-req: 1014, #queue-req: 136, 
[2025-12-16 10:05:00] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:33704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:34192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:34710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:35350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:36038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:37480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:38244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:38792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:38844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:39292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:40554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00 TP0] Prefill batch, #new-seq: 12, #new-token: 655, #cached-token: 8037, token usage: 0.12, #running-req: 1012, #queue-req: 124, 
[2025-12-16 10:05:00] INFO:     127.0.0.1:34972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:38544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:40078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:40130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:40654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:00] INFO:     127.0.0.1:40768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01 TP0] Prefill batch, #new-seq: 6, #new-token: 479, #cached-token: 4020, token usage: 0.12, #running-req: 1018, #queue-req: 118, 
[2025-12-16 10:05:01] INFO:     127.0.0.1:34940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:35988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:36620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:39194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:40232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:41194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:41460 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01 TP0] Prefill batch, #new-seq: 9, #new-token: 513, #cached-token: 6032, token usage: 0.12, #running-req: 1015, #queue-req: 109, 
[2025-12-16 10:05:01] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:34606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:34816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:35906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:36056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:36184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:37232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:37350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:41076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:01] INFO:     127.0.0.1:41202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02 TP0] Prefill batch, #new-seq: 11, #new-token: 830, #cached-token: 7372, token usage: 0.12, #running-req: 1013, #queue-req: 98, 
[2025-12-16 10:05:02] INFO:     127.0.0.1:33714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:33720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:33930 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:34430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:34784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:35294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:36034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:36168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:36534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:36924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:37760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:37876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:37914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:38958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:40438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:41458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02] INFO:     127.0.0.1:42218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:02 TP0] Prefill batch, #new-seq: 19, #new-token: 1017, #cached-token: 12732, token usage: 0.12, #running-req: 1005, #queue-req: 79, 
[2025-12-16 10:05:03] INFO:     127.0.0.1:34632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:03] INFO:     127.0.0.1:35208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:03] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:03] INFO:     127.0.0.1:38404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:03] INFO:     127.0.0.1:38656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:03] INFO:     127.0.0.1:38732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:03 TP0] Prefill batch, #new-seq: 6, #new-token: 365, #cached-token: 4018, token usage: 0.12, #running-req: 1018, #queue-req: 73, 
[2025-12-16 10:05:04] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:33582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:34666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:35830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:36206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:38158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:38910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:39004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:39486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:39982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:40076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:41180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:41326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04 TP0] Prefill batch, #new-seq: 14, #new-token: 957, #cached-token: 9375, token usage: 0.12, #running-req: 1010, #queue-req: 59, 
[2025-12-16 10:05:04] INFO:     127.0.0.1:33522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:34026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:34182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:36574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:36694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:37576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:38302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:38820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:38914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:38934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:39132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:39154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:39698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:40040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:40196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:40328 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:40996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:41032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:41060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:42124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:04] INFO:     127.0.0.1:42234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:05 TP0] Prefill batch, #new-seq: 22, #new-token: 1415, #cached-token: 14736, token usage: 0.12, #running-req: 1002, #queue-req: 37, 
[2025-12-16 10:05:07] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:07] INFO:     127.0.0.1:34322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:07] INFO:     127.0.0.1:34986 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:07] INFO:     127.0.0.1:35630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:07] INFO:     127.0.0.1:37076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:07] INFO:     127.0.0.1:37626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:07] INFO:     127.0.0.1:38112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:07] INFO:     127.0.0.1:38838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:07] INFO:     127.0.0.1:39026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:07] INFO:     127.0.0.1:39924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:07] INFO:     127.0.0.1:40578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:07] INFO:     127.0.0.1:41240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:07] INFO:     127.0.0.1:41578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:07 TP0] Prefill batch, #new-seq: 13, #new-token: 761, #cached-token: 8715, token usage: 0.12, #running-req: 1011, #queue-req: 24, 
[2025-12-16 10:05:08] INFO:     127.0.0.1:35484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:36502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:36920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:38580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:40226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:40524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:41734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08 TP0] Prefill batch, #new-seq: 7, #new-token: 457, #cached-token: 4688, token usage: 0.12, #running-req: 1017, #queue-req: 17, 
[2025-12-16 10:05:08] INFO:     127.0.0.1:33794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:33842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:33946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:34446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:35544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:35868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:36308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:37136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:38704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:39188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:39864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:40044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:40426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08] INFO:     127.0.0.1:42190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:08 TP0] Prefill batch, #new-seq: 15, #new-token: 903, #cached-token: 10049, token usage: 0.12, #running-req: 1009, #queue-req: 2, 
[2025-12-16 10:05:09] INFO:     127.0.0.1:33996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:09] INFO:     127.0.0.1:35042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:09] INFO:     127.0.0.1:35488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:09] INFO:     127.0.0.1:36966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:09] INFO:     127.0.0.1:39342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:09] INFO:     127.0.0.1:39916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:09] INFO:     127.0.0.1:41428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:09] INFO:     127.0.0.1:42110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:09 TP0] Prefill batch, #new-seq: 2, #new-token: 81, #cached-token: 1343, token usage: 0.12, #running-req: 1016, #queue-req: 0, 
[2025-12-16 10:05:10] INFO:     127.0.0.1:33566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:34520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:34688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:36646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:36678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:37388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:39280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:40302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:41392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:41966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:42280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:33460 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:33796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:34740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:36402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:39006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:39802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:41898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:34716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:34830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:35114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:35426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:37492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:37604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:38434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:39770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:41088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:41292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:35608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:35882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:36420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:37510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:38152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:39012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:39224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:39804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:40586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:33500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:34290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:34488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:35082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:36594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:37068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:37444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:37888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:40946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:41176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:10] INFO:     127.0.0.1:41618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11 TP0] Decode batch, #running-req: 980, #token: 119864, token usage: 0.12, cuda graph: False, gen throughput (token/s): 1373.21, #queue-req: 0, 
[2025-12-16 10:05:11] INFO:     127.0.0.1:33806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:34564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:36328 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:37812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:38200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:38226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:38278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:38334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:39848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:39944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:41078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:41330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:33662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:35130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:35714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:38370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:38488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:38532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:40400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:40802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:41368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:41910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:42320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:33414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:33560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:34012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:35688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:36312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:37064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:37924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:38106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:38440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:41274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:42314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:34148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:38364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:39614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:41546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:41550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:33838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:37242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:38806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:40350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:40740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:40910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:33820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:33982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:34618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:35532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:37098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:38012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:39296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:39376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:39542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:40170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:40718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:42178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:33802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:34050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:35370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:36728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:36812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:37468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:38798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:39234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:39498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:40280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:11] INFO:     127.0.0.1:40568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:33684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:35068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:36218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:37148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:37282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:39066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:40356 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:40682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:41140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:41584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:42428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:33520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:35184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:35638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:36072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:36614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:37088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:38396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:38554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:38664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:40882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:41560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:41940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:42136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:33676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:34368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:34678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:37458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:40082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:41370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:33744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:34146 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:36436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:38266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:38318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:38642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:39734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:40018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:40096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:41212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:41246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:41310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:41538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:33302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:33622 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:39602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:41028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:41424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:41502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:42128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:35326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:35984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:40060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:12] INFO:     127.0.0.1:41972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:33466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:33778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:34034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:34120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:34316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:34352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:35328 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:36446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:38720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:39850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:35728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:37418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:37952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:37990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:39052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:39684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:40140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:41674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:42388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:42848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:34246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:37164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:37772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:38992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:40980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:43076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:33290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:34204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:36666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:36994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:37324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:37736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:14] INFO:     127.0.0.1:38256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:34096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:34474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:36276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:36366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:37004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:37134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:37218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:37742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:40704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:42470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:35094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:36630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:36780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:37448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:38556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:39168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:39618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:39632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:40638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:41698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:41918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:34084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:35024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:35420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:37028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:37522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:37870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:37878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:38962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:40690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:41960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:42942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:33886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:35974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:36114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:36190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:38898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:39642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:43494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:33920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:36744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:37854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:37976 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:38182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:38570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:40720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:41862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:41876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:42200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:42246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:43442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:35318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:38026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:38518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:38740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:39082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:39406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:40216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:15] INFO:     127.0.0.1:40604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:35796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:37518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:38478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:41718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:42306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:33362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:34338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:35752 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:35766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:36394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:36990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:37022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:37370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:38872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:39798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:39846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:39974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:40962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:42068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:42998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:33386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:33666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:36054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:39978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:41298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:41714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:42542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:43162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:43216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:35440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:36544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:36712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:37040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:37552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:38494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:39116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:39458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:39822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:42704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:34956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:36866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:36910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:38348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:39482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:39530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:39874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:40760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:41688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:42156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:44542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:33278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:36900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:38694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:43192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:43634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:44160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:33426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:34098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:34262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:34768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:35940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:37822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:37922 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:38130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:40496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:41388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:41830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:42256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:43428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:43798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:36300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:38452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:39356 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:39906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:41452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:16] INFO:     127.0.0.1:42038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:34058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:34654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:35276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:37516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:37616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:38870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:39210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:41596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:42094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:43200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:33690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:34300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:35366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:35814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:36348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:37310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:37450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:38616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:39174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:40894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:41134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:41474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:41760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:42442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:44676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:33592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:34166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:34528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:35512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:35672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:37390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:37906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:39078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:33642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:34506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:36050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:37054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:38588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:39448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:40026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:40086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:41070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:43978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:33364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:35254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:35818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:37270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:37830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:39762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:40208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:41150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:42654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:43126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:33872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:34916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:35444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:35930 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:36240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:37062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:37204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:37782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:37898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:40142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:40198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:41258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:42084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:33392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:33688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:35154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:36578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:36734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:38174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:40730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:42054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:17] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:18] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:18] INFO:     127.0.0.1:36416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:18] INFO:     127.0.0.1:36448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:18] INFO:     127.0.0.1:37256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:18] INFO:     127.0.0.1:37404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:18] INFO:     127.0.0.1:39782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:18] INFO:     127.0.0.1:40670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:18] INFO:     127.0.0.1:40938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:18] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:18] INFO:     127.0.0.1:42690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:18] INFO:     127.0.0.1:42932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:19] INFO:     127.0.0.1:33532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:19] INFO:     127.0.0.1:34766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:19] INFO:     127.0.0.1:35690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:19] INFO:     127.0.0.1:37962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:19] INFO:     127.0.0.1:38744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:19] INFO:     127.0.0.1:40342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:19] INFO:     127.0.0.1:42118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:19] INFO:     127.0.0.1:42514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:19] INFO:     127.0.0.1:42628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:19] INFO:     127.0.0.1:42758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:19] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:19] INFO:     127.0.0.1:44178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20 TP0] Decode batch, #running-req: 560, #token: 86284, token usage: 0.09, cuda graph: False, gen throughput (token/s): 3391.64, #queue-req: 0, 
[2025-12-16 10:05:20] INFO:     127.0.0.1:33408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:38216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:38780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:39232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:41802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:44836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:35958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:36098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:38232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:39038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:33766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:34864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:36314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:36606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:37362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:39252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:41342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:41846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43100 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:36014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:37322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:39404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:39560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:39584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:34928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:35344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:35616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:36828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:41380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:44536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:36832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:37758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:41792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:44364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:33486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:35078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:35658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:37294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:39222 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:39476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:41444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:42664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:44224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:33718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:39306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:41512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:42744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:37334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:39890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:35812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:38604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:34850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:40898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:42530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:43292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:20] INFO:     127.0.0.1:44556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:35056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:38210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:39008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:42272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:42936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:42962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:33252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:33726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:38070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:42150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:40370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:40786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:42676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:34620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:36758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:41892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:34754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:36516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:38004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:38036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:39868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:40780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:36846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:38128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:38764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:39374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:42216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:42718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44100 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:35474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:35504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:36858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:38978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:42482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:34724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:39934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:41816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:34252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:35854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:37124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:38858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:39326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:41020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:42382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:35990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:36888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:38382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:38476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:39772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:39994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:42058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:36796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:42612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:44406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:35110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:40624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:21] INFO:     127.0.0.1:43402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:35008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:35576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:35982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:38924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:39380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:41188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:41282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:42626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:42928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:34704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:37416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:33554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:36962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:38676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:41646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:35408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:35740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:40168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:42494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:40942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:42412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:35072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:35386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:36252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:38056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:38146 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:41488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:42380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:42900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:33624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:34576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:35700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:38540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:40816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:33330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:33880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:35182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:35642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:37844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:38832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:36268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:36532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:41746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:34002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:41654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:36938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:39312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:39928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:42554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:42692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:33420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:35052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:42740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:42790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:42864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:39428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:38134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:41952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:43936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:22] INFO:     127.0.0.1:44752 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:35228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:35252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:37156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:40456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:33852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:36438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:39726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:43836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23 TP0] Decode batch, #running-req: 253, #token: 49246, token usage: 0.05, cuda graph: True, gen throughput (token/s): 5037.22, #queue-req: 0, 
[2025-12-16 10:05:23] INFO:     127.0.0.1:42454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:34460 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:35722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:40414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:43964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:33506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:37336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:38628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:42370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:36764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:38446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:42090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:43518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:43664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:43786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:38050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:38850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:42734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:35574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:38678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:37176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:43352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:37692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:39042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:41610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:36632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:40008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:43314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:43386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:35822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:42394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:43730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:35418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:38956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:40378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:42770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:43672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:43712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:35564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:38516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:39750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:42368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:42598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:42742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:42774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:33868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:37018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:43412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:23] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:36454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:36882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:37100 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:40394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:33614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:33648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:38750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:35290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:41944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:42024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:33898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:42288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:38416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:43026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:43306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:43390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:43908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:41064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:43814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:43896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44622 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:35136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:39106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:34884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:42294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:43012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:43330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:35166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:37428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:40644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:43598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:42800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:41106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:42880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:43062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:43574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:36124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:42832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:42582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:41466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:33986 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:34552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:44332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:34414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:24] INFO:     127.0.0.1:43862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:34314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:34908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:35838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:38970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:43882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:38432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:44194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:40202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:43688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:42396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:42438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:39090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:39360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:42022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:36392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:44136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25 TP0] Decode batch, #running-req: 97, #token: 22971, token usage: 0.02, cuda graph: True, gen throughput (token/s): 2982.81, #queue-req: 0, 
[2025-12-16 10:05:25] INFO:     127.0.0.1:35892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:37178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:41528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:38528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:41224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:38202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:42500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:42746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:44612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:34588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:44060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:37936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:44048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:40252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:43136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:43298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:44732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:42904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:36160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:37192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:33380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:44242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:43168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:43604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:44292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:25] INFO:     127.0.0.1:34144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:39960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:42638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:44348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:38504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:44880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:36006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:40484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:44288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:40348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:44400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:34566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:43222 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:43350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:44740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:41434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:37680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:43998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:44656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:41992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:43720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:44904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:43776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:40616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:43952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:26] INFO:     127.0.0.1:43924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27 TP0] Decode batch, #running-req: 35, #token: 10900, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1387.52, #queue-req: 0, 
[2025-12-16 10:05:27] INFO:     127.0.0.1:39672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:44320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:34800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:38650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:44172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:44784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:42240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:43854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:43244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:44770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:43622 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:42564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:44810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:36848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:27] INFO:     127.0.0.1:34376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:35980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:42358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:42978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:43110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:42892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:40158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:44404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:44474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:43046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:43606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28 TP0] Decode batch, #running-req: 10, #token: 3989, token usage: 0.00, cuda graph: True, gen throughput (token/s): 681.57, #queue-req: 0, 
[2025-12-16 10:05:28] INFO:     127.0.0.1:40502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:43906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:42958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:42332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:42330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:43700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:28] INFO:     127.0.0.1:39568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:29] INFO:     127.0.0.1:43290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:29 TP0] Decode batch, #running-req: 3, #token: 1062, token usage: 0.00, cuda graph: True, gen throughput (token/s): 183.19, #queue-req: 0, 
[2025-12-16 10:05:29] INFO:     127.0.0.1:43774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:30 TP0] Decode batch, #running-req: 1, #token: 1102, token usage: 0.00, cuda graph: True, gen throughput (token/s): 42.56, #queue-req: 0, 
[2025-12-16 10:05:31 TP0] Decode batch, #running-req: 1, #token: 1142, token usage: 0.00, cuda graph: True, gen throughput (token/s): 41.54, #queue-req: 0, 
[2025-12-16 10:05:32 TP0] Decode batch, #running-req: 1, #token: 1182, token usage: 0.00, cuda graph: True, gen throughput (token/s): 41.52, #queue-req: 0, 
[2025-12-16 10:05:33] INFO:     127.0.0.1:41408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:38] INFO:     127.0.0.1:59112 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-16 10:05:51] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:51 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:05:53 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1.86, #queue-req: 0, 
[2025-12-16 10:05:55] INFO:     127.0.0.1:41514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:05:55] INFO:     127.0.0.1:41520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:41994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:42002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:42008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:42022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:42026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:42034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:42042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:42044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:42048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:42056 - "POST /generate HTTP/1.1" 200 OK
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:05:55 TP6] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:05:55 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:05:55 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:05:55 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:05:55 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:05:55 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:05:55 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:05:55 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-16 10:05:55] INFO:     127.0.0.1:42066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:42068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:42084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:55] INFO:     127.0.0.1:42086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:05:56] INFO:     127.0.0.1:42518 - "POST /generate HTTP/1.1" 200 OK
[aiter] [32mfinish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale], cost 43.8s [0m
[2025-12-16 10:06:39 TP6] [32mfinish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale], cost 43.8s [0m
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:06:39 TP6] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:06:39 TP0] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:06:39 TP5] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:06:39 TP2] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:06:39 TP4] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:06:39 TP1] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:06:39 TP3] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:06:39 TP7] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-16 10:06:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 122, 
[2025-12-16 10:06:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.02, #running-req: 6, #queue-req: 117, 
[2025-12-16 10:06:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-12-16 10:06:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-12-16 10:06:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-12-16 10:06:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.08, #running-req: 26, #queue-req: 97, 
[2025-12-16 10:06:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-12-16 10:06:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-12-16 10:06:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-12-16 10:06:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-12-16 10:06:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-12-16 10:06:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-12-16 10:06:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.20, #running-req: 61, #queue-req: 62, 
[2025-12-16 10:06:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.22, #running-req: 66, #queue-req: 57, 
[2025-12-16 10:06:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.23, #running-req: 71, #queue-req: 52, 
[2025-12-16 10:06:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.25, #running-req: 76, #queue-req: 47, 
[2025-12-16 10:06:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.26, #running-req: 81, #queue-req: 42, 
[2025-12-16 10:06:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.28, #running-req: 86, #queue-req: 37, 
[2025-12-16 10:06:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.30, #running-req: 91, #queue-req: 32, 
[2025-12-16 10:06:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.31, #running-req: 96, #queue-req: 27, 
[2025-12-16 10:06:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.33, #running-req: 101, #queue-req: 22, 
[2025-12-16 10:06:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.35, #running-req: 106, #queue-req: 17, 
[2025-12-16 10:07:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.36, #running-req: 111, #queue-req: 12, 
[2025-12-16 10:07:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.38, #running-req: 116, #queue-req: 7, 
[2025-12-16 10:07:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.39, #running-req: 121, #queue-req: 2, 
[2025-12-16 10:07:02 TP0] Prefill batch, #new-seq: 2, #new-token: 6394, #cached-token: 8, token usage: 0.41, #running-req: 126, #queue-req: 0, 
[2025-12-16 10:07:06 TP0] Decode batch, #running-req: 128, #token: 411423, token usage: 0.42, cuda graph: True, gen throughput (token/s): 26.72, #queue-req: 0, 
[2025-12-16 10:07:09 TP0] Decode batch, #running-req: 128, #token: 416543, token usage: 0.42, cuda graph: True, gen throughput (token/s): 1988.43, #queue-req: 0, 
[2025-12-16 10:07:11 TP0] Decode batch, #running-req: 128, #token: 421663, token usage: 0.43, cuda graph: True, gen throughput (token/s): 1966.27, #queue-req: 0, 
[2025-12-16 10:07:14 TP0] Decode batch, #running-req: 128, #token: 426783, token usage: 0.43, cuda graph: True, gen throughput (token/s): 1946.42, #queue-req: 0, 
[2025-12-16 10:07:17 TP0] Decode batch, #running-req: 128, #token: 431903, token usage: 0.44, cuda graph: True, gen throughput (token/s): 1930.01, #queue-req: 0, 
[2025-12-16 10:07:19 TP0] Decode batch, #running-req: 128, #token: 437023, token usage: 0.45, cuda graph: True, gen throughput (token/s): 1934.40, #queue-req: 0, 
[2025-12-16 10:07:22 TP0] Decode batch, #running-req: 128, #token: 442143, token usage: 0.45, cuda graph: True, gen throughput (token/s): 1924.85, #queue-req: 0, 
[2025-12-16 10:07:25 TP0] Decode batch, #running-req: 128, #token: 447263, token usage: 0.46, cuda graph: True, gen throughput (token/s): 1926.12, #queue-req: 0, 
[2025-12-16 10:07:27 TP0] Decode batch, #running-req: 128, #token: 452383, token usage: 0.46, cuda graph: True, gen throughput (token/s): 1918.01, #queue-req: 0, 
[2025-12-16 10:07:30 TP0] Decode batch, #running-req: 128, #token: 457503, token usage: 0.47, cuda graph: True, gen throughput (token/s): 1907.09, #queue-req: 0, 
[2025-12-16 10:07:33 TP0] Decode batch, #running-req: 128, #token: 462623, token usage: 0.47, cuda graph: True, gen throughput (token/s): 1892.49, #queue-req: 0, 
[2025-12-16 10:07:35 TP0] Decode batch, #running-req: 128, #token: 467743, token usage: 0.48, cuda graph: True, gen throughput (token/s): 1897.26, #queue-req: 0, 
[2025-12-16 10:07:38 TP0] Decode batch, #running-req: 128, #token: 472863, token usage: 0.48, cuda graph: True, gen throughput (token/s): 1891.64, #queue-req: 0, 
[2025-12-16 10:07:41 TP0] Decode batch, #running-req: 128, #token: 477983, token usage: 0.49, cuda graph: True, gen throughput (token/s): 1887.54, #queue-req: 0, 
[2025-12-16 10:07:43 TP0] Decode batch, #running-req: 128, #token: 483103, token usage: 0.49, cuda graph: True, gen throughput (token/s): 1884.04, #queue-req: 0, 
[2025-12-16 10:07:46 TP0] Decode batch, #running-req: 128, #token: 488223, token usage: 0.50, cuda graph: True, gen throughput (token/s): 1870.92, #queue-req: 0, 
[2025-12-16 10:07:49 TP0] Decode batch, #running-req: 128, #token: 493343, token usage: 0.50, cuda graph: True, gen throughput (token/s): 1861.18, #queue-req: 0, 
[2025-12-16 10:07:52 TP0] Decode batch, #running-req: 128, #token: 498463, token usage: 0.51, cuda graph: True, gen throughput (token/s): 1859.64, #queue-req: 0, 
[2025-12-16 10:07:54 TP0] Decode batch, #running-req: 128, #token: 503583, token usage: 0.51, cuda graph: True, gen throughput (token/s): 1857.63, #queue-req: 0, 
[2025-12-16 10:07:58 TP0] Decode batch, #running-req: 128, #token: 508703, token usage: 0.52, cuda graph: True, gen throughput (token/s): 1297.26, #queue-req: 0, 
[2025-12-16 10:08:02] INFO:     127.0.0.1:45820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02 TP0] Prefill batch, #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:08:02] INFO:     127.0.0.1:45836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:45988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:02] INFO:     127.0.0.1:46218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:03] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:08:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 122, 
[2025-12-16 10:08:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 6, #queue-req: 117, 
[2025-12-16 10:08:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-12-16 10:08:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-12-16 10:08:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-12-16 10:08:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.08, #running-req: 26, #queue-req: 97, 
[2025-12-16 10:08:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-12-16 10:08:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-12-16 10:08:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-12-16 10:08:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-12-16 10:08:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-12-16 10:08:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-12-16 10:08:22 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.20, #running-req: 61, #queue-req: 61, 
[2025-12-16 10:08:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.22, #running-req: 67, #queue-req: 56, 
[2025-12-16 10:08:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15977, #cached-token: 28, token usage: 0.23, #running-req: 72, #queue-req: 51, 
[2025-12-16 10:08:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.25, #running-req: 77, #queue-req: 46, 
[2025-12-16 10:08:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.26, #running-req: 82, #queue-req: 41, 
[2025-12-16 10:08:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.28, #running-req: 87, #queue-req: 36, 
[2025-12-16 10:08:31 TP0] Prefill batch, #new-seq: 6, #new-token: 15994, #cached-token: 3212, token usage: 0.30, #running-req: 92, #queue-req: 30, 
[2025-12-16 10:08:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.32, #running-req: 98, #queue-req: 25, 
[2025-12-16 10:08:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.33, #running-req: 103, #queue-req: 20, 
[2025-12-16 10:08:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.35, #running-req: 108, #queue-req: 15, 
[2025-12-16 10:08:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.37, #running-req: 113, #queue-req: 10, 
[2025-12-16 10:08:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.38, #running-req: 118, #queue-req: 5, 
[2025-12-16 10:08:48 TP0] Prefill batch, #new-seq: 5, #new-token: 12792, #cached-token: 3213, token usage: 0.40, #running-req: 123, #queue-req: 0, 
[2025-12-16 10:08:57 TP0] Decode batch, #running-req: 128, #token: 405021, token usage: 0.41, cuda graph: True, gen throughput (token/s): 88.09, #queue-req: 0, 
[2025-12-16 10:09:03 TP0] Decode batch, #running-req: 128, #token: 410141, token usage: 0.42, cuda graph: True, gen throughput (token/s): 859.44, #queue-req: 0, 
[2025-12-16 10:09:08 TP0] Decode batch, #running-req: 128, #token: 415261, token usage: 0.42, cuda graph: True, gen throughput (token/s): 877.32, #queue-req: 0, 
[2025-12-16 10:09:14 TP0] Decode batch, #running-req: 128, #token: 420381, token usage: 0.43, cuda graph: True, gen throughput (token/s): 872.82, #queue-req: 0, 
[2025-12-16 10:09:20 TP0] Decode batch, #running-req: 128, #token: 425501, token usage: 0.43, cuda graph: True, gen throughput (token/s): 872.59, #queue-req: 0, 
[2025-12-16 10:09:26 TP0] Decode batch, #running-req: 128, #token: 430621, token usage: 0.44, cuda graph: True, gen throughput (token/s): 863.78, #queue-req: 0, 
[2025-12-16 10:09:33 TP0] Decode batch, #running-req: 128, #token: 435741, token usage: 0.44, cuda graph: True, gen throughput (token/s): 703.61, #queue-req: 0, 
[2025-12-16 10:09:39 TP0] Decode batch, #running-req: 128, #token: 440861, token usage: 0.45, cuda graph: True, gen throughput (token/s): 860.99, #queue-req: 0, 
[2025-12-16 10:09:45 TP0] Decode batch, #running-req: 128, #token: 445981, token usage: 0.45, cuda graph: True, gen throughput (token/s): 863.37, #queue-req: 0, 
[2025-12-16 10:09:51 TP0] Decode batch, #running-req: 128, #token: 451101, token usage: 0.46, cuda graph: True, gen throughput (token/s): 863.23, #queue-req: 0, 
[2025-12-16 10:09:57 TP0] Decode batch, #running-req: 128, #token: 456221, token usage: 0.46, cuda graph: True, gen throughput (token/s): 859.55, #queue-req: 0, 
[2025-12-16 10:10:03 TP0] Decode batch, #running-req: 128, #token: 461341, token usage: 0.47, cuda graph: True, gen throughput (token/s): 850.93, #queue-req: 0, 
[2025-12-16 10:10:09 TP0] Decode batch, #running-req: 128, #token: 466461, token usage: 0.48, cuda graph: True, gen throughput (token/s): 850.91, #queue-req: 0, 
[2025-12-16 10:10:15 TP0] Decode batch, #running-req: 128, #token: 471581, token usage: 0.48, cuda graph: True, gen throughput (token/s): 850.72, #queue-req: 0, 
[2025-12-16 10:10:21 TP0] Decode batch, #running-req: 128, #token: 476701, token usage: 0.49, cuda graph: True, gen throughput (token/s): 850.79, #queue-req: 0, 
[2025-12-16 10:10:27 TP0] Decode batch, #running-req: 128, #token: 481821, token usage: 0.49, cuda graph: True, gen throughput (token/s): 850.83, #queue-req: 0, 
[2025-12-16 10:10:33 TP0] Decode batch, #running-req: 128, #token: 486941, token usage: 0.50, cuda graph: True, gen throughput (token/s): 847.49, #queue-req: 0, 
[2025-12-16 10:10:39 TP0] Decode batch, #running-req: 128, #token: 492061, token usage: 0.50, cuda graph: True, gen throughput (token/s): 839.28, #queue-req: 0, 
[2025-12-16 10:10:45 TP0] Decode batch, #running-req: 128, #token: 497181, token usage: 0.51, cuda graph: True, gen throughput (token/s): 839.02, #queue-req: 0, 
[2025-12-16 10:10:51 TP0] Decode batch, #running-req: 128, #token: 502301, token usage: 0.51, cuda graph: True, gen throughput (token/s): 839.25, #queue-req: 0, 
[2025-12-16 10:10:55] INFO:     127.0.0.1:36102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55 TP0] Prefill batch, #new-seq: 2, #new-token: 6396, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:10:55] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:55] INFO:     127.0.0.1:36396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.01, #running-req: 2, #queue-req: 29, 
[2025-12-16 10:10:56] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36922 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:36994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56] INFO:     127.0.0.1:37146 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:10:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.02, #running-req: 7, #queue-req: 116, 
[2025-12-16 10:10:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.04, #running-req: 12, #queue-req: 111, 
[2025-12-16 10:10:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.06, #running-req: 17, #queue-req: 106, 
[2025-12-16 10:11:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.07, #running-req: 22, #queue-req: 101, 
[2025-12-16 10:11:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.09, #running-req: 27, #queue-req: 96, 
[2025-12-16 10:11:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.10, #running-req: 32, #queue-req: 91, 
[2025-12-16 10:11:06 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.12, #running-req: 37, #queue-req: 85, 
[2025-12-16 10:11:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.14, #running-req: 43, #queue-req: 80, 
[2025-12-16 10:11:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.16, #running-req: 48, #queue-req: 75, 
[2025-12-16 10:11:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15978, #cached-token: 27, token usage: 0.17, #running-req: 53, #queue-req: 70, 
[2025-12-16 10:11:12 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.19, #running-req: 58, #queue-req: 64, 
[2025-12-16 10:11:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.21, #running-req: 64, #queue-req: 59, 
[2025-12-16 10:11:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.22, #running-req: 69, #queue-req: 54, 
[2025-12-16 10:11:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.24, #running-req: 74, #queue-req: 49, 
[2025-12-16 10:11:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.26, #running-req: 79, #queue-req: 44, 
[2025-12-16 10:11:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.27, #running-req: 84, #queue-req: 39, 
[2025-12-16 10:11:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.29, #running-req: 89, #queue-req: 34, 
[2025-12-16 10:11:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.31, #running-req: 94, #queue-req: 29, 
[2025-12-16 10:11:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.32, #running-req: 99, #queue-req: 24, 
[2025-12-16 10:11:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.34, #running-req: 104, #queue-req: 19, 
[2025-12-16 10:11:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.36, #running-req: 109, #queue-req: 14, 
[2025-12-16 10:11:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.37, #running-req: 114, #queue-req: 9, 
[2025-12-16 10:11:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.39, #running-req: 119, #queue-req: 4, 
[2025-12-16 10:11:31 TP0] Prefill batch, #new-seq: 4, #new-token: 12795, #cached-token: 9, token usage: 0.40, #running-req: 124, #queue-req: 0, 
[2025-12-16 10:11:36 TP0] Decode batch, #running-req: 128, #token: 411420, token usage: 0.42, cuda graph: True, gen throughput (token/s): 115.01, #queue-req: 0, 
[2025-12-16 10:11:42 TP0] Decode batch, #running-req: 128, #token: 416540, token usage: 0.42, cuda graph: True, gen throughput (token/s): 877.79, #queue-req: 0, 
[2025-12-16 10:11:48 TP0] Decode batch, #running-req: 128, #token: 421660, token usage: 0.43, cuda graph: True, gen throughput (token/s): 890.15, #queue-req: 0, 
[2025-12-16 10:11:53 TP0] Decode batch, #running-req: 128, #token: 426780, token usage: 0.43, cuda graph: True, gen throughput (token/s): 887.36, #queue-req: 0, 
[2025-12-16 10:11:59 TP0] Decode batch, #running-req: 128, #token: 431900, token usage: 0.44, cuda graph: True, gen throughput (token/s): 877.04, #queue-req: 0, 
[2025-12-16 10:12:05 TP0] Decode batch, #running-req: 128, #token: 437020, token usage: 0.45, cuda graph: True, gen throughput (token/s): 876.53, #queue-req: 0, 
[2025-12-16 10:12:11 TP0] Decode batch, #running-req: 128, #token: 442140, token usage: 0.45, cuda graph: True, gen throughput (token/s): 876.02, #queue-req: 0, 
[2025-12-16 10:12:17 TP0] Decode batch, #running-req: 128, #token: 447260, token usage: 0.46, cuda graph: True, gen throughput (token/s): 869.58, #queue-req: 0, 
[2025-12-16 10:12:23 TP0] Decode batch, #running-req: 128, #token: 452380, token usage: 0.46, cuda graph: True, gen throughput (token/s): 875.73, #queue-req: 0, 
[2025-12-16 10:12:28 TP0] Decode batch, #running-req: 128, #token: 457500, token usage: 0.47, cuda graph: True, gen throughput (token/s): 875.34, #queue-req: 0, 
[2025-12-16 10:12:34 TP0] Decode batch, #running-req: 128, #token: 462620, token usage: 0.47, cuda graph: True, gen throughput (token/s): 865.86, #queue-req: 0, 
[2025-12-16 10:12:40 TP0] Decode batch, #running-req: 128, #token: 467740, token usage: 0.48, cuda graph: True, gen throughput (token/s): 864.56, #queue-req: 0, 
[2025-12-16 10:12:46 TP0] Decode batch, #running-req: 128, #token: 472860, token usage: 0.48, cuda graph: True, gen throughput (token/s): 861.14, #queue-req: 0, 
[2025-12-16 10:12:52 TP0] Decode batch, #running-req: 128, #token: 477980, token usage: 0.49, cuda graph: True, gen throughput (token/s): 864.37, #queue-req: 0, 
[2025-12-16 10:12:58 TP0] Decode batch, #running-req: 128, #token: 483100, token usage: 0.49, cuda graph: True, gen throughput (token/s): 863.92, #queue-req: 0, 
[2025-12-16 10:13:04 TP0] Decode batch, #running-req: 128, #token: 488220, token usage: 0.50, cuda graph: True, gen throughput (token/s): 863.86, #queue-req: 0, 
[2025-12-16 10:13:10 TP0] Decode batch, #running-req: 128, #token: 493340, token usage: 0.50, cuda graph: True, gen throughput (token/s): 858.79, #queue-req: 0, 
[2025-12-16 10:13:16 TP0] Decode batch, #running-req: 128, #token: 498460, token usage: 0.51, cuda graph: True, gen throughput (token/s): 848.70, #queue-req: 0, 
[2025-12-16 10:13:22 TP0] Decode batch, #running-req: 128, #token: 503580, token usage: 0.51, cuda graph: True, gen throughput (token/s): 852.39, #queue-req: 0, 
[2025-12-16 10:13:28 TP0] Decode batch, #running-req: 128, #token: 508700, token usage: 0.52, cuda graph: True, gen throughput (token/s): 852.02, #queue-req: 0, 
[2025-12-16 10:13:32] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32 TP0] Prefill batch, #new-seq: 2, #new-token: 6394, #cached-token: 8, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:13:32] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.01, #running-req: 2, #queue-req: 34, 
[2025-12-16 10:13:32] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56950 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:56992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57010 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57328 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:32] INFO:     127.0.0.1:57422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:33] INFO:     127.0.0.1:57434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:33] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:13:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 7, #queue-req: 104, 
[2025-12-16 10:13:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 12, #queue-req: 99, 
[2025-12-16 10:13:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.06, #running-req: 17, #queue-req: 94, 
[2025-12-16 10:13:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.07, #running-req: 22, #queue-req: 89, 
[2025-12-16 10:13:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 27, #queue-req: 84, 
[2025-12-16 10:13:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.10, #running-req: 32, #queue-req: 79, 
[2025-12-16 10:13:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.12, #running-req: 37, #queue-req: 74, 
[2025-12-16 10:13:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.14, #running-req: 42, #queue-req: 69, 
[2025-12-16 10:13:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 47, #queue-req: 64, 
[2025-12-16 10:13:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.17, #running-req: 52, #queue-req: 59, 
[2025-12-16 10:13:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.19, #running-req: 57, #queue-req: 54, 
[2025-12-16 10:13:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.20, #running-req: 62, #queue-req: 49, 
[2025-12-16 10:13:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.22, #running-req: 67, #queue-req: 44, 
[2025-12-16 10:13:52 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.24, #running-req: 72, #queue-req: 38, 
[2025-12-16 10:13:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.25, #running-req: 78, #queue-req: 33, 
[2025-12-16 10:13:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.27, #running-req: 83, #queue-req: 28, 
[2025-12-16 10:13:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.29, #running-req: 88, #queue-req: 23, 
[2025-12-16 10:13:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.30, #running-req: 93, #queue-req: 18, 
[2025-12-16 10:14:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.32, #running-req: 98, #queue-req: 13, 
[2025-12-16 10:14:02 TP0] Prefill batch, #new-seq: 6, #new-token: 15992, #cached-token: 3214, token usage: 0.34, #running-req: 103, #queue-req: 7, 
[2025-12-16 10:14:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.36, #running-req: 109, #queue-req: 2, 
[2025-12-16 10:14:05 TP0] Prefill batch, #new-seq: 2, #new-token: 6395, #cached-token: 7, token usage: 0.37, #running-req: 114, #queue-req: 0, 
[2025-12-16 10:14:09 TP0] Decode batch, #running-req: 116, #token: 372854, token usage: 0.38, cuda graph: True, gen throughput (token/s): 121.73, #queue-req: 0, 
[2025-12-16 10:14:14 TP0] Decode batch, #running-req: 116, #token: 377494, token usage: 0.38, cuda graph: True, gen throughput (token/s): 809.39, #queue-req: 0, 
[2025-12-16 10:14:20 TP0] Decode batch, #running-req: 116, #token: 382134, token usage: 0.39, cuda graph: True, gen throughput (token/s): 808.59, #queue-req: 0, 
[2025-12-16 10:14:26 TP0] Decode batch, #running-req: 116, #token: 386774, token usage: 0.39, cuda graph: True, gen throughput (token/s): 811.57, #queue-req: 0, 
[2025-12-16 10:14:31 TP0] Decode batch, #running-req: 116, #token: 391414, token usage: 0.40, cuda graph: True, gen throughput (token/s): 818.03, #queue-req: 0, 
[2025-12-16 10:14:37 TP0] Decode batch, #running-req: 116, #token: 396054, token usage: 0.40, cuda graph: True, gen throughput (token/s): 795.00, #queue-req: 0, 
[2025-12-16 10:14:43 TP0] Decode batch, #running-req: 116, #token: 400694, token usage: 0.41, cuda graph: True, gen throughput (token/s): 794.51, #queue-req: 0, 
[2025-12-16 10:14:49 TP0] Decode batch, #running-req: 116, #token: 405334, token usage: 0.41, cuda graph: True, gen throughput (token/s): 794.00, #queue-req: 0, 
[2025-12-16 10:14:55 TP0] Decode batch, #running-req: 116, #token: 409974, token usage: 0.42, cuda graph: True, gen throughput (token/s): 782.42, #queue-req: 0, 
[2025-12-16 10:15:01 TP0] Decode batch, #running-req: 116, #token: 414614, token usage: 0.42, cuda graph: True, gen throughput (token/s): 792.02, #queue-req: 0, 
[2025-12-16 10:15:07 TP0] Decode batch, #running-req: 116, #token: 419254, token usage: 0.43, cuda graph: True, gen throughput (token/s): 802.54, #queue-req: 0, 
[2025-12-16 10:15:12 TP0] Decode batch, #running-req: 116, #token: 423894, token usage: 0.43, cuda graph: True, gen throughput (token/s): 784.38, #queue-req: 0, 
[2025-12-16 10:15:18 TP0] Decode batch, #running-req: 116, #token: 428534, token usage: 0.44, cuda graph: True, gen throughput (token/s): 784.46, #queue-req: 0, 
[2025-12-16 10:15:24 TP0] Decode batch, #running-req: 116, #token: 433174, token usage: 0.44, cuda graph: True, gen throughput (token/s): 784.24, #queue-req: 0, 
[2025-12-16 10:15:30 TP0] Decode batch, #running-req: 116, #token: 437814, token usage: 0.45, cuda graph: True, gen throughput (token/s): 784.53, #queue-req: 0, 
[2025-12-16 10:15:36 TP0] Decode batch, #running-req: 116, #token: 442454, token usage: 0.45, cuda graph: True, gen throughput (token/s): 784.31, #queue-req: 0, 
[2025-12-16 10:15:42 TP0] Decode batch, #running-req: 116, #token: 447094, token usage: 0.46, cuda graph: True, gen throughput (token/s): 782.54, #queue-req: 0, 
[2025-12-16 10:15:48 TP0] Decode batch, #running-req: 116, #token: 451734, token usage: 0.46, cuda graph: True, gen throughput (token/s): 773.06, #queue-req: 0, 
[2025-12-16 10:15:54 TP0] Decode batch, #running-req: 116, #token: 456374, token usage: 0.47, cuda graph: True, gen throughput (token/s): 772.93, #queue-req: 0, 
[2025-12-16 10:16:00 TP0] Decode batch, #running-req: 116, #token: 461014, token usage: 0.47, cuda graph: True, gen throughput (token/s): 773.26, #queue-req: 0, 
[2025-12-16 10:16:04] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-16 10:16:04] INFO:     127.0.0.1:42420 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-16 10:16:05] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-16 10:16:05] INFO:     127.0.0.1:42428 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-16 10:16:15] INFO:     127.0.0.1:54818 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-16 10:16:22] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:22 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:16:23 TP0] Decode batch, #running-req: 1, #token: 3217, token usage: 0.00, cuda graph: True, gen throughput (token/s): 129.17, #queue-req: 0, 
[2025-12-16 10:16:24] INFO:     127.0.0.1:34840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:16:24] INFO:     127.0.0.1:34848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34930 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:34996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35010 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 22, 
[2025-12-16 10:16:24] INFO:     127.0.0.1:35054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:24] INFO:     127.0.0.1:35360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:16:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.02, #running-req: 6, #queue-req: 53, 
[2025-12-16 10:16:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-12-16 10:16:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-12-16 10:16:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-12-16 10:16:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.08, #running-req: 26, #queue-req: 33, 
[2025-12-16 10:16:29 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.10, #running-req: 31, #queue-req: 27, 
[2025-12-16 10:16:30 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.12, #running-req: 37, #queue-req: 21, 
[2025-12-16 10:16:30 TP0] Prefill batch, #new-seq: 6, #new-token: 15986, #cached-token: 3220, token usage: 0.14, #running-req: 43, #queue-req: 15, 
[2025-12-16 10:16:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.16, #running-req: 49, #queue-req: 10, 
[2025-12-16 10:16:32 TP0] Prefill batch, #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.18, #running-req: 54, #queue-req: 4, 
[2025-12-16 10:16:33 TP0] Prefill batch, #new-seq: 4, #new-token: 12793, #cached-token: 11, token usage: 0.20, #running-req: 60, #queue-req: 0, 
[2025-12-16 10:16:36 TP0] Decode batch, #running-req: 64, #token: 206248, token usage: 0.21, cuda graph: True, gen throughput (token/s): 115.45, #queue-req: 0, 
[2025-12-16 10:16:38 TP0] Decode batch, #running-req: 64, #token: 208808, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1225.28, #queue-req: 0, 
[2025-12-16 10:16:40 TP0] Decode batch, #running-req: 64, #token: 211368, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1212.51, #queue-req: 0, 
[2025-12-16 10:16:42 TP0] Decode batch, #running-req: 64, #token: 213928, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1200.08, #queue-req: 0, 
[2025-12-16 10:16:44 TP0] Decode batch, #running-req: 64, #token: 216488, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1192.52, #queue-req: 0, 
[2025-12-16 10:16:46 TP0] Decode batch, #running-req: 64, #token: 219048, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1192.12, #queue-req: 0, 
[2025-12-16 10:16:48 TP0] Decode batch, #running-req: 64, #token: 221608, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1188.35, #queue-req: 0, 
[2025-12-16 10:16:50 TP0] Decode batch, #running-req: 64, #token: 224168, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1187.47, #queue-req: 0, 
[2025-12-16 10:16:53 TP0] Decode batch, #running-req: 64, #token: 226728, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1182.35, #queue-req: 0, 
[2025-12-16 10:16:55 TP0] Decode batch, #running-req: 64, #token: 229288, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1181.46, #queue-req: 0, 
[2025-12-16 10:16:57 TP0] Decode batch, #running-req: 64, #token: 231848, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1174.40, #queue-req: 0, 
[2025-12-16 10:16:59 TP0] Decode batch, #running-req: 64, #token: 234408, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1175.15, #queue-req: 0, 
[2025-12-16 10:17:01 TP0] Decode batch, #running-req: 64, #token: 236968, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1175.13, #queue-req: 0, 
[2025-12-16 10:17:04 TP0] Decode batch, #running-req: 64, #token: 239528, token usage: 0.24, cuda graph: True, gen throughput (token/s): 894.67, #queue-req: 0, 
[2025-12-16 10:17:06 TP0] Decode batch, #running-req: 64, #token: 242088, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1146.10, #queue-req: 0, 
[2025-12-16 10:17:09 TP0] Decode batch, #running-req: 64, #token: 244648, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1169.71, #queue-req: 0, 
[2025-12-16 10:17:11 TP0] Decode batch, #running-req: 64, #token: 247208, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1166.09, #queue-req: 0, 
[2025-12-16 10:17:14 TP0] Decode batch, #running-req: 64, #token: 249768, token usage: 0.25, cuda graph: True, gen throughput (token/s): 731.04, #queue-req: 0, 
[2025-12-16 10:17:19 TP0] Decode batch, #running-req: 64, #token: 252328, token usage: 0.26, cuda graph: True, gen throughput (token/s): 511.73, #queue-req: 0, 
[2025-12-16 10:17:24 TP0] Decode batch, #running-req: 64, #token: 254888, token usage: 0.26, cuda graph: True, gen throughput (token/s): 511.89, #queue-req: 0, 
[2025-12-16 10:17:26] INFO:     127.0.0.1:33668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 3, 
[2025-12-16 10:17:26] INFO:     127.0.0.1:33748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:26] INFO:     127.0.0.1:33840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33922 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33950 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:33998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27] INFO:     127.0.0.1:34242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:17:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 5, #queue-req: 54, 
[2025-12-16 10:17:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.03, #running-req: 10, #queue-req: 49, 
[2025-12-16 10:17:30 TP0] Prefill batch, #new-seq: 6, #new-token: 15984, #cached-token: 3222, token usage: 0.05, #running-req: 15, #queue-req: 43, 
[2025-12-16 10:17:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-12-16 10:17:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.08, #running-req: 26, #queue-req: 33, 
[2025-12-16 10:17:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-12-16 10:17:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-12-16 10:17:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-12-16 10:17:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-12-16 10:17:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-12-16 10:17:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-12-16 10:17:44 TP0] Prefill batch, #new-seq: 3, #new-token: 9591, #cached-token: 12, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-12-16 10:17:51 TP0] Decode batch, #running-req: 64, #token: 206231, token usage: 0.21, cuda graph: True, gen throughput (token/s): 96.55, #queue-req: 0, 
[2025-12-16 10:17:56 TP0] Decode batch, #running-req: 64, #token: 208791, token usage: 0.21, cuda graph: True, gen throughput (token/s): 533.94, #queue-req: 0, 
[2025-12-16 10:18:00 TP0] Decode batch, #running-req: 64, #token: 211351, token usage: 0.22, cuda graph: True, gen throughput (token/s): 533.03, #queue-req: 0, 
[2025-12-16 10:18:05 TP0] Decode batch, #running-req: 64, #token: 213911, token usage: 0.22, cuda graph: True, gen throughput (token/s): 531.22, #queue-req: 0, 
[2025-12-16 10:18:10 TP0] Decode batch, #running-req: 64, #token: 216471, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.47, #queue-req: 0, 
[2025-12-16 10:18:15 TP0] Decode batch, #running-req: 64, #token: 219031, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.97, #queue-req: 0, 
[2025-12-16 10:18:20 TP0] Decode batch, #running-req: 64, #token: 221591, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.68, #queue-req: 0, 
[2025-12-16 10:18:25 TP0] Decode batch, #running-req: 64, #token: 224151, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.57, #queue-req: 0, 
[2025-12-16 10:18:30 TP0] Decode batch, #running-req: 64, #token: 226711, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.46, #queue-req: 0, 
[2025-12-16 10:18:34 TP0] Decode batch, #running-req: 64, #token: 229271, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.32, #queue-req: 0, 
[2025-12-16 10:18:39 TP0] Decode batch, #running-req: 64, #token: 231831, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.64, #queue-req: 0, 
[2025-12-16 10:18:44 TP0] Decode batch, #running-req: 64, #token: 234391, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.62, #queue-req: 0, 
[2025-12-16 10:18:49 TP0] Decode batch, #running-req: 64, #token: 236951, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.36, #queue-req: 0, 
[2025-12-16 10:18:54 TP0] Decode batch, #running-req: 64, #token: 239511, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.49, #queue-req: 0, 
[2025-12-16 10:18:59 TP0] Decode batch, #running-req: 64, #token: 242071, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.24, #queue-req: 0, 
[2025-12-16 10:19:04 TP0] Decode batch, #running-req: 64, #token: 244631, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.16, #queue-req: 0, 
[2025-12-16 10:19:09 TP0] Decode batch, #running-req: 64, #token: 247191, token usage: 0.25, cuda graph: True, gen throughput (token/s): 517.55, #queue-req: 0, 
[2025-12-16 10:19:14 TP0] Decode batch, #running-req: 64, #token: 249751, token usage: 0.25, cuda graph: True, gen throughput (token/s): 515.90, #queue-req: 0, 
[2025-12-16 10:19:19 TP0] Decode batch, #running-req: 64, #token: 252311, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.43, #queue-req: 0, 
[2025-12-16 10:19:24 TP0] Decode batch, #running-req: 64, #token: 254871, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.32, #queue-req: 0, 
[2025-12-16 10:19:26] INFO:     127.0.0.1:40614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 2, 
[2025-12-16 10:19:26] INFO:     127.0.0.1:40692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40752 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:40992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26] INFO:     127.0.0.1:41174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:19:26 TP0] Prefill batch, #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.02, #running-req: 5, #queue-req: 53, 
[2025-12-16 10:19:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-12-16 10:19:29 TP0] Prefill batch, #new-seq: 7, #new-token: 15992, #cached-token: 6415, token usage: 0.06, #running-req: 16, #queue-req: 41, 
[2025-12-16 10:19:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 23, #queue-req: 36, 
[2025-12-16 10:19:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.09, #running-req: 28, #queue-req: 31, 
[2025-12-16 10:19:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.11, #running-req: 33, #queue-req: 26, 
[2025-12-16 10:19:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.12, #running-req: 38, #queue-req: 21, 
[2025-12-16 10:19:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.14, #running-req: 43, #queue-req: 16, 
[2025-12-16 10:19:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.16, #running-req: 48, #queue-req: 11, 
[2025-12-16 10:19:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.17, #running-req: 53, #queue-req: 6, 
[2025-12-16 10:19:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.19, #running-req: 58, #queue-req: 1, 
[2025-12-16 10:19:43 TP0] Prefill batch, #new-seq: 1, #new-token: 3198, #cached-token: 3, token usage: 0.21, #running-req: 63, #queue-req: 0, 
[2025-12-16 10:19:47 TP0] Decode batch, #running-req: 64, #token: 206245, token usage: 0.21, cuda graph: True, gen throughput (token/s): 107.59, #queue-req: 0, 
[2025-12-16 10:19:52 TP0] Decode batch, #running-req: 64, #token: 208805, token usage: 0.21, cuda graph: True, gen throughput (token/s): 534.56, #queue-req: 0, 
[2025-12-16 10:19:57 TP0] Decode batch, #running-req: 64, #token: 211365, token usage: 0.22, cuda graph: True, gen throughput (token/s): 533.61, #queue-req: 0, 
[2025-12-16 10:20:02 TP0] Decode batch, #running-req: 64, #token: 213925, token usage: 0.22, cuda graph: True, gen throughput (token/s): 531.05, #queue-req: 0, 
[2025-12-16 10:20:07 TP0] Decode batch, #running-req: 64, #token: 216485, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.39, #queue-req: 0, 
[2025-12-16 10:20:12 TP0] Decode batch, #running-req: 64, #token: 219045, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.31, #queue-req: 0, 
[2025-12-16 10:20:16 TP0] Decode batch, #running-req: 64, #token: 221605, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.29, #queue-req: 0, 
[2025-12-16 10:20:21 TP0] Decode batch, #running-req: 64, #token: 224165, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.00, #queue-req: 0, 
[2025-12-16 10:20:26 TP0] Decode batch, #running-req: 64, #token: 226725, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.02, #queue-req: 0, 
[2025-12-16 10:20:31 TP0] Decode batch, #running-req: 64, #token: 229285, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.11, #queue-req: 0, 
[2025-12-16 10:20:36 TP0] Decode batch, #running-req: 64, #token: 231845, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.12, #queue-req: 0, 
[2025-12-16 10:20:41 TP0] Decode batch, #running-req: 64, #token: 234405, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.15, #queue-req: 0, 
[2025-12-16 10:20:46 TP0] Decode batch, #running-req: 64, #token: 236965, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.19, #queue-req: 0, 
[2025-12-16 10:20:51 TP0] Decode batch, #running-req: 64, #token: 239525, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.11, #queue-req: 0, 
[2025-12-16 10:20:56 TP0] Decode batch, #running-req: 64, #token: 242085, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.24, #queue-req: 0, 
[2025-12-16 10:21:01 TP0] Decode batch, #running-req: 64, #token: 244645, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.08, #queue-req: 0, 
[2025-12-16 10:21:05 TP0] Decode batch, #running-req: 64, #token: 247205, token usage: 0.25, cuda graph: True, gen throughput (token/s): 517.98, #queue-req: 0, 
[2025-12-16 10:21:10 TP0] Decode batch, #running-req: 64, #token: 249765, token usage: 0.25, cuda graph: True, gen throughput (token/s): 516.15, #queue-req: 0, 
[2025-12-16 10:21:15 TP0] Decode batch, #running-req: 64, #token: 252325, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.36, #queue-req: 0, 
[2025-12-16 10:21:20 TP0] Decode batch, #running-req: 64, #token: 254885, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.18, #queue-req: 0, 
[2025-12-16 10:21:22] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:22] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:22] INFO:     127.0.0.1:54142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:22] INFO:     127.0.0.1:54150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:22] INFO:     127.0.0.1:54158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:22] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:22] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:22] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:22] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:22 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.00, #running-req: 0, #queue-req: 1, 
[2025-12-16 10:21:22] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:22] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:21:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.02, #running-req: 6, #queue-req: 53, 
[2025-12-16 10:21:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-12-16 10:21:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-12-16 10:21:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-12-16 10:21:29 TP0] Prefill batch, #new-seq: 7, #new-token: 15992, #cached-token: 6415, token usage: 0.09, #running-req: 26, #queue-req: 31, 
[2025-12-16 10:21:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.11, #running-req: 33, #queue-req: 26, 
[2025-12-16 10:21:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 38, #queue-req: 21, 
[2025-12-16 10:21:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.14, #running-req: 43, #queue-req: 16, 
[2025-12-16 10:21:35 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.16, #running-req: 48, #queue-req: 11, 
[2025-12-16 10:21:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.17, #running-req: 53, #queue-req: 6, 
[2025-12-16 10:21:38 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.19, #running-req: 58, #queue-req: 0, 
[2025-12-16 10:21:43 TP0] Decode batch, #running-req: 64, #token: 203034, token usage: 0.21, cuda graph: True, gen throughput (token/s): 111.03, #queue-req: 0, 
[2025-12-16 10:21:48 TP0] Decode batch, #running-req: 64, #token: 205594, token usage: 0.21, cuda graph: True, gen throughput (token/s): 534.21, #queue-req: 0, 
[2025-12-16 10:21:53 TP0] Decode batch, #running-req: 64, #token: 208154, token usage: 0.21, cuda graph: True, gen throughput (token/s): 532.68, #queue-req: 0, 
[2025-12-16 10:21:58 TP0] Decode batch, #running-req: 64, #token: 210714, token usage: 0.21, cuda graph: True, gen throughput (token/s): 530.66, #queue-req: 0, 
[2025-12-16 10:22:03 TP0] Decode batch, #running-req: 64, #token: 213274, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.28, #queue-req: 0, 
[2025-12-16 10:22:08 TP0] Decode batch, #running-req: 64, #token: 215834, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.70, #queue-req: 0, 
[2025-12-16 10:22:12 TP0] Decode batch, #running-req: 64, #token: 218394, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.62, #queue-req: 0, 
[2025-12-16 10:22:17 TP0] Decode batch, #running-req: 64, #token: 220954, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.79, #queue-req: 0, 
[2025-12-16 10:22:22 TP0] Decode batch, #running-req: 64, #token: 223514, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.83, #queue-req: 0, 
[2025-12-16 10:22:27 TP0] Decode batch, #running-req: 64, #token: 226074, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.31, #queue-req: 0, 
[2025-12-16 10:22:32 TP0] Decode batch, #running-req: 64, #token: 228634, token usage: 0.23, cuda graph: True, gen throughput (token/s): 521.62, #queue-req: 0, 
[2025-12-16 10:22:37 TP0] Decode batch, #running-req: 64, #token: 231194, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.32, #queue-req: 0, 
[2025-12-16 10:22:42 TP0] Decode batch, #running-req: 64, #token: 233754, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.55, #queue-req: 0, 
[2025-12-16 10:22:47 TP0] Decode batch, #running-req: 64, #token: 236314, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.54, #queue-req: 0, 
[2025-12-16 10:22:52 TP0] Decode batch, #running-req: 64, #token: 238874, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.26, #queue-req: 0, 
[2025-12-16 10:22:56 TP0] Decode batch, #running-req: 64, #token: 241434, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.35, #queue-req: 0, 
[2025-12-16 10:23:01 TP0] Decode batch, #running-req: 64, #token: 243994, token usage: 0.25, cuda graph: True, gen throughput (token/s): 518.72, #queue-req: 0, 
[2025-12-16 10:23:06 TP0] Decode batch, #running-req: 64, #token: 246554, token usage: 0.25, cuda graph: True, gen throughput (token/s): 516.81, #queue-req: 0, 
[2025-12-16 10:23:11 TP0] Decode batch, #running-req: 64, #token: 249114, token usage: 0.25, cuda graph: True, gen throughput (token/s): 516.80, #queue-req: 0, 
[2025-12-16 10:23:16 TP0] Decode batch, #running-req: 64, #token: 251674, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.90, #queue-req: 0, 
[2025-12-16 10:23:18] INFO:     127.0.0.1:49650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.00, #running-req: 0, #queue-req: 3, 
[2025-12-16 10:23:18] INFO:     127.0.0.1:49732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:18] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:49936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:49940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:49964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:49970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:49974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:49992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:23:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.02, #running-req: 5, #queue-req: 54, 
[2025-12-16 10:23:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.03, #running-req: 10, #queue-req: 49, 
[2025-12-16 10:23:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.05, #running-req: 15, #queue-req: 44, 
[2025-12-16 10:23:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 20, #queue-req: 39, 
[2025-12-16 10:23:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.08, #running-req: 25, #queue-req: 34, 
[2025-12-16 10:23:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.10, #running-req: 30, #queue-req: 29, 
[2025-12-16 10:23:28 TP0] Prefill batch, #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.12, #running-req: 35, #queue-req: 23, 
[2025-12-16 10:23:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-12-16 10:23:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-12-16 10:23:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-12-16 10:23:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-12-16 10:23:35 TP0] Prefill batch, #new-seq: 3, #new-token: 6391, #cached-token: 3212, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-12-16 10:23:40 TP0] Decode batch, #running-req: 64, #token: 206234, token usage: 0.21, cuda graph: True, gen throughput (token/s): 107.78, #queue-req: 0, 
[2025-12-16 10:23:45 TP0] Decode batch, #running-req: 64, #token: 208794, token usage: 0.21, cuda graph: True, gen throughput (token/s): 534.17, #queue-req: 0, 
[2025-12-16 10:23:50 TP0] Decode batch, #running-req: 64, #token: 211354, token usage: 0.22, cuda graph: True, gen throughput (token/s): 533.62, #queue-req: 0, 
[2025-12-16 10:23:54 TP0] Decode batch, #running-req: 64, #token: 213914, token usage: 0.22, cuda graph: True, gen throughput (token/s): 531.35, #queue-req: 0, 
[2025-12-16 10:23:59 TP0] Decode batch, #running-req: 64, #token: 216474, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.44, #queue-req: 0, 
[2025-12-16 10:24:04 TP0] Decode batch, #running-req: 64, #token: 219034, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.80, #queue-req: 0, 
[2025-12-16 10:24:09 TP0] Decode batch, #running-req: 64, #token: 221594, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.80, #queue-req: 0, 
[2025-12-16 10:24:14 TP0] Decode batch, #running-req: 64, #token: 224154, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.50, #queue-req: 0, 
[2025-12-16 10:24:19 TP0] Decode batch, #running-req: 64, #token: 226714, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.77, #queue-req: 0, 
[2025-12-16 10:24:24 TP0] Decode batch, #running-req: 64, #token: 229274, token usage: 0.23, cuda graph: True, gen throughput (token/s): 525.76, #queue-req: 0, 
[2025-12-16 10:24:28 TP0] Decode batch, #running-req: 64, #token: 231834, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.68, #queue-req: 0, 
[2025-12-16 10:24:33 TP0] Decode batch, #running-req: 64, #token: 234394, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.08, #queue-req: 0, 
[2025-12-16 10:24:38 TP0] Decode batch, #running-req: 64, #token: 236954, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.17, #queue-req: 0, 
[2025-12-16 10:24:43 TP0] Decode batch, #running-req: 64, #token: 239514, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.39, #queue-req: 0, 
[2025-12-16 10:24:48 TP0] Decode batch, #running-req: 64, #token: 242074, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.29, #queue-req: 0, 
[2025-12-16 10:24:53 TP0] Decode batch, #running-req: 64, #token: 244634, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.85, #queue-req: 0, 
[2025-12-16 10:24:58 TP0] Decode batch, #running-req: 64, #token: 247194, token usage: 0.25, cuda graph: True, gen throughput (token/s): 519.45, #queue-req: 0, 
[2025-12-16 10:25:03 TP0] Decode batch, #running-req: 64, #token: 249754, token usage: 0.25, cuda graph: True, gen throughput (token/s): 518.54, #queue-req: 0, 
[2025-12-16 10:25:08 TP0] Decode batch, #running-req: 64, #token: 252314, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.75, #queue-req: 0, 
[2025-12-16 10:25:13 TP0] Decode batch, #running-req: 64, #token: 254874, token usage: 0.26, cuda graph: True, gen throughput (token/s): 507.05, #queue-req: 0, 
[2025-12-16 10:25:15] INFO:     127.0.0.1:59204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.00, #running-req: 0, #queue-req: 3, 
[2025-12-16 10:25:15] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:25:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 5, #queue-req: 54, 
[2025-12-16 10:25:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.03, #running-req: 10, #queue-req: 49, 
[2025-12-16 10:25:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 15, #queue-req: 44, 
[2025-12-16 10:25:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.07, #running-req: 20, #queue-req: 39, 
[2025-12-16 10:25:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.08, #running-req: 25, #queue-req: 34, 
[2025-12-16 10:25:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.10, #running-req: 30, #queue-req: 29, 
[2025-12-16 10:25:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.11, #running-req: 35, #queue-req: 24, 
[2025-12-16 10:25:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.13, #running-req: 40, #queue-req: 19, 
[2025-12-16 10:25:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.15, #running-req: 45, #queue-req: 14, 
[2025-12-16 10:25:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.16, #running-req: 50, #queue-req: 9, 
[2025-12-16 10:25:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.18, #running-req: 55, #queue-req: 4, 
[2025-12-16 10:25:32 TP0] Prefill batch, #new-seq: 4, #new-token: 12795, #cached-token: 9, token usage: 0.20, #running-req: 60, #queue-req: 0, 
[2025-12-16 10:25:37 TP0] Decode batch, #running-req: 64, #token: 206248, token usage: 0.21, cuda graph: True, gen throughput (token/s): 105.76, #queue-req: 0, 
[2025-12-16 10:25:42 TP0] Decode batch, #running-req: 64, #token: 208808, token usage: 0.21, cuda graph: True, gen throughput (token/s): 534.47, #queue-req: 0, 
[2025-12-16 10:25:47 TP0] Decode batch, #running-req: 64, #token: 211368, token usage: 0.22, cuda graph: True, gen throughput (token/s): 532.80, #queue-req: 0, 
[2025-12-16 10:25:51 TP0] Decode batch, #running-req: 64, #token: 213928, token usage: 0.22, cuda graph: True, gen throughput (token/s): 530.41, #queue-req: 0, 
[2025-12-16 10:25:56 TP0] Decode batch, #running-req: 64, #token: 216488, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.93, #queue-req: 0, 
[2025-12-16 10:26:01 TP0] Decode batch, #running-req: 64, #token: 219048, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.49, #queue-req: 0, 
[2025-12-16 10:26:06 TP0] Decode batch, #running-req: 64, #token: 221608, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.39, #queue-req: 0, 
[2025-12-16 10:26:11 TP0] Decode batch, #running-req: 64, #token: 224168, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.62, #queue-req: 0, 
[2025-12-16 10:26:16 TP0] Decode batch, #running-req: 64, #token: 226728, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.67, #queue-req: 0, 
[2025-12-16 10:26:21 TP0] Decode batch, #running-req: 64, #token: 229288, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.74, #queue-req: 0, 
[2025-12-16 10:26:26 TP0] Decode batch, #running-req: 64, #token: 231848, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.10, #queue-req: 0, 
[2025-12-16 10:26:30 TP0] Decode batch, #running-req: 64, #token: 234408, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.00, #queue-req: 0, 
[2025-12-16 10:26:35 TP0] Decode batch, #running-req: 64, #token: 236968, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.98, #queue-req: 0, 
[2025-12-16 10:26:40 TP0] Decode batch, #running-req: 64, #token: 239528, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.97, #queue-req: 0, 
[2025-12-16 10:26:45 TP0] Decode batch, #running-req: 64, #token: 242088, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.31, #queue-req: 0, 
[2025-12-16 10:26:50 TP0] Decode batch, #running-req: 64, #token: 244648, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.47, #queue-req: 0, 
[2025-12-16 10:26:55 TP0] Decode batch, #running-req: 64, #token: 247208, token usage: 0.25, cuda graph: True, gen throughput (token/s): 519.05, #queue-req: 0, 
[2025-12-16 10:27:00 TP0] Decode batch, #running-req: 64, #token: 249768, token usage: 0.25, cuda graph: True, gen throughput (token/s): 516.79, #queue-req: 0, 
[2025-12-16 10:27:05 TP0] Decode batch, #running-req: 64, #token: 252328, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.49, #queue-req: 0, 
[2025-12-16 10:27:10 TP0] Decode batch, #running-req: 64, #token: 254888, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.63, #queue-req: 0, 
[2025-12-16 10:27:12] INFO:     127.0.0.1:51022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.00, #running-req: 0, #queue-req: 3, 
[2025-12-16 10:27:12] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51222 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12] INFO:     127.0.0.1:51548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:27:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.02, #running-req: 5, #queue-req: 54, 
[2025-12-16 10:27:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.03, #running-req: 10, #queue-req: 49, 
[2025-12-16 10:27:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 15, #queue-req: 44, 
[2025-12-16 10:27:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 20, #queue-req: 39, 
[2025-12-16 10:27:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.08, #running-req: 25, #queue-req: 34, 
[2025-12-16 10:27:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.10, #running-req: 30, #queue-req: 29, 
[2025-12-16 10:27:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.11, #running-req: 35, #queue-req: 24, 
[2025-12-16 10:27:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.13, #running-req: 40, #queue-req: 19, 
[2025-12-16 10:27:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.15, #running-req: 45, #queue-req: 14, 
[2025-12-16 10:27:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.16, #running-req: 50, #queue-req: 9, 
[2025-12-16 10:27:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.18, #running-req: 55, #queue-req: 4, 
[2025-12-16 10:27:29 TP0] Prefill batch, #new-seq: 4, #new-token: 12791, #cached-token: 13, token usage: 0.20, #running-req: 60, #queue-req: 0, 
[2025-12-16 10:27:34 TP0] Decode batch, #running-req: 64, #token: 206238, token usage: 0.21, cuda graph: True, gen throughput (token/s): 105.69, #queue-req: 0, 
[2025-12-16 10:27:39 TP0] Decode batch, #running-req: 64, #token: 208798, token usage: 0.21, cuda graph: True, gen throughput (token/s): 535.01, #queue-req: 0, 
[2025-12-16 10:27:44 TP0] Decode batch, #running-req: 64, #token: 211358, token usage: 0.22, cuda graph: True, gen throughput (token/s): 533.84, #queue-req: 0, 
[2025-12-16 10:27:48 TP0] Decode batch, #running-req: 64, #token: 213918, token usage: 0.22, cuda graph: True, gen throughput (token/s): 531.65, #queue-req: 0, 
[2025-12-16 10:27:53 TP0] Decode batch, #running-req: 64, #token: 216478, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.40, #queue-req: 0, 
[2025-12-16 10:27:58 TP0] Decode batch, #running-req: 64, #token: 219038, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.70, #queue-req: 0, 
[2025-12-16 10:28:03 TP0] Decode batch, #running-req: 64, #token: 221598, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.34, #queue-req: 0, 
[2025-12-16 10:28:08 TP0] Decode batch, #running-req: 64, #token: 224158, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.43, #queue-req: 0, 
[2025-12-16 10:28:13 TP0] Decode batch, #running-req: 64, #token: 226718, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.37, #queue-req: 0, 
[2025-12-16 10:28:18 TP0] Decode batch, #running-req: 64, #token: 229278, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.62, #queue-req: 0, 
[2025-12-16 10:28:23 TP0] Decode batch, #running-req: 64, #token: 231838, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.88, #queue-req: 0, 
[2025-12-16 10:28:27 TP0] Decode batch, #running-req: 64, #token: 234398, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.86, #queue-req: 0, 
[2025-12-16 10:28:32 TP0] Decode batch, #running-req: 64, #token: 236958, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.10, #queue-req: 0, 
[2025-12-16 10:28:37 TP0] Decode batch, #running-req: 64, #token: 239518, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.91, #queue-req: 0, 
[2025-12-16 10:28:42 TP0] Decode batch, #running-req: 64, #token: 242078, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.66, #queue-req: 0, 
[2025-12-16 10:28:47 TP0] Decode batch, #running-req: 64, #token: 244638, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.81, #queue-req: 0, 
[2025-12-16 10:28:52 TP0] Decode batch, #running-req: 64, #token: 247198, token usage: 0.25, cuda graph: True, gen throughput (token/s): 519.07, #queue-req: 0, 
[2025-12-16 10:28:57 TP0] Decode batch, #running-req: 64, #token: 249758, token usage: 0.25, cuda graph: True, gen throughput (token/s): 517.34, #queue-req: 0, 
[2025-12-16 10:29:02 TP0] Decode batch, #running-req: 64, #token: 252318, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.06, #queue-req: 0, 
[2025-12-16 10:29:07 TP0] Decode batch, #running-req: 64, #token: 254878, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.16, #queue-req: 0, 
[2025-12-16 10:29:09] INFO:     127.0.0.1:35258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.00, #running-req: 0, #queue-req: 3, 
[2025-12-16 10:29:09] INFO:     127.0.0.1:35358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09] INFO:     127.0.0.1:35738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:29:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 5, #queue-req: 42, 
[2025-12-16 10:29:11 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.04, #running-req: 10, #queue-req: 36, 
[2025-12-16 10:29:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.05, #running-req: 16, #queue-req: 31, 
[2025-12-16 10:29:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 21, #queue-req: 26, 
[2025-12-16 10:29:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.08, #running-req: 26, #queue-req: 21, 
[2025-12-16 10:29:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.10, #running-req: 31, #queue-req: 16, 
[2025-12-16 10:29:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 36, #queue-req: 11, 
[2025-12-16 10:29:20 TP0] Prefill batch, #new-seq: 6, #new-token: 15992, #cached-token: 3214, token usage: 0.14, #running-req: 41, #queue-req: 5, 
[2025-12-16 10:29:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15979, #cached-token: 26, token usage: 0.15, #running-req: 47, #queue-req: 0, 
[2025-12-16 10:29:27 TP0] Decode batch, #running-req: 52, #token: 167570, token usage: 0.17, cuda graph: True, gen throughput (token/s): 115.15, #queue-req: 0, 
[2025-12-16 10:29:31 TP0] Decode batch, #running-req: 52, #token: 169650, token usage: 0.17, cuda graph: True, gen throughput (token/s): 456.93, #queue-req: 0, 
[2025-12-16 10:29:36 TP0] Decode batch, #running-req: 52, #token: 171730, token usage: 0.17, cuda graph: True, gen throughput (token/s): 455.91, #queue-req: 0, 
[2025-12-16 10:29:40 TP0] Decode batch, #running-req: 52, #token: 173810, token usage: 0.18, cuda graph: True, gen throughput (token/s): 453.81, #queue-req: 0, 
[2025-12-16 10:29:45 TP0] Decode batch, #running-req: 52, #token: 175890, token usage: 0.18, cuda graph: True, gen throughput (token/s): 451.78, #queue-req: 0, 
[2025-12-16 10:29:50 TP0] Decode batch, #running-req: 52, #token: 177970, token usage: 0.18, cuda graph: True, gen throughput (token/s): 451.42, #queue-req: 0, 
[2025-12-16 10:29:54 TP0] Decode batch, #running-req: 52, #token: 180050, token usage: 0.18, cuda graph: True, gen throughput (token/s): 451.22, #queue-req: 0, 
[2025-12-16 10:29:59 TP0] Decode batch, #running-req: 52, #token: 182130, token usage: 0.19, cuda graph: True, gen throughput (token/s): 451.31, #queue-req: 0, 
[2025-12-16 10:30:03 TP0] Decode batch, #running-req: 52, #token: 184210, token usage: 0.19, cuda graph: True, gen throughput (token/s): 451.26, #queue-req: 0, 
[2025-12-16 10:30:08 TP0] Decode batch, #running-req: 52, #token: 186290, token usage: 0.19, cuda graph: True, gen throughput (token/s): 451.19, #queue-req: 0, 
[2025-12-16 10:30:13 TP0] Decode batch, #running-req: 52, #token: 188370, token usage: 0.19, cuda graph: True, gen throughput (token/s): 447.49, #queue-req: 0, 
[2025-12-16 10:30:17 TP0] Decode batch, #running-req: 52, #token: 190450, token usage: 0.19, cuda graph: True, gen throughput (token/s): 447.92, #queue-req: 0, 
[2025-12-16 10:30:22 TP0] Decode batch, #running-req: 52, #token: 192530, token usage: 0.20, cuda graph: True, gen throughput (token/s): 447.88, #queue-req: 0, 
[2025-12-16 10:30:27 TP0] Decode batch, #running-req: 52, #token: 194610, token usage: 0.20, cuda graph: True, gen throughput (token/s): 447.94, #queue-req: 0, 
[2025-12-16 10:30:31 TP0] Decode batch, #running-req: 52, #token: 196690, token usage: 0.20, cuda graph: True, gen throughput (token/s): 447.98, #queue-req: 0, 
[2025-12-16 10:30:36 TP0] Decode batch, #running-req: 52, #token: 198770, token usage: 0.20, cuda graph: True, gen throughput (token/s): 447.79, #queue-req: 0, 
[2025-12-16 10:30:41 TP0] Decode batch, #running-req: 52, #token: 200850, token usage: 0.20, cuda graph: True, gen throughput (token/s): 445.74, #queue-req: 0, 
[2025-12-16 10:30:45 TP0] Decode batch, #running-req: 52, #token: 202930, token usage: 0.21, cuda graph: True, gen throughput (token/s): 444.28, #queue-req: 0, 
[2025-12-16 10:30:50 TP0] Decode batch, #running-req: 52, #token: 205010, token usage: 0.21, cuda graph: True, gen throughput (token/s): 444.68, #queue-req: 0, 
[2025-12-16 10:30:55 TP0] Decode batch, #running-req: 52, #token: 207090, token usage: 0.21, cuda graph: True, gen throughput (token/s): 444.32, #queue-req: 0, 
[2025-12-16 10:30:57] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-16 10:30:57] INFO:     127.0.0.1:49722 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-16 10:30:58] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-16 10:30:58] INFO:     127.0.0.1:49738 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-16 10:31:08] INFO:     127.0.0.1:47912 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-16 10:31:14] INFO:     127.0.0.1:45070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:14 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:31:15 TP0] Decode batch, #running-req: 1, #token: 3225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-12-16 10:31:16] INFO:     127.0.0.1:45078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:31:16] INFO:     127.0.0.1:45088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:16] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-12-16 10:31:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-12-16 10:31:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-12-16 10:31:20 TP0] Decode batch, #running-req: 16, #token: 51698, token usage: 0.05, cuda graph: True, gen throughput (token/s): 102.58, #queue-req: 0, 
[2025-12-16 10:31:21 TP0] Decode batch, #running-req: 16, #token: 52338, token usage: 0.05, cuda graph: True, gen throughput (token/s): 497.06, #queue-req: 0, 
[2025-12-16 10:31:23 TP0] Decode batch, #running-req: 16, #token: 52978, token usage: 0.05, cuda graph: True, gen throughput (token/s): 495.83, #queue-req: 0, 
[2025-12-16 10:31:24 TP0] Decode batch, #running-req: 16, #token: 53618, token usage: 0.05, cuda graph: True, gen throughput (token/s): 495.51, #queue-req: 0, 
[2025-12-16 10:31:25 TP0] Decode batch, #running-req: 16, #token: 54258, token usage: 0.06, cuda graph: True, gen throughput (token/s): 494.44, #queue-req: 0, 
[2025-12-16 10:31:27 TP0] Decode batch, #running-req: 16, #token: 54898, token usage: 0.06, cuda graph: True, gen throughput (token/s): 492.40, #queue-req: 0, 
[2025-12-16 10:31:28 TP0] Decode batch, #running-req: 16, #token: 55538, token usage: 0.06, cuda graph: True, gen throughput (token/s): 492.12, #queue-req: 0, 
[2025-12-16 10:31:29 TP0] Decode batch, #running-req: 16, #token: 56178, token usage: 0.06, cuda graph: True, gen throughput (token/s): 492.03, #queue-req: 0, 
[2025-12-16 10:31:30 TP0] Decode batch, #running-req: 16, #token: 56818, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.11, #queue-req: 0, 
[2025-12-16 10:31:32 TP0] Decode batch, #running-req: 16, #token: 57458, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.69, #queue-req: 0, 
[2025-12-16 10:31:33 TP0] Decode batch, #running-req: 16, #token: 58098, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.58, #queue-req: 0, 
[2025-12-16 10:31:34 TP0] Decode batch, #running-req: 16, #token: 58738, token usage: 0.06, cuda graph: True, gen throughput (token/s): 488.51, #queue-req: 0, 
[2025-12-16 10:31:36 TP0] Decode batch, #running-req: 16, #token: 59378, token usage: 0.06, cuda graph: True, gen throughput (token/s): 488.86, #queue-req: 0, 
[2025-12-16 10:31:37 TP0] Decode batch, #running-req: 16, #token: 60018, token usage: 0.06, cuda graph: True, gen throughput (token/s): 488.73, #queue-req: 0, 
[2025-12-16 10:31:38 TP0] Decode batch, #running-req: 16, #token: 60658, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.73, #queue-req: 0, 
[2025-12-16 10:31:40 TP0] Decode batch, #running-req: 16, #token: 61298, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.33, #queue-req: 0, 
[2025-12-16 10:31:41 TP0] Decode batch, #running-req: 16, #token: 61938, token usage: 0.06, cuda graph: True, gen throughput (token/s): 486.98, #queue-req: 0, 
[2025-12-16 10:31:42 TP0] Decode batch, #running-req: 16, #token: 62578, token usage: 0.06, cuda graph: True, gen throughput (token/s): 485.38, #queue-req: 0, 
[2025-12-16 10:31:44 TP0] Decode batch, #running-req: 16, #token: 63218, token usage: 0.06, cuda graph: True, gen throughput (token/s): 485.94, #queue-req: 0, 
[2025-12-16 10:31:45 TP0] Decode batch, #running-req: 16, #token: 63858, token usage: 0.07, cuda graph: True, gen throughput (token/s): 486.06, #queue-req: 0, 
[2025-12-16 10:31:45] INFO:     127.0.0.1:59744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:31:45] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:31:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-12-16 10:31:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-12-16 10:31:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-12-16 10:31:49 TP0] Decode batch, #running-req: 16, #token: 51692, token usage: 0.05, cuda graph: True, gen throughput (token/s): 158.14, #queue-req: 0, 
[2025-12-16 10:31:50 TP0] Decode batch, #running-req: 16, #token: 52332, token usage: 0.05, cuda graph: True, gen throughput (token/s): 495.30, #queue-req: 0, 
[2025-12-16 10:31:51 TP0] Decode batch, #running-req: 16, #token: 52972, token usage: 0.05, cuda graph: True, gen throughput (token/s): 494.10, #queue-req: 0, 
[2025-12-16 10:31:53 TP0] Decode batch, #running-req: 16, #token: 53612, token usage: 0.05, cuda graph: True, gen throughput (token/s): 493.21, #queue-req: 0, 
[2025-12-16 10:31:54 TP0] Decode batch, #running-req: 16, #token: 54252, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.57, #queue-req: 0, 
[2025-12-16 10:31:55 TP0] Decode batch, #running-req: 16, #token: 54892, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.81, #queue-req: 0, 
[2025-12-16 10:31:57 TP0] Decode batch, #running-req: 16, #token: 55532, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.61, #queue-req: 0, 
[2025-12-16 10:31:58 TP0] Decode batch, #running-req: 16, #token: 56172, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.40, #queue-req: 0, 
[2025-12-16 10:31:59 TP0] Decode batch, #running-req: 16, #token: 56812, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.90, #queue-req: 0, 
[2025-12-16 10:32:01 TP0] Decode batch, #running-req: 16, #token: 57452, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.36, #queue-req: 0, 
[2025-12-16 10:32:02 TP0] Decode batch, #running-req: 16, #token: 58092, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.25, #queue-req: 0, 
[2025-12-16 10:32:03 TP0] Decode batch, #running-req: 16, #token: 58732, token usage: 0.06, cuda graph: True, gen throughput (token/s): 488.86, #queue-req: 0, 
[2025-12-16 10:32:05 TP0] Decode batch, #running-req: 16, #token: 59372, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.94, #queue-req: 0, 
[2025-12-16 10:32:06 TP0] Decode batch, #running-req: 16, #token: 60012, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.93, #queue-req: 0, 
[2025-12-16 10:32:07 TP0] Decode batch, #running-req: 16, #token: 60652, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.82, #queue-req: 0, 
[2025-12-16 10:32:08 TP0] Decode batch, #running-req: 16, #token: 61292, token usage: 0.06, cuda graph: True, gen throughput (token/s): 486.59, #queue-req: 0, 
[2025-12-16 10:32:10 TP0] Decode batch, #running-req: 16, #token: 61932, token usage: 0.06, cuda graph: True, gen throughput (token/s): 485.76, #queue-req: 0, 
[2025-12-16 10:32:11 TP0] Decode batch, #running-req: 16, #token: 62572, token usage: 0.06, cuda graph: True, gen throughput (token/s): 485.09, #queue-req: 0, 
[2025-12-16 10:32:12 TP0] Decode batch, #running-req: 16, #token: 63212, token usage: 0.06, cuda graph: True, gen throughput (token/s): 485.55, #queue-req: 0, 
[2025-12-16 10:32:14 TP0] Decode batch, #running-req: 16, #token: 63852, token usage: 0.07, cuda graph: True, gen throughput (token/s): 484.07, #queue-req: 0, 
[2025-12-16 10:32:14] INFO:     127.0.0.1:34822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:32:14] INFO:     127.0.0.1:34850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14] INFO:     127.0.0.1:34954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-12-16 10:32:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-12-16 10:32:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-12-16 10:32:18 TP0] Decode batch, #running-req: 16, #token: 51694, token usage: 0.05, cuda graph: True, gen throughput (token/s): 157.18, #queue-req: 0, 
[2025-12-16 10:32:19 TP0] Decode batch, #running-req: 16, #token: 52334, token usage: 0.05, cuda graph: True, gen throughput (token/s): 500.11, #queue-req: 0, 
[2025-12-16 10:32:20 TP0] Decode batch, #running-req: 16, #token: 52974, token usage: 0.05, cuda graph: True, gen throughput (token/s): 497.50, #queue-req: 0, 
[2025-12-16 10:32:22 TP0] Decode batch, #running-req: 16, #token: 53614, token usage: 0.05, cuda graph: True, gen throughput (token/s): 496.78, #queue-req: 0, 
[2025-12-16 10:32:23 TP0] Decode batch, #running-req: 16, #token: 54254, token usage: 0.06, cuda graph: True, gen throughput (token/s): 494.38, #queue-req: 0, 
[2025-12-16 10:32:24 TP0] Decode batch, #running-req: 16, #token: 54894, token usage: 0.06, cuda graph: True, gen throughput (token/s): 492.91, #queue-req: 0, 
[2025-12-16 10:32:26 TP0] Decode batch, #running-req: 16, #token: 55534, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.60, #queue-req: 0, 
[2025-12-16 10:32:27 TP0] Decode batch, #running-req: 16, #token: 56174, token usage: 0.06, cuda graph: True, gen throughput (token/s): 492.52, #queue-req: 0, 
[2025-12-16 10:32:28 TP0] Decode batch, #running-req: 16, #token: 56814, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.42, #queue-req: 0, 
[2025-12-16 10:32:29 TP0] Decode batch, #running-req: 16, #token: 57454, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.65, #queue-req: 0, 
[2025-12-16 10:32:31 TP0] Decode batch, #running-req: 16, #token: 58094, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.90, #queue-req: 0, 
[2025-12-16 10:32:32 TP0] Decode batch, #running-req: 16, #token: 58734, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.02, #queue-req: 0, 
[2025-12-16 10:32:33 TP0] Decode batch, #running-req: 16, #token: 59374, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.78, #queue-req: 0, 
[2025-12-16 10:32:35 TP0] Decode batch, #running-req: 16, #token: 60014, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.92, #queue-req: 0, 
[2025-12-16 10:32:36 TP0] Decode batch, #running-req: 16, #token: 60654, token usage: 0.06, cuda graph: True, gen throughput (token/s): 488.98, #queue-req: 0, 
[2025-12-16 10:32:37 TP0] Decode batch, #running-req: 16, #token: 61294, token usage: 0.06, cuda graph: True, gen throughput (token/s): 488.80, #queue-req: 0, 
[2025-12-16 10:32:39 TP0] Decode batch, #running-req: 16, #token: 61934, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.64, #queue-req: 0, 
[2025-12-16 10:32:40 TP0] Decode batch, #running-req: 16, #token: 62574, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.45, #queue-req: 0, 
[2025-12-16 10:32:41 TP0] Decode batch, #running-req: 16, #token: 63214, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.89, #queue-req: 0, 
[2025-12-16 10:32:43 TP0] Decode batch, #running-req: 16, #token: 63854, token usage: 0.07, cuda graph: True, gen throughput (token/s): 487.62, #queue-req: 0, 
[2025-12-16 10:32:43] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:32:43] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43] INFO:     127.0.0.1:54244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:32:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-12-16 10:32:43 TP0] Prefill batch, #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.02, #running-req: 6, #queue-req: 4, 
[2025-12-16 10:32:44 TP0] Prefill batch, #new-seq: 4, #new-token: 12793, #cached-token: 11, token usage: 0.04, #running-req: 12, #queue-req: 0, 
[2025-12-16 10:32:46 TP0] Decode batch, #running-req: 16, #token: 51697, token usage: 0.05, cuda graph: True, gen throughput (token/s): 163.96, #queue-req: 0, 
[2025-12-16 10:32:48 TP0] Decode batch, #running-req: 16, #token: 52337, token usage: 0.05, cuda graph: True, gen throughput (token/s): 496.15, #queue-req: 0, 
[2025-12-16 10:32:49 TP0] Decode batch, #running-req: 16, #token: 52977, token usage: 0.05, cuda graph: True, gen throughput (token/s): 495.34, #queue-req: 0, 
[2025-12-16 10:32:50 TP0] Decode batch, #running-req: 16, #token: 53617, token usage: 0.05, cuda graph: True, gen throughput (token/s): 493.86, #queue-req: 0, 
[2025-12-16 10:32:52 TP0] Decode batch, #running-req: 16, #token: 54257, token usage: 0.06, cuda graph: True, gen throughput (token/s): 492.96, #queue-req: 0, 
[2025-12-16 10:32:53 TP0] Decode batch, #running-req: 16, #token: 54897, token usage: 0.06, cuda graph: True, gen throughput (token/s): 493.19, #queue-req: 0, 
[2025-12-16 10:32:54 TP0] Decode batch, #running-req: 16, #token: 55537, token usage: 0.06, cuda graph: True, gen throughput (token/s): 492.12, #queue-req: 0, 
[2025-12-16 10:32:56 TP0] Decode batch, #running-req: 16, #token: 56177, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.78, #queue-req: 0, 
[2025-12-16 10:32:57 TP0] Decode batch, #running-req: 16, #token: 56817, token usage: 0.06, cuda graph: True, gen throughput (token/s): 492.52, #queue-req: 0, 
[2025-12-16 10:32:58 TP0] Decode batch, #running-req: 16, #token: 57457, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.22, #queue-req: 0, 
[2025-12-16 10:32:59 TP0] Decode batch, #running-req: 16, #token: 58097, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.58, #queue-req: 0, 
[2025-12-16 10:33:02 TP0] Decode batch, #running-req: 16, #token: 58737, token usage: 0.06, cuda graph: True, gen throughput (token/s): 295.28, #queue-req: 0, 
[2025-12-16 10:33:06 TP0] Decode batch, #running-req: 16, #token: 59377, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.79, #queue-req: 0, 
[2025-12-16 10:33:09 TP0] Decode batch, #running-req: 16, #token: 60017, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.98, #queue-req: 0, 
[2025-12-16 10:33:13 TP0] Decode batch, #running-req: 16, #token: 60657, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.10, #queue-req: 0, 
[2025-12-16 10:33:17 TP0] Decode batch, #running-req: 16, #token: 61297, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.15, #queue-req: 0, 
[2025-12-16 10:33:21 TP0] Decode batch, #running-req: 16, #token: 61937, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.99, #queue-req: 0, 
[2025-12-16 10:33:25 TP0] Decode batch, #running-req: 16, #token: 62577, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.01, #queue-req: 0, 
[2025-12-16 10:33:29 TP0] Decode batch, #running-req: 16, #token: 63217, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.98, #queue-req: 0, 
[2025-12-16 10:33:33 TP0] Decode batch, #running-req: 16, #token: 63857, token usage: 0.07, cuda graph: True, gen throughput (token/s): 163.97, #queue-req: 0, 
[2025-12-16 10:33:34] INFO:     127.0.0.1:36862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 7, 
[2025-12-16 10:33:34] INFO:     127.0.0.1:36986 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34] INFO:     127.0.0.1:36988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:33:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.02, #running-req: 5, #queue-req: 6, 
[2025-12-16 10:33:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.03, #running-req: 10, #queue-req: 1, 
[2025-12-16 10:33:37 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.05, #running-req: 15, #queue-req: 0, 
[2025-12-16 10:33:42 TP0] Decode batch, #running-req: 16, #token: 51692, token usage: 0.05, cuda graph: True, gen throughput (token/s): 72.26, #queue-req: 0, 
[2025-12-16 10:33:46 TP0] Decode batch, #running-req: 16, #token: 52332, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.78, #queue-req: 0, 
[2025-12-16 10:33:49 TP0] Decode batch, #running-req: 16, #token: 52972, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.73, #queue-req: 0, 
[2025-12-16 10:33:53 TP0] Decode batch, #running-req: 16, #token: 53612, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.55, #queue-req: 0, 
[2025-12-16 10:33:57 TP0] Decode batch, #running-req: 16, #token: 54252, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.21, #queue-req: 0, 
[2025-12-16 10:34:01 TP0] Decode batch, #running-req: 16, #token: 54892, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.15, #queue-req: 0, 
[2025-12-16 10:34:05 TP0] Decode batch, #running-req: 16, #token: 55532, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.15, #queue-req: 0, 
[2025-12-16 10:34:09 TP0] Decode batch, #running-req: 16, #token: 56172, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.14, #queue-req: 0, 
[2025-12-16 10:34:13 TP0] Decode batch, #running-req: 16, #token: 56812, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.13, #queue-req: 0, 
[2025-12-16 10:34:17 TP0] Decode batch, #running-req: 16, #token: 57452, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.08, #queue-req: 0, 
[2025-12-16 10:34:20 TP0] Decode batch, #running-req: 16, #token: 58092, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.61, #queue-req: 0, 
[2025-12-16 10:34:24 TP0] Decode batch, #running-req: 16, #token: 58732, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.61, #queue-req: 0, 
[2025-12-16 10:34:28 TP0] Decode batch, #running-req: 16, #token: 59372, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.61, #queue-req: 0, 
[2025-12-16 10:34:32 TP0] Decode batch, #running-req: 16, #token: 60012, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.57, #queue-req: 0, 
[2025-12-16 10:34:36 TP0] Decode batch, #running-req: 16, #token: 60652, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.60, #queue-req: 0, 
[2025-12-16 10:34:40 TP0] Decode batch, #running-req: 16, #token: 61292, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.60, #queue-req: 0, 
[2025-12-16 10:34:44 TP0] Decode batch, #running-req: 16, #token: 61932, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.13, #queue-req: 0, 
[2025-12-16 10:34:48 TP0] Decode batch, #running-req: 16, #token: 62572, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.06, #queue-req: 0, 
[2025-12-16 10:34:52 TP0] Decode batch, #running-req: 16, #token: 63212, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.08, #queue-req: 0, 
[2025-12-16 10:34:55 TP0] Decode batch, #running-req: 16, #token: 63852, token usage: 0.07, cuda graph: True, gen throughput (token/s): 164.06, #queue-req: 0, 
[2025-12-16 10:34:56] INFO:     127.0.0.1:41702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.00, #running-req: 0, #queue-req: 7, 
[2025-12-16 10:34:56] INFO:     127.0.0.1:41818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:56] INFO:     127.0.0.1:41832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:34:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-12-16 10:34:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-12-16 10:35:04 TP0] Decode batch, #running-req: 16, #token: 51692, token usage: 0.05, cuda graph: True, gen throughput (token/s): 75.94, #queue-req: 0, 
[2025-12-16 10:35:08 TP0] Decode batch, #running-req: 16, #token: 52332, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.80, #queue-req: 0, 
[2025-12-16 10:35:12 TP0] Decode batch, #running-req: 16, #token: 52972, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.75, #queue-req: 0, 
[2025-12-16 10:35:15 TP0] Decode batch, #running-req: 16, #token: 53612, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.60, #queue-req: 0, 
[2025-12-16 10:35:19 TP0] Decode batch, #running-req: 16, #token: 54252, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.25, #queue-req: 0, 
[2025-12-16 10:35:23 TP0] Decode batch, #running-req: 16, #token: 54892, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.19, #queue-req: 0, 
[2025-12-16 10:35:27 TP0] Decode batch, #running-req: 16, #token: 55532, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.19, #queue-req: 0, 
[2025-12-16 10:35:31 TP0] Decode batch, #running-req: 16, #token: 56172, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.25, #queue-req: 0, 
[2025-12-16 10:35:35 TP0] Decode batch, #running-req: 16, #token: 56812, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.21, #queue-req: 0, 
[2025-12-16 10:35:39 TP0] Decode batch, #running-req: 16, #token: 57452, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.15, #queue-req: 0, 
[2025-12-16 10:35:43 TP0] Decode batch, #running-req: 16, #token: 58092, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.72, #queue-req: 0, 
[2025-12-16 10:35:47 TP0] Decode batch, #running-req: 16, #token: 58732, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.62, #queue-req: 0, 
[2025-12-16 10:35:50 TP0] Decode batch, #running-req: 16, #token: 59372, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.66, #queue-req: 0, 
[2025-12-16 10:35:54 TP0] Decode batch, #running-req: 16, #token: 60012, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.68, #queue-req: 0, 
[2025-12-16 10:35:58 TP0] Decode batch, #running-req: 16, #token: 60652, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.68, #queue-req: 0, 
[2025-12-16 10:36:02 TP0] Decode batch, #running-req: 16, #token: 61292, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.65, #queue-req: 0, 
[2025-12-16 10:36:06 TP0] Decode batch, #running-req: 16, #token: 61932, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.24, #queue-req: 0, 
[2025-12-16 10:36:10 TP0] Decode batch, #running-req: 16, #token: 62572, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.15, #queue-req: 0, 
[2025-12-16 10:36:14 TP0] Decode batch, #running-req: 16, #token: 63212, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.14, #queue-req: 0, 
[2025-12-16 10:36:18 TP0] Decode batch, #running-req: 16, #token: 63852, token usage: 0.07, cuda graph: True, gen throughput (token/s): 164.19, #queue-req: 0, 
[2025-12-16 10:36:18] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:18] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:18] INFO:     127.0.0.1:54212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:18] INFO:     127.0.0.1:54222 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:18] INFO:     127.0.0.1:54230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:18] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:18] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:18] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:18] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:18] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:19] INFO:     127.0.0.1:54324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:19] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:19] INFO:     127.0.0.1:54338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:19] INFO:     127.0.0.1:54352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 0, #queue-req: 7, 
[2025-12-16 10:36:19] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:19] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:36:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.02, #running-req: 5, #queue-req: 6, 
[2025-12-16 10:36:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.03, #running-req: 10, #queue-req: 1, 
[2025-12-16 10:36:22 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.05, #running-req: 15, #queue-req: 0, 
[2025-12-16 10:36:26 TP0] Decode batch, #running-req: 16, #token: 51694, token usage: 0.05, cuda graph: True, gen throughput (token/s): 72.31, #queue-req: 0, 
[2025-12-16 10:36:30 TP0] Decode batch, #running-req: 16, #token: 52334, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.49, #queue-req: 0, 
[2025-12-16 10:36:34 TP0] Decode batch, #running-req: 16, #token: 52974, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.45, #queue-req: 0, 
[2025-12-16 10:36:38 TP0] Decode batch, #running-req: 16, #token: 53614, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.26, #queue-req: 0, 
[2025-12-16 10:36:42 TP0] Decode batch, #running-req: 16, #token: 54254, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.02, #queue-req: 0, 
[2025-12-16 10:36:46 TP0] Decode batch, #running-req: 16, #token: 54894, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.95, #queue-req: 0, 
[2025-12-16 10:36:50 TP0] Decode batch, #running-req: 16, #token: 55534, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.95, #queue-req: 0, 
[2025-12-16 10:36:54 TP0] Decode batch, #running-req: 16, #token: 56174, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.96, #queue-req: 0, 
[2025-12-16 10:36:57 TP0] Decode batch, #running-req: 16, #token: 56814, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.95, #queue-req: 0, 
[2025-12-16 10:37:01 TP0] Decode batch, #running-req: 16, #token: 57454, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.85, #queue-req: 0, 
[2025-12-16 10:37:05 TP0] Decode batch, #running-req: 16, #token: 58094, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.42, #queue-req: 0, 
[2025-12-16 10:37:09 TP0] Decode batch, #running-req: 16, #token: 58734, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.42, #queue-req: 0, 
[2025-12-16 10:37:13 TP0] Decode batch, #running-req: 16, #token: 59374, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.42, #queue-req: 0, 
[2025-12-16 10:37:17 TP0] Decode batch, #running-req: 16, #token: 60014, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.41, #queue-req: 0, 
[2025-12-16 10:37:21 TP0] Decode batch, #running-req: 16, #token: 60654, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.42, #queue-req: 0, 
[2025-12-16 10:37:25 TP0] Decode batch, #running-req: 16, #token: 61294, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.45, #queue-req: 0, 
[2025-12-16 10:37:29 TP0] Decode batch, #running-req: 16, #token: 61934, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.92, #queue-req: 0, 
[2025-12-16 10:37:33 TP0] Decode batch, #running-req: 16, #token: 62574, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.81, #queue-req: 0, 
[2025-12-16 10:37:36 TP0] Decode batch, #running-req: 16, #token: 63214, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.85, #queue-req: 0, 
[2025-12-16 10:37:40 TP0] Decode batch, #running-req: 16, #token: 63854, token usage: 0.07, cuda graph: True, gen throughput (token/s): 163.88, #queue-req: 0, 
[2025-12-16 10:37:41] INFO:     127.0.0.1:43938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:43940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:43944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:43958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:43966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:43994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:44006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:44020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:44034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:44036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:44042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:44046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 0, #queue-req: 6, 
[2025-12-16 10:37:41] INFO:     127.0.0.1:44060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:41] INFO:     127.0.0.1:44070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:37:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 5, #queue-req: 6, 
[2025-12-16 10:37:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.03, #running-req: 10, #queue-req: 1, 
[2025-12-16 10:37:45 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.05, #running-req: 15, #queue-req: 0, 
[2025-12-16 10:37:49 TP0] Decode batch, #running-req: 16, #token: 51695, token usage: 0.05, cuda graph: True, gen throughput (token/s): 72.38, #queue-req: 0, 
[2025-12-16 10:37:53 TP0] Decode batch, #running-req: 16, #token: 52335, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.45, #queue-req: 0, 
[2025-12-16 10:37:57 TP0] Decode batch, #running-req: 16, #token: 52975, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.43, #queue-req: 0, 
[2025-12-16 10:38:01 TP0] Decode batch, #running-req: 16, #token: 53615, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.21, #queue-req: 0, 
[2025-12-16 10:38:05 TP0] Decode batch, #running-req: 16, #token: 54255, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.93, #queue-req: 0, 
[2025-12-16 10:38:09 TP0] Decode batch, #running-req: 16, #token: 54895, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.85, #queue-req: 0, 
[2025-12-16 10:38:12 TP0] Decode batch, #running-req: 16, #token: 55535, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.85, #queue-req: 0, 
[2025-12-16 10:38:16 TP0] Decode batch, #running-req: 16, #token: 56175, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.85, #queue-req: 0, 
[2025-12-16 10:38:20 TP0] Decode batch, #running-req: 16, #token: 56815, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.82, #queue-req: 0, 
[2025-12-16 10:38:24 TP0] Decode batch, #running-req: 16, #token: 57455, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.76, #queue-req: 0, 
[2025-12-16 10:38:28 TP0] Decode batch, #running-req: 16, #token: 58095, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.37, #queue-req: 0, 
[2025-12-16 10:38:32 TP0] Decode batch, #running-req: 16, #token: 58735, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.32, #queue-req: 0, 
[2025-12-16 10:38:36 TP0] Decode batch, #running-req: 16, #token: 59375, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.32, #queue-req: 0, 
[2025-12-16 10:38:40 TP0] Decode batch, #running-req: 16, #token: 60015, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.34, #queue-req: 0, 
[2025-12-16 10:38:44 TP0] Decode batch, #running-req: 16, #token: 60655, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.31, #queue-req: 0, 
[2025-12-16 10:38:47 TP0] Decode batch, #running-req: 16, #token: 61295, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.32, #queue-req: 0, 
[2025-12-16 10:38:51 TP0] Decode batch, #running-req: 16, #token: 61935, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.90, #queue-req: 0, 
[2025-12-16 10:38:55 TP0] Decode batch, #running-req: 16, #token: 62575, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.81, #queue-req: 0, 
[2025-12-16 10:38:59 TP0] Decode batch, #running-req: 16, #token: 63215, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.82, #queue-req: 0, 
[2025-12-16 10:39:03 TP0] Decode batch, #running-req: 16, #token: 63855, token usage: 0.07, cuda graph: True, gen throughput (token/s): 163.80, #queue-req: 0, 
[2025-12-16 10:39:04] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-16 10:39:04] INFO:     127.0.0.1:47574 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-16 10:39:04] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-16 10:39:04] INFO:     127.0.0.1:47584 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-16 10:39:15] INFO:     127.0.0.1:39072 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-16 10:39:21] INFO:     127.0.0.1:39086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:39:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:39:22 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 9.27, #queue-req: 0, 
[2025-12-16 10:39:23] INFO:     127.0.0.1:58950 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:39:23] INFO:     127.0.0.1:58952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:39:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:39:23] INFO:     127.0.0.1:58956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:39:23] INFO:     127.0.0.1:58970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:39:23 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-12-16 10:39:24 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 65.50, #queue-req: 0, 
[2025-12-16 10:39:25 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.89, #queue-req: 0, 
[2025-12-16 10:39:26 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.88, #queue-req: 0, 
[2025-12-16 10:39:28 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-16 10:39:29 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:39:30 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:39:31 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:39:32 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:39:33 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:39:34 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.48, #queue-req: 0, 
[2025-12-16 10:39:35 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-16 10:39:36 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:39:37 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:39:38 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:39:39 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-16 10:39:40 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-16 10:39:41 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.76, #queue-req: 0, 
[2025-12-16 10:39:42 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.74, #queue-req: 0, 
[2025-12-16 10:39:43 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.73, #queue-req: 0, 
[2025-12-16 10:39:44 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.55, #queue-req: 0, 
[2025-12-16 10:39:44] INFO:     127.0.0.1:39552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:39:44] INFO:     127.0.0.1:39562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:39:44] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:39:44] INFO:     127.0.0.1:39570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:39:44 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:39:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:39:46 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.87, #queue-req: 0, 
[2025-12-16 10:39:47 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.93, #queue-req: 0, 
[2025-12-16 10:39:48 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.87, #queue-req: 0, 
[2025-12-16 10:39:49 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-16 10:39:50 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:39:51 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:39:52 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:39:53 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:39:54 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:39:55 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.46, #queue-req: 0, 
[2025-12-16 10:39:56 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:39:57 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:39:59 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:40:00 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:40:01 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:40:02 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:40:03 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.72, #queue-req: 0, 
[2025-12-16 10:40:04 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.74, #queue-req: 0, 
[2025-12-16 10:40:05 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.73, #queue-req: 0, 
[2025-12-16 10:40:06 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.54, #queue-req: 0, 
[2025-12-16 10:40:06] INFO:     127.0.0.1:48716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:40:06] INFO:     127.0.0.1:48732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:40:06] INFO:     127.0.0.1:48740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:40:06] INFO:     127.0.0.1:48752 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:40:06 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:40:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:40:07 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.88, #queue-req: 0, 
[2025-12-16 10:40:08 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.89, #queue-req: 0, 
[2025-12-16 10:40:09 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.89, #queue-req: 0, 
[2025-12-16 10:40:11 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.75, #queue-req: 0, 
[2025-12-16 10:40:12 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:40:13 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:40:14 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:40:15 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:40:16 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-16 10:40:17 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.45, #queue-req: 0, 
[2025-12-16 10:40:18 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:40:19 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:40:20 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:40:21 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:40:22 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:40:23 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:40:24 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-16 10:40:25 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-16 10:40:26 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-16 10:40:27 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.50, #queue-req: 0, 
[2025-12-16 10:40:27] INFO:     127.0.0.1:37260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:40:27] INFO:     127.0.0.1:37268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:40:27] INFO:     127.0.0.1:37276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:40:28] INFO:     127.0.0.1:37284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:40:28 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:40:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:40:29 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.85, #queue-req: 0, 
[2025-12-16 10:40:30 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.81, #queue-req: 0, 
[2025-12-16 10:40:31 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.99, #queue-req: 0, 
[2025-12-16 10:40:32 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.77, #queue-req: 0, 
[2025-12-16 10:40:33 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:40:34 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:40:35 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:40:36 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:40:37 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:40:38 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.45, #queue-req: 0, 
[2025-12-16 10:40:39 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:40:40 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:40:42 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-16 10:40:43 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:40:44 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:40:45 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:40:46 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.68, #queue-req: 0, 
[2025-12-16 10:40:47 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-16 10:40:48 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.69, #queue-req: 0, 
[2025-12-16 10:40:49 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.50, #queue-req: 0, 
[2025-12-16 10:40:49] INFO:     127.0.0.1:40854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:40:49] INFO:     127.0.0.1:40868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:40:49] INFO:     127.0.0.1:40884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:40:49] INFO:     127.0.0.1:40888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:40:49 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:40:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:40:50 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.86, #queue-req: 0, 
[2025-12-16 10:40:51 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.88, #queue-req: 0, 
[2025-12-16 10:40:52 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.86, #queue-req: 0, 
[2025-12-16 10:40:54 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-16 10:40:55 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.52, #queue-req: 0, 
[2025-12-16 10:40:56 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:40:57 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-16 10:40:58 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:40:59 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.53, #queue-req: 0, 
[2025-12-16 10:41:00 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.44, #queue-req: 0, 
[2025-12-16 10:41:01 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:41:02 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:41:03 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:41:04 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:41:05 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-16 10:41:06 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:41:07 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.72, #queue-req: 0, 
[2025-12-16 10:41:08 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.72, #queue-req: 0, 
[2025-12-16 10:41:09 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.73, #queue-req: 0, 
[2025-12-16 10:41:10 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.52, #queue-req: 0, 
[2025-12-16 10:41:10] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:41:10] INFO:     127.0.0.1:59472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:41:11] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:41:11] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:41:11 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:41:11 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-16 10:41:12 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.80, #queue-req: 0, 
[2025-12-16 10:41:13 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-16 10:41:14 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-16 10:41:15 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-16 10:41:16 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:41:17 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:41:18 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:41:19 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:41:20 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:41:21 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.47, #queue-req: 0, 
[2025-12-16 10:41:22 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:41:24 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:41:25 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-16 10:41:26 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:41:27 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:41:28 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-16 10:41:29 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.73, #queue-req: 0, 
[2025-12-16 10:41:30 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.73, #queue-req: 0, 
[2025-12-16 10:41:31 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.73, #queue-req: 0, 
[2025-12-16 10:41:32 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.55, #queue-req: 0, 
[2025-12-16 10:41:32] INFO:     127.0.0.1:33740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:41:32] INFO:     127.0.0.1:33752 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:41:32] INFO:     127.0.0.1:33758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:41:32] INFO:     127.0.0.1:33762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:41:32 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:41:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:41:33 TP0] Decode batch, #running-req: 4, #token: 12956, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.73, #queue-req: 0, 
[2025-12-16 10:41:34 TP0] Decode batch, #running-req: 4, #token: 13116, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-16 10:41:36 TP0] Decode batch, #running-req: 4, #token: 13276, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.93, #queue-req: 0, 
[2025-12-16 10:41:37 TP0] Decode batch, #running-req: 4, #token: 13436, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.76, #queue-req: 0, 
[2025-12-16 10:41:38 TP0] Decode batch, #running-req: 4, #token: 13596, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:41:39 TP0] Decode batch, #running-req: 4, #token: 13756, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:41:40 TP0] Decode batch, #running-req: 4, #token: 13916, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:41:41 TP0] Decode batch, #running-req: 4, #token: 14076, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:41:42 TP0] Decode batch, #running-req: 4, #token: 14236, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:41:43 TP0] Decode batch, #running-req: 4, #token: 14396, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.47, #queue-req: 0, 
[2025-12-16 10:41:44 TP0] Decode batch, #running-req: 4, #token: 14556, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-16 10:41:45 TP0] Decode batch, #running-req: 4, #token: 14716, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:41:46 TP0] Decode batch, #running-req: 4, #token: 14876, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:41:47 TP0] Decode batch, #running-req: 4, #token: 15036, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-16 10:41:48 TP0] Decode batch, #running-req: 4, #token: 15196, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-16 10:41:49 TP0] Decode batch, #running-req: 4, #token: 15356, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:41:50 TP0] Decode batch, #running-req: 4, #token: 15516, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.74, #queue-req: 0, 
[2025-12-16 10:41:51 TP0] Decode batch, #running-req: 4, #token: 15676, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.76, #queue-req: 0, 
[2025-12-16 10:41:52 TP0] Decode batch, #running-req: 4, #token: 15836, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.74, #queue-req: 0, 
[2025-12-16 10:41:53 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.55, #queue-req: 0, 
[2025-12-16 10:41:53] INFO:     127.0.0.1:46810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:41:53] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:41:54] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:41:54] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:41:54 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:41:54 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:41:55 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.88, #queue-req: 0, 
[2025-12-16 10:41:56 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.95, #queue-req: 0, 
[2025-12-16 10:41:57 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-16 10:41:58 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-16 10:41:59 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:42:00 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:42:01 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:42:02 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:42:03 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:42:04 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.46, #queue-req: 0, 
[2025-12-16 10:42:05 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:42:07 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:42:08 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:42:09 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:42:10 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:42:11 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.06, #queue-req: 0, 
[2025-12-16 10:42:12 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.69, #queue-req: 0, 
[2025-12-16 10:42:13 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.71, #queue-req: 0, 
[2025-12-16 10:42:14 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.69, #queue-req: 0, 
[2025-12-16 10:42:15 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.50, #queue-req: 0, 
[2025-12-16 10:42:15] INFO:     127.0.0.1:48110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:42:15] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:42:15] INFO:     127.0.0.1:48120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:42:15] INFO:     127.0.0.1:48134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:42:15 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:42:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:42:16 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.80, #queue-req: 0, 
[2025-12-16 10:42:17 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.00, #queue-req: 0, 
[2025-12-16 10:42:19 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.02, #queue-req: 0, 
[2025-12-16 10:42:20 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.83, #queue-req: 0, 
[2025-12-16 10:42:21 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:42:22 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-16 10:42:23 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-16 10:42:24 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-16 10:42:25 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-16 10:42:26 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.52, #queue-req: 0, 
[2025-12-16 10:42:27 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.15, #queue-req: 0, 
[2025-12-16 10:42:28 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:42:29 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.15, #queue-req: 0, 
[2025-12-16 10:42:30 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:42:31 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:42:32 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-16 10:42:33 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.72, #queue-req: 0, 
[2025-12-16 10:42:34 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.73, #queue-req: 0, 
[2025-12-16 10:42:35 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.72, #queue-req: 0, 
[2025-12-16 10:42:36 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.56, #queue-req: 0, 
[2025-12-16 10:42:36] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:42:36] INFO:     127.0.0.1:47432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:42:37] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:42:37] INFO:     127.0.0.1:47456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:42:37 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:42:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:42:38 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.79, #queue-req: 0, 
[2025-12-16 10:42:39 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.96, #queue-req: 0, 
[2025-12-16 10:42:40 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-16 10:42:41 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-16 10:42:42 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-16 10:42:43 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:42:44 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:42:45 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:42:46 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:42:47 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.45, #queue-req: 0, 
[2025-12-16 10:42:48 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-16 10:42:50 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-16 10:42:51 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:42:52 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:42:53 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-16 10:42:54 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:42:55 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.72, #queue-req: 0, 
[2025-12-16 10:42:56 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.75, #queue-req: 0, 
[2025-12-16 10:42:57 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.73, #queue-req: 0, 
[2025-12-16 10:42:58 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.53, #queue-req: 0, 
[2025-12-16 10:42:58] INFO:     127.0.0.1:44422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:42:58] INFO:     127.0.0.1:44428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:42:58] INFO:     127.0.0.1:44430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:42:58] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:42:58 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:42:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:42:59 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.20, #queue-req: 0, 
[2025-12-16 10:43:00 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 150.29, #queue-req: 0, 
[2025-12-16 10:43:02 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-16 10:43:03 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-16 10:43:04 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:43:05 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:43:06 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:43:07 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:43:08 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:43:09 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.49, #queue-req: 0, 
[2025-12-16 10:43:10 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-16 10:43:11 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:43:12 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-16 10:43:13 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:43:14 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:43:15 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:43:16 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.81, #queue-req: 0, 
[2025-12-16 10:43:17 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.78, #queue-req: 0, 
[2025-12-16 10:43:18 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.74, #queue-req: 0, 
[2025-12-16 10:43:20 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.50, #queue-req: 0, 
[2025-12-16 10:43:20] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:43:20] INFO:     127.0.0.1:50208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:43:20] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:43:20] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:43:20 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:43:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:43:21 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.84, #queue-req: 0, 
[2025-12-16 10:43:22 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.98, #queue-req: 0, 
[2025-12-16 10:43:23 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-16 10:43:24 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.77, #queue-req: 0, 
[2025-12-16 10:43:25 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:43:26 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:43:27 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:43:28 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:43:29 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:43:30 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.47, #queue-req: 0, 
[2025-12-16 10:43:31 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:43:33 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:43:34 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:43:35 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-16 10:43:36 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:43:37 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:43:38 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.71, #queue-req: 0, 
[2025-12-16 10:43:39 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.73, #queue-req: 0, 
[2025-12-16 10:43:40 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.71, #queue-req: 0, 
[2025-12-16 10:43:41 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.51, #queue-req: 0, 
[2025-12-16 10:43:41] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:43:41] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:43:41] INFO:     127.0.0.1:50240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:43:41] INFO:     127.0.0.1:50250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:43:41 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:43:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:43:42 TP0] Decode batch, #running-req: 4, #token: 12957, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.26, #queue-req: 0, 
[2025-12-16 10:43:43 TP0] Decode batch, #running-req: 4, #token: 13117, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.97, #queue-req: 0, 
[2025-12-16 10:43:45 TP0] Decode batch, #running-req: 4, #token: 13277, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.94, #queue-req: 0, 
[2025-12-16 10:43:46 TP0] Decode batch, #running-req: 4, #token: 13437, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.80, #queue-req: 0, 
[2025-12-16 10:43:47 TP0] Decode batch, #running-req: 4, #token: 13597, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:43:48 TP0] Decode batch, #running-req: 4, #token: 13757, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-16 10:43:49 TP0] Decode batch, #running-req: 4, #token: 13917, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:43:50 TP0] Decode batch, #running-req: 4, #token: 14077, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:43:51 TP0] Decode batch, #running-req: 4, #token: 14237, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:43:52 TP0] Decode batch, #running-req: 4, #token: 14397, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.50, #queue-req: 0, 
[2025-12-16 10:43:53 TP0] Decode batch, #running-req: 4, #token: 14557, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-16 10:43:54 TP0] Decode batch, #running-req: 4, #token: 14717, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-16 10:43:55 TP0] Decode batch, #running-req: 4, #token: 14877, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-16 10:43:56 TP0] Decode batch, #running-req: 4, #token: 15037, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-16 10:43:57 TP0] Decode batch, #running-req: 4, #token: 15197, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-16 10:43:58 TP0] Decode batch, #running-req: 4, #token: 15357, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-16 10:43:59 TP0] Decode batch, #running-req: 4, #token: 15517, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.75, #queue-req: 0, 
[2025-12-16 10:44:00 TP0] Decode batch, #running-req: 4, #token: 15677, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.76, #queue-req: 0, 
[2025-12-16 10:44:01 TP0] Decode batch, #running-req: 4, #token: 15837, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.75, #queue-req: 0, 
[2025-12-16 10:44:03 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.57, #queue-req: 0, 
[2025-12-16 10:44:03] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:44:03] INFO:     127.0.0.1:54128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:44:03] INFO:     127.0.0.1:54144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:44:03] INFO:     127.0.0.1:54158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:44:03 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:44:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:44:04 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.97, #queue-req: 0, 
[2025-12-16 10:44:05 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-16 10:44:06 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-16 10:44:07 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-16 10:44:08 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:44:09 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:44:10 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:44:11 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:44:12 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:44:13 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.43, #queue-req: 0, 
[2025-12-16 10:44:14 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:44:16 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:44:17 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:44:18 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:44:19 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.06, #queue-req: 0, 
[2025-12-16 10:44:20 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:44:21 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.71, #queue-req: 0, 
[2025-12-16 10:44:22 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.72, #queue-req: 0, 
[2025-12-16 10:44:23 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.73, #queue-req: 0, 
[2025-12-16 10:44:24 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.54, #queue-req: 0, 
[2025-12-16 10:44:24] INFO:     127.0.0.1:33136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:44:24] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:44:24] INFO:     127.0.0.1:33164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:44:24] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:44:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:44:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-16 10:44:25 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 114.03, #queue-req: 0, 
[2025-12-16 10:44:26 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-16 10:44:28 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.88, #queue-req: 0, 
[2025-12-16 10:44:29 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-16 10:44:30 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:44:31 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:44:32 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:44:33 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:44:34 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:44:35 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.43, #queue-req: 0, 
[2025-12-16 10:44:36 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:44:37 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:44:38 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:44:39 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:44:40 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:44:41 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:44:42 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.71, #queue-req: 0, 
[2025-12-16 10:44:43 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.74, #queue-req: 0, 
[2025-12-16 10:44:44 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.74, #queue-req: 0, 
[2025-12-16 10:44:46 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.58, #queue-req: 0, 
[2025-12-16 10:44:46] INFO:     127.0.0.1:42700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:44:46] INFO:     127.0.0.1:42710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:44:46] INFO:     127.0.0.1:42716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:44:46] INFO:     127.0.0.1:42724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:44:46 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:44:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:44:47 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.87, #queue-req: 0, 
[2025-12-16 10:44:48 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.85, #queue-req: 0, 
[2025-12-16 10:44:49 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.83, #queue-req: 0, 
[2025-12-16 10:44:50 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-16 10:44:51 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.50, #queue-req: 0, 
[2025-12-16 10:44:52 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.51, #queue-req: 0, 
[2025-12-16 10:44:53 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.49, #queue-req: 0, 
[2025-12-16 10:44:54 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.49, #queue-req: 0, 
[2025-12-16 10:44:55 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.48, #queue-req: 0, 
[2025-12-16 10:44:56 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.33, #queue-req: 0, 
[2025-12-16 10:44:57 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.00, #queue-req: 0, 
[2025-12-16 10:44:59 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.02, #queue-req: 0, 
[2025-12-16 10:45:00 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.99, #queue-req: 0, 
[2025-12-16 10:45:01 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.02, #queue-req: 0, 
[2025-12-16 10:45:02 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.03, #queue-req: 0, 
[2025-12-16 10:45:03 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.01, #queue-req: 0, 
[2025-12-16 10:45:04 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.60, #queue-req: 0, 
[2025-12-16 10:45:05 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.57, #queue-req: 0, 
[2025-12-16 10:45:06 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.57, #queue-req: 0, 
[2025-12-16 10:45:07 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.40, #queue-req: 0, 
[2025-12-16 10:45:07] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:45:07] INFO:     127.0.0.1:59048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:45:07] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:45:07] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:45:07 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:45:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:45:08 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 114.07, #queue-req: 0, 
[2025-12-16 10:45:09 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.87, #queue-req: 0, 
[2025-12-16 10:45:11 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.85, #queue-req: 0, 
[2025-12-16 10:45:12 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-16 10:45:13 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-16 10:45:14 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:45:15 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:45:16 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:45:17 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:45:18 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.44, #queue-req: 0, 
[2025-12-16 10:45:19 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-16 10:45:20 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.06, #queue-req: 0, 
[2025-12-16 10:45:21 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.06, #queue-req: 0, 
[2025-12-16 10:45:22 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.06, #queue-req: 0, 
[2025-12-16 10:45:23 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.06, #queue-req: 0, 
[2025-12-16 10:45:24 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:45:25 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.65, #queue-req: 0, 
[2025-12-16 10:45:26 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.66, #queue-req: 0, 
[2025-12-16 10:45:27 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.66, #queue-req: 0, 
[2025-12-16 10:45:29 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.46, #queue-req: 0, 
[2025-12-16 10:45:29] INFO:     127.0.0.1:48378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:45:29] INFO:     127.0.0.1:48384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:45:29] INFO:     127.0.0.1:48392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:45:29] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:45:29 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:45:29 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-16 10:45:30 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 114.07, #queue-req: 0, 
[2025-12-16 10:45:31 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.99, #queue-req: 0, 
[2025-12-16 10:45:32 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.94, #queue-req: 0, 
[2025-12-16 10:45:33 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.79, #queue-req: 0, 
[2025-12-16 10:45:34 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:45:35 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:45:36 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:45:37 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:45:38 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-16 10:45:39 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.52, #queue-req: 0, 
[2025-12-16 10:45:40 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-16 10:45:42 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-16 10:45:43 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.25, #queue-req: 0, 
[2025-12-16 10:45:44 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.26, #queue-req: 0, 
[2025-12-16 10:45:45 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.23, #queue-req: 0, 
[2025-12-16 10:45:46 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.28, #queue-req: 0, 
[2025-12-16 10:45:47 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-16 10:45:48 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.87, #queue-req: 0, 
[2025-12-16 10:45:49 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.85, #queue-req: 0, 
[2025-12-16 10:45:50 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.68, #queue-req: 0, 
[2025-12-16 10:45:50] INFO:     127.0.0.1:44110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:45:50] INFO:     127.0.0.1:44122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:45:50] INFO:     127.0.0.1:44138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:45:50] INFO:     127.0.0.1:44150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:45:50 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:45:50 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-16 10:45:51 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 114.06, #queue-req: 0, 
[2025-12-16 10:45:52 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.90, #queue-req: 0, 
[2025-12-16 10:45:54 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-16 10:45:55 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.76, #queue-req: 0, 
[2025-12-16 10:45:56 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:45:57 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-16 10:45:58 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:45:59 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-16 10:46:00 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:46:01 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.46, #queue-req: 0, 
[2025-12-16 10:46:02 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:46:03 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:46:04 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:46:05 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-16 10:46:06 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:46:07 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:46:08 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.73, #queue-req: 0, 
[2025-12-16 10:46:09 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.76, #queue-req: 0, 
[2025-12-16 10:46:10 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.75, #queue-req: 0, 
[2025-12-16 10:46:12 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.57, #queue-req: 0, 
[2025-12-16 10:46:12] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:46:12] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:46:12] INFO:     127.0.0.1:50206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:46:12] INFO:     127.0.0.1:50222 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:46:12 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:46:12 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-16 10:46:13 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 114.20, #queue-req: 0, 
[2025-12-16 10:46:14 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.97, #queue-req: 0, 
[2025-12-16 10:46:15 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.96, #queue-req: 0, 
[2025-12-16 10:46:16 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.80, #queue-req: 0, 
[2025-12-16 10:46:17 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-16 10:46:18 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:46:19 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:46:20 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:46:21 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:46:22 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.47, #queue-req: 0, 
[2025-12-16 10:46:23 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:46:25 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-16 10:46:26 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-16 10:46:27 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:46:28 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-16 10:46:29 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:46:30 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-16 10:46:31 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-16 10:46:32 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-16 10:46:33 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.50, #queue-req: 0, 
[2025-12-16 10:46:33] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:46:33] INFO:     127.0.0.1:33112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:46:33] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:46:33] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:46:33 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:46:33 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-16 10:46:34 TP0] Decode batch, #running-req: 4, #token: 12957, token usage: 0.01, cuda graph: True, gen throughput (token/s): 114.07, #queue-req: 0, 
[2025-12-16 10:46:35 TP0] Decode batch, #running-req: 4, #token: 13117, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.99, #queue-req: 0, 
[2025-12-16 10:46:37 TP0] Decode batch, #running-req: 4, #token: 13277, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.00, #queue-req: 0, 
[2025-12-16 10:46:38 TP0] Decode batch, #running-req: 4, #token: 13437, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.84, #queue-req: 0, 
[2025-12-16 10:46:39 TP0] Decode batch, #running-req: 4, #token: 13597, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-16 10:46:40 TP0] Decode batch, #running-req: 4, #token: 13757, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-16 10:46:41 TP0] Decode batch, #running-req: 4, #token: 13917, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-16 10:46:42 TP0] Decode batch, #running-req: 4, #token: 14077, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-16 10:46:43 TP0] Decode batch, #running-req: 4, #token: 14237, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:46:44 TP0] Decode batch, #running-req: 4, #token: 14397, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.52, #queue-req: 0, 
[2025-12-16 10:46:45 TP0] Decode batch, #running-req: 4, #token: 14557, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-16 10:46:46 TP0] Decode batch, #running-req: 4, #token: 14717, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-16 10:46:47 TP0] Decode batch, #running-req: 4, #token: 14877, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-16 10:46:48 TP0] Decode batch, #running-req: 4, #token: 15037, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-16 10:46:49 TP0] Decode batch, #running-req: 4, #token: 15197, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-16 10:46:50 TP0] Decode batch, #running-req: 4, #token: 15357, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-16 10:46:51 TP0] Decode batch, #running-req: 4, #token: 15517, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.79, #queue-req: 0, 
[2025-12-16 10:46:52 TP0] Decode batch, #running-req: 4, #token: 15677, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.83, #queue-req: 0, 
[2025-12-16 10:46:53 TP0] Decode batch, #running-req: 4, #token: 15837, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.84, #queue-req: 0, 
[2025-12-16 10:46:54 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.67, #queue-req: 0, 
[2025-12-16 10:46:54] INFO:     127.0.0.1:34348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:46:55] INFO:     127.0.0.1:34356 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:46:55] INFO:     127.0.0.1:34366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:46:55] INFO:     127.0.0.1:34372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:46:55 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:46:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:46:56 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.89, #queue-req: 0, 
[2025-12-16 10:46:57 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.93, #queue-req: 0, 
[2025-12-16 10:46:58 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-16 10:46:59 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.77, #queue-req: 0, 
[2025-12-16 10:47:00 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:47:01 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:47:02 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:47:03 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-16 10:47:04 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:47:05 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.50, #queue-req: 0, 
[2025-12-16 10:47:06 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.15, #queue-req: 0, 
[2025-12-16 10:47:08 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:47:09 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-16 10:47:10 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-16 10:47:11 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-16 10:47:12 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-16 10:47:13 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.76, #queue-req: 0, 
[2025-12-16 10:47:14 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.76, #queue-req: 0, 
[2025-12-16 10:47:15 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.76, #queue-req: 0, 
[2025-12-16 10:47:16 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.57, #queue-req: 0, 
[2025-12-16 10:47:16] INFO:     127.0.0.1:42940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:47:16] INFO:     127.0.0.1:42954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:47:16] INFO:     127.0.0.1:42970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:47:16] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:47:16 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:47:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:47:17 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 114.00, #queue-req: 0, 
[2025-12-16 10:47:18 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-16 10:47:19 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.89, #queue-req: 0, 
[2025-12-16 10:47:21 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-16 10:47:22 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-16 10:47:23 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:47:24 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:47:25 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-16 10:47:26 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:47:27 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.41, #queue-req: 0, 
[2025-12-16 10:47:28 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-16 10:47:29 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-16 10:47:30 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-16 10:47:31 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-16 10:47:32 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-16 10:47:33 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.05, #queue-req: 0, 
[2025-12-16 10:47:34 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.68, #queue-req: 0, 
[2025-12-16 10:47:35 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.68, #queue-req: 0, 
[2025-12-16 10:47:36 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.69, #queue-req: 0, 
[2025-12-16 10:47:37 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.50, #queue-req: 0, 
[2025-12-16 10:47:37] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:47:38] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:47:38] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:47:38] INFO:     127.0.0.1:55434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:47:38 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:47:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:47:39 TP0] Decode batch, #running-req: 4, #token: 12957, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.92, #queue-req: 0, 
[2025-12-16 10:47:40 TP0] Decode batch, #running-req: 4, #token: 13117, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.95, #queue-req: 0, 
[2025-12-16 10:47:41 TP0] Decode batch, #running-req: 4, #token: 13277, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.90, #queue-req: 0, 
[2025-12-16 10:47:42 TP0] Decode batch, #running-req: 4, #token: 13437, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.75, #queue-req: 0, 
[2025-12-16 10:47:43 TP0] Decode batch, #running-req: 4, #token: 13597, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:47:44 TP0] Decode batch, #running-req: 4, #token: 13757, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:47:45 TP0] Decode batch, #running-req: 4, #token: 13917, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:47:46 TP0] Decode batch, #running-req: 4, #token: 14077, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:47:47 TP0] Decode batch, #running-req: 4, #token: 14237, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:47:48 TP0] Decode batch, #running-req: 4, #token: 14397, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.44, #queue-req: 0, 
[2025-12-16 10:47:49 TP0] Decode batch, #running-req: 4, #token: 14557, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-16 10:47:51 TP0] Decode batch, #running-req: 4, #token: 14717, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:47:52 TP0] Decode batch, #running-req: 4, #token: 14877, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:47:53 TP0] Decode batch, #running-req: 4, #token: 15037, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:47:54 TP0] Decode batch, #running-req: 4, #token: 15197, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:47:55 TP0] Decode batch, #running-req: 4, #token: 15357, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:47:56 TP0] Decode batch, #running-req: 4, #token: 15517, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.69, #queue-req: 0, 
[2025-12-16 10:47:57 TP0] Decode batch, #running-req: 4, #token: 15677, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.71, #queue-req: 0, 
[2025-12-16 10:47:58 TP0] Decode batch, #running-req: 4, #token: 15837, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-16 10:47:59 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.52, #queue-req: 0, 
[2025-12-16 10:47:59] INFO:     127.0.0.1:32912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:47:59] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:47:59] INFO:     127.0.0.1:32918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:47:59] INFO:     127.0.0.1:32928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:47:59 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:47:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:48:00 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.95, #queue-req: 0, 
[2025-12-16 10:48:01 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.87, #queue-req: 0, 
[2025-12-16 10:48:03 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.87, #queue-req: 0, 
[2025-12-16 10:48:04 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-16 10:48:05 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.51, #queue-req: 0, 
[2025-12-16 10:48:06 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:48:07 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-16 10:48:08 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.52, #queue-req: 0, 
[2025-12-16 10:48:09 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.52, #queue-req: 0, 
[2025-12-16 10:48:10 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.44, #queue-req: 0, 
[2025-12-16 10:48:11 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:48:12 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:48:13 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:48:14 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-16 10:48:15 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:48:16 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:48:17 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.67, #queue-req: 0, 
[2025-12-16 10:48:18 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.68, #queue-req: 0, 
[2025-12-16 10:48:19 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.68, #queue-req: 0, 
[2025-12-16 10:48:20 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.49, #queue-req: 0, 
[2025-12-16 10:48:21] INFO:     127.0.0.1:41910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:48:21] INFO:     127.0.0.1:41924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:48:21] INFO:     127.0.0.1:41936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:48:21] INFO:     127.0.0.1:41942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:48:21 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:48:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:48:22 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.93, #queue-req: 0, 
[2025-12-16 10:48:23 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.87, #queue-req: 0, 
[2025-12-16 10:48:24 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.86, #queue-req: 0, 
[2025-12-16 10:48:25 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.67, #queue-req: 0, 
[2025-12-16 10:48:26 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.53, #queue-req: 0, 
[2025-12-16 10:48:27 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-16 10:48:28 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-16 10:48:29 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-16 10:48:30 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-16 10:48:31 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.40, #queue-req: 0, 
[2025-12-16 10:48:32 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.03, #queue-req: 0, 
[2025-12-16 10:48:34 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:48:35 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.06, #queue-req: 0, 
[2025-12-16 10:48:36 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.04, #queue-req: 0, 
[2025-12-16 10:48:37 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.02, #queue-req: 0, 
[2025-12-16 10:48:38 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.04, #queue-req: 0, 
[2025-12-16 10:48:39 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.67, #queue-req: 0, 
[2025-12-16 10:48:40 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.69, #queue-req: 0, 
[2025-12-16 10:48:41 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.69, #queue-req: 0, 
[2025-12-16 10:48:42 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.49, #queue-req: 0, 
[2025-12-16 10:48:42] INFO:     127.0.0.1:38832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:48:42] INFO:     127.0.0.1:38836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:48:42] INFO:     127.0.0.1:38840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:48:42] INFO:     127.0.0.1:38852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:48:42 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:48:42 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:48:43 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 114.05, #queue-req: 0, 
[2025-12-16 10:48:44 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.82, #queue-req: 0, 
[2025-12-16 10:48:46 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.79, #queue-req: 0, 
[2025-12-16 10:48:47 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-16 10:48:48 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.47, #queue-req: 0, 
[2025-12-16 10:48:49 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.45, #queue-req: 0, 
[2025-12-16 10:48:50 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.44, #queue-req: 0, 
[2025-12-16 10:48:51 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.46, #queue-req: 0, 
[2025-12-16 10:48:52 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.46, #queue-req: 0, 
[2025-12-16 10:48:53 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.34, #queue-req: 0, 
[2025-12-16 10:48:54 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 150.99, #queue-req: 0, 
[2025-12-16 10:48:55 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.01, #queue-req: 0, 
[2025-12-16 10:48:56 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.00, #queue-req: 0, 
[2025-12-16 10:48:57 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.00, #queue-req: 0, 
[2025-12-16 10:48:58 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.00, #queue-req: 0, 
[2025-12-16 10:48:59 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.99, #queue-req: 0, 
[2025-12-16 10:49:00 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.59, #queue-req: 0, 
[2025-12-16 10:49:01 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.57, #queue-req: 0, 
[2025-12-16 10:49:02 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.59, #queue-req: 0, 
[2025-12-16 10:49:04 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.43, #queue-req: 0, 
[2025-12-16 10:49:04] INFO:     127.0.0.1:43760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:49:04] INFO:     127.0.0.1:43766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:49:04] INFO:     127.0.0.1:43780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:49:04] INFO:     127.0.0.1:43788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:49:04 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:49:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:49:05 TP0] Decode batch, #running-req: 4, #token: 12957, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.97, #queue-req: 0, 
[2025-12-16 10:49:06 TP0] Decode batch, #running-req: 4, #token: 13117, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.86, #queue-req: 0, 
[2025-12-16 10:49:07 TP0] Decode batch, #running-req: 4, #token: 13277, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.87, #queue-req: 0, 
[2025-12-16 10:49:08 TP0] Decode batch, #running-req: 4, #token: 13437, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-16 10:49:09 TP0] Decode batch, #running-req: 4, #token: 13597, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-16 10:49:10 TP0] Decode batch, #running-req: 4, #token: 13757, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:49:11 TP0] Decode batch, #running-req: 4, #token: 13917, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-16 10:49:12 TP0] Decode batch, #running-req: 4, #token: 14077, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:49:13 TP0] Decode batch, #running-req: 4, #token: 14237, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.53, #queue-req: 0, 
[2025-12-16 10:49:14 TP0] Decode batch, #running-req: 4, #token: 14397, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.45, #queue-req: 0, 
[2025-12-16 10:49:15 TP0] Decode batch, #running-req: 4, #token: 14557, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:49:17 TP0] Decode batch, #running-req: 4, #token: 14717, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:49:18 TP0] Decode batch, #running-req: 4, #token: 14877, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:49:19 TP0] Decode batch, #running-req: 4, #token: 15037, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:49:20 TP0] Decode batch, #running-req: 4, #token: 15197, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:49:21 TP0] Decode batch, #running-req: 4, #token: 15357, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.06, #queue-req: 0, 
[2025-12-16 10:49:22 TP0] Decode batch, #running-req: 4, #token: 15517, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.72, #queue-req: 0, 
[2025-12-16 10:49:23 TP0] Decode batch, #running-req: 4, #token: 15677, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-16 10:49:24 TP0] Decode batch, #running-req: 4, #token: 15837, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-16 10:49:25 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.53, #queue-req: 0, 
[2025-12-16 10:49:25] INFO:     127.0.0.1:39760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:49:25] INFO:     127.0.0.1:39766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:49:25] INFO:     127.0.0.1:39772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:49:25 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:49:25] INFO:     127.0.0.1:39780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:49:25 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-16 10:49:26 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 114.03, #queue-req: 0, 
[2025-12-16 10:49:27 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.89, #queue-req: 0, 
[2025-12-16 10:49:29 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.89, #queue-req: 0, 
[2025-12-16 10:49:30 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-16 10:49:31 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.53, #queue-req: 0, 
[2025-12-16 10:49:32 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:49:33 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:49:34 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:49:35 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:49:36 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.46, #queue-req: 0, 
[2025-12-16 10:49:37 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:49:38 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:49:39 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:49:40 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:49:41 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-16 10:49:42 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:49:43 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-16 10:49:44 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.71, #queue-req: 0, 
[2025-12-16 10:49:45 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-16 10:49:47 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.53, #queue-req: 0, 
[2025-12-16 10:49:47] INFO:     127.0.0.1:51066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:49:47] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:49:47] INFO:     127.0.0.1:51080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:49:47] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:49:47 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:49:47 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-16 10:49:48 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 114.15, #queue-req: 0, 
[2025-12-16 10:49:49 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.86, #queue-req: 0, 
[2025-12-16 10:49:50 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.87, #queue-req: 0, 
[2025-12-16 10:49:51 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-16 10:49:52 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-16 10:49:53 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:49:54 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:49:55 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-16 10:49:56 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-16 10:49:57 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.44, #queue-req: 0, 
[2025-12-16 10:49:58 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:50:00 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:50:01 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-16 10:50:02 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:50:03 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:50:04 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:50:05 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.69, #queue-req: 0, 
[2025-12-16 10:50:06 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.70, #queue-req: 0, 
[2025-12-16 10:50:07 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.72, #queue-req: 0, 
[2025-12-16 10:50:08 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.53, #queue-req: 0, 
[2025-12-16 10:50:08] INFO:     127.0.0.1:45038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:50:08] INFO:     127.0.0.1:45054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:50:08] INFO:     127.0.0.1:45068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:50:08] INFO:     127.0.0.1:45078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:50:08 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:50:08 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-16 10:50:09 TP0] Decode batch, #running-req: 4, #token: 12958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 114.11, #queue-req: 0, 
[2025-12-16 10:50:10 TP0] Decode batch, #running-req: 4, #token: 13118, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.89, #queue-req: 0, 
[2025-12-16 10:50:12 TP0] Decode batch, #running-req: 4, #token: 13278, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.88, #queue-req: 0, 
[2025-12-16 10:50:13 TP0] Decode batch, #running-req: 4, #token: 13438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-16 10:50:14 TP0] Decode batch, #running-req: 4, #token: 13598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.53, #queue-req: 0, 
[2025-12-16 10:50:15 TP0] Decode batch, #running-req: 4, #token: 13758, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.52, #queue-req: 0, 
[2025-12-16 10:50:16 TP0] Decode batch, #running-req: 4, #token: 13918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.53, #queue-req: 0, 
[2025-12-16 10:50:17 TP0] Decode batch, #running-req: 4, #token: 14078, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.52, #queue-req: 0, 
[2025-12-16 10:50:18 TP0] Decode batch, #running-req: 4, #token: 14238, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-16 10:50:19 TP0] Decode batch, #running-req: 4, #token: 14398, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.40, #queue-req: 0, 
[2025-12-16 10:50:20 TP0] Decode batch, #running-req: 4, #token: 14558, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:50:21 TP0] Decode batch, #running-req: 4, #token: 14718, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-16 10:50:22 TP0] Decode batch, #running-req: 4, #token: 14878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:50:23 TP0] Decode batch, #running-req: 4, #token: 15038, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-16 10:50:24 TP0] Decode batch, #running-req: 4, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-16 10:50:25 TP0] Decode batch, #running-req: 4, #token: 15358, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-16 10:50:26 TP0] Decode batch, #running-req: 4, #token: 15518, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.68, #queue-req: 0, 
[2025-12-16 10:50:27 TP0] Decode batch, #running-req: 4, #token: 15678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.69, #queue-req: 0, 
[2025-12-16 10:50:28 TP0] Decode batch, #running-req: 4, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.68, #queue-req: 0, 
[2025-12-16 10:50:30 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.52, #queue-req: 0, 
[2025-12-16 10:50:30] INFO:     127.0.0.1:38086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:50:30] INFO:     127.0.0.1:38098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:50:30] INFO:     127.0.0.1:38114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:50:30] INFO:     127.0.0.1:38130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:50:30 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:50:30 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-16 10:50:31 TP0] Decode batch, #running-req: 4, #token: 12956, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.83, #queue-req: 0, 
[2025-12-16 10:50:32 TP0] Decode batch, #running-req: 4, #token: 13116, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-16 10:50:33 TP0] Decode batch, #running-req: 4, #token: 13276, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-16 10:50:34 TP0] Decode batch, #running-req: 4, #token: 13436, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-16 10:50:35 TP0] Decode batch, #running-req: 4, #token: 13596, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.40, #queue-req: 0, 
[2025-12-16 10:50:36 TP0] Decode batch, #running-req: 4, #token: 13756, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.36, #queue-req: 0, 
[2025-12-16 10:50:37 TP0] Decode batch, #running-req: 4, #token: 13916, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-16 10:50:38 TP0] Decode batch, #running-req: 4, #token: 14076, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.30, #queue-req: 0, 
[2025-12-16 10:50:39 TP0] Decode batch, #running-req: 4, #token: 14236, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.33, #queue-req: 0, 
[2025-12-16 10:50:40 TP0] Decode batch, #running-req: 4, #token: 14396, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-16 10:50:41 TP0] Decode batch, #running-req: 4, #token: 14556, token usage: 0.01, cuda graph: True, gen throughput (token/s): 150.90, #queue-req: 0, 
[2025-12-16 10:50:43 TP0] Decode batch, #running-req: 4, #token: 14716, token usage: 0.01, cuda graph: True, gen throughput (token/s): 150.90, #queue-req: 0, 
[2025-12-16 10:50:44 TP0] Decode batch, #running-req: 4, #token: 14876, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.91, #queue-req: 0, 
[2025-12-16 10:50:45 TP0] Decode batch, #running-req: 4, #token: 15036, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.91, #queue-req: 0, 
[2025-12-16 10:50:46 TP0] Decode batch, #running-req: 4, #token: 15196, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-16 10:50:47 TP0] Decode batch, #running-req: 4, #token: 15356, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.87, #queue-req: 0, 
[2025-12-16 10:50:48 TP0] Decode batch, #running-req: 4, #token: 15516, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.50, #queue-req: 0, 
[2025-12-16 10:50:49 TP0] Decode batch, #running-req: 4, #token: 15676, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.49, #queue-req: 0, 
[2025-12-16 10:50:50 TP0] Decode batch, #running-req: 4, #token: 15836, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.47, #queue-req: 0, 
[2025-12-16 10:50:51 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.34, #queue-req: 0, 
[2025-12-16 10:50:51] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-16 10:50:51] INFO:     127.0.0.1:40524 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-16 10:50:51] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-16 10:50:51] INFO:     127.0.0.1:40532 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-16 10:51:02] INFO:     127.0.0.1:37568 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-16 10:51:08] INFO:     127.0.0.1:54352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:51:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:51:10] INFO:     127.0.0.1:54364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:51:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:51:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2.22, #queue-req: 0, 
[2025-12-16 10:51:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:51:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:51:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:51:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:51:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:51:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 10:51:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:51:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:51:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:51:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-16 10:51:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:51:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:51:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:51:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:51:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:51:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:51:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.80, #queue-req: 0, 
[2025-12-16 10:51:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.17, #queue-req: 0, 
[2025-12-16 10:51:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 10:51:30] INFO:     127.0.0.1:47070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:51:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:51:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 10:51:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:51:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:51:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:51:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:51:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:51:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 10:51:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:51:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:51:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:51:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:51:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:51:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:51:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:51:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:51:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:51:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:51:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 10:51:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:51:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 10:51:50] INFO:     127.0.0.1:56868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:51:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:51:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 10:51:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:51:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:51:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:51:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:51:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:51:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:51:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:51:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:51:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:52:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:52:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:52:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:52:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:52:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:52:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:52:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:52:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:52:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:52:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:52:10] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:52:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:52:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 10:52:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:52:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:52:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:52:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:52:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:52:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:52:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:52:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:52:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:52:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:52:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:52:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:52:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:52:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:52:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:52:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:52:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:52:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:52:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:52:30] INFO:     127.0.0.1:58860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:52:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:52:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 10:52:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:52:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:52:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:52:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:52:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:52:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:52:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:52:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:52:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:52:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:52:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:52:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:52:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:52:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:52:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:52:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:52:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:52:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:52:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:52:50] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:52:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:52:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 10:52:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:52:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:52:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:52:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:52:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:52:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:52:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:52:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:52:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 10:53:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 10:53:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:53:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:53:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:53:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:53:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:53:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 10:53:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 10:53:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:53:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:53:10] INFO:     127.0.0.1:44532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:53:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:53:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 10:53:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:53:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:53:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:53:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:53:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:53:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:53:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:53:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:53:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:53:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:53:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:53:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:53:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:53:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:53:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:53:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:53:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:53:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 10:53:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:53:30] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:53:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:53:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 10:53:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:53:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:53:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:53:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:53:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:53:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:53:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:53:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:53:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:53:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:53:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 10:53:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:53:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:53:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:53:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:53:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 10:53:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 10:53:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:53:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 10:53:50] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:53:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:53:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 10:53:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:53:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:53:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:53:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 10:53:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:53:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:53:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:53:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:53:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:54:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:54:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:54:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:54:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:54:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:54:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:54:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 10:54:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:54:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:54:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:54:10] INFO:     127.0.0.1:42862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:54:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:54:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 10:54:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:54:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:54:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:54:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:54:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:54:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:54:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:54:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:54:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:54:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:54:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:54:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:54:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:54:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:54:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:54:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:54:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:54:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:54:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:54:30] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:54:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:54:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 10:54:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:54:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:54:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:54:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:54:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:54:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:54:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:54:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:54:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:54:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:54:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:54:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:54:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:54:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:54:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:54:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:54:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:54:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 10:54:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:54:50] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:54:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:54:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 10:54:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:54:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:54:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:54:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:54:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:54:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:54:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:54:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:54:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:55:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:55:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:55:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:55:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:55:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:55:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:55:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:55:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:55:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:55:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:55:10] INFO:     127.0.0.1:43446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:55:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:55:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 10:55:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:55:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:55:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:55:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 10:55:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:55:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:55:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:55:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:55:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:55:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:55:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:55:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:55:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:55:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:55:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:55:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 10:55:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:55:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:55:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:55:30] INFO:     127.0.0.1:37628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:55:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:55:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 10:55:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:55:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:55:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:55:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 10:55:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:55:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:55:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:55:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:55:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:55:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:55:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:55:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:55:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 10:55:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:55:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:55:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:55:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:55:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 10:55:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:55:50] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:55:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:55:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.11, #queue-req: 0, 
[2025-12-16 10:55:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:55:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:55:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:55:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 10:55:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:55:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:55:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:55:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:55:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:56:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:56:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:56:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:56:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:56:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:56:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:56:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:56:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:56:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:56:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:56:10] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:56:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:56:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 10:56:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:56:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 10:56:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 10:56:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:56:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:56:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:56:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 10:56:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:56:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:56:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:56:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:56:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:56:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:56:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:56:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:56:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:56:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:56:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 10:56:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:56:30] INFO:     127.0.0.1:51640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:56:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:56:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.17, #queue-req: 0, 
[2025-12-16 10:56:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:56:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:56:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:56:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 10:56:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:56:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:56:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:56:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:56:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:56:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:56:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:56:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:56:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:56:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:56:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:56:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:56:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 10:56:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:56:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:56:50] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:56:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:56:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.11, #queue-req: 0, 
[2025-12-16 10:56:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:56:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:56:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:56:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:56:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:56:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:56:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:56:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:56:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:57:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:57:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:57:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:57:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:57:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:57:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:57:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 10:57:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 10:57:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 10:57:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:57:10] INFO:     127.0.0.1:53534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:57:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:57:11 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 10:57:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:57:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:57:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:57:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:57:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:57:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:57:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:57:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:57:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:57:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:57:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:57:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:57:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:57:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:57:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:57:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:57:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:57:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:57:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:57:30] INFO:     127.0.0.1:44070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:57:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:57:31 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 10:57:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:57:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:57:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:57:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:57:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:57:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:57:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:57:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:57:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:57:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-16 10:57:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:57:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:57:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:57:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:57:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:57:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 10:57:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 10:57:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:57:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:57:50] INFO:     127.0.0.1:37884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:57:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:57:51 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.17, #queue-req: 0, 
[2025-12-16 10:57:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:57:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:57:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 10:57:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:57:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:57:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:57:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:57:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 10:57:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:58:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:58:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:58:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:58:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:58:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:58:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:58:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 10:58:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:58:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:58:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:58:10] INFO:     127.0.0.1:35458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:58:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:58:11 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 10:58:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 10:58:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 10:58:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 10:58:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:58:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:58:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:58:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:58:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:58:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 10:58:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 10:58:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:58:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:58:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:58:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:58:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:58:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 10:58:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:58:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 10:58:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:58:30] INFO:     127.0.0.1:37698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:58:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:58:31 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.17, #queue-req: 0, 
[2025-12-16 10:58:32 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 10:58:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 10:58:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 10:58:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 10:58:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:58:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 10:58:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 10:58:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:58:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:58:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 10:58:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:58:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 10:58:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 10:58:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:58:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:58:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:58:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 10:58:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 10:58:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 10:58:50] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:58:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:58:51 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 10:58:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 10:58:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 10:58:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 10:58:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 10:58:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:58:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 10:58:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:58:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:58:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:59:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 10:59:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:59:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:59:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 10:59:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 10:59:10] INFO:     127.0.0.1:50998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:59:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:59:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 10:59:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 10:59:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 10:59:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 10:59:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 10:59:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:59:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:59:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:59:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:59:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:59:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 10:59:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 10:59:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 10:59:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 10:59:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:59:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 10:59:30] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:59:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:59:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 10:59:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 10:59:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 10:59:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 10:59:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-16 10:59:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:59:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:59:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:59:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 10:59:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:59:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 10:59:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 10:59:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 10:59:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:59:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 10:59:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 10:59:50] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 10:59:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 10:59:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 10:59:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 10:59:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 10:59:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 10:59:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 10:59:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:59:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 10:59:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 10:59:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 10:59:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:00:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:00:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:00:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:00:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:00:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:00:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:00:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:00:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:00:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:00:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:00:10] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:00:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:00:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-16 11:00:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:00:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:00:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:00:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:00:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:00:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:00:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:00:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:00:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:00:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:00:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:00:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:00:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:00:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:00:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:00:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:00:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:00:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:00:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:00:30] INFO:     127.0.0.1:42440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:00:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:00:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-16 11:00:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:00:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:00:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:00:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:00:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:00:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:00:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:00:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:00:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:00:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:00:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:00:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:00:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:00:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:00:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:00:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:00:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:00:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:00:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:00:50] INFO:     127.0.0.1:40490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:00:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:00:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:00:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:00:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:00:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:00:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:00:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:00:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:00:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:00:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:00:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:01:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:01:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:01:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:01:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:01:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:01:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:01:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:01:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:01:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:01:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:01:10] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:01:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:01:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 11:01:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:01:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:01:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:01:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:01:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:01:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:01:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:01:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:01:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:01:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:01:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:01:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:01:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:01:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:01:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:01:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:01:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:01:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:01:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:01:30] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:01:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:01:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 11:01:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:01:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:01:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:01:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:01:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:01:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:01:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:01:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:01:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:01:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:01:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:01:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:01:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:01:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:01:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:01:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:01:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:01:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:01:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:01:50] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:01:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:01:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 11:01:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:01:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:01:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:01:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:01:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:01:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:01:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:01:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:01:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:02:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:02:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:02:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:02:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:02:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:02:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:02:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:02:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:02:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:02:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:02:10] INFO:     127.0.0.1:42662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:02:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:02:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 11:02:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:02:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:02:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:02:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:02:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:02:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:02:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:02:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:02:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:02:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:02:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:02:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:02:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:02:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:02:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:02:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:02:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:02:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:02:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:02:30] INFO:     127.0.0.1:44962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:02:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:02:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 11:02:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:02:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:02:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:02:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-16 11:02:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:02:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:02:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:02:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:02:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:02:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:02:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:02:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:02:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:02:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:02:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:02:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:02:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:02:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:02:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:02:50] INFO:     127.0.0.1:46140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:02:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:02:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:02:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:02:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:02:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:02:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:02:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:02:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:02:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:02:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:02:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:03:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:03:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:03:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:03:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:03:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:03:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:03:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:03:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:03:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:03:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:03:10] INFO:     127.0.0.1:52750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:03:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:03:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:03:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:03:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:03:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:03:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:03:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:03:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:03:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:03:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:03:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:03:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-16 11:03:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:03:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:03:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:03:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:03:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:03:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:03:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:03:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:03:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:03:30] INFO:     127.0.0.1:45002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:03:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:03:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-16 11:03:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:03:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:03:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:03:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:03:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:03:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:03:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:03:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:03:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:03:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:03:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:03:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:03:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:03:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:03:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:03:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:03:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:03:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:03:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:03:50] INFO:     127.0.0.1:40700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:03:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:03:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-16 11:03:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:03:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:03:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:03:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:03:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:03:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:03:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:03:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:03:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:04:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:04:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:04:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:04:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:04:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:04:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:04:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:04:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:04:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:04:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:04:10] INFO:     127.0.0.1:45840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:04:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:04:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:04:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:04:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:04:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:04:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:04:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:04:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:04:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:04:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:04:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:04:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:04:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:04:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:04:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:04:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:04:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:04:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:04:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:04:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:04:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:04:30] INFO:     127.0.0.1:37026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:04:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:04:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:04:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:04:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:04:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:04:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-16 11:04:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:04:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:04:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:04:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:04:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:04:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:04:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:04:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:04:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:04:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:04:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:04:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:04:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:04:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:04:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:04:50] INFO:     127.0.0.1:51762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:04:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:04:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.25, #queue-req: 0, 
[2025-12-16 11:04:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:04:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:04:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:04:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-16 11:04:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:04:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:04:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:04:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:04:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:05:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:05:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:05:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:05:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:05:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:05:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:05:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:05:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:05:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:05:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:05:10] INFO:     127.0.0.1:34878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:05:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:05:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-16 11:05:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:05:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:05:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:05:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:05:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:05:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:05:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:05:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:05:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:05:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:05:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:05:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:05:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:05:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:05:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:05:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:05:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:05:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:05:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:05:30] INFO:     127.0.0.1:40030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:05:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:05:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-16 11:05:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:05:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:05:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:05:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:05:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:05:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:05:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:05:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:05:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:05:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:05:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:05:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:05:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:05:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:05:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:05:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:05:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:05:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:05:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:05:50] INFO:     127.0.0.1:49692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:05:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:05:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-16 11:05:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:05:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:05:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:05:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:05:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:05:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:05:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:05:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:05:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:06:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:06:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:06:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:06:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:06:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:06:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:06:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:06:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:06:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:06:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:06:10] INFO:     127.0.0.1:38046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:06:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:06:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-16 11:06:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:06:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:06:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:06:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-16 11:06:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:06:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:06:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:06:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:06:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:06:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:06:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:06:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:06:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:06:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:06:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:06:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:06:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:06:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:06:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:06:30] INFO:     127.0.0.1:46312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:06:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:06:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 11:06:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:06:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:06:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:06:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:06:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:06:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:06:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:06:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:06:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:06:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:06:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:06:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:06:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:06:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:06:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:06:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:06:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:06:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:06:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:06:50] INFO:     127.0.0.1:44894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:06:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:06:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 11:06:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:06:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:06:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:06:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:06:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:06:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:06:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:06:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:06:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:07:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:07:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:07:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:07:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:07:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:07:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:07:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:07:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:07:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:07:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:07:10] INFO:     127.0.0.1:56464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:07:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:07:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:07:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:07:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:07:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:07:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:07:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:07:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:07:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:07:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:07:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:07:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:07:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:07:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:07:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:07:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:07:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:07:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:07:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:07:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:07:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:07:30] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:07:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:07:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 11:07:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:07:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:07:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:07:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:07:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:07:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:07:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:07:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:07:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:07:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-16 11:07:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:07:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:07:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:07:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:07:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:07:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:07:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:07:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:07:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:07:50] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:07:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:07:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:07:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:07:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:07:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:07:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:07:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:07:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:07:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:07:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:07:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:08:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:08:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:08:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:08:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:08:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:08:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:08:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:08:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:08:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:08:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:08:10] INFO:     127.0.0.1:36520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:08:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:08:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-16 11:08:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:08:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:08:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:08:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:08:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:08:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:08:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:08:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:08:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:08:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:08:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:08:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:08:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:08:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:08:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:08:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:08:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:08:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:08:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:08:30] INFO:     127.0.0.1:59764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:08:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:08:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:08:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:08:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:08:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:08:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:08:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:08:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:08:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:08:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:08:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:08:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-16 11:08:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:08:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:08:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:08:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:08:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:08:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:08:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:08:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:08:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:08:50] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:08:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:08:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:08:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:08:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:08:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:08:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:08:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:08:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:08:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:08:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:08:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:09:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:09:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:09:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:09:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:09:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:09:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:09:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:09:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:09:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:09:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:09:10] INFO:     127.0.0.1:45082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:09:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:09:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-16 11:09:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:09:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-16 11:09:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:09:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:09:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:09:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:09:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:09:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:09:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:09:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:09:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:09:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:09:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:09:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:09:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:09:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:09:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:09:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:09:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:09:30] INFO:     127.0.0.1:41352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:09:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:09:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 11:09:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:09:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:09:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:09:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:09:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:09:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:09:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:09:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:09:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:09:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:09:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:09:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:09:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:09:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:09:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:09:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:09:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:09:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:09:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:09:50] INFO:     127.0.0.1:58956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:09:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:09:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:09:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:09:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:09:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:09:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:09:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:09:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:09:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:09:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:09:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:10:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:10:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:10:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:10:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:10:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:10:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:10:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:10:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:10:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:10:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:10:10] INFO:     127.0.0.1:49188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:10:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:10:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 11:10:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:10:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:10:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:10:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:10:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:10:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:10:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:10:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:10:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:10:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:10:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:10:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:10:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:10:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:10:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:10:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:10:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:10:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:10:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:10:30] INFO:     127.0.0.1:38966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:10:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:10:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:10:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:10:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:10:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:10:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:10:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:10:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:10:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:10:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:10:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:10:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:10:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:10:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:10:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:10:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:10:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:10:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:10:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:10:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:10:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:10:50] INFO:     127.0.0.1:52408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:10:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:10:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:10:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:10:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:10:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:10:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:10:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:10:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:10:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:10:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:10:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:11:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:11:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:11:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:11:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:11:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:11:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:11:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:11:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:11:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:11:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:11:10] INFO:     127.0.0.1:34182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:11:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:11:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:11:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:11:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:11:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:11:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:11:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:11:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:11:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:11:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:11:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:11:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:11:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:11:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:11:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:11:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:11:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:11:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 11:11:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.16, #queue-req: 0, 
[2025-12-16 11:11:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.17, #queue-req: 0, 
[2025-12-16 11:11:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.17, #queue-req: 0, 
[2025-12-16 11:11:30] INFO:     127.0.0.1:37938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:11:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:11:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.17, #queue-req: 0, 
[2025-12-16 11:11:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:11:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:11:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:11:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:11:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:11:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:11:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:11:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:11:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:11:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:11:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:11:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:11:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:11:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:11:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:11:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:11:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:11:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:11:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:11:50] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:11:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:11:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:11:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:11:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:11:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:11:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:11:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:11:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:11:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:11:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:11:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:12:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:12:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:12:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:12:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:12:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:12:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:12:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:12:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:12:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:12:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:12:10] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:12:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:12:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 11:12:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:12:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:12:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:12:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:12:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:12:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:12:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:12:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:12:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:12:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-16 11:12:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:12:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:12:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:12:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:12:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:12:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:12:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:12:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:12:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:12:30] INFO:     127.0.0.1:43898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:12:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:12:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 11:12:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:12:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:12:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:12:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:12:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:12:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:12:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:12:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:12:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:12:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 11:12:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:12:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:12:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:12:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:12:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:12:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:12:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:12:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:12:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:12:50] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:12:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:12:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 11:12:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-16 11:12:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:12:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:12:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:12:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:12:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:12:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:12:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:12:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:13:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:13:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:13:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.17, #queue-req: 0, 
[2025-12-16 11:13:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:13:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:13:10] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:13:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:13:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.17, #queue-req: 0, 
[2025-12-16 11:13:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:13:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:13:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:13:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:13:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:13:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:13:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:13:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:13:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:13:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:13:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:13:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 11:13:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.17, #queue-req: 0, 
[2025-12-16 11:13:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.18, #queue-req: 0, 
[2025-12-16 11:13:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-16 11:13:30] INFO:     127.0.0.1:35734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:13:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:13:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:13:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-16 11:13:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:13:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:13:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:13:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:13:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:13:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:13:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:13:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:13:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:13:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:13:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:13:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.18, #queue-req: 0, 
[2025-12-16 11:13:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.17, #queue-req: 0, 
[2025-12-16 11:13:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-16 11:13:50] INFO:     127.0.0.1:56712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:13:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:13:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 11:13:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:13:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:13:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:13:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:13:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:13:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:13:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:13:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:13:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:14:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:14:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:14:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:14:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:14:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:14:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:14:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 11:14:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:14:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:14:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:14:10] INFO:     127.0.0.1:48752 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:14:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:14:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:14:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:14:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:14:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:14:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:14:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:14:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:14:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:14:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:14:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:14:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:14:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:14:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:14:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:14:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:14:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:14:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:14:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:14:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:14:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:14:30] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:14:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:14:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 11:14:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:14:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:14:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:14:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:14:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:14:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:14:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:14:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:14:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:14:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-16 11:14:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:14:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:14:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:14:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:14:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:14:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:14:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:14:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:14:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:14:50] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:14:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:14:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:14:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:14:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:14:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:14:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:14:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:14:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:14:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:14:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:14:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:15:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-16 11:15:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:15:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:15:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:15:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:15:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:15:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:15:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:15:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:15:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:15:10] INFO:     127.0.0.1:41954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:15:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:15:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:15:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:15:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:15:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:15:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:15:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:15:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:15:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:15:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:15:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:15:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-16 11:15:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:15:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:15:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:15:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:15:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:15:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:15:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:15:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:15:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:15:30] INFO:     127.0.0.1:55442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:15:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:15:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:15:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:15:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:15:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:15:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:15:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:15:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:15:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:15:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:15:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:15:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:15:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:15:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:15:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:15:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:15:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:15:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:15:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:15:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:15:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:15:50] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:15:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:15:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:15:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:15:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:15:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:15:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:15:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:15:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:15:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:15:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:15:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:16:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-16 11:16:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:16:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:16:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:16:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:16:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:16:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:16:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:16:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:16:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:16:10] INFO:     127.0.0.1:41860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:16:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:16:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-16 11:16:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:16:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:16:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:16:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:16:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:16:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:16:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:16:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:16:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:16:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:16:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:16:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:16:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:16:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:16:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:16:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:16:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:16:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:16:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:16:30] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:16:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:16:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:16:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:16:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:16:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:16:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:16:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:16:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:16:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:16:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:16:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:16:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-16 11:16:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:16:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:16:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:16:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:16:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:16:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:16:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:16:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:16:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:16:50] INFO:     127.0.0.1:52350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:16:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:16:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 11:16:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:16:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:16:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:16:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:16:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:16:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:16:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:16:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:16:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:17:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:17:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:17:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:17:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:17:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:17:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:17:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:17:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:17:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:17:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:17:10] INFO:     127.0.0.1:36408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:17:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:17:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.05, #queue-req: 0, 
[2025-12-16 11:17:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-16 11:17:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:17:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:17:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:17:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:17:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:17:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:17:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:17:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:17:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:17:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:17:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:17:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:17:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:17:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:17:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:17:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:17:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:17:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:17:30] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:17:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:17:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:17:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:17:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:17:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:17:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:17:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:17:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:17:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:17:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:17:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:17:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:17:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:17:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:17:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:17:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:17:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:17:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:17:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:17:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:17:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 11:17:50] INFO:     127.0.0.1:42108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:17:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:17:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 11:17:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:17:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:17:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:17:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:17:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:17:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:17:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:17:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:17:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:18:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:18:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:18:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:18:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:18:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:18:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:18:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:18:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:18:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:18:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:18:10] INFO:     127.0.0.1:39988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:18:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:18:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:18:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:18:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:18:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:18:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:18:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:18:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:18:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:18:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:18:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:18:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:18:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:18:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:18:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:18:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:18:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:18:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:18:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:18:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:18:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:18:30] INFO:     127.0.0.1:37206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:18:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:18:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:18:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:18:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:18:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:18:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:18:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:18:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:18:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:18:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:18:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:18:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:18:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:18:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:18:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:18:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:18:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:18:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:18:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:18:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:18:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:18:50] INFO:     127.0.0.1:42050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:18:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:18:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-16 11:18:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:18:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:18:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:18:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:18:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:18:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:18:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:18:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:18:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:19:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:19:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:19:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:19:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:19:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:19:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:19:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:19:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:19:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:19:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:19:10] INFO:     127.0.0.1:42722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:19:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:19:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:19:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:19:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:19:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:19:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:19:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:19:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:19:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:19:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:19:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:19:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:19:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:19:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:19:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:19:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:19:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:19:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:19:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:19:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:19:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:19:30] INFO:     127.0.0.1:43894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:19:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:19:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:19:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:19:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:19:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:19:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:19:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:19:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:19:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:19:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:19:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:19:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:19:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:19:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:19:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:19:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:19:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:19:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:19:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:19:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:19:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:19:50] INFO:     127.0.0.1:42718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:19:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:19:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:19:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:19:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:19:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:19:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:19:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:19:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:19:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:19:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:19:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:20:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:20:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:20:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:20:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:20:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:20:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:20:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:20:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:20:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:20:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:20:10] INFO:     127.0.0.1:60142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:20:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:20:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 11:20:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:20:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:20:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:20:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:20:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:20:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:20:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:20:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:20:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:20:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:20:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:20:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:20:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:20:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:20:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:20:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:20:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:20:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:20:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:20:30] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:20:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:20:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:20:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:20:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:20:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:20:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:20:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:20:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:20:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:20:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:20:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:20:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:20:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:20:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:20:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:20:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:20:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:20:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:20:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:20:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:20:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:20:50] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:20:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:20:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-16 11:20:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:20:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:20:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:20:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:20:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:20:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:20:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:20:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:20:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:21:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:21:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:21:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:21:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:21:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:21:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:21:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:21:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:21:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:21:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:21:10] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:21:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:21:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:21:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:21:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:21:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:21:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:21:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:21:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:21:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:21:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:21:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:21:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:21:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:21:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:21:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:21:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:21:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:21:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:21:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:21:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:21:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:21:30] INFO:     127.0.0.1:60336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:21:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:21:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-16 11:21:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:21:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:21:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:21:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:21:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:21:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:21:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:21:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:21:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:21:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:21:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:21:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:21:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:21:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:21:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:21:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:21:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:21:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:21:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:21:50] INFO:     127.0.0.1:37472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:21:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:21:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.25, #queue-req: 0, 
[2025-12-16 11:21:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:21:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:21:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:21:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:21:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:21:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:21:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:21:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:21:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:22:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:22:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:22:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:22:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:22:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:22:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:22:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:22:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:22:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:22:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:22:10] INFO:     127.0.0.1:40658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:22:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:22:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 11:22:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:22:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:22:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:22:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:22:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:22:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:22:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:22:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:22:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:22:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:22:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:22:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:22:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:22:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:22:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:22:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:22:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:22:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:22:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:22:30] INFO:     127.0.0.1:34918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:22:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:22:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 11:22:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:22:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:22:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:22:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:22:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:22:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:22:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:22:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:22:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:22:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:22:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:22:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:22:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:22:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:22:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:22:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:22:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:22:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:22:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:22:50] INFO:     127.0.0.1:35392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:22:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:22:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:22:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:22:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:22:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:22:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:22:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:22:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:22:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:22:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:22:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:23:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:23:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:23:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:23:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:23:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:23:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:23:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:23:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:23:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:23:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:23:10] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:23:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:23:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 11:23:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:23:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:23:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:23:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:23:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:23:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:23:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:23:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:23:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:23:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 11:23:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:23:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:23:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:23:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:23:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:23:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:23:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:23:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:23:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:23:30] INFO:     127.0.0.1:45170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:23:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:23:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 11:23:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:23:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:23:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:23:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:23:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:23:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:23:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:23:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:23:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:23:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:23:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:23:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:23:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:23:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:23:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:23:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:23:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:23:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:23:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:23:50] INFO:     127.0.0.1:60658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:23:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:23:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 11:23:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:23:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:23:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:23:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:23:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:23:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:23:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:23:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:23:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:24:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:24:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:24:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:24:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:24:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:24:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:24:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 11:24:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:24:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:24:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:24:10] INFO:     127.0.0.1:36400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:24:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:24:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 11:24:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-16 11:24:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-16 11:24:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-16 11:24:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:24:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:24:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:24:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:24:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:24:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:24:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:24:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:24:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:24:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:24:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:24:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:24:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 11:24:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.17, #queue-req: 0, 
[2025-12-16 11:24:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-16 11:24:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:24:30] INFO:     127.0.0.1:45330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:24:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:24:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 11:24:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-16 11:24:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-16 11:24:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-16 11:24:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:24:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:24:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:24:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:24:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:24:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:24:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:24:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:24:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:24:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:24:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:24:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:24:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 11:24:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:24:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:24:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:24:50] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:24:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:24:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-16 11:24:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-16 11:24:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-16 11:24:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-16 11:24:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:24:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:24:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:24:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:24:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:24:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:25:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:25:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:25:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:25:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:25:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:25:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:25:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:25:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.18, #queue-req: 0, 
[2025-12-16 11:25:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-16 11:25:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:25:10] INFO:     127.0.0.1:37768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:25:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:25:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.16, #queue-req: 0, 
[2025-12-16 11:25:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:25:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:25:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:25:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:25:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:25:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:25:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:25:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:25:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:25:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:25:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:25:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:25:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:25:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:25:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:25:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:25:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:25:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:25:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:25:30] INFO:     127.0.0.1:44812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:25:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:25:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-16 11:25:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:25:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:25:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:25:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-16 11:25:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:25:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:25:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:25:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:25:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:25:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:25:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:25:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:25:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:25:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:25:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:25:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:25:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:25:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:25:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:25:50] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:25:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:25:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:25:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:25:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:25:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:25:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:25:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:25:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:25:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:25:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:25:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:26:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 11:26:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:26:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:26:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:26:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:26:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:26:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:26:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:26:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:26:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:26:10] INFO:     127.0.0.1:33736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:26:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:26:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:26:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-16 11:26:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-16 11:26:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-16 11:26:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:26:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-16 11:26:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:26:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:26:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:26:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:26:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:26:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 11:26:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:26:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:26:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 11:26:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 11:26:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:26:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.12, #queue-req: 0, 
[2025-12-16 11:26:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.11, #queue-req: 0, 
[2025-12-16 11:26:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.11, #queue-req: 0, 
[2025-12-16 11:26:30] INFO:     127.0.0.1:42174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:26:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:26:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.16, #queue-req: 0, 
[2025-12-16 11:26:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:26:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:26:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:26:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:26:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:26:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:26:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:26:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:26:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:26:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 11:26:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:26:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:26:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:26:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:26:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:26:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:26:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:26:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:26:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:26:50] INFO:     127.0.0.1:33174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:26:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:26:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 11:26:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:26:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:26:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:26:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:26:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:26:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:26:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:26:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:26:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:27:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 11:27:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:27:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:27:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:27:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:27:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:27:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 11:27:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:27:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:27:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:27:10] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:27:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:27:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 11:27:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:27:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:27:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:27:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:27:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:27:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:27:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:27:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:27:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:27:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:27:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:27:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:27:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:27:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:27:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:27:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 11:27:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:27:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:27:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:27:30] INFO:     127.0.0.1:48802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:27:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:27:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.16, #queue-req: 0, 
[2025-12-16 11:27:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:27:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:27:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:27:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:27:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:27:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:27:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:27:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:27:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:27:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:27:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:27:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:27:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:27:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:27:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:27:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:27:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:27:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:27:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:27:50] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:27:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:27:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 11:27:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:27:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:27:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:27:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:27:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:27:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:27:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:27:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:27:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:28:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 11:28:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:28:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:28:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:28:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:28:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:28:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:28:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:28:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:28:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:28:10] INFO:     127.0.0.1:37010 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:28:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:28:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:28:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:28:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:28:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:28:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:28:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:28:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:28:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:28:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:28:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:28:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 11:28:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:28:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:28:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:28:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:28:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:28:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:28:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-16 11:28:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:28:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:28:30] INFO:     127.0.0.1:52156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:28:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:28:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.17, #queue-req: 0, 
[2025-12-16 11:28:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:28:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:28:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:28:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:28:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:28:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:28:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:28:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:28:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:28:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:28:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:28:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:28:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:28:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:28:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:28:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:28:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-16 11:28:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:28:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:28:50] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:28:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:28:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:28:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:28:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:28:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:28:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:28:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:28:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:28:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:28:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:28:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:29:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:29:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:29:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:29:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:29:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:29:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:29:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:29:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:29:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:29:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-16 11:29:10] INFO:     127.0.0.1:59536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:29:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:29:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 11:29:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:29:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:29:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:29:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:29:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:29:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:29:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:29:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:29:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:29:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 11:29:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:29:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:29:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:29:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:29:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:29:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-16 11:29:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:29:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:29:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:29:30] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:29:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:29:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:29:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:29:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:29:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:29:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:29:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:29:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:29:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:29:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:29:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:29:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:29:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:29:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:29:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:29:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:29:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:29:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:29:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:29:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:29:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:29:50] INFO:     127.0.0.1:45800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:29:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:29:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 11:29:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:29:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:29:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:29:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:29:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:29:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:29:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:29:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:29:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:30:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:30:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:30:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:30:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:30:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:30:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:30:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:30:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:30:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-16 11:30:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:30:10] INFO:     127.0.0.1:44074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:30:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:30:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.13, #queue-req: 0, 
[2025-12-16 11:30:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:30:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:30:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:30:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:30:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:30:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:30:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:30:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:30:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:30:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 11:30:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:30:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:30:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:30:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:30:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:30:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:30:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:30:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:30:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:30:30] INFO:     127.0.0.1:34810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:30:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:30:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.17, #queue-req: 0, 
[2025-12-16 11:30:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:30:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:30:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:30:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:30:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:30:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:30:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:30:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:30:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:30:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:30:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:30:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:30:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:30:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:30:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:30:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:30:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:30:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:30:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:30:50] INFO:     127.0.0.1:55610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:30:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:30:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-16 11:30:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-16 11:30:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:30:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:30:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:30:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:30:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:30:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:30:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:30:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:31:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 11:31:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:31:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:31:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:31:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:31:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:31:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:31:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:31:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:31:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:31:10] INFO:     127.0.0.1:49050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:31:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:31:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-16 11:31:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:31:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:31:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:31:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-16 11:31:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:31:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:31:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:31:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:31:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:31:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:31:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:31:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:31:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:31:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:31:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:31:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:31:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:31:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:31:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:31:30] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:31:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:31:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:31:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:31:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:31:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:31:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:31:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:31:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:31:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:31:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:31:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:31:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:31:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:31:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:31:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:31:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:31:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:31:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:31:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:31:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:31:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:31:50] INFO:     127.0.0.1:44720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:31:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:31:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:31:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:31:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:31:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-16 11:31:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:31:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:31:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-16 11:31:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:31:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:31:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:32:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-16 11:32:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:32:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:32:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:32:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:32:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:32:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:32:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:32:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-16 11:32:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:32:10] INFO:     127.0.0.1:43510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:32:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:32:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.19, #queue-req: 0, 
[2025-12-16 11:32:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:32:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:32:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:32:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:32:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:32:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:32:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:32:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-16 11:32:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:32:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 11:32:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:32:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:32:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:32:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:32:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:32:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:32:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.18, #queue-req: 0, 
[2025-12-16 11:32:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-16 11:32:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:32:30] INFO:     127.0.0.1:46102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:32:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:32:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.17, #queue-req: 0, 
[2025-12-16 11:32:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:32:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:32:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-16 11:32:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 11:32:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:32:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:32:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:32:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:32:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-16 11:32:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:32:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:32:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:32:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:32:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:32:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:32:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:32:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.08, #queue-req: 0, 
[2025-12-16 11:32:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.08, #queue-req: 0, 
[2025-12-16 11:32:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.08, #queue-req: 0, 
[2025-12-16 11:32:50] INFO:     127.0.0.1:60354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:32:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:32:50 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.09, #queue-req: 0, 
[2025-12-16 11:32:51 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:32:52 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:32:53 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-16 11:32:54 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-16 11:32:55 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-16 11:32:56 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:32:57 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:32:58 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-16 11:32:59 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-16 11:33:00 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-16 11:33:01 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:33:02 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-16 11:33:03 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:33:04 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:33:05 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-16 11:33:06 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-16 11:33:07 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:33:08 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:33:09 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-16 11:33:10] INFO:     127.0.0.1:44136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:33:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:33:10 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:33:11 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:33:12 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:33:13 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:33:14 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:33:15 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:33:16 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:33:17 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:33:18 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:33:19 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:33:20 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-16 11:33:21 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:33:22 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:33:23 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-16 11:33:24 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:33:25 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:33:26 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:33:27 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-16 11:33:28 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:33:29 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-16 11:33:30] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 11:33:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 11:33:30 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-16 11:33:31 TP0] Decode batch, #running-req: 1, #token: 3249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-16 11:33:32 TP0] Decode batch, #running-req: 1, #token: 3289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:33:33 TP0] Decode batch, #running-req: 1, #token: 3329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-16 11:33:34 TP0] Decode batch, #running-req: 1, #token: 3369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-16 11:33:35 TP0] Decode batch, #running-req: 1, #token: 3409, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:33:36 TP0] Decode batch, #running-req: 1, #token: 3449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:33:37 TP0] Decode batch, #running-req: 1, #token: 3489, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:33:38 TP0] Decode batch, #running-req: 1, #token: 3529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-16 11:33:39 TP0] Decode batch, #running-req: 1, #token: 3569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-16 11:33:40 TP0] Decode batch, #running-req: 1, #token: 3609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-16 11:33:41 TP0] Decode batch, #running-req: 1, #token: 3649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:33:42 TP0] Decode batch, #running-req: 1, #token: 3689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:33:43 TP0] Decode batch, #running-req: 1, #token: 3729, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:33:44 TP0] Decode batch, #running-req: 1, #token: 3769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:33:45 TP0] Decode batch, #running-req: 1, #token: 3809, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-16 11:33:46 TP0] Decode batch, #running-req: 1, #token: 3849, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-16 11:33:47 TP0] Decode batch, #running-req: 1, #token: 3889, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:33:48 TP0] Decode batch, #running-req: 1, #token: 3929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:33:49 TP0] Decode batch, #running-req: 1, #token: 3969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-16 11:33:50] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-16 11:33:50] INFO:     127.0.0.1:41100 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-16 11:33:50] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-16 11:33:50] INFO:     127.0.0.1:41104 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-16 11:33:53] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-12-16 11:33:56] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
