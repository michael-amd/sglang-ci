INFO 10-23 11:30:40 __init__.py:179] Automatically detected platform rocm.
WARNING 10-23 11:30:40 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:30:42] WARNING server_args.py:1173: DP attention is enabled. The chunked prefill size is adjusted to 16384 to avoid MoE kernel issues. 
[2025-10-23 11:30:42] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 11:30:42] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=0.3, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', elastic_ep_backend=None, mooncake_ib_device=None, tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=219773322, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=8, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_beta_spec=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=True, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:30:43] Using default HuggingFace chat template with detected content format: string
INFO 10-23 11:30:50 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:30:50 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:30:52] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 11:30:52] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
INFO 10-23 11:31:00 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:31:00 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:31:00 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:31:00 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:31:00 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:31:00 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:31:00 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:31:00 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:31:02] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 11:31:02 DP0 TP0] Process 815 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:31:02 DP0 TP0] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:31:02] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:31:02] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:31:02 DP1 TP1] Process 816 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:31:02 DP4 TP4] Process 819 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:31:03] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 11:31:03] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:31:03 DP5 TP5] Process 820 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:31:03 DP3 TP3] Process 818 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:31:03] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 11:31:03 DP7 TP7] Process 822 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:31:03 DP1 TP1] Init torch distributed begin.
[2025-10-23 11:31:03 DP4 TP4] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:31:03 DP3 TP3] Init torch distributed begin.
[2025-10-23 11:31:03 DP5 TP5] Init torch distributed begin.
[2025-10-23 11:31:03] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 11:31:03 DP6 TP6] Process 821 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:31:03] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 11:31:03 DP2 TP2] Process 817 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:31:03 DP7 TP7] Init torch distributed begin.
[2025-10-23 11:31:03 DP6 TP6] Init torch distributed begin.
[2025-10-23 11:31:03 DP2 TP2] Init torch distributed begin.
[2025-10-23 11:31:04 DP0 TP0] sglang is using nccl==2.21.5
[2025-10-23 11:31:05 DP0 TP0] Init torch distributed ends. mem usage=3.63 GB
[2025-10-23 11:31:05 DP7 TP7] Init torch distributed ends. mem usage=3.92 GB
[2025-10-23 11:31:05 DP6 TP6] Init torch distributed ends. mem usage=3.93 GB
[2025-10-23 11:31:05 DP5 TP5] Init torch distributed ends. mem usage=3.91 GB
[2025-10-23 11:31:05 DP4 TP4] Init torch distributed ends. mem usage=3.99 GB
[2025-10-23 11:31:05 DP3 TP3] Init torch distributed ends. mem usage=4.04 GB
[2025-10-23 11:31:05 DP2 TP2] Init torch distributed ends. mem usage=4.05 GB
[2025-10-23 11:31:05 DP1 TP1] Init torch distributed ends. mem usage=4.05 GB
[2025-10-23 11:31:07 DP5 TP5] Load weight begin. avail mem=187.35 GB
[2025-10-23 11:31:07 DP4 TP4] Load weight begin. avail mem=187.27 GB
[2025-10-23 11:31:07 DP7 TP7] Load weight begin. avail mem=187.34 GB
[2025-10-23 11:31:07 DP6 TP6] Load weight begin. avail mem=187.33 GB
[2025-10-23 11:31:07 DP2 TP2] Load weight begin. avail mem=187.21 GB
[2025-10-23 11:31:07 DP3 TP3] Load weight begin. avail mem=187.22 GB
[2025-10-23 11:31:07 DP0 TP0] Load weight begin. avail mem=187.63 GB
[2025-10-23 11:31:07 DP0 TP0] Detected fp8 checkpoint.
[2025-10-23 11:31:07 DP0 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-10-23 11:31:07 DP1 TP1] Load weight begin. avail mem=187.21 GB
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:30,  5.24it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:31,  5.18it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:24,  6.54it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:16,  9.70it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:00<00:11, 13.98it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:00<00:09, 15.40it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:01<00:25,  5.88it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:01<00:20,  7.13it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:01<00:14,  9.88it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:02<00:16,  8.67it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:02<00:18,  7.56it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:03<00:26,  5.34it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:03<00:30,  4.57it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:04<00:32,  4.18it/s]
Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:04<00:33,  4.01it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:04<00:35,  3.81it/s]
Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:05<00:43,  3.10it/s]
Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:05<00:45,  2.90it/s]
Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:06<00:44,  2.95it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:06<00:30,  4.27it/s]
Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:06<00:31,  4.13it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:06<00:30,  4.25it/s]
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:07<00:33,  3.75it/s]
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:07<00:49,  2.54it/s]
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:07<00:39,  3.15it/s]
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:08<00:27,  4.45it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:08<00:27,  4.48it/s]
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:08<00:29,  4.05it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:08<00:28,  4.26it/s]
Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:08<00:24,  4.83it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:09<00:24,  4.90it/s]
Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:09<00:20,  5.69it/s]
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:09<00:23,  4.90it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:09<00:20,  5.60it/s]
Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:09<00:18,  6.25it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:09<00:19,  5.83it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:10<00:20,  5.52it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:10<00:20,  5.32it/s]
Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:10<00:18,  5.93it/s]
Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:10<00:18,  5.78it/s]
Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:10<00:17,  6.35it/s]
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:11<00:18,  5.69it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:11<00:17,  6.16it/s]
Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:11<00:17,  5.87it/s]
Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:11<00:18,  5.69it/s]
Loading safetensors checkpoint shards:  37% Completed | 61/163 [00:11<00:12,  8.16it/s]
Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:11<00:08, 12.28it/s]
Loading safetensors checkpoint shards:  40% Completed | 66/163 [00:11<00:07, 13.38it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:13<00:23,  4.12it/s]
Loading safetensors checkpoint shards:  43% Completed | 70/163 [00:13<00:20,  4.51it/s]
Loading safetensors checkpoint shards:  44% Completed | 72/163 [00:13<00:15,  5.72it/s]
Loading safetensors checkpoint shards:  45% Completed | 74/163 [00:13<00:12,  7.09it/s]
Loading safetensors checkpoint shards:  47% Completed | 76/163 [00:13<00:11,  7.67it/s]
Loading safetensors checkpoint shards:  48% Completed | 79/163 [00:14<00:08,  9.92it/s]
Loading safetensors checkpoint shards:  50% Completed | 82/163 [00:14<00:07, 10.71it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:14<00:06, 11.85it/s]
Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:14<00:05, 13.27it/s]
Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:14<00:05, 14.06it/s]
Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:14<00:05, 12.59it/s]
Loading safetensors checkpoint shards:  57% Completed | 93/163 [00:15<00:04, 14.69it/s]
Loading safetensors checkpoint shards:  58% Completed | 95/163 [00:15<00:04, 15.11it/s]
Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:15<00:05, 11.26it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:15<00:05, 12.21it/s]
Loading safetensors checkpoint shards:  62% Completed | 101/163 [00:15<00:05, 11.41it/s]
Loading safetensors checkpoint shards:  64% Completed | 104/163 [00:15<00:04, 14.03it/s]
Loading safetensors checkpoint shards:  65% Completed | 106/163 [00:16<00:10,  5.46it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:17<00:08,  6.67it/s]
Loading safetensors checkpoint shards:  68% Completed | 111/163 [00:17<00:05,  8.96it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:17<00:04, 10.07it/s]
Loading safetensors checkpoint shards:  72% Completed | 117/163 [00:17<00:03, 12.25it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:17<00:03, 12.23it/s]
Loading safetensors checkpoint shards:  74% Completed | 121/163 [00:17<00:03, 13.48it/s]
Loading safetensors checkpoint shards:  75% Completed | 123/163 [00:17<00:02, 14.55it/s]
Loading safetensors checkpoint shards:  77% Completed | 126/163 [00:18<00:02, 16.89it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:18<00:02, 13.47it/s]
Loading safetensors checkpoint shards:  81% Completed | 132/163 [00:18<00:01, 15.57it/s]
Loading safetensors checkpoint shards:  82% Completed | 134/163 [00:18<00:01, 14.66it/s]
Loading safetensors checkpoint shards:  83% Completed | 136/163 [00:18<00:01, 15.53it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:18<00:01, 16.42it/s]
Loading safetensors checkpoint shards:  86% Completed | 140/163 [00:18<00:01, 16.78it/s]
Loading safetensors checkpoint shards:  87% Completed | 142/163 [00:19<00:01, 16.89it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:19<00:00, 18.46it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:19<00:01, 15.21it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:19<00:01, 13.33it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:19<00:00, 14.51it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:20<00:01,  5.07it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:20<00:01,  6.37it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:20<00:00,  7.91it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:21<00:00,  9.60it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:21<00:00, 11.08it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:21<00:00,  9.49it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:21<00:00,  7.59it/s]

[2025-10-23 11:31:58 DP5 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.88 GB, mem usage=90.47 GB.
[2025-10-23 11:31:58 DP4 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.80 GB, mem usage=90.47 GB.
[2025-10-23 11:31:58 DP7 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.88 GB, mem usage=90.47 GB.
[2025-10-23 11:31:59 DP6 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.87 GB, mem usage=90.47 GB.
[2025-10-23 11:32:00 DP2 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.74 GB, mem usage=90.47 GB.
[2025-10-23 11:32:01 DP1 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.74 GB, mem usage=90.47 GB.
[2025-10-23 11:32:01 DP3 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.75 GB, mem usage=90.47 GB.
[2025-10-23 11:32:02 DP0 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=97.16 GB, mem usage=90.47 GB.
[2025-10-23 11:32:02 DP0 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-23 11:32:02 DP4 TP4] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-23 11:32:02 DP4 TP4] Memory pool end. avail mem=58.73 GB
[2025-10-23 11:32:02 DP7 TP7] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-23 11:32:02 DP7 TP7] Memory pool end. avail mem=58.81 GB
[2025-10-23 11:32:02 DP5 TP5] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-23 11:32:02 DP6 TP6] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-23 11:32:02 DP5 TP5] Memory pool end. avail mem=58.82 GB
[2025-10-23 11:32:02 DP1 TP1] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-23 11:32:02 DP6 TP6] Memory pool end. avail mem=58.80 GB
[2025-10-23 11:32:02 DP1 TP1] Memory pool end. avail mem=58.68 GB
[2025-10-23 11:32:02 DP3 TP3] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-23 11:32:02 DP3 TP3] Memory pool end. avail mem=58.68 GB
[2025-10-23 11:32:02 DP0 TP0] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-23 11:32:02 DP0 TP0] Memory pool end. avail mem=59.09 GB
[2025-10-23 11:32:02 DP2 TP2] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-23 11:32:02 DP2 TP2] Memory pool end. avail mem=58.67 GB
[2025-10-23 11:32:03 DP7 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=58.60 GB
[2025-10-23 11:32:03 DP5 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=58.61 GB
[2025-10-23 11:32:03 DP2 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=58.46 GB
[2025-10-23 11:32:04 DP6 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=58.59 GB
[2025-10-23 11:32:04 DP4 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=58.53 GB
[2025-10-23 11:32:04 DP0 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=58.89 GB
[2025-10-23 11:32:04 DP0 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-10-23 11:32:04 DP3 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=58.48 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=58.25 GB):   0%|          | 0/52 [00:00<?, ?it/s][2025-10-23 11:32:05 DP1 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=58.47 GB
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:32:05 DP4 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:32:05 DP6 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:32:05 DP5 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:32:05 DP1 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:32:05 DP2 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:32:05 DP7 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:32:05 DP3 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:32:05 DP0 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:32:07 DP5 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:32:07 DP3 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:32:07 DP2 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:32:07 DP7 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:32:07 DP6 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:32:07 DP0 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:07 DP5 TP5] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:07 DP5 TP5] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:32:07 DP4 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:08 DP3 TP3] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:08 DP3 TP3] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:08 DP2 TP2] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:08 DP2 TP2] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:08 DP7 TP7] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:08 DP7 TP7] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:08 DP6 TP6] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:08 DP6 TP6] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:08 DP0 TP0] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:08 DP0 TP0] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:08 DP4 TP4] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:08 DP4 TP4] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:32:09 DP1 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:09 DP1 TP1] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:09 DP1 TP1] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP1 TP1] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP7 TP7] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP1 TP1] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP6 TP6] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP3 TP3] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP4 TP4] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP5 TP5] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP2 TP2] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP0 TP0] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP7 TP7] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP6 TP6] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP3 TP3] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP4 TP4] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP5 TP5] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP2 TP2] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:10 DP0 TP0] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:10 DP1 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:10 DP3 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:10 DP6 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:10 DP4 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:10 DP2 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:10 DP7 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:10 DP5 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:10 DP0 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[rank7]:[W1023 11:32:10.956643752 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank4]:[W1023 11:32:10.956654078 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank6]:[W1023 11:32:10.956680850 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank5]:[W1023 11:32:10.956693068 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank3]:[W1023 11:32:10.956654126 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank1]:[W1023 11:32:10.956741073 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank2]:[W1023 11:32:10.956692155 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank0]:[W1023 11:32:10.956741106 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Capturing batches (bs=512 avail_mem=58.25 GB):   2%|         | 1/52 [00:06<05:32,  6.52s/it]Capturing batches (bs=496 avail_mem=50.25 GB):   2%|         | 1/52 [00:06<05:32,  6.52s/it]Capturing batches (bs=496 avail_mem=50.25 GB):   4%|         | 2/52 [00:07<02:34,  3.09s/it]Capturing batches (bs=480 avail_mem=50.24 GB):   4%|         | 2/52 [00:07<02:34,  3.09s/it]Capturing batches (bs=480 avail_mem=50.24 GB):   6%|         | 3/52 [00:07<01:38,  2.00s/it]Capturing batches (bs=464 avail_mem=50.23 GB):   6%|         | 3/52 [00:07<01:38,  2.00s/it]Capturing batches (bs=464 avail_mem=50.23 GB):   8%|         | 4/52 [00:08<01:11,  1.49s/it]Capturing batches (bs=448 avail_mem=50.22 GB):   8%|         | 4/52 [00:08<01:11,  1.49s/it]Capturing batches (bs=448 avail_mem=50.22 GB):  10%|         | 5/52 [00:09<00:56,  1.20s/it]Capturing batches (bs=432 avail_mem=50.22 GB):  10%|         | 5/52 [00:09<00:56,  1.20s/it]Capturing batches (bs=432 avail_mem=50.22 GB):  12%|        | 6/52 [00:09<00:45,  1.00it/s]Capturing batches (bs=416 avail_mem=50.21 GB):  12%|        | 6/52 [00:09<00:45,  1.00it/s]Capturing batches (bs=416 avail_mem=50.21 GB):  13%|        | 7/52 [00:10<00:40,  1.11it/s]Capturing batches (bs=400 avail_mem=50.20 GB):  13%|        | 7/52 [00:10<00:40,  1.11it/s]Capturing batches (bs=400 avail_mem=50.20 GB):  15%|        | 8/52 [00:11<00:36,  1.19it/s]Capturing batches (bs=384 avail_mem=50.20 GB):  15%|        | 8/52 [00:11<00:36,  1.19it/s]Capturing batches (bs=384 avail_mem=50.20 GB):  17%|        | 9/52 [00:11<00:31,  1.37it/s]Capturing batches (bs=368 avail_mem=50.19 GB):  17%|        | 9/52 [00:11<00:31,  1.37it/s]Capturing batches (bs=368 avail_mem=50.19 GB):  19%|        | 10/52 [00:12<00:29,  1.44it/s]Capturing batches (bs=352 avail_mem=50.18 GB):  19%|        | 10/52 [00:12<00:29,  1.44it/s]Capturing batches (bs=352 avail_mem=50.18 GB):  21%|        | 11/52 [00:13<00:27,  1.50it/s]Capturing batches (bs=336 avail_mem=50.18 GB):  21%|        | 11/52 [00:13<00:27,  1.50it/s]Capturing batches (bs=336 avail_mem=50.18 GB):  23%|       | 12/52 [00:13<00:27,  1.48it/s]Capturing batches (bs=320 avail_mem=50.17 GB):  23%|       | 12/52 [00:13<00:27,  1.48it/s]Capturing batches (bs=320 avail_mem=50.17 GB):  25%|       | 13/52 [00:14<00:25,  1.53it/s]Capturing batches (bs=304 avail_mem=50.16 GB):  25%|       | 13/52 [00:14<00:25,  1.53it/s]Capturing batches (bs=304 avail_mem=50.16 GB):  27%|       | 14/52 [00:14<00:22,  1.66it/s]Capturing batches (bs=288 avail_mem=50.15 GB):  27%|       | 14/52 [00:14<00:22,  1.66it/s]Capturing batches (bs=288 avail_mem=50.15 GB):  29%|       | 15/52 [00:15<00:20,  1.85it/s]Capturing batches (bs=272 avail_mem=50.15 GB):  29%|       | 15/52 [00:15<00:20,  1.85it/s]Capturing batches (bs=272 avail_mem=50.15 GB):  31%|       | 16/52 [00:15<00:20,  1.77it/s]Capturing batches (bs=256 avail_mem=50.14 GB):  31%|       | 16/52 [00:15<00:20,  1.77it/s][aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP7 TP7] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP7 TP7] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP7 TP7] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP7 TP7] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP5 TP5] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP5 TP5] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP5 TP5] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP5 TP5] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP1 TP1] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP1 TP1] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP1 TP1] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP1 TP1] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP6 TP6] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP6 TP6] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP6 TP6] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP6 TP6] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP0 TP0] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP0 TP0] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP0 TP0] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP0 TP0] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP2 TP2] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP2 TP2] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP2 TP2] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP2 TP2] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP4 TP4] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP4 TP4] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP4 TP4] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP4 TP4] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP3 TP3] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:20 DP3 TP3] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP3 TP3] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:20 DP3 TP3] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=256 avail_mem=50.14 GB):  33%|      | 17/52 [00:16<00:20,  1.74it/s]Capturing batches (bs=248 avail_mem=50.13 GB):  33%|      | 17/52 [00:16<00:20,  1.74it/s]Capturing batches (bs=248 avail_mem=50.13 GB):  35%|      | 18/52 [00:17<00:19,  1.72it/s]Capturing batches (bs=240 avail_mem=50.13 GB):  35%|      | 18/52 [00:17<00:19,  1.72it/s]Capturing batches (bs=240 avail_mem=50.13 GB):  37%|      | 19/52 [00:17<00:19,  1.70it/s]Capturing batches (bs=232 avail_mem=50.12 GB):  37%|      | 19/52 [00:17<00:19,  1.70it/s]Capturing batches (bs=232 avail_mem=50.12 GB):  38%|      | 20/52 [00:18<00:17,  1.79it/s]Capturing batches (bs=224 avail_mem=50.11 GB):  38%|      | 20/52 [00:18<00:17,  1.79it/s]Capturing batches (bs=224 avail_mem=50.11 GB):  40%|      | 21/52 [00:18<00:17,  1.74it/s]Capturing batches (bs=216 avail_mem=50.11 GB):  40%|      | 21/52 [00:18<00:17,  1.74it/s]Capturing batches (bs=216 avail_mem=50.11 GB):  42%|     | 22/52 [00:19<00:17,  1.72it/s]Capturing batches (bs=208 avail_mem=50.10 GB):  42%|     | 22/52 [00:19<00:17,  1.72it/s]Capturing batches (bs=208 avail_mem=50.10 GB):  44%|     | 23/52 [00:19<00:15,  1.90it/s]Capturing batches (bs=200 avail_mem=50.09 GB):  44%|     | 23/52 [00:19<00:15,  1.90it/s]Capturing batches (bs=200 avail_mem=50.09 GB):  46%|     | 24/52 [00:20<00:15,  1.82it/s]Capturing batches (bs=192 avail_mem=50.09 GB):  46%|     | 24/52 [00:20<00:15,  1.82it/s][aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP5 TP5] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP7 TP7] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP4 TP4] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP6 TP6] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP3 TP3] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP1 TP1] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP0 TP0] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP2 TP2] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:24 DP5 TP5] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:24 DP7 TP7] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:24 DP4 TP4] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:24 DP6 TP6] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:24 DP3 TP3] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:24 DP1 TP1] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:24 DP0 TP0] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:24 DP2 TP2] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP5 TP5] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP7 TP7] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP1 TP1] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP4 TP4] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP2 TP2] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP0 TP0] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP5 TP5] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP3 TP3] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP7 TP7] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP1 TP1] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP4 TP4] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP0 TP0] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP2 TP2] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP6 TP6] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP3 TP3] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:24 DP6 TP6] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=192 avail_mem=50.09 GB):  48%|     | 25/52 [00:20<00:13,  1.98it/s]Capturing batches (bs=184 avail_mem=50.08 GB):  48%|     | 25/52 [00:20<00:13,  1.98it/s]Capturing batches (bs=184 avail_mem=50.08 GB):  50%|     | 26/52 [00:21<00:12,  2.11it/s]Capturing batches (bs=176 avail_mem=50.08 GB):  50%|     | 26/52 [00:21<00:12,  2.11it/s]Capturing batches (bs=176 avail_mem=50.08 GB):  52%|    | 27/52 [00:21<00:11,  2.21it/s]Capturing batches (bs=168 avail_mem=50.07 GB):  52%|    | 27/52 [00:21<00:11,  2.21it/s]Capturing batches (bs=168 avail_mem=50.07 GB):  54%|    | 28/52 [00:22<00:11,  2.02it/s]Capturing batches (bs=160 avail_mem=50.06 GB):  54%|    | 28/52 [00:22<00:11,  2.02it/s]Capturing batches (bs=160 avail_mem=50.06 GB):  56%|    | 29/52 [00:22<00:10,  2.14it/s]Capturing batches (bs=152 avail_mem=50.06 GB):  56%|    | 29/52 [00:22<00:10,  2.14it/s]Capturing batches (bs=152 avail_mem=50.06 GB):  58%|    | 30/52 [00:23<00:11,  1.97it/s]Capturing batches (bs=144 avail_mem=50.05 GB):  58%|    | 30/52 [00:23<00:11,  1.97it/s]Capturing batches (bs=144 avail_mem=50.05 GB):  60%|    | 31/52 [00:23<00:09,  2.10it/s]Capturing batches (bs=136 avail_mem=50.01 GB):  60%|    | 31/52 [00:23<00:09,  2.10it/s]Capturing batches (bs=136 avail_mem=50.01 GB):  62%|   | 32/52 [00:23<00:09,  2.20it/s]Capturing batches (bs=128 avail_mem=49.99 GB):  62%|   | 32/52 [00:23<00:09,  2.20it/s][aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP2 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP0 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP5 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP1 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP7 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP4 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP3 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP6 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP2 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP0 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP5 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP1 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP7 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP4 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP3 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:32:28 DP6 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP2 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP1 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP0 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP2 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP5 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP7 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP4 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP1 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP0 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP5 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP3 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP6 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP7 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP4 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP3 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:28 DP6 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=128 avail_mem=49.99 GB):  63%|   | 33/52 [00:24<00:08,  2.29it/s]Capturing batches (bs=120 avail_mem=49.98 GB):  63%|   | 33/52 [00:24<00:08,  2.29it/s][aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:28 DP2 TP2] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:28 DP3 TP3] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:28 DP6 TP6] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:28 DP4 TP4] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:28 DP0 TP0] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:28 DP7 TP7] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:28 DP5 TP5] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:28 DP1 TP1] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=49.98 GB):  65%|   | 34/52 [00:24<00:08,  2.05it/s]Capturing batches (bs=112 avail_mem=49.97 GB):  65%|   | 34/52 [00:24<00:08,  2.05it/s][aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP2 TP2] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP0 TP0] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP7 TP7] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP5 TP5] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP1 TP1] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP4 TP4] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP3 TP3] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP6 TP6] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=49.97 GB):  67%|   | 35/52 [00:25<00:07,  2.17it/s]Capturing batches (bs=104 avail_mem=49.97 GB):  67%|   | 35/52 [00:25<00:07,  2.17it/s][aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP0 TP0] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP4 TP4] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP5 TP5] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP2 TP2] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP3 TP3] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP1 TP1] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP7 TP7] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:29 DP6 TP6] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=49.97 GB):  69%|   | 36/52 [00:25<00:07,  2.12it/s]Capturing batches (bs=96 avail_mem=49.96 GB):  69%|   | 36/52 [00:25<00:07,  2.12it/s] [aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP1 TP1] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP2 TP2] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP0 TP0] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP5 TP5] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP6 TP6] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP4 TP4] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP7 TP7] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP3 TP3] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=49.96 GB):  71%|   | 37/52 [00:26<00:07,  2.09it/s]Capturing batches (bs=88 avail_mem=49.94 GB):  71%|   | 37/52 [00:26<00:07,  2.09it/s][aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP7 TP7] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP4 TP4] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP5 TP5] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP6 TP6] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP3 TP3] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP0 TP0] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP1 TP1] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:30 DP2 TP2] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=49.94 GB):  73%|  | 38/52 [00:26<00:07,  1.94it/s]Capturing batches (bs=80 avail_mem=49.91 GB):  73%|  | 38/52 [00:26<00:07,  1.94it/s][aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP5 TP5] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP1 TP1] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP7 TP7] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP3 TP3] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP2 TP2] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP0 TP0] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP4 TP4] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP6 TP6] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=49.91 GB):  75%|  | 39/52 [00:27<00:06,  2.07it/s]Capturing batches (bs=72 avail_mem=49.89 GB):  75%|  | 39/52 [00:27<00:06,  2.07it/s][aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP0 TP0] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP6 TP6] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP7 TP7] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP4 TP4] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP5 TP5] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP1 TP1] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP3 TP3] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:31 DP2 TP2] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=49.89 GB):  77%|  | 40/52 [00:27<00:06,  1.91it/s]Capturing batches (bs=64 avail_mem=49.88 GB):  77%|  | 40/52 [00:27<00:06,  1.91it/s][aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP3 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP2 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP0 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP7 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP5 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP4 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP1 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP6 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP3 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP2 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP0 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP7 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP5 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP4 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP1 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP6 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP1 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP3 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP0 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP2 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP7 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP5 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP4 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP1 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP3 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP0 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP2 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP7 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP5 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP6 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP4 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:32:32 DP6 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP1 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP3 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP0 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP7 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP2 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP5 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP4 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP6 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=49.88 GB):  79%|  | 41/52 [00:28<00:05,  2.08it/s]Capturing batches (bs=56 avail_mem=49.87 GB):  79%|  | 41/52 [00:28<00:05,  2.08it/s][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP1 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP4 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP7 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP3 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP0 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP5 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP6 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:32 DP2 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=49.87 GB):  81%|  | 42/52 [00:28<00:05,  1.93it/s]Capturing batches (bs=48 avail_mem=49.86 GB):  81%|  | 42/52 [00:28<00:05,  1.93it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP1 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP0 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP7 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP5 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP2 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP3 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP6 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP4 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=49.86 GB):  83%| | 43/52 [00:29<00:04,  2.07it/s]Capturing batches (bs=40 avail_mem=49.86 GB):  83%| | 43/52 [00:29<00:04,  2.07it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP0 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP4 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP2 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP1 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP3 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP6 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP7 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:32:33 DP5 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=49.86 GB):  85%| | 44/52 [00:29<00:04,  1.93it/s]Capturing batches (bs=32 avail_mem=49.85 GB):  85%| | 44/52 [00:29<00:04,  1.93it/s][rank6]:W1023 11:32:36.666000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:32:36.682000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1023 11:32:36.692000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:32:36.703000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1023 11:32:36.720000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1023 11:32:36.740000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:32:36.741000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1023 11:32:36.751000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:32:36.757000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1023 11:32:36.767000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1023 11:32:36.778000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:32:36.795000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:32:36.815000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:32:36.826000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:32:36.838000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:32:36.855000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:32:36.864000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:32:36.875000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:32:36.892000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:32:36.912000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:32:36.923000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1023 11:32:37.185000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1023 11:32:37.260000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:32:37.358000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:32:37.394000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:32:37.424000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:32:37.431000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:32:37.446000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1023 11:32:37.454000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:32:37.466000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:32:37.475000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:32:37.507000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:32:37.539000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:32:37.543000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:32:37.557000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:32:37.568000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:32:37.577000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:32:37.586000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:32:37.590000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:32:37.624000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:32:37.627000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:32:37.641000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:32:37.652000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:32:37.658000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:32:37.661000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:32:37.670000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:32:37.691000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:32:37.693000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:32:37.708000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:32:37.719000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:32:37.727000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:32:37.736000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:32:37.916000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:32:38.044000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:32:38.111000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:32:38.177000 821 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank3]:W1023 11:32:38.178000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:32:38.224000 817 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank7]:W1023 11:32:38.227000 822 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank0]:W1023 11:32:38.227000 815 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank5]:W1023 11:32:38.248000 820 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank1]:W1023 11:32:38.254000 816 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank4]:W1023 11:32:38.255000 819 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank3]:W1023 11:32:38.861000 818 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0106 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0107 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0108 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0108 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0113 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0115 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0115 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0116 ms 90.3% 
  triton_bmm_20 0.0117 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3086 seconds and 0.4667 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0106 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0107 ms 97.4% 
  triton_bmm_25 0.0108 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_19 0.0109 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0109 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0113 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0115 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0116 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_20 0.0118 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3168 seconds and 0.7153 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_23 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_17 0.0103 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0105 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0106 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0108 ms 94.8% 
  triton_bmm_15 0.0110 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0113 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0113 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0114 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3310 seconds and 0.6547 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0107 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0109 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0111 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0111 ms 96.7% 
  triton_bmm_21 0.0112 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0112 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0113 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0117 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0118 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0118 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2677 seconds and 0.5573 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0106 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0107 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0107 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_19 0.0108 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0111 ms 95.3% 
  triton_bmm_15 0.0112 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0115 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0116 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0116 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2972 seconds and 0.7309 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0106 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0106 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0108 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0108 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0111 ms 94.6% 
  triton_bmm_15 0.0112 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_9 0.0116 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0116 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0116 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3395 seconds and 0.5175 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0106 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0107 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0108 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0109 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_25 0.0111 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0111 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0113 ms 94.3% 
  triton_bmm_9 0.0114 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0115 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0115 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3250 seconds and 0.5251 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0107 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0108 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0109 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0109 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_19 0.0110 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0111 ms 96.0% 
  triton_bmm_15 0.0112 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0116 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0117 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0117 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2917 seconds and 0.4632 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1023 11:32:49.942000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:32:50.122000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:32:50.404000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:32:50.417000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:32:50.455000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:32:50.462000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:32:50.483000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:32:50.663000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:32:50.755000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:32:50.916000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:32:50.926000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:32:50.958000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:32:50.966000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:32:51.259000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:32:51.311000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1023 11:32:51.820000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1023 11:32:51.977000 821 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1023 11:32:52.166000 816 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank7]:W1023 11:32:52.301000 822 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank4]:W1023 11:32:52.450000 819 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank2]:W1023 11:32:52.481000 817 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank0]:W1023 11:32:52.515000 815 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1023 11:32:52.771000 818 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank5]:W1023 11:32:53.372000 820 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0104 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_46 0.0104 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0105 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0105 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0108 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0110 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0111 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0112 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0114 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3442 seconds and 0.6155 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0104 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0104 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0105 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0107 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0109 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0109 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0109 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0111 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0113 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0115 ms 90.9% 
SingleProcess AUTOTUNE benchmarking takes 5.3334 seconds and 0.6164 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_48 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0103 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0104 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_41 0.0104 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0106 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0107 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0107 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0108 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0113 ms 90.4% 
  triton_bmm_31 0.0114 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3643 seconds and 0.6185 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_46 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0103 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0106 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0106 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0107 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0109 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0111 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2413 seconds and 0.6201 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_39 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_41 0.0100 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0101 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0104 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0105 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0106 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_45 0.0109 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_42 0.0112 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2907 seconds and 0.6194 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_39 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_41 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0103 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0104 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0106 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_37 0.0108 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0108 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0108 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0113 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0115 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3189 seconds and 0.6258 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0104 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0105 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_46 0.0105 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0107 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0107 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0109 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0109 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0112 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0113 ms 91.2% 
SingleProcess AUTOTUNE benchmarking takes 5.2762 seconds and 0.6144 seconds precompiling for 27 choices
[rank6]:W1023 11:32:59.292000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:32:59.367000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_48 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0104 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_41 0.0104 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0104 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0108 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0108 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_51 0.0108 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0110 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_45 0.0113 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3327 seconds and 0.6084 seconds precompiling for 27 choices
[rank6]:W1023 11:32:59.465000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:32:59.644000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:32:59.669000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:32:59.695000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:32:59.745000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:32:59.747000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:32:59.771000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:32:59.824000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:32:59.844000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:32:59.844000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:32:59.879000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:32:59.880000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:32:59.912000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:32:59.958000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:00.012000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:00.059000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:00.295000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:00.371000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:00.472000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:00.711000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:00.786000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:00.884000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:04.249000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:04.324000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:04.421000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:04 DP6 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank4]:W1023 11:33:04.526000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:04.575000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:04.600000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:04.614000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:04.651000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:04.688000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:04.698000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:04.750000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:04 DP4 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1023 11:33:04.791000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:04.799000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:04 DP7 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:04 DP2 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1023 11:33:04.866000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:04.869000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:04.943000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:04.964000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:05 DP1 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1023 11:33:05.041000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:05 DP0 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1023 11:33:05.228000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:05.303000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:05.401000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:05 DP3 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1023 11:33:05.621000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:05.695000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:05.793000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:05 DP5 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1023 11:33:06.367000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:06.389000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:06.412000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:06.433000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:06.442000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:06.444000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:06.464000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:06.487000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:06.508000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:06.521000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:06.562000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:06.572000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:06.580000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:06.588000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:06 DP4 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1023 11:33:06.622000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:06 DP2 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:06 DP7 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1023 11:33:06.646000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:06.647000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:06 DP0 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:06 DP6 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1023 11:33:06.755000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:06 DP3 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1023 11:33:06.853000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:06.928000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:07.018000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:07.026000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:07 DP1 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1023 11:33:07.093000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:07.192000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:33:07 DP5 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1023 11:33:07.759000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:07.768000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:07.778000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:07.787000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:07.796000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:07.804000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:07.815000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:07.839000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:08.003000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:08.021000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:08.029000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:08.038000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:08.047000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:08.058000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:08.069000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:08.080000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:08.247000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:08.265000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:08.273000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:08.282000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:08.290000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:08.302000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:08.314000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:08.325000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:33:08 DP6 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:33:08 DP1 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:33:08 DP3 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:33:08 DP2 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:33:08 DP4 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:33:08 DP0 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:33:08 DP7 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:33:08 DP5 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1023 11:33:09.894000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:09.926000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:09.931000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:09.941000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:09.950000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:09.972000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:09.982000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:10.001000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:10.005000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:10.014000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:10.016000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:10.028000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:10.046000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:10.047000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:10.057000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:10.076000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:10.079000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:10.090000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:10.092000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:10.104000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:10.121000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:33:10 DP6 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[rank5]:W1023 11:33:10.132000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:33:10 DP4 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:33:10 DP1 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:33:10 DP2 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:33:10 DP0 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[rank3]:W1023 11:33:10.165000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:33:10 DP7 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:33:10 DP5 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1023 11:33:10.240000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:33:10 DP3 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0146 ms 100.0% 
  triton_mm_55 0.0278 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 34.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0583 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0609 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0749 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1920 seconds and 0.4559 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0147 ms 100.0% 
  triton_mm_55 0.0274 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0422 ms 34.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0535 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_67 0.0583 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_69 0.0583 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_66 0.0609 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0754 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1935 seconds and 0.5160 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0140 ms 100.0% 
  triton_mm_55 0.0273 ms 51.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 33.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0455 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0507 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 26.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_67 0.0583 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_69 0.0584 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_66 0.0609 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0766 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.3013 seconds and 0.7243 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0144 ms 100.0% 
  triton_mm_55 0.0277 ms 52.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 34.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0456 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0505 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0533 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0581 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0611 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0753 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2423 seconds and 0.4043 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0144 ms 100.0% 
  triton_mm_55 0.0317 ms 45.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0420 ms 34.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0455 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0532 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0584 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0610 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_59 0.0837 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.3098 seconds and 0.7520 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0146 ms 100.0% 
  triton_mm_55 0.0310 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 34.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0455 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0504 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0583 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0610 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0832 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2225 seconds and 0.4293 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0148 ms 100.0% 
  triton_mm_55 0.0310 ms 47.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 35.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0505 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0585 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0610 ms 24.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0752 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2911 seconds and 0.7305 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0148 ms 100.0% 
  triton_mm_55 0.0287 ms 51.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 35.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0505 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0583 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0611 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_59 0.0837 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.2822 seconds and 0.7440 seconds precompiling for 39 choices
[rank0]:W1023 11:33:21.032000 815 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faab87b5ad0>
[rank0]:W1023 11:33:21.063000 815 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faab87b5980>
[rank1]:W1023 11:33:21.137000 816 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f17ff505890>
[rank6]:W1023 11:33:21.163000 821 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ed15d2d2400>
[rank2]:W1023 11:33:21.176000 817 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7740909aa0>
[rank1]:W1023 11:33:21.178000 816 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f17ff505b30>
[rank5]:W1023 11:33:21.179000 820 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef392bddda0>
[rank6]:W1023 11:33:21.194000 821 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ed15d2d2190>
[rank7]:W1023 11:33:21.198000 822 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f94b03b5e30>
[rank4]:W1023 11:33:21.200000 819 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3220b967f0>
[rank2]:W1023 11:33:21.209000 817 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f77409099e0>
[rank5]:W1023 11:33:21.210000 820 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef392bddad0>
[rank7]:W1023 11:33:21.229000 822 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f94b03b59e0>
[rank4]:W1023 11:33:21.231000 819 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3220b965b0>
[rank3]:W1023 11:33:21.236000 818 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8825499e60>
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:33:21 DP0 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1023 11:33:21.267000 818 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f882549a070>
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:33:21 DP1 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:33:21 DP6 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:33:21 DP2 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:33:21 DP4 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:33:21 DP5 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:33:21 DP3 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:33:21 DP7 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1023 11:33:22.659000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:22.680000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:22.689000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:22.703000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:22.712000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:22.722000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:22.734000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:22.767000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:22.907000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:22.924000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:22.932000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:22.949000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:22.959000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:22.968000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:22.978000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:23.003000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:23.151000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:23.164000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:23.174000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:23.195000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:23.204000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:23.213000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:23.223000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:23.239000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:23.391000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:23.404000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:23.413000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:23.435000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:23.450000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:23.459000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:23.468000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:23.479000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:23.630000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:23.642000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:23.674000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:23.700000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:23.711000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:23.721000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:23.733000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:23.741000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:23.873000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:23.882000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:23.914000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:23.944000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:23.954000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:23.965000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:23.977000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:23.986000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:24.117000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:24.126000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:24.154000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:24.188000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:24.198000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:24.209000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:24.220000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:24.234000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:24.365000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:24.373000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:24.394000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:24.432000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:24.442000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:24.454000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:24.466000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:24.476000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:24.618000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:24.634000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:24.661000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:24.676000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:24.695000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:24.714000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:24.735000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:24.760000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:24.889000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:24.897000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:24.905000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:24.919000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:24.935000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:24.954000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:24.976000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:25.000000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:25.142000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:25.153000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:25.179000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:25.188000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:25.197000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:25.216000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:25.225000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:25.240000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:25.386000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:25.397000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:25.424000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:25.432000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:25.442000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:25.461000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:25.471000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:25.484000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:25.649000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:25.668000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:25.682000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:25.691000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:25.709000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:25.718000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:25.730000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:25.740000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:25.897000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:25.912000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:25.926000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:25.935000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:25.951000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:25.962000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:25.974000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:25.983000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:26.141000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:26.156000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:26.173000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:26.183000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:26.195000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:26.206000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:26.217000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:26.227000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:26.399000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:26.406000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:26.417000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:26.426000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:26.439000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:26.450000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:26.462000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:26.471000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:26.643000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:26.653000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:26.661000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:26.670000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:26.683000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:26.694000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:26.706000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:26.715000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:26.887000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:26.897000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:26.905000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:26.915000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:26.925000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:26.936000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:26.947000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:26.957000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:27.141000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:27.150000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:27.158000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:27.172000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:27.182000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:27.194000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:27.204000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:27.214000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:27.385000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:27.393000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:27.402000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:27.416000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:27.426000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:27.438000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:27.448000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:27.458000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:27.633000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:27.641000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:27.650000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:27.661000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:27.670000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:27.681000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:27.693000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:27.706000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:27.878000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:27.885000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:27.894000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:27.904000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:27.915000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:27.926000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:27.938000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:27.948000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:28.122000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:28.138000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:28.152000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:28.172000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:28.189000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:28.198000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:28.216000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:28.236000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:28.370000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:28.381000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:28.403000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:28.415000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:28.437000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:28.446000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:28.460000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:28.476000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:28.630000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:28.660000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:28.670000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:28.685000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:28.694000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:28.706000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:28.716000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:28.728000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:28.878000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:28.904000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:28.914000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:28.930000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:28.940000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:28.950000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:28.962000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:28.974000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:29.148000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:29.162000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:29.177000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:29.187000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:29.198000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:29.209000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:29.220000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:29.242000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:29.390000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:29.407000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:29.427000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:29.438000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:29.447000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:29.457000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:29.466000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:29.483000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:29.633000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:29.663000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:29.682000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:29.694000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:29.702000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:29.712000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:29.721000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:29.732000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:29.880000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:29.906000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:29.926000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:29.934000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:29.947000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:29.960000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:29.970000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:29.980000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:30.123000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:30.150000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:30.169000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:30.182000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:30.192000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:30.204000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:30.215000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:30.225000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:30.367000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:30.393000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:30.413000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:30.426000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:30.436000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:30.447000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:30.459000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:30.468000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:30.611000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:30.637000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:30.657000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:30.670000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:30.680000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:30.691000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:30.703000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:30.712000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:30.882000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:30.901000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:30.914000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:30.924000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:30.934000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:30.945000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:30.957000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:30.966000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:31.126000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:31.145000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:31.158000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:31.168000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:31.179000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:31.192000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:31.201000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:31.211000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:31.370000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:31.395000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:31.408000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:31.416000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:31.426000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:31.435000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:31.445000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:31.456000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:31.611000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:31.638000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:31.651000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:31.662000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:31.675000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:31.684000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:31.692000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:31.703000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:31.854000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:31.898000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:31.912000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:31.932000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:31.946000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:31.954000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:31.988000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:32.004000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:32.138000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:32.146000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:32.159000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:32.179000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:32.190000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:32.201000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:32.232000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:32.244000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:32.394000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:32.408000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:32.434000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:32.446000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:32.453000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:32.481000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:32.494000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:32.504000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:32.650000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:32.659000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:32.686000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:32.698000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:32.709000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:32.728000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:32.740000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:32.751000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:32.898000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:32.907000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:32.926000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:32.942000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:32.953000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:32.972000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:32.986000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:32.997000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:33.152000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:33.190000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:33.201000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:33.216000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:33.224000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:33.236000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:33.247000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:33.257000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:33.395000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:33.438000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:33.449000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:33.459000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:33.470000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:33.482000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:33.492000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:33.502000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:33.639000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:33.686000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:33.697000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:33.707000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:33.718000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:33.730000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:33.740000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:33.749000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:33.887000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:33.930000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:33.945000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:33.955000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:33.966000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:33.977000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:33.989000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:33.998000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:34.135000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:34.176000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:34.193000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:34.203000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:34.214000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:34.225000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:34.237000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:34.246000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:34.383000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:34.421000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:34.441000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:34.451000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:34.462000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:34.472000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:34.484000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:34.493000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:34.631000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:34.666000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:34.689000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:34.699000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:34.710000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:34.722000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:34.732000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:34.742000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:34.910000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:34.937000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:34.948000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:34.958000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:34.967000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:34.979000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:34.989000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:34.999000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:35.154000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:35.185000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:35.195000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:35.206000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:35.215000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:35.227000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:35.237000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:35.247000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:35.398000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:35.433000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:35.444000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:35.454000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:35.463000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:35.474000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:35.486000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:35.495000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:35.641000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:35.681000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:35.692000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:35.701000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:35.711000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:35.723000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:35.733000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:35.742000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:35.886000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:35.931000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:35.940000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:35.949000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:35.961000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:35.971000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:35.980000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:35.991000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:36.132000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:36.190000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:36.211000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:36.222000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:36.235000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:36.250000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:36.270000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:36.298000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:36.435000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:36.444000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:36.459000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:36.474000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:36.483000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:36.498000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:36.518000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:36.543000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:36.698000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:36.712000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:36.736000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:36.751000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:36.763000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:36.772000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:36.790000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:36.801000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:37.541000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:37.549000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:37.573000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:37.583000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:37.591000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:37.603000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:37.629000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:37.641000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1840 ms 100.0% 
  triton_mm_127 0.2501 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2569 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2620 ms 70.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2665 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_124 0.2670 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2698 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2735 ms 67.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2829 ms 65.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_120 0.2982 ms 61.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9440 seconds and 0.4601 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1818 ms 100.0% 
  triton_mm_127 0.2468 ms 73.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2549 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2613 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2657 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_124 0.2665 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2702 ms 67.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2716 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2812 ms 64.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_120 0.2960 ms 61.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9115 seconds and 0.5541 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1854 ms 100.0% 
  triton_mm_127 0.2485 ms 74.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2570 ms 72.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2633 ms 70.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2677 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_124 0.2680 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2701 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2756 ms 67.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2839 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.3006 ms 61.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9864 seconds and 0.5985 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1813 ms 100.0% 
  triton_mm_127 0.2468 ms 73.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2541 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2592 ms 70.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2632 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_107 0.2639 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2641 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2681 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2707 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2931 ms 61.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9308 seconds and 0.3622 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1845 ms 100.0% 
  triton_mm_127 0.2487 ms 74.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2567 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2625 ms 70.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2660 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2674 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2710 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2758 ms 66.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2855 ms 64.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2984 ms 61.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9232 seconds and 0.5358 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1798 ms 100.0% 
  triton_mm_127 0.2450 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2518 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_124 0.2601 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_113 0.2606 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_125 0.2652 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2662 ms 67.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_107 0.2704 ms 66.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2826 ms 63.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.2928 ms 61.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9312 seconds and 0.7034 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1878 ms 100.0% 
  triton_mm_127 0.2450 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2523 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2584 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2610 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2630 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_107 0.2648 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_125 0.2651 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2737 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2921 ms 64.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7961 seconds and 0.4957 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1821 ms 100.0% 
  triton_mm_127 0.2492 ms 73.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2557 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2628 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2666 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2682 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2683 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_107 0.2751 ms 66.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2832 ms 64.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2988 ms 60.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9324 seconds and 0.3640 seconds precompiling for 39 choices
Capturing batches (bs=32 avail_mem=49.85 GB):  87%| | 45/52 [01:48<02:47, 23.92s/it]Capturing batches (bs=24 avail_mem=49.21 GB):  87%| | 45/52 [01:48<02:47, 23.92s/it][rank6]:W1023 11:33:54.295000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:54.302000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:54.315000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:54.324000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:54.337000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:54.346000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:54.357000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:54.362000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:54.368000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:54.382000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:54.391000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:54.404000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:54.413000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:54.425000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:54.433000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:54.438000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:54.453000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:54.462000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:54.475000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:54.485000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:54.497000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:54.773000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:54.840000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:54.911000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:54.917000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:54.925000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:54.941000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:54.949000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:54.961000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:54.969000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:54.984000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:54.996000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:55.002000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:55.018000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:55.028000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:55.042000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:55.048000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:55.064000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:55.066000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:55.069000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:55.087000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:55.096000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:55.110000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:55.116000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:55.130000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:55.135000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:33:55.136000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:55.154000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:33:55.163000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:33:55.177000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:33:55.183000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:33:55.203000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:55.399000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:55.477000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:33:55.544000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:33:55.591000 821 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank4]:W1023 11:33:55.594000 819 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank7]:W1023 11:33:55.610000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:33:55.617000 820 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank2]:W1023 11:33:55.632000 817 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank1]:W1023 11:33:55.650000 816 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank3]:W1023 11:33:55.656000 818 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank0]:W1023 11:33:55.681000 815 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank7]:W1023 11:33:56.113000 822 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0102 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0102 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0102 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0102 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0103 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0104 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0104 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0106 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2390 seconds and 0.4183 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0103 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_153 0.0103 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0104 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0105 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0105 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0108 ms 93.3% 
SingleProcess AUTOTUNE benchmarking takes 5.3461 seconds and 0.4624 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0103 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0105 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0108 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3271 seconds and 0.4294 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_144 0.0102 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0102 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0102 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_148 0.0103 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0104 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0105 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0108 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3120 seconds and 0.3916 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0097 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0097 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0099 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0099 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0099 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0099 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_149 0.0100 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0101 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0101 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_137 0.0103 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2609 seconds and 0.4660 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_151 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_148 0.0104 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0104 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0104 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0105 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0105 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0110 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3328 seconds and 0.3601 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_144 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0103 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0104 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0106 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2209 seconds and 0.3846 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_153 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_145 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_144 0.0103 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0103 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_148 0.0103 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0104 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0105 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0105 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2409 seconds and 0.3765 seconds precompiling for 27 choices
[rank5]:W1023 11:34:06.758000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:06.802000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:06.821000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:07.097000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:07.129000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:07.260000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:07.300000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:07.320000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:07.394000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:07.447000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:07.476000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:07.597000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:07.633000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:07.896000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:07.944000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:07.983000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:08.564000 820 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank4]:W1023 11:34:08.759000 819 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank6]:W1023 11:34:08.788000 821 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank2]:W1023 11:34:09.067000 817 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank1]:W1023 11:34:09.287000 816 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank7]:W1023 11:34:09.361000 822 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank3]:W1023 11:34:09.405000 818 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank0]:W1023 11:34:09.955000 815 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0097 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0098 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_165 0.0099 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0100 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0101 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0104 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0107 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0107 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0108 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0109 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3131 seconds and 0.5319 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0100 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_165 0.0102 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0103 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0104 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_179 0.0106 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_159 0.0107 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_175 0.0107 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0107 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0111 ms 89.5% 
SingleProcess AUTOTUNE benchmarking takes 5.2399 seconds and 0.5445 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0099 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_174 0.0100 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0100 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0101 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0102 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0103 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0106 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0106 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0108 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3025 seconds and 0.5252 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0097 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0097 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0098 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0099 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0100 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0102 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0103 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_179 0.0104 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0105 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_173 0.0105 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2362 seconds and 0.5243 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_174 0.0102 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0104 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_175 0.0106 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0108 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0109 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_159 0.0111 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0113 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2677 seconds and 0.5317 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0099 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0101 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0103 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0105 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_175 0.0107 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0107 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0108 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_159 0.0111 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0112 ms 88.2% 
SingleProcess AUTOTUNE benchmarking takes 5.3353 seconds and 0.5222 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_176 0.0102 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_174 0.0105 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0108 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0109 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_175 0.0110 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0110 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_159 0.0111 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0113 ms 87.9% 
SingleProcess AUTOTUNE benchmarking takes 5.3111 seconds and 0.5188 seconds precompiling for 27 choices
[rank5]:W1023 11:34:15.625000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:15.700000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:15.798000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:15.820000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:15.839000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0100 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_174 0.0101 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0103 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0103 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0105 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0107 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0107 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_159 0.0108 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0112 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3126 seconds and 0.5169 seconds precompiling for 27 choices
[rank6]:W1023 11:34:15.895000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:15.914000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:15.993000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:16.012000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:16.057000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:16.132000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:16.231000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:16.623000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:16.699000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:16.758000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:16.797000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:16.832000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:16.895000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:16.931000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:16.970000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:16.988000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:17.065000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:17.068000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:17.170000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:19.820000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:19.894000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:19.992000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:20.073000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:20.149000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:20.251000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:20.335000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:20.411000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:20.443000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:20.511000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:20.519000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:20.618000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:20.796000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:20.869000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:20.913000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:20.967000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:20.988000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:21.188000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:21.282000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:21.356000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:21.454000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:21.836000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:21.924000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:22.030000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:22.495000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:22.502000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:22.512000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:22.523000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:22.531000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:22.571000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:22.579000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:22.595000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:22.598000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:22.607000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:22.636000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:22.670000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:22.672000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:22.685000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:22.706000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:22.710000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:22.770000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:22.810000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:22.827000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:22.831000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:22.921000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:23.226000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:23.301000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:23.400000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:23.980000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:23.988000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:23.998000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:24.008000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:24.017000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:24.029000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:24.038000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:24.050000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:24.228000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:24.236000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:24.247000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:24.258000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:24.266000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:24.279000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:24.290000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:24.300000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:24.476000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:24.484000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:24.495000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:24.506000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:24.515000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:24.526000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:24.538000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:24.547000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:26.090000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:26.109000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:26.127000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:26.139000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:26.157000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:26.164000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:26.165000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:26.174000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:26.184000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:26.202000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:26.213000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:26.215000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:26.233000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:26.233000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:26.239000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:26.250000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:26.251000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:26.263000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:26.281000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:26.287000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:26.298000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:26.450000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:26.525000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:26.573000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0134 ms 100.0% 
  triton_mm_183 0.0275 ms 48.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0409 ms 32.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0454 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0532 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 22.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0756 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2079 seconds and 0.6842 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0128 ms 100.0% 
  triton_mm_183 0.0275 ms 46.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 25.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0533 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0597 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0755 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1893 seconds and 0.5013 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0132 ms 100.0% 
  triton_mm_183 0.0275 ms 48.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0409 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0498 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0533 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 22.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0755 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2308 seconds and 0.7577 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0136 ms 100.0% 
  triton_mm_183 0.0274 ms 49.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 33.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0498 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0533 ms 25.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0583 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0612 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0755 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2135 seconds and 0.7392 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0129 ms 100.0% 
  triton_mm_183 0.0275 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0454 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0597 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0754 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2311 seconds and 0.5250 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0130 ms 100.0% 
  triton_mm_183 0.0275 ms 47.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0533 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 21.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0611 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0752 ms 17.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2253 seconds and 0.4913 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0129 ms 100.0% 
  triton_mm_183 0.0274 ms 47.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0407 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0453 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0495 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0532 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0611 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0756 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2462 seconds and 0.8016 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0129 ms 100.0% 
  triton_mm_183 0.0275 ms 47.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0496 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0581 ms 22.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0611 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0753 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2354 seconds and 0.4709 seconds precompiling for 39 choices
[rank4]:W1023 11:34:36.938000 819 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3220b967f0>
[rank4]:W1023 11:34:36.961000 819 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3220b965b0>
[rank1]:W1023 11:34:36.994000 816 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f17ff505890>
[rank3]:W1023 11:34:37.002000 818 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8825499e60>
[rank2]:W1023 11:34:37.008000 817 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7740909aa0>
[rank1]:W1023 11:34:37.017000 816 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f17ff505b30>
[rank3]:W1023 11:34:37.025000 818 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f882549a070>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:34:37 DP4 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1023 11:34:37.030000 817 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f77409099e0>
[rank5]:W1023 11:34:37.062000 820 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef392bddda0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:34:37 DP1 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1023 11:34:37.085000 820 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef392bddad0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:34:37 DP3 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:34:37 DP2 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1023 11:34:37.129000 821 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ed15d2d2400>
[rank6]:W1023 11:34:37.151000 821 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ed15d2d2190>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:34:37 DP5 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1023 11:34:37.200000 815 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faab87b5ad0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:34:37 DP6 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1023 11:34:37.223000 815 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faab87b5980>
[rank7]:W1023 11:34:37.290000 822 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f94b03b5e30>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:34:37 DP0 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1023 11:34:37.312000 822 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f94b03b59e0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:34:37 DP7 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1023 11:34:38.648000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:38.656000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:38.664000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:38.675000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:38.683000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:38.693000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:38.704000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:38.760000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:38.900000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:38.918000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:38.927000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:38.937000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:38.947000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:38.957000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:39.008000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:39.080000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:39.221000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:39.231000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:39.241000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:39.258000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:39.282000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:39.314000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:39.332000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:39.342000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:39.479000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:39.487000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:39.496000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:39.508000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:39.532000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:39.564000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:39.578000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:39.592000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:39.730000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:39.738000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:39.747000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:39.758000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:39.779000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:39.807000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:39.826000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:39.840000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:39.982000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:39.990000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:39.999000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:40.010000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:40.028000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:40.052000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:40.074000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:40.086000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:40.234000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:40.242000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:40.251000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:40.262000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:40.276000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:40.296000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:40.322000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:40.335000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:40.486000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:40.494000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:40.503000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:40.514000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:40.525000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:40.540000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:40.571000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:40.582000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:40.734000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:40.745000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:40.754000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:40.765000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:40.778000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:40.792000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:40.819000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:40.830000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:40.994000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:41.005000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:41.016000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:41.024000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:41.035000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:41.047000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:41.066000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:41.079000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:41.242000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:41.254000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:41.264000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:41.274000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:41.285000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:41.296000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:41.315000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:41.327000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:41.496000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:41.507000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:41.516000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:41.528000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:41.538000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:41.546000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:41.569000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:41.579000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:41.748000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:41.758000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:41.767000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:41.778000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:41.789000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:41.797000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:41.815000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:41.827000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:41.996000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:42.006000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:42.015000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:42.026000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:42.037000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:42.045000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:42.059000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:42.074000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:42.244000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:42.253000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:42.262000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:42.276000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:42.287000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:42.295000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:42.307000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:42.322000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:42.495000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:42.503000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:42.514000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:42.523000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:42.536000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:42.547000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:42.558000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:42.572000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:42.746000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:42.754000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:42.764000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:42.775000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:42.785000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:42.797000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:42.807000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:42.820000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:42.998000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:43.006000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:43.016000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:43.025000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:43.037000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:43.048000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:43.058000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:43.071000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:43.250000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:43.258000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:43.268000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:43.278000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:43.289000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:43.301000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:43.316000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:43.324000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:43.503000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:43.511000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:43.522000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:43.532000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:43.544000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:43.554000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:43.567000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:43.577000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:43.755000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:43.768000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:43.776000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:43.786000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:43.797000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:43.808000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:43.820000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:43.830000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:44.010000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:44.021000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:44.033000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:44.043000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:44.054000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:44.065000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:44.078000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:44.088000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:44.312000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:44.323000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:44.332000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:44.342000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:44.351000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:44.363000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:44.373000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:44.421000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:44.614000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:44.625000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:44.634000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:44.644000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:44.653000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:44.664000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:44.675000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:44.723000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:44.896000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:44.904000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:44.912000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:44.922000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:44.932000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:44.943000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:44.970000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:45.004000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:45.152000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:45.160000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:45.168000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:45.178000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:45.188000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:45.199000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:45.218000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:45.252000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:45.414000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:45.422000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:45.434000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:45.445000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:45.456000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:45.466000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:45.504000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:45.533000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:45.725000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:45.733000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:45.743000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:45.752000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:45.761000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:45.772000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:45.785000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:45.836000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:46.052000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:46.063000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:46.073000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:46.082000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:46.094000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:46.116000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:46.150000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:46.160000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:46.308000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:46.319000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:46.329000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:46.338000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:46.350000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:46.368000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:46.398000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:46.410000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:46.560000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:46.572000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:46.582000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:46.591000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:46.606000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:46.619000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:46.646000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:46.658000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:46.812000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:46.824000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:46.834000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:46.843000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:46.863000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:46.873000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:46.894000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:46.906000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:47.064000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:47.076000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:47.085000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:47.095000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:47.118000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:47.128000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:47.147000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:47.156000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:47.315000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:47.326000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:47.335000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:47.348000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:47.376000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:47.385000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:47.400000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:47.412000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:47.570000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:47.579000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:47.589000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:47.600000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:47.628000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:47.648000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:47.658000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:47.669000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:47.823000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:47.831000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:47.841000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:47.852000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:47.880000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:47.896000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:47.910000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:47.922000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:48.078000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:48.087000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:48.097000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:48.108000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:48.132000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:48.144000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:48.162000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:48.174000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:48.334000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:48.343000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:48.352000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:48.362000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:48.385000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:48.395000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:48.415000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:48.426000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:48.590000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:48.598000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:48.607000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:48.618000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:48.637000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:48.647000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:48.666000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:48.678000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:48.842000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:48.851000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:48.861000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:48.872000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:48.889000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:48.899000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:48.919000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:48.930000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:49.098000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:49.107000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:49.117000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:49.128000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:49.141000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:49.151000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:49.170000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:49.182000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:49.354000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:49.363000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:49.372000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:49.382000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:49.394000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:49.404000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:49.422000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:49.434000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:49.610000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:49.618000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:49.627000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:49.638000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:49.649000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:49.661000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:49.674000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:49.686000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:49.866000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:49.874000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:49.883000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:49.894000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:49.905000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:49.917000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:49.936000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:49.945000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:50.122000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:50.131000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:50.140000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:50.151000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:50.163000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:50.174000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:50.187000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:50.198000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:50.379000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:50.387000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:50.397000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:50.408000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:50.420000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:50.431000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:50.443000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:50.453000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:50.634000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:50.643000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:50.653000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:50.664000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:50.675000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:50.687000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:50.699000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:50.709000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:50.890000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:50.899000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:50.909000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:50.920000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:50.932000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:50.942000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:50.955000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:50.965000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:51.208000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:51.219000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:51.229000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:51.238000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:51.247000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:51.258000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:51.267000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:51.317000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:51.548000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:51.558000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:51.567000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:51.578000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:51.586000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:51.595000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:51.605000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:51.657000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:51.879000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:51.889000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:51.900000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:51.910000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:51.918000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:51.927000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:51.940000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:51.987000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:52.138000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:52.149000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:52.160000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:52.170000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:52.180000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:52.189000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:52.201000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:52.238000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:52.398000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:52.409000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:52.420000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:52.430000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:52.440000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:52.450000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:52.461000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:52.490000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:52.748000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:52.759000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:52.767000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:52.778000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:52.788000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:52.797000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:52.806000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:52.854000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:53.048000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:53.058000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:53.067000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:53.078000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:53.087000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:53.096000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:53.107000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:53.161000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:53.392000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:53.401000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:53.409000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:53.418000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:53.430000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:53.484000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:53.498000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:53.510000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:53.656000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:53.664000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:53.673000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:53.682000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:53.694000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:53.736000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:53.754000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:53.767000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:34:54.456000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:34:54.465000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:34:54.472000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:34:54.484000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:34:54.494000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:34:54.504000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:34:54.513000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:34:54.522000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1464 ms 100.0% 
  triton_mm_235 0.2161 ms 67.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2165 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2387 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2442 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_227 0.2446 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_241 0.2478 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2501 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2509 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2606 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7878 seconds and 0.5730 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1480 ms 100.0% 
  triton_mm_235 0.2239 ms 66.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2240 ms 66.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2422 ms 61.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2472 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2495 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2524 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2537 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2547 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2644 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8020 seconds and 0.5681 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1480 ms 100.0% 
  triton_mm_235 0.2208 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2214 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2414 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2462 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2492 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2518 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2523 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2540 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2634 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8546 seconds and 0.5294 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1465 ms 100.0% 
  triton_mm_234 0.2162 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_235 0.2182 ms 67.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_233 0.2392 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_227 0.2444 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_255 0.2445 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2487 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2509 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2524 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2620 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8845 seconds and 0.6750 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1504 ms 100.0% 
  triton_mm_235 0.2254 ms 66.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2267 ms 66.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2422 ms 62.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2482 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2502 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2533 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2546 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2566 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2659 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.9011 seconds and 0.4975 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1469 ms 100.0% 
  triton_mm_235 0.2199 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2219 ms 66.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2406 ms 61.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2440 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2469 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2493 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2509 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2511 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2594 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8769 seconds and 0.5146 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1499 ms 100.0% 
  triton_mm_235 0.2257 ms 66.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2263 ms 66.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2436 ms 61.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2485 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2509 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2531 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2543 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2548 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2642 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.9086 seconds and 0.4754 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1487 ms 100.0% 
  triton_mm_235 0.2224 ms 66.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2238 ms 66.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2431 ms 61.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2498 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2499 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2523 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2549 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2554 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2641 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8863 seconds and 0.4297 seconds precompiling for 39 choices
Capturing batches (bs=24 avail_mem=49.21 GB):  88%| | 46/52 [03:05<03:59, 39.85s/it]Capturing batches (bs=16 avail_mem=48.60 GB):  88%| | 46/52 [03:05<03:59, 39.85s/it][rank5]:W1023 11:35:11.297000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:11.314000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:11.321000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:11.350000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:11.363000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:11.364000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:11.368000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:11.380000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:11.382000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:11.388000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:11.419000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:11.432000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:11.436000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:11.436000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:11.448000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:11.454000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:11.460000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:11.492000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:11.505000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:11.508000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:11.521000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:11.793000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:11.860000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:11.925000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:11.932000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:11.941000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:11.950000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:11.983000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:11.998000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:12.006000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:12.008000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:12.017000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:12.024000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:12.029000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:12.063000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:12.075000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:12.078000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:12.089000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:12.093000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:12.096000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:12.096000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:12.133000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:12.143000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:12.147000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:12.157000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:12.161000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:12.164000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:12.165000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:12.200000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:12.216000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:12.225000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:12.232000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:12.419000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:12.497000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:12.565000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:12.599000 820 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank6]:W1023 11:35:12.616000 821 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank3]:W1023 11:35:12.626000 818 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank4]:W1023 11:35:12.632000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:12.669000 816 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank7]:W1023 11:35:12.677000 822 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank0]:W1023 11:35:12.684000 815 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank2]:W1023 11:35:12.713000 817 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank4]:W1023 11:35:13.117000 819 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_277 0.0091 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0091 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_276 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_272 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8184 seconds and 0.1756 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_271 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_270 0.0091 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_272 0.0092 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_279 0.0092 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_273 0.0093 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_278 0.0093 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8326 seconds and 0.1962 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_271 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_270 0.0089 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_272 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_273 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_276 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_275 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8005 seconds and 0.1645 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0092 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_273 0.0093 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_272 0.0093 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0093 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_275 0.0094 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0094 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8952 seconds and 0.1963 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0089 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_273 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_277 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_272 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0092 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0092 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0092 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_279 0.0093 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9016 seconds and 0.1651 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0093 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0093 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_275 0.0094 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_277 0.0094 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0094 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_274 0.0094 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_276 0.0094 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0094 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8926 seconds and 0.1181 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0088 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_274 0.0088 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_272 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_276 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_278 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_277 0.0090 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8562 seconds and 0.1390 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_271 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_270 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_276 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_279 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_277 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_274 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0092 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8905 seconds and 0.1373 seconds precompiling for 25 choices
[rank5]:W1023 11:35:22.855000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:22.930000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:23.102000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:23.111000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:23.355000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:23.432000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:23.493000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:23.545000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:23.616000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:23.625000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:23.991000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:24.063000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:24.227000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:24.248000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:24.731000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:24.749000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:24.902000 816 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank6]:W1023 11:35:25.086000 821 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank7]:W1023 11:35:25.306000 822 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank2]:W1023 11:35:25.555000 817 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank0]:W1023 11:35:25.616000 815 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank5]:W1023 11:35:25.851000 820 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank4]:W1023 11:35:26.234000 819 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank3]:W1023 11:35:26.264000 818 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0087 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0096 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0096 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_284 0.0097 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0099 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_298 0.0099 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0100 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0401 seconds and 0.4046 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0091 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0092 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_302 0.0096 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_285 0.0097 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0097 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_299 0.0099 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_298 0.0099 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_284 0.0100 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0290 seconds and 0.4118 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0095 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0097 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_298 0.0098 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0098 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_285 0.0101 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_300 0.0101 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8800 seconds and 0.3982 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_303 0.0094 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0095 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_284 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0097 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_298 0.0097 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0098 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8665 seconds and 0.3987 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0085 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_293 0.0091 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0091 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_284 0.0095 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0095 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0096 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0097 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_298 0.0097 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0097 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0467 seconds and 0.3995 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_292 0.0094 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0094 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0095 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_303 0.0095 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0096 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_285 0.0097 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_299 0.0098 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_298 0.0098 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.1301 seconds and 0.4198 seconds precompiling for 25 choices
[rank1]:W1023 11:35:31.612000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0085 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_293 0.0091 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0091 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_298 0.0096 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0097 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0097 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_285 0.0097 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_299 0.0097 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0098 ms 86.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9144 seconds and 0.4055 seconds precompiling for 25 choices
[rank1]:W1023 11:35:31.689000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:31.766000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:31.788000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:31.844000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0089 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0096 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0097 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_298 0.0098 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0099 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_301 0.0102 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0103 ms 85.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.1231 seconds and 0.4082 seconds precompiling for 25 choices
[rank6]:W1023 11:35:31.945000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:31.997000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:32.074000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:32.174000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:32.429000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:32.505000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:32.617000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:32.692000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:32.696000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:32.779000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:32.798000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:32.800000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:32.879000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:33.019000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:33.053000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:33.095000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:33.129000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:33.194000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:33.415000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:35.891000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:35.966000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:36.066000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:36 DP7 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1023 11:35:36.270000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:36.350000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:36.441000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:36.453000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:36.517000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:36 DP1 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1023 11:35:36.617000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:36 DP6 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1023 11:35:36.694000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:36.769000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:36.870000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:36 DP2 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1023 11:35:37.358000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:37.374000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:37.435000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:37.450000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:37.534000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:37.549000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:37 DP0 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:37 DP5 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1023 11:35:37.838000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:37.914000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:37.937000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:38.012000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:38.014000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:38 DP3 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank4]:W1023 11:35:38.111000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:38 DP4 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank7]:W1023 11:35:38.679000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:38.724000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:38.753000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:38.755000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:38.799000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:38.829000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:38.855000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:38.900000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:38 DP7 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1023 11:35:38.928000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:38 DP2 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:38 DP5 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1023 11:35:39.155000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:39.191000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:39.199000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:39.210000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:39.231000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:39.267000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:39.274000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:39.274000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:39.286000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:39.331000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:39.349000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:39.366000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:39.373000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:39 DP6 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1023 11:35:39.386000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:39 DP1 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:39 DP3 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank4]:W1023 11:35:39.448000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:39 DP0 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:35:39 DP4 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1023 11:35:40.012000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:40.023000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:40.031000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:40.043000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:40.053000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:40.063000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:40.072000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:40.088000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:40.268000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:40.279000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:40.288000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:40.301000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:40.315000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:40.325000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:40.335000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:40.348000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:40.520000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:40.532000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:40.556000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:40.564000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:40.574000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:40.585000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:40.596000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:40.607000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:35:40 DP5 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:35:40 DP7 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:35:40 DP6 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:35:40 DP2 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:35:40 DP1 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:35:40 DP3 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:35:40 DP0 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:35:40 DP4 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1023 11:35:42.010000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:42.040000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:42.051000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:42.086000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:42.116000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:42.129000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:42.148000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:42.162000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:42.187000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:42.192000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:42.214000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:35:42 DP7 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank5]:W1023 11:35:42.223000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:42.227000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:42.236000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:42.238000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:35:42 DP2 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank6]:W1023 11:35:42.263000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:35:42 DP1 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank5]:W1023 11:35:42.299000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:42.303000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:42.312000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:42.314000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:42.338000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:35:42 DP5 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank3]:W1023 11:35:42.379000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:42.388000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:42.389000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:35:42 DP6 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:35:42 DP3 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:35:42 DP0 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:35:42 DP4 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0126 ms 100.0% 
  triton_mm_307 0.0271 ms 46.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0419 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0455 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 25.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0598 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0611 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1903 seconds and 0.5983 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0128 ms 100.0% 
  triton_mm_307 0.0271 ms 47.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 25.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 22.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2427 seconds and 0.6758 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0130 ms 100.0% 
  triton_mm_307 0.0271 ms 47.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0454 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0496 ms 26.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0533 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0583 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0602 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1913 seconds and 0.7024 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0124 ms 100.0% 
  triton_mm_307 0.0271 ms 45.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0421 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0455 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0496 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0533 ms 23.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0583 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2485 seconds and 0.5721 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0127 ms 100.0% 
  triton_mm_307 0.0270 ms 47.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0419 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0452 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 25.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2045 seconds and 0.4262 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0125 ms 100.0% 
  triton_mm_307 0.0272 ms 46.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0419 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0454 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0581 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0601 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0744 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2422 seconds and 0.5655 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0122 ms 100.0% 
  triton_mm_307 0.0273 ms 44.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0496 ms 24.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0602 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0618 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0745 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.3107 seconds and 0.4348 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0123 ms 100.0% 
  triton_mm_307 0.0271 ms 45.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0455 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0601 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0745 ms 16.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2591 seconds and 0.4113 seconds precompiling for 39 choices
[rank2]:W1023 11:35:52.658000 817 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7740909aa0>
[rank2]:W1023 11:35:52.680000 817 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f77409099e0>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:35:52 DP2 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1023 11:35:52.873000 822 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f94b03b5e30>
[rank7]:W1023 11:35:52.895000 822 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f94b03b59e0>
[rank1]:W1023 11:35:52.922000 816 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f17ff505890>
[rank1]:W1023 11:35:52.945000 816 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f17ff505b30>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:35:52 DP7 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:35:53 DP1 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1023 11:35:53.051000 820 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef392bddda0>
[rank4]:W1023 11:35:53.070000 819 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3220b967f0>
[rank5]:W1023 11:35:53.074000 820 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef392bddad0>
[rank4]:W1023 11:35:53.092000 819 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3220b965b0>
[rank3]:W1023 11:35:53.104000 818 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8825499e60>
[rank6]:W1023 11:35:53.124000 821 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ed15d2d2400>
[rank3]:W1023 11:35:53.126000 818 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f882549a070>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:35:53 DP5 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1023 11:35:53.146000 815 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faab87b5ad0>
[rank6]:W1023 11:35:53.147000 821 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ed15d2d2190>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:35:53 DP4 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1023 11:35:53.169000 815 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faab87b5980>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:35:53 DP3 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:35:53 DP6 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:35:53 DP0 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1023 11:35:54.383000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:54.395000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:54.409000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:54.422000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:54.435000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:54.461000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:54.498000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:54.688000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:54.835000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:54.846000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:54.854000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:54.865000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:54.891000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:54.924000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:54.937000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:54.948000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:55.215000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:55.225000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:55.234000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:55.242000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:55.253000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:55.263000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:55.274000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:55.326000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:55.625000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:55.635000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:55.646000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:55.655000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:55.664000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:55.674000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:55.702000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:55.732000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:55.888000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:55.899000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:55.909000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:55.920000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:55.930000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:55.939000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:55.957000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:55.984000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:56.149000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:56.159000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:56.169000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:56.182000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:56.191000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:56.202000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:56.213000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:56.236000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:56.408000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:56.419000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:56.429000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:56.454000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:56.464000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:56.473000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:56.484000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:56.496000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:56.668000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:56.679000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:56.689000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:56.715000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:56.725000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:56.734000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:56.745000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:56.757000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:56.927000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:56.935000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:56.944000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:56.977000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:56.989000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:57.001000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:57.014000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:57.024000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:57.186000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:57.194000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:57.202000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:57.243000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:57.255000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:57.265000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:57.277000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:57.290000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:57.448000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:57.459000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:57.469000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:57.502000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:57.513000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:57.523000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:57.534000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:57.546000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:57.708000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:57.719000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:57.729000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:57.763000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:57.773000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:57.782000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:57.793000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:57.805000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:57.962000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:57.971000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:57.982000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:58.025000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:58.037000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:58.049000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:58.062000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:58.072000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:58.222000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:58.231000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:58.239000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:58.284000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:58.297000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:58.309000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:58.321000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:58.332000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:58.482000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:58.490000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:58.499000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:58.540000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:58.553000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:58.565000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:58.576000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:58.588000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:58.742000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:58.751000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:58.759000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:58.797000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:58.810000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:58.822000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:58.833000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:58.845000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:59.002000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:59.011000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:59.019000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:59.064000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:59.076000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:59.088000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:59.099000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:59.111000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:59.262000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:59.271000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:59.279000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:59.328000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:59.344000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:59.357000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:59.369000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:59.379000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:59.526000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:59.535000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:59.543000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:59.584000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:59.601000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:59.613000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:59.624000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:59.635000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:35:59.786000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:35:59.795000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:35:59.803000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:35:59.844000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:35:59.857000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:35:59.869000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:35:59.880000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:35:59.892000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:00.046000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:00.054000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:00.063000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:00.100000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:00.112000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:00.125000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:00.136000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:00.148000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:00.310000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:00.319000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:00.327000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:00.356000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:00.372000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:00.385000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:00.396000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:00.407000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:00.570000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:00.579000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:00.587000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:00.616000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:00.628000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:00.641000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:00.652000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:00.664000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:00.834000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:00.843000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:00.851000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:00.889000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:00.900000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:00.913000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:00.925000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:00.934000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:01.094000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:01.103000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:01.111000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:01.149000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:01.160000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:01.173000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:01.183000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:01.195000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:01.354000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:01.363000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:01.371000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:01.408000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:01.421000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:01.433000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:01.444000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:01.455000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:01.618000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:01.627000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:01.635000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:01.668000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:01.681000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:01.693000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:01.704000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:01.715000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:01.882000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:01.891000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:01.899000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:01.928000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:01.937000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:01.949000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:01.961000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:01.972000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:02.144000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:02.155000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:02.165000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:02.190000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:02.200000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:02.210000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:02.223000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:02.233000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:02.402000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:02.412000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:02.422000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:02.457000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:02.468000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:02.481000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:02.491000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:02.503000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:02.666000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:02.675000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:02.683000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:02.717000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:02.728000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:02.741000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:02.751000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:02.762000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:02.930000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:02.938000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:02.946000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:02.989000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:03.000000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:03.012000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:03.023000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:03.035000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:03.194000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:03.202000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:03.211000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:03.248000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:03.260000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:03.271000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:03.283000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:03.295000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:03.457000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:03.467000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:03.478000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:03.510000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:03.520000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:03.532000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:03.543000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:03.553000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:03.716000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:03.728000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:03.739000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:03.786000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:03.797000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:03.806000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:03.817000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:03.827000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:03.975000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:03.986000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:03.995000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:04.052000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:04.062000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:04.074000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:04.087000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:04.099000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:04.454000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:04.462000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:04.474000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:04.483000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:04.491000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:04.503000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:04.513000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:04.565000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:04.923000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:04.933000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:04.942000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:04.953000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:04.986000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:04.996000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:05.008000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:05.033000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:05.327000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:05.338000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:05.349000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:05.359000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:05.368000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:05.378000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:05.388000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:05.442000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:05.753000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:05.762000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:05.772000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:05.781000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:05.792000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:05.803000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:05.854000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:05.867000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:06.020000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:06.029000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:06.039000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:06.049000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:06.061000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:06.073000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:06.115000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:06.127000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:06.285000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:06.293000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:06.312000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:06.323000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:06.334000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:06.346000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:06.378000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:06.389000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:06.547000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:06.569000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:06.581000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:06.592000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:06.602000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:06.612000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:06.640000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:06.651000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:06.810000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:06.832000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:06.845000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:06.856000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:06.867000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:06.876000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:06.899000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:06.911000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:07.081000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:07.091000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:07.107000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:07.117000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:07.130000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:07.141000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:07.158000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:07.172000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:07.344000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:07.354000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:07.371000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:07.381000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:07.392000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:07.404000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:07.419000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:07.431000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:07.607000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:07.621000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:07.640000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:07.652000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:07.662000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:07.672000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:07.684000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:07.694000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:07.870000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:07.884000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:07.904000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:07.916000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:07.927000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:07.936000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:07.947000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:07.958000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:08.137000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:08.146000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:08.167000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:08.176000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:08.189000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:08.200000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:08.210000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:08.223000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:08.400000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:08.409000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:08.443000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:08.452000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:08.463000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:08.475000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:08.488000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:08.497000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:08.664000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:08.673000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:08.706000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:08.716000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:08.728000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:08.739000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:08.752000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:08.762000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:08.924000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:08.934000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:08.971000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:08.981000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:08.992000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:09.004000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:09.016000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:09.026000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:09.185000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:09.198000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:09.246000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:09.257000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:09.267000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:09.278000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:09.291000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:09.301000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:09.453000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:09.462000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:09.511000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:09.522000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:09.532000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:09.545000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:09.557000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:09.567000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:09.721000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:09.729000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:09.779000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:09.790000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:09.799000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:09.813000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:09.824000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:09.834000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:09.988000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:09.997000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:10.043000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:10.054000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:10.064000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:10.076000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:10.088000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:10.099000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:10.252000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:10.261000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:10.306000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:10.318000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:10.328000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:10.340000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:10.353000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:10.363000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:11.061000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:11.073000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:11.091000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:11.101000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:11.111000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:11.120000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:11.130000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:11.139000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1058 ms 100.0% 
  triton_mm_359 0.1488 ms 71.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1537 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1683 ms 62.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1711 ms 61.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1784 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1785 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_357 0.1858 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_351 0.1867 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_345 0.2081 ms 50.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6876 seconds and 0.4837 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1046 ms 100.0% 
  triton_mm_359 0.1450 ms 72.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1482 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1640 ms 63.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1657 ms 63.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1763 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1767 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1788 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1838 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2050 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6599 seconds and 0.7107 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1059 ms 100.0% 
  triton_mm_359 0.1486 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1522 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1642 ms 64.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1661 ms 63.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1761 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1766 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1811 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1837 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2078 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6854 seconds and 0.6915 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1056 ms 100.0% 
  triton_mm_359 0.1495 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1526 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1662 ms 63.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1684 ms 62.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1770 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1771 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1818 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1848 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2066 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6625 seconds and 0.5715 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1035 ms 100.0% 
  triton_mm_359 0.1439 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1478 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1631 ms 63.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1648 ms 62.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1757 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1758 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1806 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1832 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2014 ms 51.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6438 seconds and 0.4743 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1076 ms 100.0% 
  triton_mm_359 0.1507 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1553 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1673 ms 64.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1690 ms 63.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1778 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1781 ms 60.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1845 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1853 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2086 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6721 seconds and 0.6686 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1066 ms 100.0% 
  triton_mm_359 0.1502 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1540 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1681 ms 63.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1694 ms 62.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1782 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1783 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_357 0.1862 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_351 0.1863 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_345 0.2080 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6666 seconds and 0.4333 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1064 ms 100.0% 
  triton_mm_359 0.1479 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1541 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1690 ms 62.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1704 ms 62.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1775 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1778 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1854 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_351 0.1858 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_345 0.2090 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6746 seconds and 0.4272 seconds precompiling for 39 choices
Capturing batches (bs=16 avail_mem=48.60 GB):  90%| | 47/52 [04:22<04:15, 51.07s/it]Capturing batches (bs=12 avail_mem=47.98 GB):  90%| | 47/52 [04:22<04:15, 51.07s/it][rank4]:W1023 11:36:28.540000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:28.566000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:28.590000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:28.599000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:28.607000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:28.609000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:28.618000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:28.632000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:28.636000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:28.642000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:28.660000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:28.668000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:28.676000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:28.681000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:28.687000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:28.702000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:28.709000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:28.711000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:28.734000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:28.741000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:28.749000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:28.759000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:28.776000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:28.785000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:29.177000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:29.205000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:29.234000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:29.243000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:29.253000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:29.260000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:29.263000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:29.275000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:29.286000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:29.288000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:29.319000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:29.330000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:29.337000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:29.345000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:29.345000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:29.359000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:29.368000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:29.378000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:29.389000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:29.399000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:29.408000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:29.415000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:29.415000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:29.429000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:29.439000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:29.448000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:29.458000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:29.478000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:29.483000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:29.484000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:29.508000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:29.517000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:30.060000 819 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank3]:W1023 11:36:30.070000 818 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank5]:W1023 11:36:30.093000 820 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank0]:W1023 11:36:30.096000 815 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank2]:W1023 11:36:30.103000 817 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank6]:W1023 11:36:30.125000 821 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank7]:W1023 11:36:30.146000 822 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank1]:W1023 11:36:30.157000 816 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_395 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0091 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_397 0.0091 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_396 0.0091 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_403 0.0091 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_402 0.0092 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_398 0.0092 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0093 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8048 seconds and 0.2169 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_395 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0089 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0089 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_402 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_399 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_403 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_393 0.0092 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7918 seconds and 0.2558 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0088 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0088 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0090 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_402 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_403 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_393 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_392 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8482 seconds and 0.1067 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_400 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_394 0.0089 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0089 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_401 0.0090 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_399 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_392 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_398 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8788 seconds and 0.2872 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0089 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0090 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_396 0.0090 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_401 0.0090 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_392 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_400 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_403 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_393 0.0092 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8826 seconds and 0.1338 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_401 0.0088 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_392 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_393 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_398 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_403 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_397 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8594 seconds and 0.1400 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_395 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0091 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0091 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0091 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0092 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_402 0.0092 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_403 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_392 0.0093 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_398 0.0094 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9284 seconds and 0.2031 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0089 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_403 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_401 0.0091 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_398 0.0091 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0091 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0092 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9227 seconds and 0.2413 seconds precompiling for 25 choices
[rank4]:W1023 11:36:40.218000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:40.357000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:40.533000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:40.552000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:40.723000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:40.877000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:40.877000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:40.908000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:41.032000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:41.055000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:41.206000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:41.228000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:41.372000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:41.408000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:41.701000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:41.731000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:42.417000 819 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank1]:W1023 11:36:42.421000 816 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank2]:W1023 11:36:42.516000 817 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank7]:W1023 11:36:42.715000 822 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank5]:W1023 11:36:42.845000 820 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank3]:W1023 11:36:42.903000 818 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank6]:W1023 11:36:43.054000 821 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank0]:W1023 11:36:43.116000 815 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_415 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0086 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_427 0.0094 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0095 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_423 0.0096 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0097 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_408 0.0098 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0099 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7881 seconds and 0.4388 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_426 0.0094 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0094 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_422 0.0095 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0095 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0099 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0099 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9486 seconds and 0.4223 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0090 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0090 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_427 0.0091 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0092 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_408 0.0094 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0095 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_423 0.0096 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0097 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8885 seconds and 0.4269 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_415 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_427 0.0093 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_423 0.0094 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_426 0.0094 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_422 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_408 0.0098 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0098 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9182 seconds and 0.4231 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0092 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0092 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_426 0.0096 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0097 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_422 0.0098 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0098 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0099 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0101 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8947 seconds and 0.4131 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_415 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0083 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0089 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0089 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_409 0.0091 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_426 0.0095 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_408 0.0096 ms 85.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_427 0.0096 ms 85.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_422 0.0097 ms 84.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0097 ms 84.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9647 seconds and 0.4223 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0092 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_427 0.0095 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0096 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_409 0.0097 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_423 0.0097 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0098 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_422 0.0098 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9305 seconds and 0.4212 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0090 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0090 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_426 0.0092 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0092 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_423 0.0093 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0094 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0096 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0097 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9109 seconds and 0.4123 seconds precompiling for 25 choices
[rank1]:W1023 11:36:48.862000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:48.938000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:49.036000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:49.269000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:49.346000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:49.447000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:49.461000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:49.480000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:49.515000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:49.538000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:49.556000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:49.592000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:49.636000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:49.639000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:49.656000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:49.692000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:49.715000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:49.753000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:49.816000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:49.830000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:49.931000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:49.947000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:50.024000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:50.124000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:53.435000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:53.512000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:53.524000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:53.601000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:53.614000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:53.642000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:53.702000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:53.719000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:53.821000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:53.981000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:54.058000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:54.160000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:54.196000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:54.273000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:54.274000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:54.349000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:54.375000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:54.459000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:54.527000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:54.603000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:54.704000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:54.739000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:54.818000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:54.919000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:55.486000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:55.499000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:55.534000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:55.541000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:55.563000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:55.576000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:55.611000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:55.619000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:55.667000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:55.673000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:55.712000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:55.727000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:55.750000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:55.856000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:55.880000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:55.926000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:55.958000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:55.977000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:56.003000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:56.062000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:56.104000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:56.110000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:56.190000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:56.293000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:57.123000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:57.141000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:57.153000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:57.165000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:57.176000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:57.187000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:57.198000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:57.215000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:57.387000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:57.405000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:57.417000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:57.429000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:57.441000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:57.453000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:57.467000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:57.480000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:57.890000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:57.899000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:57.908000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:57.918000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:57.928000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:57.947000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:57.997000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:58.009000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:59.504000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:59.558000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:59.569000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:59.582000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:59.582000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:59.592000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:59.600000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:59.611000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:36:59.631000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:59.635000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:59.636000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:59.646000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:59.659000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:59.669000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:59.677000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:36:59.684000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:59.688000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:36:59.696000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:36:59.708000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:59.713000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:36:59.719000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:36:59.727000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:36:59.738000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:36:59.763000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0110 ms 100.0% 
  triton_mm_431 0.0278 ms 39.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0420 ms 26.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0495 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0534 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1779 seconds and 0.7772 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0113 ms 100.0% 
  triton_mm_431 0.0279 ms 40.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0420 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0455 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0495 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0534 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0583 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0756 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2206 seconds and 0.6583 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0111 ms 100.0% 
  triton_mm_431 0.0277 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0492 ms 22.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0533 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0594 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2628 seconds and 0.4708 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0113 ms 100.0% 
  triton_mm_431 0.0278 ms 40.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0494 ms 22.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0610 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 15.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2872 seconds and 0.5734 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0111 ms 100.0% 
  triton_mm_431 0.0278 ms 40.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0418 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0452 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0491 ms 22.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0530 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0609 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2792 seconds and 0.5452 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0112 ms 100.0% 
  triton_mm_431 0.0277 ms 40.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0420 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0452 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0492 ms 22.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0595 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0755 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2648 seconds and 0.7126 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0113 ms 100.0% 
  triton_mm_431 0.0279 ms 40.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0491 ms 22.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0598 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0609 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2469 seconds and 0.6719 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0114 ms 100.0% 
  triton_mm_431 0.0278 ms 40.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 23.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0531 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0580 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 15.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2913 seconds and 0.8409 seconds precompiling for 39 choices
[rank1]:W1023 11:37:10.401000 816 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f17ff505890>
[rank1]:W1023 11:37:10.424000 816 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f17ff505b30>
[rank7]:W1023 11:37:10.431000 822 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f94b03b5e30>
[rank7]:W1023 11:37:10.454000 822 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f94b03b59e0>
[rank3]:W1023 11:37:10.483000 818 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8825499e60>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:37:10 DP1 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1023 11:37:10.506000 818 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f882549a070>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:37:10 DP7 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:37:10 DP3 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1023 11:37:10.579000 817 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7740909aa0>
[rank4]:W1023 11:37:10.592000 819 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3220b967f0>
[rank5]:W1023 11:37:10.601000 820 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef392bddda0>
[rank2]:W1023 11:37:10.602000 817 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f77409099e0>
[rank6]:W1023 11:37:10.611000 821 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ed15d2d2400>
[rank4]:W1023 11:37:10.615000 819 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3220b965b0>
[rank5]:W1023 11:37:10.624000 820 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef392bddad0>
[rank6]:W1023 11:37:10.634000 821 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ed15d2d2190>
[rank0]:W1023 11:37:10.649000 815 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faab87b5ad0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:37:10 DP2 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1023 11:37:10.672000 815 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faab87b5980>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:37:10 DP4 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:37:10 DP5 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:37:10 DP6 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:37:10 DP0 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1023 11:37:12.198000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:12.208000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:12.217000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:12.229000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:12.240000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:12.251000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:12.264000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:12.311000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:12.467000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:12.477000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:12.487000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:12.499000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:12.511000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:12.523000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:12.537000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:12.571000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:12.734000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:12.744000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:12.755000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:12.766000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:12.779000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:12.791000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:12.805000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:12.831000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:13.002000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:13.012000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:13.023000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:13.035000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:13.047000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:13.060000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:13.073000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:13.091000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:13.270000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:13.280000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:13.291000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:13.302000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:13.315000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:13.327000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:13.340000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:13.351000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:13.538000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:13.548000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:13.559000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:13.570000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:13.583000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:13.595000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:13.608000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:13.619000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:13.806000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:13.816000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:13.827000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:13.838000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:13.851000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:13.863000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:13.876000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:13.887000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:14.077000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:14.089000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:14.101000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:14.111000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:14.121000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:14.132000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:14.143000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:14.156000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:14.356000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:14.369000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:14.379000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:14.390000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:14.402000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:14.411000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:14.422000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:14.434000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:14.624000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:14.637000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:14.647000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:14.659000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:14.670000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:14.680000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:14.690000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:14.703000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:14.888000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:14.905000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:14.914000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:14.926000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:14.937000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:14.947000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:14.958000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:14.971000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:15.151000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:15.168000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:15.181000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:15.191000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:15.205000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:15.217000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:15.230000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:15.241000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:15.418000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:15.431000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:15.445000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:15.459000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:15.472000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:15.484000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:15.497000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:15.508000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:15.686000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:15.696000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:15.709000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:15.727000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:15.740000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:15.752000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:15.765000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:15.776000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:15.954000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:15.964000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:15.975000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:15.995000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:16.007000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:16.019000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:16.032000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:16.043000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:16.225000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:16.236000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:16.246000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:16.265000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:16.275000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:16.285000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:16.296000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:16.309000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:16.492000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:16.505000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:16.515000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:16.537000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:16.549000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:16.559000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:16.568000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:16.581000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:16.755000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:16.767000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:16.781000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:16.803000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:16.817000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:16.830000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:16.843000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:16.853000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:17.022000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:17.032000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:17.045000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:17.071000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:17.084000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:17.096000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:17.109000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:17.120000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:17.290000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:17.300000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:17.311000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:17.339000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:17.352000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:17.364000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:17.377000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:17.388000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:17.558000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:17.568000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:17.579000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:17.607000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:17.620000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:17.631000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:17.644000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:17.655000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:17.826000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:17.836000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:17.847000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:17.885000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:17.895000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:17.907000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:17.920000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:17.931000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:18.094000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:18.104000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:18.115000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:18.149000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:18.173000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:18.183000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:18.195000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:18.206000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:18.363000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:18.372000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:18.383000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:18.413000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:18.437000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:18.451000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:18.463000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:18.474000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:18.895000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:18.905000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:18.916000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:18.926000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:18.937000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:18.949000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:18.958000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:19.001000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:19.166000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:19.177000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:19.188000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:19.200000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:19.211000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:19.221000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:19.233000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:19.263000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:19.689000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:19.700000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:19.712000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:19.721000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:19.731000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:19.742000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:19.752000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:19.799000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:19.969000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:19.981000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:19.991000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:20.002000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:20.014000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:20.026000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:20.067000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:20.278000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:20.483000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:20.492000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:20.502000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:20.512000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:20.524000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:20.536000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:20.549000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:20.595000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:20.759000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:20.767000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:20.779000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:20.788000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:20.801000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:20.814000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:20.828000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:20.863000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:21.331000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:21.340000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:21.350000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:21.361000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:21.372000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:21.383000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:21.439000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:21.451000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:21.606000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:21.615000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:21.624000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:21.645000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:21.655000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:21.666000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:21.709000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:21.722000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:21.878000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:21.887000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:21.896000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:21.913000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:21.923000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:21.936000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:21.973000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:21.988000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:22.151000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:22.160000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:22.168000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:22.181000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:22.191000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:22.204000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:22.237000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:22.256000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:22.422000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:22.435000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:22.444000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:22.456000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:22.466000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:22.478000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:22.505000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:22.525000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:22.690000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:22.707000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:22.716000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:22.727000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:22.738000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:22.750000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:22.773000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:22.792000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:22.961000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:22.981000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:22.992000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:23.002000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:23.015000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:23.025000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:23.038000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:23.059000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:23.233000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:23.253000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:23.264000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:23.280000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:23.290000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:23.302000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:23.318000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:23.338000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:23.505000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:23.525000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:23.536000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:23.547000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:23.559000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:23.570000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:23.587000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:23.607000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:23.773000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:23.793000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:23.805000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:23.816000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:23.826000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:23.838000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:23.855000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:23.879000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:24.041000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:24.065000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:24.077000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:24.088000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:24.102000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:24.118000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:24.135000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:24.159000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:24.315000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:24.331000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:24.343000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:24.352000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:24.373000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:24.388000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:24.405000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:24.433000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:24.587000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:24.603000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:24.612000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:24.621000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:24.641000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:24.653000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:24.673000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:24.700000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:24.858000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:24.875000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:24.884000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:24.892000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:24.909000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:24.921000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:24.941000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:24.969000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:25.130000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:25.147000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:25.156000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:25.165000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:25.177000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:25.189000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:25.209000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:25.236000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:25.398000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:25.419000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:25.428000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:25.437000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:25.450000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:25.462000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:25.476000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:25.504000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:25.666000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:25.690000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:25.699000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:25.709000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:25.721000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:25.734000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:25.747000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:25.772000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:25.934000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:25.962000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:25.971000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:25.980000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:25.996000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:26.012000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:26.025000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:26.048000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:26.202000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:26.235000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:26.244000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:26.252000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:26.265000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:26.280000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:26.293000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:26.316000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:26.471000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:26.507000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:26.516000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:26.525000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:26.537000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:26.549000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:26.563000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:26.584000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:26.738000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:26.779000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:26.788000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:26.797000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:26.810000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:26.822000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:26.835000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:26.852000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:27.009000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:27.053000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:27.065000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:27.076000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:27.086000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:27.097000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:27.108000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:27.120000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:27.281000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:27.325000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:27.337000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:27.347000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:27.358000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:27.369000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:27.379000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:27.390000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:27.553000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:27.597000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:27.609000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:27.620000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:27.630000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:27.641000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:27.651000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:27.662000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:27.825000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:27.869000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:27.881000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:27.891000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:27.910000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:27.923000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:27.933000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:27.944000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:28.105000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:28.141000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:28.153000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:28.163000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:28.190000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:28.203000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:28.212000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:28.227000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:28.383000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:28.411000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:28.420000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:28.430000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:28.461000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:28.476000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:28.489000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:28.503000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:29.206000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:29.215000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:29.224000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:29.233000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:29.242000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:29.252000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:29.265000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:29.275000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0894 ms 100.0% 
  triton_mm_483 0.1448 ms 61.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1473 ms 60.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1611 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1636 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1681 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1752 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1754 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1795 ms 49.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1821 ms 49.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.6369 seconds and 0.4444 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0900 ms 100.0% 
  triton_mm_483 0.1439 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1486 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1629 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1651 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1690 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1760 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1763 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1778 ms 50.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1829 ms 49.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.6708 seconds and 0.6539 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0908 ms 100.0% 
  triton_mm_483 0.1480 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1512 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1634 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1643 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1708 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1754 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1755 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1803 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1823 ms 49.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.6435 seconds and 0.6448 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0908 ms 100.0% 
  triton_mm_483 0.1469 ms 61.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1521 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1645 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1671 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1705 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1765 ms 51.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1774 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1814 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1836 ms 49.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7105 seconds and 0.5348 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0924 ms 100.0% 
  triton_mm_483 0.1487 ms 62.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1543 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1654 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1678 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1745 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1772 ms 52.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1773 ms 52.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_481 0.1842 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_475 0.1849 ms 50.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7540 seconds and 0.7399 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0926 ms 100.0% 
  triton_mm_483 0.1494 ms 62.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1556 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1677 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1692 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1729 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1775 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1776 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_481 0.1850 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_475 0.1852 ms 50.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7426 seconds and 0.5701 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0920 ms 100.0% 
  triton_mm_483 0.1462 ms 62.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1534 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1672 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1684 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1718 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1777 ms 51.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1782 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1847 ms 49.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_475 0.1848 ms 49.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6965 seconds and 0.4768 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0929 ms 100.0% 
  triton_mm_483 0.1499 ms 62.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1536 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1674 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1694 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1729 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1776 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1779 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1847 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1852 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7498 seconds and 0.3763 seconds precompiling for 39 choices
Capturing batches (bs=12 avail_mem=47.98 GB):  92%|| 48/52 [05:39<03:55, 58.76s/it]Capturing batches (bs=8 avail_mem=47.39 GB):  92%|| 48/52 [05:39<03:55, 58.76s/it] [rank2]:W1023 11:37:45.435000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:45.450000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:45.461000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:45.479000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:45.486000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:45.500000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:45.504000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:45.511000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:45.519000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:45.531000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:45.549000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:45.556000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:45.569000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:45.578000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:45.580000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:45.593000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:45.604000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:45.623000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:45.630000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:45.632000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:45.643000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:45.654000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:45.702000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:45.776000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:46.075000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:46.085000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:46.094000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:46.119000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:46.128000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:46.139000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:46.158000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:46.162000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:46.172000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:46.178000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:46.203000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:46.212000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:46.221000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:46.228000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:46.243000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:46.246000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:46.249000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:46.271000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:46.274000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:46.284000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:46.292000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:46.298000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:46.313000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:46.317000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:46.319000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:46.345000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:46.354000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:46.356000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:46.363000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:46.388000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:46.428000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:46.499000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:46.760000 817 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank6]:W1023 11:37:46.776000 821 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank7]:W1023 11:37:46.780000 822 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank1]:W1023 11:37:46.813000 816 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank5]:W1023 11:37:46.819000 820 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank3]:W1023 11:37:46.827000 818 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank4]:W1023 11:37:46.848000 819 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank0]:W1023 11:37:46.977000 815 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_517 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0086 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_516 0.0086 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_521 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_523 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_522 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7640 seconds and 0.1505 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0089 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0089 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_523 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0091 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0091 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_522 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8109 seconds and 0.2035 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_519 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0087 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_521 0.0087 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_524 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_522 0.0088 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_523 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_527 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8262 seconds and 0.1603 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_521 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_522 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_523 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_527 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_524 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8803 seconds and 0.2434 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0086 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0086 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_521 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_525 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0088 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_526 0.0088 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0088 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_524 0.0088 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8966 seconds and 0.2405 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_519 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0087 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_521 0.0087 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_524 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_525 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_527 0.0088 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_526 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8617 seconds and 0.1532 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_519 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0089 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_520 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_524 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_527 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_525 0.0089 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9208 seconds and 0.1624 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_522 0.0088 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_523 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_527 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_525 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8780 seconds and 0.1637 seconds precompiling for 25 choices
[rank1]:W1023 11:37:57.047000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:57.461000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:57.487000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:57.519000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:57.524000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:57.545000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:57.687000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:57.921000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:37:57.967000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:37:57.996000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:37:58.017000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:37:58.027000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:58.086000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:37:58.194000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:37:58.440000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:37:58.603000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:37:58.927000 816 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank4]:W1023 11:37:59.359000 819 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank3]:W1023 11:37:59.407000 818 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank6]:W1023 11:37:59.446000 821 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank2]:W1023 11:37:59.586000 817 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank7]:W1023 11:37:59.850000 822 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank0]:W1023 11:37:59.928000 815 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank5]:W1023 11:38:00.351000 820 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_539 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_538 0.0086 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0087 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_551 0.0093 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0095 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_546 0.0096 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0096 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_533 0.0100 ms 84.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0101 ms 83.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8751 seconds and 0.4176 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_539 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_538 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_540 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_551 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0095 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_547 0.0095 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0096 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0100 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_548 0.0100 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9190 seconds and 0.4099 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0088 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0089 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0096 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_550 0.0096 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_532 0.0097 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0097 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_551 0.0097 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_533 0.0099 ms 83.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9358 seconds and 0.4296 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0088 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0088 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0095 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0095 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_532 0.0096 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_546 0.0097 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0097 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_547 0.0097 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9302 seconds and 0.4140 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0087 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0087 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0092 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0092 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_547 0.0095 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0095 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_532 0.0095 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_548 0.0097 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8528 seconds and 0.4281 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_547 0.0092 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0092 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0093 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_546 0.0094 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_532 0.0096 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0097 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8951 seconds and 0.4045 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_541 0.0088 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0089 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_550 0.0093 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0093 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_547 0.0095 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0095 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_532 0.0099 ms 85.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0099 ms 85.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8711 seconds and 0.3972 seconds precompiling for 25 choices
[rank1]:W1023 11:38:05.771000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0088 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0088 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0095 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_547 0.0096 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0096 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_532 0.0097 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0097 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_551 0.0097 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9274 seconds and 0.4248 seconds precompiling for 25 choices
[rank1]:W1023 11:38:05.848000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:38:05.937000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:38:05.948000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:38:05.996000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:38:06.015000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:38:06.030000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:38:06.074000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:38:06.104000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:38:06.108000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:38:06.116000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:38:06.174000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:38:06.181000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:38:06.211000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:38:06.281000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:38:06.369000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:38:06.436000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:38:06.446000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:38:06.514000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:38:06.547000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:38:06.617000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:38:06.899000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:38:06.978000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:38:07.081000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:38:11.639000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:38:11.639000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:38:11.639000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:38:11.639000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:38:11.641000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:38:11.641000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:38:11.642000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:38:11.644000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:38:11.715000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:38:11.716000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:38:11.716000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:38:11.716000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:38:11.718000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:38:11.719000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:38:11.719000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:38:11.722000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:38:11.817000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:38:11.817000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:38:11.818000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:38:11.818000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:38:11.820000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:38:11.821000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:38:11.822000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:38:11.825000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:38:11 DP7 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:38:11 DP2 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:38:11 DP5 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:38:11 DP4 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:38:11 DP1 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:38:11 DP3 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:38:11 DP0 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:38:11 DP6 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1023 11:38:13.003000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:38:13.026000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:38:13.051000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:38:13.060000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:38:13.072000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:38:13.082000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:38:13.082000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:38:13.090000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:38:13.103000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:38:13.105000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:38:13.127000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:38:13.137000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:38:13.148000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:38:13.159000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:38:13.167000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:38:13.183000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:38:13.185000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:38:13.205000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:38:13.228000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:38:13.240000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:38:13 DP0 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1023 11:38:13.250000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:38:13.260000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:38:13 DP7 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1023 11:38:13.269000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:38:13 DP4 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1023 11:38:13.285000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:38:13 DP3 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:38:13 DP5 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:38:13 DP1 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:38:13 DP2 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:38:13 DP6 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1023 11:38:13.859000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:38:13.870000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:38:13.882000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:38:13.904000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:38:13.915000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:38:13.927000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:38:13.938000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:38:13.952000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:38:14.135000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:38:14.146000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:38:14.158000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:38:14.175000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:38:14.188000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:38:14.200000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:38:14.210000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:38:14.224000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:38:14.407000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:38:14.419000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:38:14.430000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:38:14.444000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:38:14.456000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:38:14.477000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:38:14.488000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:38:14.502000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:38:14 DP6 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
