INFO 11-05 14:15:42 __init__.py:179] Automatically detected platform rocm.
WARNING 11-05 14:15:42 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-05 14:15:42] WARNING server_args.py:1165: Attention backend not explicitly specified. Use aiter backend by default.
[2025-11-05 14:15:42] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-05 14:15:42] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.765, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=149244306, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, decrypted_config_file=None, decrypted_draft_config_file=None)
[2025-11-05 14:15:42] Using default HuggingFace chat template with detected content format: string
INFO 11-05 14:15:52 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
INFO 11-05 14:15:52 __init__.py:179] Automatically detected platform rocm.
[2025-11-05 14:15:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-05 14:15:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 11-05 14:15:52 __init__.py:179] Automatically detected platform rocm.
INFO 11-05 14:15:52 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-05 14:15:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 11-05 14:15:52 __init__.py:179] Automatically detected platform rocm.
[2025-11-05 14:15:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-05 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-05 14:15:53 TP4] Process 291 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[2025-11-05 14:15:53 TP0] Process 287 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
INFO 11-05 14:15:53 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
INFO 11-05 14:15:53 __init__.py:179] Automatically detected platform rocm.
INFO 11-05 14:15:53 __init__.py:179] Automatically detected platform rocm.
[2025-11-05 14:15:53 TP6] Process 293 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
[2025-11-05 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 11-05 14:15:53 __init__.py:179] Automatically detected platform rocm.
[2025-11-05 14:15:53 TP7] Process 294 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-05 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-05 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-05 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-05 14:15:53 TP4] Init torch distributed begin.
[2025-11-05 14:15:53 TP0] Init torch distributed begin.
[2025-11-05 14:15:53 TP5] Process 292 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-11-05 14:15:53 TP6] Init torch distributed begin.
[2025-11-05 14:15:53 TP7] Init torch distributed begin.
[2025-11-05 14:15:53 TP1] Process 288 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2025-11-05 14:15:53 TP3] Process 290 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-11-05 14:15:53 TP2] Process 289 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
[2025-11-05 14:15:53 TP5] Init torch distributed begin.
[2025-11-05 14:15:54 TP1] Init torch distributed begin.
[2025-11-05 14:15:54 TP3] Init torch distributed begin.
[2025-11-05 14:15:54 TP2] Init torch distributed begin.
[2025-11-05 14:15:54 TP0] sglang is using nccl==2.21.5
[2025-11-05 14:15:56 TP7] Init torch distributed ends. mem usage=3.94 GB
[2025-11-05 14:15:56 TP5] Init torch distributed ends. mem usage=3.93 GB
[2025-11-05 14:15:56 TP6] Init torch distributed ends. mem usage=3.95 GB
[2025-11-05 14:15:56 TP0] Init torch distributed ends. mem usage=3.65 GB
[2025-11-05 14:15:56 TP4] Init torch distributed ends. mem usage=4.01 GB
[2025-11-05 14:15:56 TP3] Init torch distributed ends. mem usage=4.06 GB
[2025-11-05 14:15:56 TP2] Init torch distributed ends. mem usage=4.07 GB
[2025-11-05 14:15:56 TP1] Init torch distributed ends. mem usage=4.07 GB
[2025-11-05 14:15:57 TP7] Load weight begin. avail mem=187.32 GB
[2025-11-05 14:15:57 TP6] Load weight begin. avail mem=187.31 GB
[2025-11-05 14:15:57 TP2] Load weight begin. avail mem=187.19 GB
[2025-11-05 14:15:57 TP5] Load weight begin. avail mem=187.33 GB
[2025-11-05 14:15:57 TP4] Load weight begin. avail mem=187.25 GB
[2025-11-05 14:15:57 TP3] Load weight begin. avail mem=187.20 GB
[2025-11-05 14:15:57 TP0] Load weight begin. avail mem=187.61 GB
[2025-11-05 14:15:57 TP0] Detected fp8 checkpoint.
[2025-11-05 14:15:57 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-11-05 14:15:57 TP1] Load weight begin. avail mem=187.19 GB
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:17,  9.26it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:29,  5.51it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:13, 11.50it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:00<00:09, 15.79it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:22,  6.76it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:01<00:17,  8.45it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:01<00:16,  8.82it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:01<00:12, 11.60it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:01<00:12, 11.12it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:02<00:10, 13.42it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:02<00:09, 14.42it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:02<00:10, 12.82it/s]
Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:02<00:08, 15.16it/s]
Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:02<00:07, 18.24it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:03<00:15,  8.23it/s]
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:03<00:13,  9.52it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:03<00:11, 10.90it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:03<00:09, 12.37it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:03<00:08, 13.60it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:03<00:08, 14.65it/s]
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:04<00:09, 11.94it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:04<00:07, 14.33it/s]
Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:04<00:06, 16.03it/s]
Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:04<00:07, 15.19it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:04<00:06, 16.17it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:04<00:05, 17.59it/s]
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:04<00:06, 16.01it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:05<00:12,  7.94it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:05<00:09, 10.17it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:05<00:07, 12.31it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:06<00:06, 13.16it/s]
Loading safetensors checkpoint shards:  47% Completed | 76/163 [00:06<00:05, 15.48it/s]
Loading safetensors checkpoint shards:  48% Completed | 79/163 [00:06<00:04, 18.20it/s]
Loading safetensors checkpoint shards:  50% Completed | 82/163 [00:06<00:04, 17.06it/s]
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:06<00:04, 18.66it/s]
Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:06<00:03, 19.54it/s]
Loading safetensors checkpoint shards:  56% Completed | 91/163 [00:06<00:03, 21.19it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:07<00:03, 21.66it/s]
Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:07<00:03, 21.63it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:07<00:03, 16.40it/s]
Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:07<00:03, 17.34it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:08<00:07,  7.95it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:08<00:05,  9.72it/s]
Loading safetensors checkpoint shards:  67% Completed | 110/163 [00:08<00:05, 10.18it/s]
Loading safetensors checkpoint shards:  69% Completed | 113/163 [00:08<00:03, 12.70it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:08<00:03, 15.24it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:09<00:02, 17.43it/s]
Loading safetensors checkpoint shards:  75% Completed | 122/163 [00:09<00:02, 18.29it/s]
Loading safetensors checkpoint shards:  77% Completed | 125/163 [00:09<00:02, 13.17it/s]
Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:09<00:02, 14.21it/s]
Loading safetensors checkpoint shards:  80% Completed | 130/163 [00:09<00:02, 16.36it/s]
Loading safetensors checkpoint shards:  81% Completed | 132/163 [00:09<00:01, 16.61it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:10<00:01, 17.91it/s]
Loading safetensors checkpoint shards:  84% Completed | 137/163 [00:10<00:01, 13.79it/s]
Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:10<00:01, 14.63it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:10<00:01, 15.48it/s]
Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:10<00:01, 14.04it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:10<00:01, 14.60it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:11<00:01, 11.30it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:11<00:01, 12.77it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:11<00:00, 14.21it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:12<00:01,  5.68it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:12<00:01,  7.09it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:12<00:00,  8.73it/s]
Loading safetensors checkpoint shards:  98% Completed | 160/163 [00:12<00:00, 11.16it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:12<00:00, 10.69it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:12<00:00, 12.72it/s]

[2025-11-05 14:16:45 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.69 GB, mem usage=79.56 GB.
[2025-11-05 14:16:45 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.77 GB, mem usage=79.56 GB.
[2025-11-05 14:16:45 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.78 GB, mem usage=79.56 GB.
[2025-11-05 14:16:45 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.76 GB, mem usage=79.56 GB.
[2025-11-05 14:16:45 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-11-05 14:16:45 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.64 GB, mem usage=79.56 GB.
[2025-11-05 14:16:46 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-11-05 14:16:46 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.05 GB, mem usage=79.56 GB.
[2025-11-05 14:16:46 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-05 14:16:46 TP5] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 14:16:46 TP5] Memory pool end. avail mem=43.51 GB
[2025-11-05 14:16:46 TP7] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 14:16:46 TP6] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 14:16:46 TP7] Memory pool end. avail mem=43.50 GB
[2025-11-05 14:16:46 TP6] Memory pool end. avail mem=43.49 GB
[2025-11-05 14:16:46 TP0] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 14:16:46 TP3] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 14:16:46 TP0] Memory pool end. avail mem=43.78 GB
[2025-11-05 14:16:46 TP3] Memory pool end. avail mem=43.37 GB
[2025-11-05 14:16:46 TP2] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 14:16:46 TP2] Memory pool end. avail mem=43.36 GB
[2025-11-05 14:16:46 TP4] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 14:16:46 TP4] Memory pool end. avail mem=43.42 GB
[2025-11-05 14:16:46 TP1] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 14:16:46 TP1] Memory pool end. avail mem=43.37 GB
[2025-11-05 14:16:48 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.30 GB
[2025-11-05 14:16:48 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.22 GB
[2025-11-05 14:16:48 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
[2025-11-05 14:16:48 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-11-05 14:16:48 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-11-05 14:16:48 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
[2025-11-05 14:16:48 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.17 GB
[2025-11-05 14:16:49 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.58 GB
[2025-11-05 14:16:49 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.94 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 14:16:50 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 14:16:50 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 14:16:50 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 14:16:50 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 14:16:50 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 14:16:50 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 14:16:50 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 14:16:50 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:50 TP7] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:50 TP5] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:50 TP6] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:50 TP3] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:50 TP4] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:50 TP1] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:50 TP2] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:50 TP0] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 14:16:52 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 14:16:52 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 14:16:52 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 14:16:52 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 14:16:52 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 14:16:52 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP6] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP4] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP5] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:52 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP1] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:52 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:52 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:52 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP7] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP2] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:52 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:52 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:52 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 14:16:53 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:53 TP3] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:53 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:53 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:16:53 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:53 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 14:16:53 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:53 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:53 TP0] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:53 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:53 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:16:53 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:16:53 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:53 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
Capturing batches (bs=512 avail_mem=42.94 GB):   2%|         | 1/52 [00:04<03:53,  4.59s/it]Capturing batches (bs=496 avail_mem=42.28 GB):   2%|         | 1/52 [00:04<03:53,  4.59s/it][aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP4] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP5] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP6] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP7] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP2] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP3] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP1] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP0] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=496 avail_mem=42.28 GB):   4%|         | 2/52 [00:05<01:56,  2.34s/it]Capturing batches (bs=480 avail_mem=42.27 GB):   4%|         | 2/52 [00:05<01:56,  2.34s/it][aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP6] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP2] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP4] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP3] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP5] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP1] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP0] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:54 TP7] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=480 avail_mem=42.27 GB):   6%|         | 3/52 [00:05<01:11,  1.45s/it]Capturing batches (bs=464 avail_mem=42.27 GB):   6%|         | 3/52 [00:05<01:11,  1.45s/it][aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP5] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP6] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP2] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP1] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP4] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP3] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP0] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP7] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=464 avail_mem=42.27 GB):   8%|         | 4/52 [00:06<00:49,  1.04s/it]Capturing batches (bs=448 avail_mem=42.26 GB):   8%|         | 4/52 [00:06<00:49,  1.04s/it][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:55 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=448 avail_mem=42.26 GB):  10%|         | 5/52 [00:06<00:37,  1.24it/s]Capturing batches (bs=432 avail_mem=42.26 GB):  10%|         | 5/52 [00:06<00:37,  1.24it/s][aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP5] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP6] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP2] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP3] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP0] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP4] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP7] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP1] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=432 avail_mem=42.26 GB):  12%|        | 6/52 [00:06<00:30,  1.50it/s]Capturing batches (bs=416 avail_mem=42.25 GB):  12%|        | 6/52 [00:06<00:30,  1.50it/s][aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP2] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP6] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP4] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP0] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP1] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP7] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP5] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP3] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=416 avail_mem=42.25 GB):  13%|        | 7/52 [00:07<00:26,  1.72it/s]Capturing batches (bs=400 avail_mem=42.25 GB):  13%|        | 7/52 [00:07<00:26,  1.72it/s][aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP0] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP2] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP1] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP3] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP5] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP7] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP4] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:56 TP6] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=400 avail_mem=42.25 GB):  15%|        | 8/52 [00:07<00:23,  1.90it/s]Capturing batches (bs=384 avail_mem=42.24 GB):  15%|        | 8/52 [00:07<00:23,  1.90it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=384 avail_mem=42.24 GB):  17%|        | 9/52 [00:08<00:19,  2.25it/s]Capturing batches (bs=368 avail_mem=42.24 GB):  17%|        | 9/52 [00:08<00:19,  2.25it/s][aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP4] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP7] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP3] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP0] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP5] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP6] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP1] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP2] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=368 avail_mem=42.24 GB):  19%|        | 10/52 [00:08<00:18,  2.31it/s]Capturing batches (bs=352 avail_mem=42.23 GB):  19%|        | 10/52 [00:08<00:18,  2.31it/s][aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP4] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP6] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP2] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP0] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP3] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:57 TP5] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP7] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP1] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=352 avail_mem=42.23 GB):  21%|        | 11/52 [00:08<00:17,  2.35it/s]Capturing batches (bs=336 avail_mem=42.23 GB):  21%|        | 11/52 [00:08<00:17,  2.35it/s][aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP6] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP1] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP3] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP0] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP4] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP2] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP5] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP7] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=336 avail_mem=42.23 GB):  23%|       | 12/52 [00:09<00:16,  2.38it/s]Capturing batches (bs=320 avail_mem=42.22 GB):  23%|       | 12/52 [00:09<00:16,  2.38it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:58 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=320 avail_mem=42.22 GB):  25%|       | 13/52 [00:09<00:14,  2.68it/s]Capturing batches (bs=304 avail_mem=42.22 GB):  25%|       | 13/52 [00:09<00:14,  2.68it/s][aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP4] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP2] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP5] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP7] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP0] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP6] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP3] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP1] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=304 avail_mem=42.22 GB):  27%|       | 14/52 [00:09<00:14,  2.61it/s]Capturing batches (bs=288 avail_mem=42.22 GB):  27%|       | 14/52 [00:09<00:14,  2.61it/s][aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP5] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP1] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP3] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP4] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP2] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP6] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP7] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP0] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=288 avail_mem=42.22 GB):  29%|       | 15/52 [00:10<00:12,  2.88it/s]Capturing batches (bs=272 avail_mem=42.21 GB):  29%|       | 15/52 [00:10<00:12,  2.88it/s][aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP2] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP1] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP0] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP3] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP4] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP5] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP6] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:16:59 TP7] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=272 avail_mem=42.21 GB):  31%|       | 16/52 [00:10<00:13,  2.73it/s]Capturing batches (bs=256 avail_mem=42.21 GB):  31%|       | 16/52 [00:10<00:13,  2.73it/s][aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP4] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP6] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP5] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP1] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP2] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP3] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP7] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP0] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP3] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP2] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:00 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:00 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP6] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:00 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP4] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:00 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP7] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP0] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:00 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:00 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP5] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP1] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:00 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:00 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:00 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=256 avail_mem=42.21 GB):  33%|      | 17/52 [00:11<00:13,  2.64it/s]Capturing batches (bs=248 avail_mem=42.20 GB):  33%|      | 17/52 [00:11<00:13,  2.64it/s][aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP0] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP2] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP3] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP1] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP5] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP7] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP6] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP4] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=248 avail_mem=42.20 GB):  35%|      | 18/52 [00:11<00:13,  2.57it/s]Capturing batches (bs=240 avail_mem=42.20 GB):  35%|      | 18/52 [00:11<00:13,  2.57it/s][aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP1] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP0] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP2] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP3] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP4] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP6] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP5] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:00 TP7] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=240 avail_mem=42.20 GB):  37%|      | 19/52 [00:11<00:13,  2.53it/s]Capturing batches (bs=232 avail_mem=42.19 GB):  37%|      | 19/52 [00:11<00:13,  2.53it/s][aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP5] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP4] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP2] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP1] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP0] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP7] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP6] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP3] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=232 avail_mem=42.19 GB):  38%|      | 20/52 [00:12<00:12,  2.50it/s]Capturing batches (bs=224 avail_mem=42.19 GB):  38%|      | 20/52 [00:12<00:12,  2.50it/s][aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP5] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP6] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP3] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP2] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP1] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP4] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP0] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:01 TP7] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=224 avail_mem=42.19 GB):  40%|      | 21/52 [00:12<00:12,  2.48it/s]Capturing batches (bs=216 avail_mem=42.18 GB):  40%|      | 21/52 [00:12<00:12,  2.48it/s][aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP5] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP6] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP1] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP0] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP3] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP7] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP4] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP2] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=216 avail_mem=42.18 GB):  42%|     | 22/52 [00:13<00:12,  2.46it/s]Capturing batches (bs=208 avail_mem=42.18 GB):  42%|     | 22/52 [00:13<00:12,  2.46it/s][aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP5] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP1] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP3] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP0] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP4] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP6] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP2] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP7] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=208 avail_mem=42.18 GB):  44%|     | 23/52 [00:13<00:10,  2.75it/s]Capturing batches (bs=200 avail_mem=42.17 GB):  44%|     | 23/52 [00:13<00:10,  2.75it/s][aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP5] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP4] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP1] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP6] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP7] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP3] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP2] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:02 TP0] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=200 avail_mem=42.17 GB):  46%|     | 24/52 [00:13<00:10,  2.65it/s]Capturing batches (bs=192 avail_mem=42.17 GB):  46%|     | 24/52 [00:13<00:10,  2.65it/s][aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=192 avail_mem=42.17 GB):  48%|     | 25/52 [00:14<00:09,  2.90it/s]Capturing batches (bs=184 avail_mem=42.16 GB):  48%|     | 25/52 [00:14<00:09,  2.90it/s][aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP5] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP4] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP1] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP0] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP3] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP6] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP2] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP7] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=184 avail_mem=42.16 GB):  50%|     | 26/52 [00:14<00:08,  3.12it/s]Capturing batches (bs=176 avail_mem=42.16 GB):  50%|     | 26/52 [00:14<00:08,  3.12it/s][aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP5] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP1] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP0] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP4] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP3] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP6] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP2] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:03 TP7] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=176 avail_mem=42.16 GB):  52%|    | 27/52 [00:14<00:07,  3.25it/s]Capturing batches (bs=168 avail_mem=42.16 GB):  52%|    | 27/52 [00:14<00:07,  3.25it/s][aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP7] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP2] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP3] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP1] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP6] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP5] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP0] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP4] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=168 avail_mem=42.16 GB):  54%|    | 28/52 [00:14<00:08,  2.95it/s]Capturing batches (bs=160 avail_mem=42.15 GB):  54%|    | 28/52 [00:14<00:08,  2.95it/s][aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP1] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP5] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP3] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP0] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP4] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP6] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP7] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP2] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=160 avail_mem=42.15 GB):  56%|    | 29/52 [00:15<00:07,  3.16it/s]Capturing batches (bs=152 avail_mem=42.15 GB):  56%|    | 29/52 [00:15<00:07,  3.16it/s][aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP5] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP6] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP2] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP0] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP7] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP4] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP1] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:04 TP3] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=152 avail_mem=42.15 GB):  58%|    | 30/52 [00:15<00:07,  2.92it/s]Capturing batches (bs=144 avail_mem=42.15 GB):  58%|    | 30/52 [00:15<00:07,  2.92it/s][aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP1] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP5] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP3] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP4] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP0] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP2] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP6] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP7] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=144 avail_mem=42.15 GB):  60%|    | 31/52 [00:15<00:06,  3.14it/s]Capturing batches (bs=136 avail_mem=42.14 GB):  60%|    | 31/52 [00:15<00:06,  3.14it/s][aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP5] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP0] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP1] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP4] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP3] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP6] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP7] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP2] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=136 avail_mem=42.14 GB):  62%|   | 32/52 [00:16<00:06,  3.32it/s]Capturing batches (bs=128 avail_mem=42.14 GB):  62%|   | 32/52 [00:16<00:06,  3.32it/s][aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:05 TP5] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:05 TP6] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:05 TP4] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:05 TP1] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:05 TP3] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:05 TP2] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:05 TP7] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:05 TP0] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP5] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP6] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP3] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP2] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP4] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP1] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP0] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP7] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:05 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:05 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:05 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:05 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:05 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:05 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:05 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:05 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:05 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=128 avail_mem=42.14 GB):  63%|   | 33/52 [00:16<00:05,  3.37it/s]Capturing batches (bs=120 avail_mem=42.14 GB):  63%|   | 33/52 [00:16<00:05,  3.37it/s][aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP4] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP5] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP2] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP3] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:05 TP0] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP1] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP6] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP7] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.14 GB):  65%|   | 34/52 [00:16<00:05,  3.02it/s]Capturing batches (bs=112 avail_mem=42.13 GB):  65%|   | 34/52 [00:16<00:05,  3.02it/s][aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP5] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP0] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP1] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP3] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP4] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP6] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP7] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP2] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.13 GB):  67%|   | 35/52 [00:17<00:05,  3.21it/s]Capturing batches (bs=104 avail_mem=42.13 GB):  67%|   | 35/52 [00:17<00:05,  3.21it/s][aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP1] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP3] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP4] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP6] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP7] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP2] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP0] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP5] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.13 GB):  69%|   | 36/52 [00:17<00:05,  2.94it/s]Capturing batches (bs=96 avail_mem=42.12 GB):  69%|   | 36/52 [00:17<00:05,  2.94it/s] [aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:06 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.12 GB):  71%|   | 37/52 [00:17<00:04,  3.14it/s]Capturing batches (bs=88 avail_mem=42.12 GB):  71%|   | 37/52 [00:17<00:04,  3.14it/s][aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP4] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP0] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP2] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP5] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP7] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP1] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP3] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP6] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.12 GB):  73%|  | 38/52 [00:18<00:04,  2.86it/s]Capturing batches (bs=80 avail_mem=42.12 GB):  73%|  | 38/52 [00:18<00:04,  2.86it/s][aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP5] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP3] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP4] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP1] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP0] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP7] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP2] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:07 TP6] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.12 GB):  75%|  | 39/52 [00:18<00:04,  3.09it/s]Capturing batches (bs=72 avail_mem=42.11 GB):  75%|  | 39/52 [00:18<00:04,  3.09it/s][aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP2] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP0] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP4] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP5] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP1] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP3] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP6] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP7] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.11 GB):  77%|  | 40/52 [00:18<00:04,  2.86it/s]Capturing batches (bs=64 avail_mem=42.11 GB):  77%|  | 40/52 [00:18<00:04,  2.86it/s][aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:08 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:08 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:08 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:08 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:08 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:08 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:08 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 14:17:08 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:08 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:08 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:08 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:08 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:08 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:08 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:08 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:08 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:17:08 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.11 GB):  79%|  | 41/52 [00:19<00:03,  3.08it/s]Capturing batches (bs=56 avail_mem=42.10 GB):  79%|  | 41/52 [00:19<00:03,  3.08it/s][aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP3] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP1] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP0] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP2] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP5] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP6] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP4] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP7] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.10 GB):  81%|  | 42/52 [00:19<00:03,  2.85it/s]Capturing batches (bs=48 avail_mem=42.10 GB):  81%|  | 42/52 [00:19<00:03,  2.85it/s][aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP5] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP1] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP4] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP0] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP3] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP6] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP7] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:08 TP2] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.10 GB):  83%| | 43/52 [00:19<00:02,  3.08it/s]Capturing batches (bs=40 avail_mem=42.09 GB):  83%| | 43/52 [00:19<00:02,  3.08it/s][aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP1] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP6] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP2] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP4] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP3] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP7] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP0] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP5] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.09 GB):  85%| | 44/52 [00:20<00:02,  2.84it/s]Capturing batches (bs=32 avail_mem=42.09 GB):  85%| | 44/52 [00:20<00:02,  2.84it/s][aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:09 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:09 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=32 avail_mem=42.09 GB):  87%| | 45/52 [00:20<00:02,  3.07it/s]Capturing batches (bs=24 avail_mem=42.08 GB):  87%| | 45/52 [00:20<00:02,  3.07it/s][aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP1] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP6] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP5] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP2] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP3] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP4] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP0] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP7] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=24 avail_mem=42.08 GB):  88%| | 46/52 [00:20<00:02,  2.82it/s]Capturing batches (bs=16 avail_mem=42.08 GB):  88%| | 46/52 [00:20<00:02,  2.82it/s][aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:17:10 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=16 avail_mem=42.08 GB):  90%| | 47/52 [00:21<00:01,  3.06it/s]Capturing batches (bs=12 avail_mem=42.07 GB):  90%| | 47/52 [00:21<00:01,  3.06it/s][aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP4] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP7] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP5] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP3] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP1] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP0] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP6] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP2] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=12 avail_mem=42.07 GB):  92%|| 48/52 [00:21<00:01,  3.26it/s]Capturing batches (bs=8 avail_mem=42.07 GB):  92%|| 48/52 [00:21<00:01,  3.26it/s] [aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:10 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=8 avail_mem=42.07 GB):  94%|| 49/52 [00:21<00:00,  3.42it/s]Capturing batches (bs=4 avail_mem=42.07 GB):  94%|| 49/52 [00:21<00:00,  3.42it/s][aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP3] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP0] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP1] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP5] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP7] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP6] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP4] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP2] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=4 avail_mem=42.07 GB):  96%|| 50/52 [00:21<00:00,  3.54it/s]Capturing batches (bs=2 avail_mem=42.07 GB):  96%|| 50/52 [00:21<00:00,  3.54it/s][aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP5] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP0] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP1] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP3] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP4] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP7] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP6] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:11 TP2] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=2 avail_mem=42.07 GB):  98%|| 51/52 [00:22<00:00,  3.63it/s]Capturing batches (bs=1 avail_mem=42.06 GB):  98%|| 51/52 [00:22<00:00,  3.63it/s][aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:12 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:12 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:12 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:12 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:12 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:12 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:12 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:12 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=1 avail_mem=42.06 GB): 100%|| 52/52 [00:22<00:00,  2.40it/s]Capturing batches (bs=1 avail_mem=42.06 GB): 100%|| 52/52 [00:22<00:00,  2.26it/s]
[2025-11-05 14:17:12 TP0] Registering 6396 cuda graph addresses
[2025-11-05 14:17:13 TP6] Capture cuda graph end. Time elapsed: 25.06 s. mem usage=1.52 GB. avail mem=41.76 GB.
[2025-11-05 14:17:13 TP4] Capture cuda graph end. Time elapsed: 25.13 s. mem usage=1.52 GB. avail mem=41.69 GB.
[2025-11-05 14:17:13 TP5] Capture cuda graph end. Time elapsed: 25.16 s. mem usage=1.52 GB. avail mem=41.78 GB.
[2025-11-05 14:17:13 TP1] Capture cuda graph end. Time elapsed: 25.13 s. mem usage=1.52 GB. avail mem=41.64 GB.
[2025-11-05 14:17:13 TP3] Capture cuda graph end. Time elapsed: 24.92 s. mem usage=1.52 GB. avail mem=41.64 GB.
[2025-11-05 14:17:13 TP2] Capture cuda graph end. Time elapsed: 25.09 s. mem usage=1.52 GB. avail mem=41.63 GB.
[2025-11-05 14:17:13 TP0] Capture cuda graph end. Time elapsed: 24.14 s. mem usage=1.52 GB. avail mem=42.05 GB.
[2025-11-05 14:17:13 TP7] Capture cuda graph end. Time elapsed: 25.19 s. mem usage=1.52 GB. avail mem=41.77 GB.
[2025-11-05 14:17:13 TP0] max_total_num_tokens=971639, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=42.05 GB
[2025-11-05 14:17:13] INFO:     Started server process [47]
[2025-11-05 14:17:13] INFO:     Waiting for application startup.
[2025-11-05 14:17:13] INFO:     Application startup complete.
[2025-11-05 14:17:13] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-11-05 14:17:14] INFO:     127.0.0.1:51102 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-05 14:17:14 TP0] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:15 TP0] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:15 TP2] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:15 TP3] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:15 TP1] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:15 TP5] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:15 TP4] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:15 TP6] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:15 TP7] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:17] INFO:     127.0.0.1:51110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:17] The server is fired up and ready to roll!
[2025-11-05 14:17:18] INFO:     127.0.0.1:51124 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-05 14:17:25] INFO:     127.0.0.1:60712 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-05 14:17:25 TP0] Prefill batch, #new-seq: 1, #new-token: 666, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:25 TP3] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:25 TP4] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:25 TP0] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:25 TP6] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:25 TP2] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:25 TP1] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:25 TP5] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:25 TP7] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26] INFO:     127.0.0.1:60716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:26 TP0] Prefill batch, #new-seq: 1, #new-token: 67, #cached-token: 667, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP4] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP6] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP2] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP3] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP0] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP7] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP1] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP5] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP0] Prefill batch, #new-seq: 44, #new-token: 2589, #cached-token: 29348, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:26 TP0] Prefill batch, #new-seq: 132, #new-token: 8172, #cached-token: 88309, token usage: 0.00, #running-req: 45, #queue-req: 0, 
[2025-11-05 14:17:26 TP0] Prefill batch, #new-seq: 128, #new-token: 7558, #cached-token: 85667, token usage: 0.01, #running-req: 177, #queue-req: 0, 
[2025-11-05 14:17:27 TP0] Prefill batch, #new-seq: 279, #new-token: 16357, #cached-token: 186789, token usage: 0.02, #running-req: 305, #queue-req: 54, 
[2025-11-05 14:17:29 TP0] Prefill batch, #new-seq: 272, #new-token: 16335, #cached-token: 182157, token usage: 0.04, #running-req: 584, #queue-req: 89, 
[2025-11-05 14:17:30 TP0] Prefill batch, #new-seq: 168, #new-token: 10411, #cached-token: 112527, token usage: 0.05, #running-req: 856, #queue-req: 295, 
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP2] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP2] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP3] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP3] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP0] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP0] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP1] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP1] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP7] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP7] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP6] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP6] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP5] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP5] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP4] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP4] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 14:17:31 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:31 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 14:17:34] INFO:     127.0.0.1:35360 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP4] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP1] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP0] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP2] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP6] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP3] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP7] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP5] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP0] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 669, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP1] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP6] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP3] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP0] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP4] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP2] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP5] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP7] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34] INFO:     127.0.0.1:41224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:34 TP0] Decode batch, #running-req: 1024, #token: 95335, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1577.76, #queue-req: 294, 
[2025-11-05 14:17:34] INFO:     127.0.0.1:33028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:34 TP0] Prefill batch, #new-seq: 1, #new-token: 41, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP1] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP6] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP4] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP2] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP0] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP3] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP5] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP7] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP0] Prefill batch, #new-seq: 1, #new-token: 69, #cached-token: 669, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP4] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP6] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP0] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP2] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP3] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP1] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP7] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:34 TP5] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:35 TP0] Prefill batch, #new-seq: 1, #new-token: 66, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 291, 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP0] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP3] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP1] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP4] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP2] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP6] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP7] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP5] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35] INFO:     127.0.0.1:33152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:35] INFO:     127.0.0.1:38284 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP4] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP6] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP1] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP0] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP7] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP2] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP3] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP5] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:35] INFO:     127.0.0.1:34054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:35 TP0] Prefill batch, #new-seq: 2, #new-token: 182, #cached-token: 1339, token usage: 0.11, #running-req: 1022, #queue-req: 289, 
[2025-11-05 14:17:35] INFO:     127.0.0.1:37200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:35] INFO:     127.0.0.1:41180 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP1] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP4] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP6] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP5] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP3] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP2] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP0] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP7] [fused_moe] using default for (182, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35] INFO:     127.0.0.1:33468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:35] INFO:     127.0.0.1:34888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:35] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:35] INFO:     127.0.0.1:36062 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP4] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP0] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP2] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP6] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP3] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP1] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP7] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP5] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:35 TP0] Prefill batch, #new-seq: 8, #new-token: 552, #cached-token: 5362, token usage: 0.11, #running-req: 1016, #queue-req: 281, 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP4] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP6] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP2] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP3] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP1] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP0] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP7] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP5] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36] INFO:     127.0.0.1:33700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36 TP0] Prefill batch, #new-seq: 1, #new-token: 80, #cached-token: 670, token usage: 0.11, #running-req: 1023, #queue-req: 280, 
[2025-11-05 14:17:36] INFO:     127.0.0.1:32826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:32956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:36826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:36866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:37158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:39898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:40448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36 TP0] Prefill batch, #new-seq: 8, #new-token: 435, #cached-token: 5363, token usage: 0.11, #running-req: 1016, #queue-req: 272, 
[aiter] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP4] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP6] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP2] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP3] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP0] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP1] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP7] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP5] [fused_moe] using default for (435, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36] INFO:     127.0.0.1:34166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:37434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:40102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:41310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:41482 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP4] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP3] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP0] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP2] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP7] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP6] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP1] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP5] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP0] Prefill batch, #new-seq: 5, #new-token: 236, #cached-token: 3352, token usage: 0.11, #running-req: 1019, #queue-req: 267, 
[aiter] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP4] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP0] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP2] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP6] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP3] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP1] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP7] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP5] [fused_moe] using default for (236, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36] INFO:     127.0.0.1:32882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:34728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:35666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:36224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:37790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:39384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:40642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:41328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36 TP0] Prefill batch, #new-seq: 8, #new-token: 445, #cached-token: 5357, token usage: 0.11, #running-req: 1016, #queue-req: 259, 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP4] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP2] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP6] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP0] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP1] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP3] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP7] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP5] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36] INFO:     127.0.0.1:36938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:37030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:37696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:38562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36] INFO:     127.0.0.1:40888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:36 TP0] Prefill batch, #new-seq: 5, #new-token: 277, #cached-token: 3351, token usage: 0.11, #running-req: 1019, #queue-req: 254, 
[aiter] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP4] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP6] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP0] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP1] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP3] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP2] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP7] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:36 TP5] [fused_moe] using default for (277, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37] INFO:     127.0.0.1:32940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:36880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:37172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:40204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:40606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:40788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:41384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37 TP0] Prefill batch, #new-seq: 8, #new-token: 523, #cached-token: 5359, token usage: 0.11, #running-req: 1016, #queue-req: 246, 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP6] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP4] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP0] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP2] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP1] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP3] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP7] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP5] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37] INFO:     127.0.0.1:60740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:35336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:38454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:38886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:39944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:41926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37 TP0] Prefill batch, #new-seq: 8, #new-token: 546, #cached-token: 5360, token usage: 0.11, #running-req: 1016, #queue-req: 238, 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP4] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP1] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP2] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP0] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP6] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP3] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP5] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP7] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP4] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP1] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP6] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP2] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP0] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP3] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP7] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP5] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37] INFO:     127.0.0.1:33508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:35874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:37128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:40266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37 TP0] Prefill batch, #new-seq: 4, #new-token: 203, #cached-token: 2681, token usage: 0.11, #running-req: 1020, #queue-req: 234, 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP4] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP6] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP0] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP3] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP1] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP7] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP2] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP5] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37] INFO:     127.0.0.1:33916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:36420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:36838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:37354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:38176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:38956 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP1] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP4] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP3] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP7] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP0] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP6] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP5] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP2] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP0] Prefill batch, #new-seq: 7, #new-token: 461, #cached-token: 4692, token usage: 0.11, #running-req: 1017, #queue-req: 227, 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP4] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP6] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP0] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP3] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP1] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP2] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP7] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37 TP5] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:37] INFO:     127.0.0.1:34978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:35770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:36626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:38078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:38376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:38844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:40524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37] INFO:     127.0.0.1:40666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:37 TP0] Prefill batch, #new-seq: 8, #new-token: 520, #cached-token: 5360, token usage: 0.12, #running-req: 1016, #queue-req: 219, 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP4] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP6] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP1] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP0] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP3] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP7] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP2] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP5] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38] INFO:     127.0.0.1:33194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:33664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:33886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:34454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:34838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:35798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:37794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:40598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:40752 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP4] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP3] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP7] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP0] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP6] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP2] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP1] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP5] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP0] Prefill batch, #new-seq: 12, #new-token: 615, #cached-token: 8035, token usage: 0.12, #running-req: 1012, #queue-req: 207, 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP4] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP6] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP0] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP3] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP1] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP2] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP7] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP5] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38] INFO:     127.0.0.1:32836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:33312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:35762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:36036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:36566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:37316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:38476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38 TP0] Prefill batch, #new-seq: 7, #new-token: 336, #cached-token: 4688, token usage: 0.12, #running-req: 1017, #queue-req: 200, 
[2025-11-05 14:17:38] INFO:     127.0.0.1:33980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:34214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:37214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:37652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:38932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:39366 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP4] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP0] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP6] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP2] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP3] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP7] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP1] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP5] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP0] Prefill batch, #new-seq: 6, #new-token: 397, #cached-token: 4020, token usage: 0.12, #running-req: 1018, #queue-req: 194, 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP3] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP4] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP1] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP0] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP6] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP2] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP7] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP5] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:34294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:34330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:35016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:38574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:38740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:38942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:39046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:39872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:41548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38 TP0] Prefill batch, #new-seq: 12, #new-token: 686, #cached-token: 8038, token usage: 0.12, #running-req: 1012, #queue-req: 182, 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP4] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP6] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP1] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP3] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP0] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP2] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP5] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP7] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38] INFO:     127.0.0.1:60968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:33810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:33898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:34546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:34626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:35222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:35848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:38] INFO:     127.0.0.1:40462 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP4] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP0] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP6] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP2] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP1] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP5] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP3] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP7] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:38 TP0] Prefill batch, #new-seq: 9, #new-token: 520, #cached-token: 6029, token usage: 0.12, #running-req: 1015, #queue-req: 173, 
[2025-11-05 14:17:39] INFO:     127.0.0.1:33542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:34654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:35446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:35778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:38546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39 TP0] Prefill batch, #new-seq: 5, #new-token: 253, #cached-token: 3349, token usage: 0.12, #running-req: 1019, #queue-req: 168, 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP6] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP4] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP0] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP3] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP2] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP1] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP7] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP5] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39] INFO:     127.0.0.1:34664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:35078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:36682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:38678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:39038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:39278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:41306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39 TP0] Prefill batch, #new-seq: 9, #new-token: 478, #cached-token: 6031, token usage: 0.12, #running-req: 1015, #queue-req: 159, 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP6] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP4] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP3] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP0] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP1] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP2] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP7] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP5] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39] INFO:     127.0.0.1:34404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:36730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:37092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:38116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:38994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:39154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39 TP0] Prefill batch, #new-seq: 7, #new-token: 429, #cached-token: 4687, token usage: 0.12, #running-req: 1017, #queue-req: 152, 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP6] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP4] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP3] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP0] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP1] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP2] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP5] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP7] [fused_moe] using default for (429, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39] INFO:     127.0.0.1:33864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:34308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:37144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:37188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:38274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:39698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:40408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39 TP0] Prefill batch, #new-seq: 9, #new-token: 551, #cached-token: 6031, token usage: 0.12, #running-req: 1015, #queue-req: 143, 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP3] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP4] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP6] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP0] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP2] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP1] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP5] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP7] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:60942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:32788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:33342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:34690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:35944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:37084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:38126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:38202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:39974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:39] INFO:     127.0.0.1:41192 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP4] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP0] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP6] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP2] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP3] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP7] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP1] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP5] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP0] Prefill batch, #new-seq: 11, #new-token: 653, #cached-token: 7373, token usage: 0.12, #running-req: 1013, #queue-req: 132, 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP6] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP4] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP3] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP0] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP1] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP2] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP7] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:39 TP5] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:35110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:37586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:38864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:39198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:40688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40 TP0] Prefill batch, #new-seq: 8, #new-token: 490, #cached-token: 5357, token usage: 0.12, #running-req: 1016, #queue-req: 124, 
[aiter] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP0] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP4] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP3] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP6] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP2] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP1] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP5] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP7] [fused_moe] using default for (490, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40] INFO:     127.0.0.1:33066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:35208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:35936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:37114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:37414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:37992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:38340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:38492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:38594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:39658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:40700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40 TP0] Prefill batch, #new-seq: 11, #new-token: 776, #cached-token: 7371, token usage: 0.12, #running-req: 1013, #queue-req: 113, 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP4] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP6] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP0] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP3] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP2] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP1] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP7] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP5] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40] INFO:     127.0.0.1:32886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:33280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:34250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:34670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:35366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:35494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:35516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:36162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:36512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:37560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:40544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:40720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:41080 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP4] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP0] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP2] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP6] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP1] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP3] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP5] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP7] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP0] Prefill batch, #new-seq: 14, #new-token: 1000, #cached-token: 9384, token usage: 0.12, #running-req: 1010, #queue-req: 99, 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP4] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP3] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP6] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP2] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP0] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP1] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP7] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP5] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40] INFO:     127.0.0.1:60962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:32930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:34128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:34934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:35348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:35486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:36194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:37094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:37716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:38334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:38770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:39840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:41014 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP4] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP6] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP3] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP0] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP1] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP2] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP5] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP7] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP0] Prefill batch, #new-seq: 15, #new-token: 774, #cached-token: 10050, token usage: 0.12, #running-req: 1009, #queue-req: 84, 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP4] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP2] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP0] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP3] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP6] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP1] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP5] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40 TP7] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:40] INFO:     127.0.0.1:32810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:33462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:33796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:33872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:35176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40] INFO:     127.0.0.1:38098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:40 TP0] Prefill batch, #new-seq: 6, #new-token: 324, #cached-token: 4021, token usage: 0.12, #running-req: 1018, #queue-req: 78, 
[aiter] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP4] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP0] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP2] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP6] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP3] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP7] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP1] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP5] [fused_moe] using default for (324, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41] INFO:     127.0.0.1:60916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:33418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:34744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:34970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:35166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:37662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:38366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:38924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:39452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:40712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41 TP0] Prefill batch, #new-seq: 12, #new-token: 850, #cached-token: 8035, token usage: 0.12, #running-req: 1012, #queue-req: 66, 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP6] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP0] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP4] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP3] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP2] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP1] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP7] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP5] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41] INFO:     127.0.0.1:34238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:35842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:38160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:38262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:39432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:39588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:39734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:40460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:40526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:40858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:41022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41 TP0] Prefill batch, #new-seq: 12, #new-token: 899, #cached-token: 8037, token usage: 0.12, #running-req: 1012, #queue-req: 54, 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP4] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP6] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP3] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP2] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP0] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP1] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP7] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP5] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41] INFO:     127.0.0.1:33484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:33674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:36394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:37042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:37310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:37496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:37680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:38192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:38422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:38558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:40022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:41618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41 TP0] Prefill batch, #new-seq: 12, #new-token: 700, #cached-token: 8040, token usage: 0.13, #running-req: 1012, #queue-req: 42, 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP4] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP3] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP6] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP0] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP2] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP1] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP7] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP5] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41] INFO:     127.0.0.1:34806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:35226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:35298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:35688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:36154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:36904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:37934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:39228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:41842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41 TP0] Prefill batch, #new-seq: 9, #new-token: 526, #cached-token: 6028, token usage: 0.13, #running-req: 1015, #queue-req: 33, 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP6] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP0] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP3] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP2] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP4] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP1] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP7] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP5] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41] INFO:     127.0.0.1:33100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:33328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:34284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:34640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:35814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:36642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:37436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:37668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:38064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:38554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:39072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:39352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:39440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:41604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:41] INFO:     127.0.0.1:41714 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP4] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP0] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP2] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP6] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP3] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP7] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP1] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP5] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:41 TP0] Prefill batch, #new-seq: 16, #new-token: 945, #cached-token: 10722, token usage: 0.13, #running-req: 1008, #queue-req: 17, 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP4] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP0] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP6] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP1] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP3] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP2] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP7] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP5] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP0] Decode batch, #running-req: 1008, #token: 121006, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5503.58, #queue-req: 17, 
[2025-11-05 14:17:42] INFO:     127.0.0.1:32778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:33248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:34332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:34498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:35956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:38750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:38752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:39268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:39826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:40580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:40984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:41804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:41862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42 TP0] Prefill batch, #new-seq: 14, #new-token: 831, #cached-token: 9379, token usage: 0.13, #running-req: 1010, #queue-req: 3, 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP4] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP1] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP6] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP7] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP0] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP3] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP2] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP5] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42] INFO:     127.0.0.1:33848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:34514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:35960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:41270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42 TP0] Prefill batch, #new-seq: 3, #new-token: 153, #cached-token: 2013, token usage: 0.13, #running-req: 1020, #queue-req: 0, 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP6] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP3] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP0] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP2] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP4] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP1] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP7] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP5] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42] INFO:     127.0.0.1:60928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:33868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:35716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:37568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:38724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:39164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:40374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:40510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:41120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:41440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:41830 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP4] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP3] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP7] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP6] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP0] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP2] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP1] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP5] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42] INFO:     127.0.0.1:33616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:34158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:34608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:37070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:40630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:40952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:41506 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP0] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP2] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP1] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP3] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP4] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP7] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP6] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP5] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42] INFO:     127.0.0.1:33116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:34230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:34784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:37466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:38406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:38626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:39520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:40010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:41824 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP4] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP0] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP6] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP2] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP7] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP3] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP5] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP1] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:33526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:34016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:34420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:34564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:35430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:35868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:37556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:38392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:42] INFO:     127.0.0.1:41160 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP0] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP3] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP6] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP7] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP4] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP2] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP5] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:42 TP1] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43] INFO:     127.0.0.1:33132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:34010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:34058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:37008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:37026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:39222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:39630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP0] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP7] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP4] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP6] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP3] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP2] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP1] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP5] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43] INFO:     127.0.0.1:32800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:33014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:33186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:33406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:33536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:37674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:37870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:38154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:39490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP4] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP6] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP3] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP7] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP0] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP2] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP5] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP1] [fused_moe] using default for (954, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:33130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:34612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:35588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:37756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:38928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:39744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:41810 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP6] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP7] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP0] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP4] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP2] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP3] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP5] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP1] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43] INFO:     127.0.0.1:60892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:33308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:33738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:34532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:35962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:37404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:37828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:39014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:41092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:41764 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP2] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP0] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP6] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP4] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP3] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP7] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP1] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP5] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43] INFO:     127.0.0.1:32924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:34450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:34796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:35758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:38790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:41432 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP2] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP0] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP6] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP7] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP4] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP3] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP5] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP1] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43] INFO:     127.0.0.1:33198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:35134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:37378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40174 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP6] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP2] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP4] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP0] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP3] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP7] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP1] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP5] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43] INFO:     127.0.0.1:60978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:33090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:34360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:35968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:37040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:38136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:39688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:39982 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP7] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP5] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP3] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP0] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP1] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP2] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP4] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP6] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43] INFO:     127.0.0.1:60778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:33042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:33210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:35054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:37640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:37772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:38460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:39482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:39756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:42026 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP4] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP7] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP5] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP1] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP0] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP3] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP6] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP2] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43] INFO:     127.0.0.1:35370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:36978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:37882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:38004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:39772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:39964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:40650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:41104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:41468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:43] INFO:     127.0.0.1:41668 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP4] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP5] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP3] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP1] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP7] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP6] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP0] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:43 TP2] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:33820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:35022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:35860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:36262 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP7] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP1] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP3] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP5] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP4] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP0] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP6] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP2] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44] INFO:     127.0.0.1:60862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:33162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:33844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:34772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:35426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:35500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:35708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:36040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:36366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:37638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:37978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:38698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:41070 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP4] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP0] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP6] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP2] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP7] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP3] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP1] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP5] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44] INFO:     127.0.0.1:38624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:41748 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP4] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP6] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP2] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP0] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP3] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP7] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP1] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP5] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44] INFO:     127.0.0.1:33326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:34068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:34740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:35942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:37602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:38530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:38686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:40438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:40476 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP4] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP6] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP2] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP0] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP7] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP3] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP1] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP5] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:34142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:34846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:35738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:35870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:40776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:41076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:41582 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP4] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP2] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP6] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP0] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP7] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP3] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP1] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP5] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44] INFO:     127.0.0.1:60768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:33770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:33930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:37430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:41398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:41642 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP4] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP2] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP6] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP0] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP3] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP7] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP1] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP5] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44] INFO:     127.0.0.1:33286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:36494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:36962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:37552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:38088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:38362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:39994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:40214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:40436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:40678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:41272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:41574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:42412 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP4] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP2] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP6] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP0] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP7] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP3] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP1] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP5] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:42652 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP2] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP6] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP4] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP0] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP3] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP7] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP1] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP5] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44] INFO:     127.0.0.1:60772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:34400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:35668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:36376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:37808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:37916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:41984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:42064 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP4] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP2] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP6] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP0] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP7] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP3] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP1] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44 TP5] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:44] INFO:     127.0.0.1:32818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:34948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:35916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:36232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:36690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:37572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:40052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:40628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:44] INFO:     127.0.0.1:41234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:38236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:39028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:39646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:40760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:41522 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP4] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP2] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP6] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP0] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP7] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP3] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP1] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP5] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45] INFO:     127.0.0.1:60922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:38050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:38256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:39034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:40536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:41532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:41882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:42572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:43110 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP4] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP2] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP6] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP0] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP7] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP3] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP1] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP5] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP4] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP6] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP2] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP0] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP7] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP3] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP1] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP5] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45] INFO:     127.0.0.1:33562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:38658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:39168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:40156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:41400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:33428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:34864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:34918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:38506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:39614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:40026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:41010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:41854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:42126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:43134 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP4] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP6] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP2] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP0] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP7] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP3] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP1] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP5] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:36984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:39332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:40828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:41778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:41960 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP6] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP4] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP2] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP0] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP7] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP3] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP1] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP5] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:32770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:33572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:33658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:36298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:38244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:40422 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP4] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP6] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP2] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP0] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP7] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP3] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP1] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP5] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:33720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:40742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:42762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP6] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP4] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP2] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP0] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP7] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP3] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP1] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP5] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45] INFO:     127.0.0.1:35982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:36118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:36246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:36310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:39016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:39206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:40284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:42672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:43046 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP2] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP6] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP4] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP0] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP3] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP7] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP1] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP5] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45] INFO:     127.0.0.1:33970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:34474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:38904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:39254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:39618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:42730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:42792 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP4] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP6] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP2] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP0] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP7] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP3] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP1] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP5] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:34256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:35994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:37960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:38830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:41340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:41898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:42192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:45] INFO:     127.0.0.1:44248 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP4] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP2] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP6] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP7] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP0] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP3] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP1] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:45 TP5] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:33386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:34184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:34814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:36144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:36894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:37170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:37536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:37776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:40300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:41124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:41256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:43258 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP4] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP2] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP6] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP0] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP7] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP3] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP1] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP5] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:34700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:36616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:37500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:38646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:38756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:41698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:41870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:42802 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP4] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP6] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP2] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP0] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP7] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP3] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP1] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP5] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46] INFO:     127.0.0.1:33354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:34622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:37898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:37982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:38430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:38952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:40238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:41140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:41210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:41588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP4] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP6] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP2] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP0] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP7] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP3] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP1] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP5] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:33492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:36964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:38094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:38224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:38560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:42052 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP2] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP6] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP4] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP0] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP7] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP3] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP1] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP5] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46] INFO:     127.0.0.1:33778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:33830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:37480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:40130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:40564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:41298 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP6] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP4] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP2] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP0] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP3] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP7] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP1] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP5] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP0] Decode batch, #running-req: 627, #token: 95069, token usage: 0.10, cuda graph: False, gen throughput (token/s): 7501.67, #queue-req: 0, 
[2025-11-05 14:17:46] INFO:     127.0.0.1:35244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:38872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:43452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:44418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:60838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:35146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:36032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:36074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:36542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:36574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:37948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:37956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:40654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:41682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:43222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:43692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:44228 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP4] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP6] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP2] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP3] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP0] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP7] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP1] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP5] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46] INFO:     127.0.0.1:60954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:34932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:35308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:35894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:36698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:37914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:38478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:38566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:38586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:39416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:39494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:39600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:42224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:42704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP4] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP2] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP6] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP0] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP7] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP3] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP1] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP5] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46] INFO:     127.0.0.1:32864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:34462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:34816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:34994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:38440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:40190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:41324 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP4] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP6] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP2] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP0] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP7] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP3] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP1] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP5] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46] INFO:     127.0.0.1:33716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:34270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:35602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:35702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:36518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:36818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:37230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:39144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:39940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:40110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:41036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:42250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:46] INFO:     127.0.0.1:42272 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP4] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP2] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP6] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP0] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP3] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP7] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP1] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:46 TP5] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47] INFO:     127.0.0.1:32904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:33684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:34108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:34324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:34754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:36996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:41746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:44284 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP4] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP2] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP6] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP7] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP0] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP3] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP1] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP5] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47] INFO:     127.0.0.1:60822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:60880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:36076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:44540 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP6] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP4] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP2] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP3] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP7] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP0] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP1] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP5] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47] INFO:     127.0.0.1:35124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:39126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:39670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:40900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43824 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP4] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP6] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP2] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP0] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP3] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP7] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP1] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP5] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:36622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:40160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:41368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43218 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP4] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP6] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP2] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP0] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP3] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP7] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP1] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47 TP5] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:17:47] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:36586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:37244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:37822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:39708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:40252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:34726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:36192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:36648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:37056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:40044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:40940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:41910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:34206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:39234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:39294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:32958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:33454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:34946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:36548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:40348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:40380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:41002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:41352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:32954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:39956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:40084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:36590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:39258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:39574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:41060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:37326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:39558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:40220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:40322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:41942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:34336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:35384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:39128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:60742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:34348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:36010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:36802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:38980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:41284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:40318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:41416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:41788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:42106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:47] INFO:     127.0.0.1:43886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:33518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:34904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:36520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:36852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:36926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:36918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:38032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:40866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:41606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:44052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:35640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:44038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:44176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:44528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:44020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:34082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:36038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:37288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:38114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:44332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:44398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:32878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:33726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:36092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:38212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:35276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:38356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:39800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:40802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:44500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:36136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:39570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:39858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:40490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:40504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:34318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:35562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:36466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:37300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:39376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:41242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:37362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:39784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:40756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:41958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:44126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:34428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:36210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:32938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:39194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:39326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:44442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:33338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:34962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:41972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:33264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:41166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:41836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:34522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:43926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:44236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:33218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:42630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:48] INFO:     127.0.0.1:44268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:33140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:34242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:35038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:35810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:41028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:33900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:34424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:34764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:38208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:38796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:34606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:37272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:37852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:34190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:35064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:36612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:40250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:33390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:41200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:34710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:36476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49 TP0] Decode batch, #running-req: 289, #token: 53487, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6103.80, #queue-req: 0, 
[2025-11-05 14:17:49] INFO:     127.0.0.1:38424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:38848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:60794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:34172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:35624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:36660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:33586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:33794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:35730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:41492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:34850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:39882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:35548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:36022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:36178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:39078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:38298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:39484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:34724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:39854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:34792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:37766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:32902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:41688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:42376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:41464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:41970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:43170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:49] INFO:     127.0.0.1:44162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:34032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:39098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:39310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:34642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:35400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:41936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:33222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:36056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:36404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:60912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:34830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:38324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:39110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:41146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:60806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:37336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:37984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:39808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:37836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:39610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:40094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:33996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:34552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:37706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:37860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:38092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:41654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:34570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:36478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:40528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:32974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:38524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:33754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:34452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:44308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:35418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:35532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:40696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:42330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:36676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:37570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:50] INFO:     127.0.0.1:43178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:33602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:44436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:41024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:38172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:38808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:43620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:33580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:42176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:43528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:43228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:43948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51 TP0] Decode batch, #running-req: 113, #token: 26780, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3918.45, #queue-req: 0, 
[2025-11-05 14:17:51] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:37964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:40592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:37746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:42158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:43034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:44022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:38782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:41930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:43308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:44158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:44564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:41730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:42040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:34198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:41990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:43844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:41048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:41690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:42746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:44444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:37456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:34874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:35454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:42082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:60844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:33630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:35170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:38516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:43536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:39346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:37624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:42954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:35196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:39676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:43774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:36496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:43600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:35680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:43872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:36282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:37834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:42156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:51] INFO:     127.0.0.1:43010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:33410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:43246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:43980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:44074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:43340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:44380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:38964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:43946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:42700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:44062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:44512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:39912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:42462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:44130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:35324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:39746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:42828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:40982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:41648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:43352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:44652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:37504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:44010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:44612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:34048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:44110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:43506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52 TP0] Decode batch, #running-req: 36, #token: 11067, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1836.26, #queue-req: 0, 
[2025-11-05 14:17:52] INFO:     127.0.0.1:41872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:42620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:40066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:40990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:39048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:43838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:44454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:52] INFO:     127.0.0.1:36108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:36122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:44364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:43192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:44482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:36106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:35282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:44140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:43266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:42870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:39936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:42682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:42424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:34150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53 TP0] Decode batch, #running-req: 14, #token: 4922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 839.88, #queue-req: 0, 
[2025-11-05 14:17:53] INFO:     127.0.0.1:43700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:53] INFO:     127.0.0.1:42564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:54] INFO:     127.0.0.1:43748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:54] INFO:     127.0.0.1:39550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:54] INFO:     127.0.0.1:43252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:54] INFO:     127.0.0.1:43584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:54] INFO:     127.0.0.1:42522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:54] INFO:     127.0.0.1:41626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:54] INFO:     127.0.0.1:41558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:54] INFO:     127.0.0.1:43326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:54 TP0] Decode batch, #running-req: 4, #token: 2084, token usage: 0.00, cuda graph: True, gen throughput (token/s): 328.98, #queue-req: 0, 
[2025-11-05 14:17:54] INFO:     127.0.0.1:42936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:54] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:55 TP0] Decode batch, #running-req: 2, #token: 1123, token usage: 0.00, cuda graph: True, gen throughput (token/s): 116.79, #queue-req: 0, 
[2025-11-05 14:17:55] INFO:     127.0.0.1:34374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:17:56 TP0] Decode batch, #running-req: 1, #token: 1162, token usage: 0.00, cuda graph: True, gen throughput (token/s): 50.83, #queue-req: 0, 
[2025-11-05 14:17:57 TP0] Decode batch, #running-req: 1, #token: 1202, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.61, #queue-req: 0, 
[2025-11-05 14:17:57 TP0] Decode batch, #running-req: 1, #token: 1242, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.61, #queue-req: 0, 
[2025-11-05 14:17:58] INFO:     127.0.0.1:36744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:10] INFO:     127.0.0.1:35068 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-05 14:18:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:18:11] INFO:     127.0.0.1:35076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:18:11 TP0] Prefill batch, #new-seq: 38, #new-token: 38, #cached-token: 27565, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP0] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP4] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP7] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP6] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP2] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP5] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP1] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP3] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP0] Prefill batch, #new-seq: 45, #new-token: 45, #cached-token: 32836, token usage: 0.01, #running-req: 39, #queue-req: 0, 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP0] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP4] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP7] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP1] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP5] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP6] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP3] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP2] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP0] Prefill batch, #new-seq: 50, #new-token: 50, #cached-token: 36487, token usage: 0.01, #running-req: 84, #queue-req: 0, 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP4] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP7] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP0] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP5] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP1] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP2] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP3] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP6] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP0] Prefill batch, #new-seq: 52, #new-token: 52, #cached-token: 37965, token usage: 0.01, #running-req: 134, #queue-req: 0, 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP7] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP4] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP1] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP6] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP2] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP5] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP3] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP0] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP0] Prefill batch, #new-seq: 58, #new-token: 58, #cached-token: 42098, token usage: 0.02, #running-req: 186, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP0] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP4] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP7] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP6] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP5] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP1] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP2] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:11 TP3] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 42843, token usage: 0.02, #running-req: 244, #queue-req: 0, 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP4] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP7] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP5] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP6] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP1] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP3] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP2] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP0] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP0] Prefill batch, #new-seq: 62, #new-token: 62, #cached-token: 45263, token usage: 0.02, #running-req: 303, #queue-req: 0, 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP4] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP7] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP6] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP5] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP1] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP2] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP0] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP3] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP0] Prefill batch, #new-seq: 64, #new-token: 64, #cached-token: 46725, token usage: 0.03, #running-req: 365, #queue-req: 0, 
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:18:12 TP4] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:18:12 TP7] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:18:12 TP6] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:18:12 TP5] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:18:12 TP0] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:18:12 TP2] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:18:12 TP1] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:18:12 TP3] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 14:18:12 TP0] Prefill batch, #new-seq: 67, #new-token: 67, #cached-token: 48676, token usage: 0.03, #running-req: 429, #queue-req: 0, 
[2025-11-05 14:18:12 TP0] Prefill batch, #new-seq: 71, #new-token: 71, #cached-token: 51319, token usage: 0.04, #running-req: 496, #queue-req: 0, 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP0] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP4] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP2] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP1] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP6] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP7] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP3] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP5] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP0] Prefill batch, #new-seq: 71, #new-token: 71, #cached-token: 51716, token usage: 0.04, #running-req: 567, #queue-req: 0, 
[2025-11-05 14:18:12 TP0] Prefill batch, #new-seq: 76, #new-token: 76, #cached-token: 55326, token usage: 0.04, #running-req: 638, #queue-req: 0, 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP4] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP7] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP2] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP0] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP6] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP1] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP3] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:12 TP5] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP0] Prefill batch, #new-seq: 76, #new-token: 76, #cached-token: 55403, token usage: 0.05, #running-req: 714, #queue-req: 0, 
[2025-11-05 14:18:13 TP0] Prefill batch, #new-seq: 81, #new-token: 81, #cached-token: 59066, token usage: 0.05, #running-req: 790, #queue-req: 0, 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP4] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP7] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP2] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP5] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP1] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP6] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP3] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP0] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP0] Prefill batch, #new-seq: 80, #new-token: 80, #cached-token: 58262, token usage: 0.06, #running-req: 871, #queue-req: 0, 
[2025-11-05 14:18:13 TP0] Prefill batch, #new-seq: 33, #new-token: 33, #cached-token: 24158, token usage: 0.06, #running-req: 951, #queue-req: 0, 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP4] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP5] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP6] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP7] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP1] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP2] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP3] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP0] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP4] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP5] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP7] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP1] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP6] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP3] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP2] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP0] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:13 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1497, token usage: 0.06, #running-req: 984, #queue-req: 0, 
[2025-11-05 14:18:13 TP0] Prefill batch, #new-seq: 38, #new-token: 38, #cached-token: 27991, token usage: 0.06, #running-req: 986, #queue-req: 10, 
[2025-11-05 14:18:16 TP0] Decode batch, #running-req: 1024, #token: 87333, token usage: 0.09, cuda graph: False, gen throughput (token/s): 1336.42, #queue-req: 295, 
[2025-11-05 14:18:17] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:17 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.10, #running-req: 1023, #queue-req: 294, 
[2025-11-05 14:18:17] INFO:     127.0.0.1:38254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:17 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-05 14:18:17] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:17] INFO:     127.0.0.1:35810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:17] INFO:     127.0.0.1:38910 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP0] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP6] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP2] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP4] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP1] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP7] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP3] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP5] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18] INFO:     127.0.0.1:36168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:39582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:40874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2201, token usage: 0.11, #running-req: 1021, #queue-req: 290, 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP0] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP6] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP4] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP2] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP3] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP7] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP1] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18 TP5] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:18] INFO:     127.0.0.1:36146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:37418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:37950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:40838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5247, token usage: 0.11, #running-req: 1017, #queue-req: 283, 
[2025-11-05 14:18:18] INFO:     127.0.0.1:35912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:39964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:43686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2198, token usage: 0.11, #running-req: 1021, #queue-req: 280, 
[2025-11-05 14:18:18] INFO:     127.0.0.1:36670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:36958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:39526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:39772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:39968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:40060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5083, token usage: 0.11, #running-req: 1017, #queue-req: 273, 
[2025-11-05 14:18:18] INFO:     127.0.0.1:36002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:40132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:42614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:43810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:43822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:43938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:44144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5027, token usage: 0.11, #running-req: 1017, #queue-req: 266, 
[2025-11-05 14:18:18] INFO:     127.0.0.1:38114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:38484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:39096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:40308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:41914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:18] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4353, token usage: 0.11, #running-req: 1018, #queue-req: 260, 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP0] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP1] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP2] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP7] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP6] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP3] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP4] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP5] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19] INFO:     127.0.0.1:39732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:40228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:41124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:43370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2879, token usage: 0.11, #running-req: 1020, #queue-req: 256, 
[2025-11-05 14:18:19] INFO:     127.0.0.1:35682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:35880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:38742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:39962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:42738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:42930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:43062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:43272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:43866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6584, token usage: 0.11, #running-req: 1015, #queue-req: 247, 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP0] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP4] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP1] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP7] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP2] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP6] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP3] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19 TP5] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:19] INFO:     127.0.0.1:35096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:37068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:38140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:38200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:38216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:41442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:42490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:42808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5934, token usage: 0.11, #running-req: 1016, #queue-req: 239, 
[2025-11-05 14:18:19] INFO:     127.0.0.1:36294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:39372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:39678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:42270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:42796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:44322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4280, token usage: 0.11, #running-req: 1018, #queue-req: 233, 
[2025-11-05 14:18:19] INFO:     127.0.0.1:36454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:39830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:40084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:19] INFO:     127.0.0.1:40746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 3001, token usage: 0.12, #running-req: 1020, #queue-req: 229, 
[2025-11-05 14:18:20] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:38622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:39438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:40590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:40940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:41396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:41866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:42982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:43124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6583, token usage: 0.12, #running-req: 1015, #queue-req: 220, 
[2025-11-05 14:18:20] INFO:     127.0.0.1:35462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:36914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:37522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:38400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:39224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:40324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:41502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP4] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP0] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP2] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP6] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP7] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP3] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP1] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP5] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7203, token usage: 0.12, #running-req: 1014, #queue-req: 210, 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP7] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP1] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP2] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP0] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP6] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP3] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP4] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP5] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20] INFO:     127.0.0.1:35622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:36390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:37036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:37256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:38608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:38884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:41044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5028, token usage: 0.12, #running-req: 1017, #queue-req: 203, 
[2025-11-05 14:18:20] INFO:     127.0.0.1:36508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:36974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:39684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:39696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:41456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3653, token usage: 0.12, #running-req: 1019, #queue-req: 198, 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP0] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP7] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP4] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP6] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP2] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP3] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP1] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20 TP5] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:20] INFO:     127.0.0.1:35080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:35230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:37144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:37360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:38684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:41140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:41290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:41568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:41986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:20] INFO:     127.0.0.1:43976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8023, token usage: 0.12, #running-req: 1013, #queue-req: 187, 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:21 TP0] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:21 TP4] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:21 TP6] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:21 TP1] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:21 TP2] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:21 TP7] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:21 TP3] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:21 TP5] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:21] INFO:     127.0.0.1:35940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:36048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:37402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:37458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:38078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:39920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:40670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:41038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:42140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:42412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:42950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8690, token usage: 0.12, #running-req: 1012, #queue-req: 175, 
[2025-11-05 14:18:21] INFO:     127.0.0.1:35960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:36422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:37892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:37936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:38284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:38634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:38808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:41104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5757, token usage: 0.12, #running-req: 1016, #queue-req: 167, 
[2025-11-05 14:18:21] INFO:     127.0.0.1:37422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:41218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:41560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:42430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:42512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:42886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:43798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5064, token usage: 0.12, #running-req: 1017, #queue-req: 160, 
[2025-11-05 14:18:21] INFO:     127.0.0.1:37200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:38788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:39666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:39720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:40128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:41676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:41696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:42254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:43454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6537, token usage: 0.12, #running-req: 1015, #queue-req: 151, 
[2025-11-05 14:18:21] INFO:     127.0.0.1:35924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:37108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:40822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:43400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:21] INFO:     127.0.0.1:43612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4386, token usage: 0.12, #running-req: 1018, #queue-req: 145, 
[2025-11-05 14:18:22] INFO:     127.0.0.1:36270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:36316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:37046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:37698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:38764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:39746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:40112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:40668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:40760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:40876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:43654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8095, token usage: 0.12, #running-req: 1013, #queue-req: 134, 
[2025-11-05 14:18:22] INFO:     127.0.0.1:37096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:37584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:37722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:39050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:39660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:40392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:41402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:42050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:42756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:43308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7251, token usage: 0.12, #running-req: 1014, #queue-req: 124, 
[2025-11-05 14:18:22] INFO:     127.0.0.1:35378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:37304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:37530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:38082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:38762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:40512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:41982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:42524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:43158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:43186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8866, token usage: 0.12, #running-req: 1012, #queue-req: 112, 
[2025-11-05 14:18:22 TP0] Decode batch, #running-req: 1012, #token: 118432, token usage: 0.12, cuda graph: False, gen throughput (token/s): 6375.50, #queue-req: 112, 
[2025-11-05 14:18:22] INFO:     127.0.0.1:35510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:35830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:37342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:38234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:38328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:38356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:38796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:39334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:41060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:42224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:43144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8931, token usage: 0.12, #running-req: 1012, #queue-req: 100, 
[2025-11-05 14:18:22] INFO:     127.0.0.1:35344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:35572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:36558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:38226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:38318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:38482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:39596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:40186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:42272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:42390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:22] INFO:     127.0.0.1:43494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7902, token usage: 0.12, #running-req: 1013, #queue-req: 89, 
[2025-11-05 14:18:23] INFO:     127.0.0.1:35476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:36304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:36804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:36838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:37626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:40640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4349, token usage: 0.12, #running-req: 1018, #queue-req: 83, 
[2025-11-05 14:18:23] INFO:     127.0.0.1:35278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:35390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:36100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:37788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:37990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:38334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:40056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:40244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:40934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:41454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:43148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:43358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10308, token usage: 0.13, #running-req: 1010, #queue-req: 69, 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP0] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP6] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP7] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP5] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP3] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP1] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP4] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP2] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23] INFO:     127.0.0.1:37002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:37014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:37494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:38700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:39046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:39832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:40164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:40714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:40850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:41246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:41930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:42284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:42372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:42932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:42998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:43496 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP4] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP0] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP7] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP1] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP3] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP2] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP6] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP5] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP0] Prefill batch, #new-seq: 17, #new-token: 17, #cached-token: 12524, token usage: 0.13, #running-req: 1007, #queue-req: 52, 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP1] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP0] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP6] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP4] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP2] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP5] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP7] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23 TP3] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:23] INFO:     127.0.0.1:36634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:37228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:39214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:39626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:39984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:40732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:41106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:41728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:42540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:44084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7299, token usage: 0.13, #running-req: 1014, #queue-req: 42, 
[2025-11-05 14:18:23] INFO:     127.0.0.1:35864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:37886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:38524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:38666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:39036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:39900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:40422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:40986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:41464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:42212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:42484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:43064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23] INFO:     127.0.0.1:44208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:23 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9465, token usage: 0.13, #running-req: 1011, #queue-req: 29, 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP6] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP0] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP4] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP1] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP2] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP5] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP7] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP3] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:35956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:36156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:36674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:37080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:37838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:38428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:38802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:39234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:39442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:39612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:41306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:41604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:41834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:41966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11730, token usage: 0.13, #running-req: 1008, #queue-req: 13, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:18:24] INFO:     127.0.0.1:37160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:37486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:38478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:41316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:41768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:42368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:43050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:43614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:44160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:44278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7197, token usage: 0.13, #running-req: 1014, #queue-req: 3, 
[2025-11-05 14:18:24] INFO:     127.0.0.1:36726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:36796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:38794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:40574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:43768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:44136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:44290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2163, token usage: 0.13, #running-req: 1017, #queue-req: 0, 
[2025-11-05 14:18:24] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:38532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:39844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:40950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:41276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:41678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:41904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:42878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:42978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:36198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:36910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:37914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:39160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:39514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:39676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:40088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:40106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:43438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:43894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:43958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:44060 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP0] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP2] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP6] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP7] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP1] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP3] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP5] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24 TP4] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:24] INFO:     127.0.0.1:35324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:36940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:37318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:37810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:38758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:39878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:40200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:42018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:42542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:43098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:24] INFO:     127.0.0.1:44184 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP4] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP0] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP6] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP2] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP5] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP3] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP7] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP1] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25] INFO:     127.0.0.1:36122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:36354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:37196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:37638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:38286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:39482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:39670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:40974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:41740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43668 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP4] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP6] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP2] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP0] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP5] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP7] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP1] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP3] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25] INFO:     127.0.0.1:35578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:35886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:36594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:40152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:41652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43252 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP6] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP0] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP2] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP4] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP5] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP3] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP7] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP1] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:35990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:36034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:37378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:39184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:39628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:40348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:40366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:42012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:42790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:42904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:36334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:36386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:37566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:38184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:39966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:36626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:37660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:41536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:42664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43890 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP4] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP2] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP0] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP6] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP5] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP7] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP3] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP1] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25] INFO:     127.0.0.1:35634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:36006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:37242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:39352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:39534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:41252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:41348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:41790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:42324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:42552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:42740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:42858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:44174 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP4] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP2] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP0] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP6] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP5] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP1] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP7] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP3] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25] INFO:     127.0.0.1:35798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:37396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:37750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:38440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:38540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:39186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:41484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:41682 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP0] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP6] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP2] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP4] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP5] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP7] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP3] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP1] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25] INFO:     127.0.0.1:35826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:36406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:37176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:38932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:42244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43606 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP0] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP2] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP4] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP6] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP7] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP3] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP5] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP1] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25] INFO:     127.0.0.1:35138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:35400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:35330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:35742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:36848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:36896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:36988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:39126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:39762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:39860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:40688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:42656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:43202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:44252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:44408 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP4] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP2] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP0] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP6] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP1] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP7] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP3] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25 TP5] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:25] INFO:     127.0.0.1:35586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:38252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:39112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:40222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:40416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:40538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:42338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:25] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP4] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP0] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP2] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP6] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP3] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP7] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP1] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP5] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26] INFO:     127.0.0.1:35128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:35176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:35360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:36376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:36524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:37420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:43332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:44086 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP4] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP2] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP6] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP0] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP7] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP3] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP1] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP5] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26] INFO:     127.0.0.1:35712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:36552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:37442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:38342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:38702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:38900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:40170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:40484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:42006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:42448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:43026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:43572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:43944 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP4] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP2] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP6] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP0] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP7] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP3] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP1] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP5] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26] INFO:     127.0.0.1:36136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:36178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:40862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:43258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:43446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:43468 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP4] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP6] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP2] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP0] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP7] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP3] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP1] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP5] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26] INFO:     127.0.0.1:36490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:40044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:40256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:42294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:42916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:42944 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP4] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP2] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP6] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP0] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP7] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP3] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP1] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP5] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:37710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:38712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:43482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:44024 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP4] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP2] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP6] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP0] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP7] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP3] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP1] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP5] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26] INFO:     127.0.0.1:35124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:35408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:36582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:37776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:38772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:40430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:42040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:43850 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP4] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP2] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP6] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP0] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP7] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP3] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP1] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP5] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26] INFO:     127.0.0.1:37730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:38282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:40406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:40602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:40902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:43140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:43780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:44360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:44800 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP2] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP6] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP4] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP0] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP7] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP3] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP1] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP5] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26] INFO:     127.0.0.1:38822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:45006 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP2] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP4] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP6] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP0] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP1] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP7] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP3] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP5] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26] INFO:     127.0.0.1:35524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:37696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:38466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:38558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41468 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP4] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP2] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP6] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP0] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP5] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP1] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP7] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP3] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:35704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:35964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:37188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:37670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:38744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:39742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:40138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:41110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:43630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:26] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP4] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP2] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP6] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP0] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP1] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP5] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP7] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:26 TP3] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27] INFO:     127.0.0.1:36368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:37510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:40328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:41192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:43734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:43952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:44022 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP2] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP4] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP6] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP0] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP5] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP1] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP3] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP7] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27] INFO:     127.0.0.1:35294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:35782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:37684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:38322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:40812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:41554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:45510 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP2] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP4] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP6] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP0] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP5] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP1] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP3] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP7] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27] INFO:     127.0.0.1:35724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:38104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:40082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:40262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:42700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:43034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:43874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:44928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:35638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:36842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:37938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:41070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:42168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:43528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:43980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:44266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:45534 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP4] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP6] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP2] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP0] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP5] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP1] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP7] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP3] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27] INFO:     127.0.0.1:35542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:37300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:38256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:44082 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP6] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP2] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP4] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP0] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP5] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP1] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP7] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP3] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27] INFO:     127.0.0.1:35170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:35538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:36374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:36746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:37330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:37350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:38496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:38706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:40806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:41206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:41658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:41700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:41848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:42680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:42686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:42888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:44298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:45168 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP2] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP4] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP6] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP0] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP5] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP1] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP7] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP3] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27] INFO:     127.0.0.1:35652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:38246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:41876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:43322 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP4] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP2] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP6] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP0] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP5] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP1] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP7] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP3] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP0] Decode batch, #running-req: 712, #token: 101549, token usage: 0.10, cuda graph: False, gen throughput (token/s): 7151.95, #queue-req: 0, 
[2025-11-05 14:18:27] INFO:     127.0.0.1:36764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:37830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:38414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:38678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:38846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:38990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:40100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:41090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:43760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:44104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:44504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:45042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:45204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:45450 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP4] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP6] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP2] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP0] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP5] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP1] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP7] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP3] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27] INFO:     127.0.0.1:35418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:35668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:36710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:37050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:41446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:41756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:42746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:45104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP4] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP2] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP6] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP0] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP5] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP1] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP7] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP3] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:35216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:37122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:40194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:40418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:40446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:40598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:41378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:42468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:43846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:44350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:44580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:45660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:27] INFO:     127.0.0.1:46558 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP4] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP2] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP6] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP0] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP5] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP1] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP7] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:27 TP3] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28] INFO:     127.0.0.1:35268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:36816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:37470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:38232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:39956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:41540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:42198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:42818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:43540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:43750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:45498 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP2] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP4] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP6] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP0] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP5] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP1] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP7] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP3] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28] INFO:     127.0.0.1:36826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:37866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:39412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:40298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:41318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:43024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:45834 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP4] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP2] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP6] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP0] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP5] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP1] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP3] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP7] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28] INFO:     127.0.0.1:35772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:40496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:42712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:43646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:43716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:44120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:44192 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP2] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP4] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP6] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP0] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP5] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP1] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP7] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP3] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28] INFO:     127.0.0.1:36654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:36876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:37284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:39932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:40612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:40774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:41122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:41752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:42778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:42810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:44230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:44414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:45196 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP4] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP2] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP6] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP0] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP5] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP1] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP7] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP3] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28] INFO:     127.0.0.1:35236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:35370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:35994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:36214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:36928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:38050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:43790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:44874 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP4] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP2] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP6] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP0] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP5] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP1] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP7] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP3] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28] INFO:     127.0.0.1:36888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:39422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:46698 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP2] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP4] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP6] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP0] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP5] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP1] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP7] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP3] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28] INFO:     127.0.0.1:35482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:37752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:38022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:38876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:39356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:39374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:39928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:40010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:42182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:42798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:43122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:44636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:45614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:46006 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP4] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP6] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP2] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP0] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP5] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP1] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP7] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP3] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28] INFO:     127.0.0.1:35978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:36758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:37870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:37948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:39640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:39864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:40390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:40456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:41148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:41270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:41418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:41944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:42056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:42150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:44272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:44554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP4] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP2] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP6] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP0] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP5] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP1] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP7] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP3] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28] INFO:     127.0.0.1:36688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:37268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:37742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:41022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:41066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:42722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:44242 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP4] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP2] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP6] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP0] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP5] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP1] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP7] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP3] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28] INFO:     127.0.0.1:35180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:38594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:38944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:39468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:40074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:41668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:42630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:42876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:28] INFO:     127.0.0.1:43398 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP4] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP2] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP6] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP0] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP5] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP1] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP7] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:28 TP3] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29] INFO:     127.0.0.1:35654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:36870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:37354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:38378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:39690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:40558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:40648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:40900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:40998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:44220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:44718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:44832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:46590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:35232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:35976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:36478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:38562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:40672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:44476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:44682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:44998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:46214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:40122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:44666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:46014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:46142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:46866 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP4] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP2] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP6] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP0] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP5] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP1] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP7] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP3] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29] INFO:     127.0.0.1:37020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:38006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:38444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:38734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:39384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:43362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:43842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45608 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP4] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP2] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP6] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP0] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP5] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP1] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP3] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29 TP7] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:29] INFO:     127.0.0.1:37326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:37632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:39146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:41506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:37290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:37472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:39458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:40786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:41170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:41262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:41742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:43814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:43838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:35732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:37066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:38388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:39642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:39884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:43598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:35156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:36810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:37186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:37324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:37382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:37796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:38854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:39370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:44858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:35448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:38928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:39168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:41080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:38230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:38578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:39904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:39922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:43170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:44704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:39200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:39510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:46176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:37946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:38364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:38836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:41426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:42562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:44332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:45292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:35104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:37172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:38512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:40090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:44760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:29] INFO:     127.0.0.1:46102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:36060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:39066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:40284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:41200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:42844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:43226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:43878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:44900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:35608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:37254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:37978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:39320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:39940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:40550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:44650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:40918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:41444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:42092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:38650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:39238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:39562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:44076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:44628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:38168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:40000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:40660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:40332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:40790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:41710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:44048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:39524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:41320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:43418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:43724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:36738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:37132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:39020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:42396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:37900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:39068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:40522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:41642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:41890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:44088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:42340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:36190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:40362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:41340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:42586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:42974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:37534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:43282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:35252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:36566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:37214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:39402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:43638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:44442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:44618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:44846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:36322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:37212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:37854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:40026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30 TP0] Decode batch, #running-req: 340, #token: 59801, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6444.91, #queue-req: 0, 
[2025-11-05 14:18:30] INFO:     127.0.0.1:37594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:37768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:39494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:44340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:44402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:44892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:43684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:44972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:45068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:30] INFO:     127.0.0.1:46908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:36150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:37872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:38098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:37178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:37780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:41376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:37834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:38970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:40030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:39408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:38452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:40766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:41390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:41474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:35506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:37150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:39284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:38462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:36466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:36586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:43702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:43950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:35852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:37550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:37578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:38064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:38550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:42420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:37544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:38868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:39062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:35836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:36036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:38394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:35574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:37376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:41798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:42378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:43926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:37868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:35150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:35758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:40294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:41812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:38984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:37644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:44334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:45696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:31] INFO:     127.0.0.1:46682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:42342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:44696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:39268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:38264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:43240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:44964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:36538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:38768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:40500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:43828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:37924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:41014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:37826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:40270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:40866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:41630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:44494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:44716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:44744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:44352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:37436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:44308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:40238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:35154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:39834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:42358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:36280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:40628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:43002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:44792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:35492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:44852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:39474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:37610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:38280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:44570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:44722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:44738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32 TP0] Decode batch, #running-req: 139, #token: 30774, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4318.13, #queue-req: 0, 
[2025-11-05 14:18:32] INFO:     127.0.0.1:36224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:45738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:32] INFO:     127.0.0.1:46422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:44914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:46282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:46730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:38922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:46496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:46606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:40726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:41778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:44796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:44862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:45412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:45644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:46316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:43060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:44552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:36076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:44772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:38124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:45584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:40268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:45628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:39306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:41336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:45436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:45706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:45888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:40472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:40890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:45898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:36976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:38488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:44080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:46748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:35696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:44384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:44808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:44830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:45132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:36120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:40098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:41500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:45942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:37962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:40378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:39114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:44592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:45100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:45752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:44532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:42156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:42444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:33] INFO:     127.0.0.1:45324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:36092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:36438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:38156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:45662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:46272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:46328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:46402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:46070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:45394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:42308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:46838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:44056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:46388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:36978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:41490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:41850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:43470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:36860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:39706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:46444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:42232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:43448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:36786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34 TP0] Decode batch, #running-req: 49, #token: 13451, token usage: 0.01, cuda graph: True, gen throughput (token/s): 2219.01, #queue-req: 0, 
[2025-11-05 14:18:34] INFO:     127.0.0.1:44012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:45220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:46018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:43508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:39294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:42576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:45970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:35434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:44158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:41580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:46764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:36492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:37732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:34] INFO:     127.0.0.1:39750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:46816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:39004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:36498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:46822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:45592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:38954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:46358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:46456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:45254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:45760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:45886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:45672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35 TP0] Decode batch, #running-req: 17, #token: 6263, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1024.85, #queue-req: 0, 
[2025-11-05 14:18:35] INFO:     127.0.0.1:44878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:35] INFO:     127.0.0.1:45054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:36] INFO:     127.0.0.1:41614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:36] INFO:     127.0.0.1:46066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:36] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:36] INFO:     127.0.0.1:42460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:36] INFO:     127.0.0.1:44816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:36] INFO:     127.0.0.1:45652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:36] INFO:     127.0.0.1:44038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:36] INFO:     127.0.0.1:45928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:36] INFO:     127.0.0.1:44028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:36] INFO:     127.0.0.1:45816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:36 TP0] Decode batch, #running-req: 5, #token: 2487, token usage: 0.00, cuda graph: True, gen throughput (token/s): 454.13, #queue-req: 0, 
[2025-11-05 14:18:36] INFO:     127.0.0.1:40360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:36] INFO:     127.0.0.1:45314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:36] INFO:     127.0.0.1:38302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:37] INFO:     127.0.0.1:44458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:37 TP0] Decode batch, #running-req: 1, #token: 1114, token usage: 0.00, cuda graph: True, gen throughput (token/s): 105.37, #queue-req: 0, 
[2025-11-05 14:18:37] INFO:     127.0.0.1:39632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:49] INFO:     127.0.0.1:54912 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-05 14:18:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:18:50] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:18:50 TP0] Prefill batch, #new-seq: 27, #new-token: 27, #cached-token: 19618, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP0] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP6] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP3] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP4] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP5] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP1] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP7] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP2] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP0] Prefill batch, #new-seq: 44, #new-token: 44, #cached-token: 32135, token usage: 0.01, #running-req: 28, #queue-req: 0, 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP0] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP4] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP6] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP1] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP3] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP5] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP2] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP7] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP0] Prefill batch, #new-seq: 51, #new-token: 51, #cached-token: 37096, token usage: 0.01, #running-req: 72, #queue-req: 0, 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP4] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP1] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP0] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP6] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP5] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP3] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP7] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP2] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP0] Prefill batch, #new-seq: 52, #new-token: 52, #cached-token: 37830, token usage: 0.01, #running-req: 123, #queue-req: 0, 
[2025-11-05 14:18:50 TP0] Prefill batch, #new-seq: 57, #new-token: 57, #cached-token: 41493, token usage: 0.01, #running-req: 175, #queue-req: 0, 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP0] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP4] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP1] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP3] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP6] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP2] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP5] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP7] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP0] Prefill batch, #new-seq: 57, #new-token: 57, #cached-token: 41419, token usage: 0.02, #running-req: 232, #queue-req: 0, 
[2025-11-05 14:18:50 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44426, token usage: 0.02, #running-req: 289, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP1] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP3] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP2] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP0] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP6] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP4] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP5] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP7] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP0] Prefill batch, #new-seq: 63, #new-token: 63, #cached-token: 45897, token usage: 0.03, #running-req: 350, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP0] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP1] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP3] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP2] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP4] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP6] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP5] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:50 TP7] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 43127, token usage: 0.03, #running-req: 413, #queue-req: 0, 
[2025-11-05 14:18:51 TP0] Prefill batch, #new-seq: 68, #new-token: 68, #cached-token: 49302, token usage: 0.03, #running-req: 472, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP6] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP4] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP7] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP1] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP3] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP5] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP0] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP2] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP0] Prefill batch, #new-seq: 65, #new-token: 65, #cached-token: 47231, token usage: 0.04, #running-req: 540, #queue-req: 0, 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP4] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP0] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP6] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP1] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP3] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP7] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP2] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP5] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:51 TP0] Prefill batch, #new-seq: 68, #new-token: 68, #cached-token: 49399, token usage: 0.04, #running-req: 605, #queue-req: 0, 
[2025-11-05 14:18:51 TP0] Prefill batch, #new-seq: 69, #new-token: 69, #cached-token: 50212, token usage: 0.05, #running-req: 673, #queue-req: 0, 
[2025-11-05 14:18:51 TP0] Prefill batch, #new-seq: 72, #new-token: 72, #cached-token: 52660, token usage: 0.05, #running-req: 742, #queue-req: 0, 
[2025-11-05 14:18:51 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2203, token usage: 0.05, #running-req: 814, #queue-req: 0, 
[2025-11-05 14:18:52 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9510, token usage: 0.05, #running-req: 817, #queue-req: 0, 
[2025-11-05 14:18:52 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 34797, token usage: 0.05, #running-req: 830, #queue-req: 0, 
[2025-11-05 14:18:52 TP0] Prefill batch, #new-seq: 47, #new-token: 47, #cached-token: 34047, token usage: 0.06, #running-req: 878, #queue-req: 0, 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP0] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP4] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP6] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP1] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP7] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP5] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP3] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP2] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP0] Prefill batch, #new-seq: 57, #new-token: 57, #cached-token: 41862, token usage: 0.06, #running-req: 925, #queue-req: 0, 
[2025-11-05 14:18:52 TP0] Prefill batch, #new-seq: 42, #new-token: 42, #cached-token: 30814, token usage: 0.06, #running-req: 982, #queue-req: 11, 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP0] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP6] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP4] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP7] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP1] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP3] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP5] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:52 TP2] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:18:55] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 704, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-05 14:18:55] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 750, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-05 14:18:56] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 730, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-11-05 14:18:56 TP0] Decode batch, #running-req: 1024, #token: 100261, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1964.96, #queue-req: 292, 
[2025-11-05 14:18:56] INFO:     127.0.0.1:54960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:56] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:56] INFO:     127.0.0.1:58504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:56] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:56] INFO:     127.0.0.1:59374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:56 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2264, token usage: 0.11, #running-req: 1021, #queue-req: 289, 
[2025-11-05 14:18:57] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:57396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:57696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:60568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:35294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5140, token usage: 0.11, #running-req: 1017, #queue-req: 282, 
[2025-11-05 14:18:57] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:60626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2261, token usage: 0.11, #running-req: 1021, #queue-req: 279, 
[2025-11-05 14:18:57] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:56790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:59362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:59444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:59700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:34196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5874, token usage: 0.11, #running-req: 1016, #queue-req: 271, 
[2025-11-05 14:18:57] INFO:     127.0.0.1:55850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:59882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:35714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2130, token usage: 0.11, #running-req: 1021, #queue-req: 268, 
[2025-11-05 14:18:57] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:58694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:33494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:35442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:57 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5795, token usage: 0.11, #running-req: 1016, #queue-req: 260, 
[2025-11-05 14:18:58] INFO:     127.0.0.1:60888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:34706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:35454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2167, token usage: 0.11, #running-req: 1021, #queue-req: 257, 
[2025-11-05 14:18:58] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:56540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:59512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2862, token usage: 0.11, #running-req: 1020, #queue-req: 253, 
[2025-11-05 14:18:58] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:56744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:60754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:32958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:34316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:34528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:34646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:35516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8021, token usage: 0.11, #running-req: 1013, #queue-req: 242, 
[2025-11-05 14:18:58] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:59448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:33816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:34388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:34868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5929, token usage: 0.11, #running-req: 1016, #queue-req: 234, 
[2025-11-05 14:18:58] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:58818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:59140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:59784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:60470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58] INFO:     127.0.0.1:34372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:58 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5146, token usage: 0.12, #running-req: 1017, #queue-req: 227, 
[2025-11-05 14:18:59] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:59100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:60364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5872, token usage: 0.12, #running-req: 1016, #queue-req: 219, 
[2025-11-05 14:18:59] INFO:     127.0.0.1:55490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:56532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:58214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:60070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4344, token usage: 0.12, #running-req: 1018, #queue-req: 213, 
[2025-11-05 14:18:59] INFO:     127.0.0.1:55592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:55740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:55916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:57188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:58176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:60756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:33652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:34660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:34820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8559, token usage: 0.12, #running-req: 1012, #queue-req: 201, 
[2025-11-05 14:18:59] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:59480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:33002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:35922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5887, token usage: 0.12, #running-req: 1016, #queue-req: 193, 
[2025-11-05 14:18:59] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:55706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:60900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:32842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:33154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59] INFO:     127.0.0.1:33570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:18:59 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5846, token usage: 0.12, #running-req: 1016, #queue-req: 185, 
[2025-11-05 14:19:00] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:56688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:57678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:35616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:35738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6524, token usage: 0.12, #running-req: 1015, #queue-req: 176, 
[2025-11-05 14:19:00] INFO:     127.0.0.1:55280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:56496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:57890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:59854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:60840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:33086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6458, token usage: 0.12, #running-req: 1015, #queue-req: 167, 
[2025-11-05 14:19:00] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:60980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:33986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2862, token usage: 0.12, #running-req: 1020, #queue-req: 163, 
[2025-11-05 14:19:00] INFO:     127.0.0.1:56238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:59436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:34478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:35428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6523, token usage: 0.12, #running-req: 1015, #queue-req: 154, 
[2025-11-05 14:19:00] INFO:     127.0.0.1:56632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:59430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:33792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00] INFO:     127.0.0.1:35058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:00 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2937, token usage: 0.12, #running-req: 1020, #queue-req: 150, 
[2025-11-05 14:19:01] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:56418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:57066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:59892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:60424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:33762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:34098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:34968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:35868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10331, token usage: 0.12, #running-req: 1010, #queue-req: 136, 
[2025-11-05 14:19:01] INFO:     127.0.0.1:55688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:59992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:33626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:35034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:35256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7957, token usage: 0.12, #running-req: 1013, #queue-req: 125, 
[2025-11-05 14:19:01] INFO:     127.0.0.1:55568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:60270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:60770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:60932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:34346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:34756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:34856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6656, token usage: 0.12, #running-req: 1015, #queue-req: 116, 
[2025-11-05 14:19:01] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:58388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:34816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6635, token usage: 0.12, #running-req: 1015, #queue-req: 107, 
[2025-11-05 14:19:01] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:56288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:56678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:56754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:57124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:57914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:58656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:59344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:59938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:60642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:33936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:34604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:01] INFO:     127.0.0.1:34790 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP4] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP3] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP1] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP7] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP5] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP0] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP2] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP6] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP0] Prefill batch, #new-seq: 18, #new-token: 18, #cached-token: 13184, token usage: 0.12, #running-req: 1006, #queue-req: 89, 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP1] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP0] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP7] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP3] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP6] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP4] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP2] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:01 TP5] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:02] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:56308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:60244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5031, token usage: 0.12, #running-req: 1017, #queue-req: 82, 
[2025-11-05 14:19:02] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:56220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:60534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:60692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9573, token usage: 0.12, #running-req: 1011, #queue-req: 69, 
[2025-11-05 14:19:02] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:59322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:59928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:60544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:33574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:33696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:33842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:34772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:35210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10297, token usage: 0.13, #running-req: 1010, #queue-req: 55, 
[2025-11-05 14:19:02] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:59204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:60472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:60718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:33332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:34514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:34562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:34942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:35982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10231, token usage: 0.13, #running-req: 1010, #queue-req: 41, 
[2025-11-05 14:19:02] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:60196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:60826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:34042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:34694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02] INFO:     127.0.0.1:35234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:02 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7275, token usage: 0.13, #running-req: 1014, #queue-req: 31, 
[2025-11-05 14:19:03] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:56086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:32854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:33018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:33184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:33566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:33910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:34672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:35122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10950, token usage: 0.13, #running-req: 1009, #queue-req: 16, 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP0] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP3] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP1] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP4] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP7] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP2] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP5] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP6] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:33400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:35670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5118, token usage: 0.13, #running-req: 1017, #queue-req: 9, 
[2025-11-05 14:19:03] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:59102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:60988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:34620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:35938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5050, token usage: 0.13, #running-req: 1017, #queue-req: 2, 
[2025-11-05 14:19:03] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:55958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:58364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:59350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:32826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:33282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:33502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:35408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1457, token usage: 0.13, #running-req: 1013, #queue-req: 0, 
[2025-11-05 14:19:03] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:55816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:59320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:59366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:59404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:59412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:33250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:03] INFO:     127.0.0.1:35644 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP4] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP3] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP7] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP1] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP5] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP0] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP6] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:03 TP2] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04] INFO:     127.0.0.1:56506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:58778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:60714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:60950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:33610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:33900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:35022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:35534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:35588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:35966 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP0] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP1] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP7] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP3] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP4] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP2] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP6] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP5] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP0] Decode batch, #running-req: 1004, #token: 121681, token usage: 0.13, cuda graph: False, gen throughput (token/s): 5420.78, #queue-req: 0, 
[2025-11-05 14:19:04] INFO:     127.0.0.1:55240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:33322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34114 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP4] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP3] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP7] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP2] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP6] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP0] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP5] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP1] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04] INFO:     127.0.0.1:55608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:55716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:33738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP0] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP4] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP7] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP2] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP6] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP3] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP5] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP1] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04] INFO:     127.0.0.1:55446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:57272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:58800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:33588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34924 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP4] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP3] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP7] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP6] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP2] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP0] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP1] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP5] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04] INFO:     127.0.0.1:55064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:56210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:35540 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP0] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP3] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP2] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP4] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP6] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP7] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP5] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP1] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04] INFO:     127.0.0.1:55260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34920 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP3] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP0] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP7] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP6] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP2] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP4] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP5] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP1] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:56866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:32778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:32892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:33412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:35194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:35198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:35810 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP0] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP6] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP3] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP7] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP2] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP4] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP1] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP5] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:33038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34984 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP0] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP7] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP3] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP6] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP2] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP4] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP5] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP1] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:58412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:60438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP4] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP7] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP6] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP5] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP3] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP2] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP0] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04 TP1] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:04] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:57526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:58758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:58886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:59582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:60764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:33838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:33848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:04] INFO:     127.0.0.1:34082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:57844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:60046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:60162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:34218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:34400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:34722 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP4] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP7] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP5] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP1] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP3] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP6] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP0] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP2] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:56734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:33286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:33576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:33818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:35560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:35914 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP4] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP1] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP3] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP7] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP5] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP0] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP6] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP2] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05] INFO:     127.0.0.1:55272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:55900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:56108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:57324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:57930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:59490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:59910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:60238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:33524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:34000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:35226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP4] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP3] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP7] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP1] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP5] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP0] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP6] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP2] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:33342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:34928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:35182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:57790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:32772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:33102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:33554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:34532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:34800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:35064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:56588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:57068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:57140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:34504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:34736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:35600 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP4] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP3] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP7] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP1] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP5] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP0] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP6] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP2] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05] INFO:     127.0.0.1:54968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:55378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:33098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:33168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:33624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:34882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:35412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:35824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:36232 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP4] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP3] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP7] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP1] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP5] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP0] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP6] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP2] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05] INFO:     127.0.0.1:59954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:33586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:36464 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP3] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP7] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP4] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP0] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP2] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP6] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP1] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP5] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:60686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:34740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:35898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:36678 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP4] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP3] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP7] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP0] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP6] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP2] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP1] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP5] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:58914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:59242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:05] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP4] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP3] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP7] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP0] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP6] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP2] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP1] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:05 TP5] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:59052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:59134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:59238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:60786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:60878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:36030 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP4] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP3] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP7] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP0] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP6] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP2] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP1] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP5] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06] INFO:     127.0.0.1:55550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:33492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:35380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:35892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:36034 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP4] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP3] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP7] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP0] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP6] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP2] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP1] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP5] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:55222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:60276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:60554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:60972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:33136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:33752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:35084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:35582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:36192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:37092 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP4] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP7] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP3] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP0] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP6] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP2] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP1] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP5] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP4] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP7] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP3] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP6] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP0] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP2] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP1] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP5] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06] INFO:     127.0.0.1:55402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:58430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:34152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:34598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:35758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:36614 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP4] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06] INFO:     127.0.0.1:59282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:33726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:34268 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP6] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP2] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06] INFO:     127.0.0.1:35654 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP0] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP3] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP7] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06] INFO:     127.0.0.1:35886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:37126 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP1] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP5] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06] INFO:     127.0.0.1:55518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:60010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:33642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:35130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:35502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:35510 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP4] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP6] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP2] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP7] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP5] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP1] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP0] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP3] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06] INFO:     127.0.0.1:55034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:58736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:33454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:34334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:37080 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP4] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP3] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP0] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP7] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP6] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP2] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP1] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP5] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:56324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:58876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:33048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:34484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:35642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:36014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:36800 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP4] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP3] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP7] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP0] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP6] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP2] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP1] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06 TP5] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:06] INFO:     127.0.0.1:57544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:58640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:60050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:34014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:34908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:35392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:35700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:36704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:36738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:59932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:33380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:33716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:34392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:35018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:35806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:06] INFO:     127.0.0.1:37018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:60194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:60336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:32930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:35366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:36746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:38182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:55090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:55186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:33122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:35462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:36778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:37070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:37270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:57208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:34402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:35148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:35722 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP4] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP7] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP3] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP0] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP6] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP2] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP1] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP5] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:57250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:60600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:60916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:32864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:33354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:35418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:36560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:36796 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP4] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP3] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP7] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP0] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP6] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP2] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP1] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP5] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07] INFO:     127.0.0.1:55060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:55822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:60862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:34280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:34358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:35324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:36088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:36322 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP4] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP3] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP7] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP0] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP6] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP2] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP1] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP5] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:57474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:59824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:60792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:34678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:35790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:35952 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP4] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP3] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP7] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP0] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP2] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP6] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP1] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP5] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:56646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:58766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:59092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:35836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:38336 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP4] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP3] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP7] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP0] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP6] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP2] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP1] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP5] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:60738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:33706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:33824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:35242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:37396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:37594 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP4] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP3] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP7] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP0] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP2] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP6] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP1] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP5] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07] INFO:     127.0.0.1:54982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:56572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:57006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:58462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:59372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:60214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:34762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:35718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:36200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP4] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP0] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP6] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP2] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP3] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP7] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP1] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP5] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:60752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:33600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:36164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:36154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:07] INFO:     127.0.0.1:37602 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP4] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP2] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP6] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP0] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP3] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP7] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP1] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:07 TP5] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08] INFO:     127.0.0.1:55040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:55160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:55380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:33258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:33796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:35166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36340 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP4] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP6] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP2] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP0] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP7] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP3] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP1] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP5] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP0] Decode batch, #running-req: 582, #token: 88047, token usage: 0.09, cuda graph: False, gen throughput (token/s): 7869.73, #queue-req: 0, 
[2025-11-05 14:19:08] INFO:     127.0.0.1:56034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:56990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:57970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:60650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:35884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:38208 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP4] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP2] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP6] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP0] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP3] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP7] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP1] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP5] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:55800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:60004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:60428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:33668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:35474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:37776 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP4] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP6] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP2] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP0] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP3] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP7] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP1] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP5] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08] INFO:     127.0.0.1:55316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:56276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:57514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:33774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:35458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:37704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:37904 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP4] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP2] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP6] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP0] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP7] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP3] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP1] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08 TP5] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:08] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:59038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:32912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:37008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:37176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:37224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:56014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:33076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:37368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:56900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:59576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:59762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:32794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:33368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:55998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:56728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:57706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:59116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:60946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:32978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:35450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:38194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:57602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:37148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:37236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:35092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:59046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:59266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:33690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:37730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:37832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:58264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:59642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:60708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:32970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:35636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:33198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:33234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:34416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:35724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:36878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:08] INFO:     127.0.0.1:37434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:54954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:57020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:34420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:55004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:59814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:60960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:58792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:33686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:34370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:34840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:38090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:38260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:38484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:56494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:58228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:60672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:35526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:38496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:58486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:35772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:38322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:57806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:60084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:35852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:56516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:32882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:38456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:55194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:56810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:33952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:38148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:55184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:56620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:60290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:33072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:34558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:35352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:33864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:38036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:33436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:35416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:34166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:34556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:59126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:60584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:35244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:37026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:34898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:09] INFO:     127.0.0.1:36698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:35278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:35992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:33306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:56974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:57304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:60408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:32900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:35134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:59744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:60116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:35572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:35768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:32810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:33426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:59030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:35336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:32940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:35728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:55976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:57300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:33964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10 TP0] Decode batch, #running-req: 259, #token: 49951, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6001.97, #queue-req: 0, 
[2025-11-05 14:19:10] INFO:     127.0.0.1:56826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:57532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:58672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:58586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:57878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:59076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:36536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:37958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:38512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:58446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:60030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:60614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:10] INFO:     127.0.0.1:35162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:35548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:38120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:60496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:33220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:38104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:38312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:56944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:33888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:38136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:55802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:58816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:33206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:38068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:38232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:38356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:58864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:55302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:59656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:38288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:33872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:38002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:59414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:56114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:33518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:34586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:38248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:36914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:37996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:11] INFO:     127.0.0.1:38054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:56584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:34180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:37054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:37318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:37320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:57886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:34786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:37458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:37860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:38350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:38074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:60468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:33388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:55602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:57470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:37238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:37546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:37890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:38532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:55766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:57618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:55442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:34640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:37192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:32880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:38300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:34828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:37310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:38016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12 TP0] Decode batch, #running-req: 105, #token: 25818, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3683.36, #queue-req: 0, 
[2025-11-05 14:19:12] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:60658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:60814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:38372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:59802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:35126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:35686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:37722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:57144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:38236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:33484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:38216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:60094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:59246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:60134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:37984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:37640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:12] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:58920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:33776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:36304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:36724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:37524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:56574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:33710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:36890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:37326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:56444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:37892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:37998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:33826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:37218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:37244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:33064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:37476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:57908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:36984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:38470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:37720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:38566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:35078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:37840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:36802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:37940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:35630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:37456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:59298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:38022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:38548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:37746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:60236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:34136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:37378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:33164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:13] INFO:     127.0.0.1:36056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:37206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:38380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14 TP0] Decode batch, #running-req: 35, #token: 10246, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1732.08, #queue-req: 0, 
[2025-11-05 14:19:14] INFO:     127.0.0.1:38416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:35776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:38440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:55168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:35044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:37338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:36248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:36848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:37274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:38282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:36708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:14] INFO:     127.0.0.1:38114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15] INFO:     127.0.0.1:38048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15] INFO:     127.0.0.1:37592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15 TP0] Decode batch, #running-req: 14, #token: 4726, token usage: 0.00, cuda graph: True, gen throughput (token/s): 864.66, #queue-req: 0, 
[2025-11-05 14:19:15] INFO:     127.0.0.1:34030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15] INFO:     127.0.0.1:37634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15] INFO:     127.0.0.1:33676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15] INFO:     127.0.0.1:37508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15] INFO:     127.0.0.1:36434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15] INFO:     127.0.0.1:36102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15] INFO:     127.0.0.1:36574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15] INFO:     127.0.0.1:36008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15] INFO:     127.0.0.1:37260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15] INFO:     127.0.0.1:35746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:15] INFO:     127.0.0.1:36882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:16 TP0] Decode batch, #running-req: 2, #token: 1383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 232.46, #queue-req: 0, 
[2025-11-05 14:19:16] INFO:     127.0.0.1:37376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:16 TP0] Decode batch, #running-req: 1, #token: 1127, token usage: 0.00, cuda graph: True, gen throughput (token/s): 53.36, #queue-req: 0, 
[2025-11-05 14:19:17 TP0] Decode batch, #running-req: 1, #token: 1167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.62, #queue-req: 0, 
[2025-11-05 14:19:17] INFO:     127.0.0.1:59502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:30] INFO:     127.0.0.1:41106 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-05 14:19:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:19:30] INFO:     127.0.0.1:41114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:19:30 TP0] Prefill batch, #new-seq: 37, #new-token: 37, #cached-token: 26896, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP0] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP4] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP6] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP2] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP5] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP1] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP7] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP3] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP0] Prefill batch, #new-seq: 46, #new-token: 46, #cached-token: 33608, token usage: 0.01, #running-req: 38, #queue-req: 0, 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP0] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP5] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP6] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP4] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP7] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP1] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP2] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP3] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP0] Prefill batch, #new-seq: 49, #new-token: 49, #cached-token: 35653, token usage: 0.01, #running-req: 84, #queue-req: 0, 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP5] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP6] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP4] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP7] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP1] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP3] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP2] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP0] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP0] Prefill batch, #new-seq: 55, #new-token: 55, #cached-token: 40147, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP5] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP4] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP6] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP1] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP7] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP2] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP3] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP0] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:30 TP0] Prefill batch, #new-seq: 56, #new-token: 56, #cached-token: 40661, token usage: 0.02, #running-req: 188, #queue-req: 0, 
[2025-11-05 14:19:30 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44374, token usage: 0.02, #running-req: 244, #queue-req: 0, 
[2025-11-05 14:19:31 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 43718, token usage: 0.02, #running-req: 305, #queue-req: 0, 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP5] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP6] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP4] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP7] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP1] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP2] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP3] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP0] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP0] Prefill batch, #new-seq: 66, #new-token: 66, #cached-token: 48175, token usage: 0.03, #running-req: 365, #queue-req: 0, 
[2025-11-05 14:19:31 TP0] Prefill batch, #new-seq: 65, #new-token: 65, #cached-token: 47156, token usage: 0.03, #running-req: 431, #queue-req: 0, 
[2025-11-05 14:19:31 TP0] Prefill batch, #new-seq: 72, #new-token: 72, #cached-token: 52150, token usage: 0.04, #running-req: 496, #queue-req: 0, 
[2025-11-05 14:19:31 TP0] Prefill batch, #new-seq: 74, #new-token: 74, #cached-token: 53900, token usage: 0.04, #running-req: 568, #queue-req: 0, 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP6] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP4] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP5] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP1] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP3] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP2] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP7] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP0] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP0] Prefill batch, #new-seq: 71, #new-token: 71, #cached-token: 51670, token usage: 0.04, #running-req: 642, #queue-req: 0, 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP5] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP4] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP7] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP1] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP3] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP6] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP0] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:31 TP2] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10215, token usage: 0.05, #running-req: 713, #queue-req: 0, 
[2025-11-05 14:19:32 TP0] Prefill batch, #new-seq: 47, #new-token: 47, #cached-token: 34180, token usage: 0.05, #running-req: 727, #queue-req: 0, 
[2025-11-05 14:19:32 TP0] Prefill batch, #new-seq: 47, #new-token: 47, #cached-token: 34427, token usage: 0.05, #running-req: 774, #queue-req: 0, 
[2025-11-05 14:19:32 TP0] Prefill batch, #new-seq: 57, #new-token: 57, #cached-token: 41425, token usage: 0.06, #running-req: 821, #queue-req: 0, 
[2025-11-05 14:19:32 TP0] Prefill batch, #new-seq: 54, #new-token: 54, #cached-token: 39261, token usage: 0.06, #running-req: 878, #queue-req: 0, 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP5] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP6] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP4] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP2] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP1] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP0] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP7] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP3] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP0] Prefill batch, #new-seq: 63, #new-token: 63, #cached-token: 46173, token usage: 0.06, #running-req: 932, #queue-req: 0, 
[2025-11-05 14:19:32 TP0] Prefill batch, #new-seq: 29, #new-token: 29, #cached-token: 21368, token usage: 0.06, #running-req: 995, #queue-req: 30, 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP5] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP4] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP1] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP7] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP6] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP0] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP2] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:32 TP3] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:35] INFO:     127.0.0.1:43970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 718, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-05 14:19:36 TP0] Decode batch, #running-req: 1024, #token: 95800, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1788.95, #queue-req: 294, 
[2025-11-05 14:19:36] INFO:     127.0.0.1:49618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:36] INFO:     127.0.0.1:44000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 736, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-05 14:19:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 757, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-11-05 14:19:36] INFO:     127.0.0.1:42478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:36] INFO:     127.0.0.1:44750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:41150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:42496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:45552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1511, token usage: 0.11, #running-req: 1022, #queue-req: 290, 
[2025-11-05 14:19:37] INFO:     127.0.0.1:42064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:42802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:43568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:43838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:46940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5902, token usage: 0.11, #running-req: 1016, #queue-req: 282, 
[2025-11-05 14:19:37] INFO:     127.0.0.1:41404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:49592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2210, token usage: 0.11, #running-req: 1021, #queue-req: 279, 
[2025-11-05 14:19:37] INFO:     127.0.0.1:41454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:41906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:42586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:43646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:45796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:46032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:49700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5805, token usage: 0.11, #running-req: 1016, #queue-req: 271, 
[2025-11-05 14:19:37] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:44402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:46228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2135, token usage: 0.11, #running-req: 1021, #queue-req: 268, 
[2025-11-05 14:19:37] INFO:     127.0.0.1:44268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:44914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:37] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2202, token usage: 0.11, #running-req: 1021, #queue-req: 265, 
[2025-11-05 14:19:38] INFO:     127.0.0.1:45686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:47218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:49672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:49812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2872, token usage: 0.11, #running-req: 1020, #queue-req: 261, 
[2025-11-05 14:19:38] INFO:     127.0.0.1:41354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:41600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:41698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:42534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:44526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:45790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:46028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:48984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5750, token usage: 0.11, #running-req: 1016, #queue-req: 253, 
[2025-11-05 14:19:38] INFO:     127.0.0.1:41136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:43280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:43616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:43954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:46338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:47490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:49222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5139, token usage: 0.11, #running-req: 1017, #queue-req: 246, 
[2025-11-05 14:19:38] INFO:     127.0.0.1:42370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:42874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:45260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:45720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:47532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:47988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:48850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:49136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:49738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7377, token usage: 0.11, #running-req: 1014, #queue-req: 236, 
[2025-11-05 14:19:38] INFO:     127.0.0.1:42346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:45076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:45788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:46132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:46816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:38] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5842, token usage: 0.11, #running-req: 1016, #queue-req: 228, 
[2025-11-05 14:19:39] INFO:     127.0.0.1:42624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:43394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:45332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:46690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:48256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:48684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5874, token usage: 0.12, #running-req: 1016, #queue-req: 220, 
[2025-11-05 14:19:39] INFO:     127.0.0.1:42002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:44172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:46416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3659, token usage: 0.12, #running-req: 1019, #queue-req: 215, 
[2025-11-05 14:19:39] INFO:     127.0.0.1:41590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:42044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:42844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:44710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:47028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:47114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7131, token usage: 0.12, #running-req: 1014, #queue-req: 205, 
[2025-11-05 14:19:39] INFO:     127.0.0.1:42176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:45722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:46760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4379, token usage: 0.12, #running-req: 1018, #queue-req: 199, 
[2025-11-05 14:19:39] INFO:     127.0.0.1:41122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:42890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:43132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:44462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:47102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:39] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5831, token usage: 0.12, #running-req: 1016, #queue-req: 191, 
[2025-11-05 14:19:40] INFO:     127.0.0.1:42278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:43034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:43372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:43436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:43792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:47656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:47960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5800, token usage: 0.12, #running-req: 1016, #queue-req: 183, 
[2025-11-05 14:19:40] INFO:     127.0.0.1:41648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:41920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:43222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:44048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:44418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:45454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:45884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:47090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:47176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:47650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:48064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:48130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9386, token usage: 0.12, #running-req: 1011, #queue-req: 170, 
[2025-11-05 14:19:40] INFO:     127.0.0.1:41562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:46030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:48166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:48856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3620, token usage: 0.12, #running-req: 1019, #queue-req: 165, 
[2025-11-05 14:19:40] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:44596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:45968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:48404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2872, token usage: 0.12, #running-req: 1020, #queue-req: 161, 
[2025-11-05 14:19:40] INFO:     127.0.0.1:42900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:46216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:46918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:48958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:40] INFO:     127.0.0.1:49668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4329, token usage: 0.12, #running-req: 1018, #queue-req: 155, 
[2025-11-05 14:19:41] INFO:     127.0.0.1:41248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:41366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:41710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:42692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:43242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:45738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:46768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:46840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:46976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:47606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:48268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:48418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:49314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11754, token usage: 0.12, #running-req: 1008, #queue-req: 139, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 14:19:41] INFO:     127.0.0.1:41486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:43310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:43672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:45596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:46346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:47910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:49254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:49516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6510, token usage: 0.12, #running-req: 1015, #queue-req: 130, 
[2025-11-05 14:19:41] INFO:     127.0.0.1:42052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:43790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:44556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:45756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:47240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:47482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:48230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:48470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:49288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:49580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:49896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8909, token usage: 0.12, #running-req: 1012, #queue-req: 118, 
[2025-11-05 14:19:41] INFO:     127.0.0.1:41426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:41682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:41824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:44086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:44114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:45190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:45616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:49164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8809, token usage: 0.12, #running-req: 1012, #queue-req: 106, 
[2025-11-05 14:19:41] INFO:     127.0.0.1:41412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:42168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:42734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:42858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:43340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:44064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:44870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:41] INFO:     127.0.0.1:49078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8094, token usage: 0.12, #running-req: 1013, #queue-req: 95, 
[2025-11-05 14:19:42] INFO:     127.0.0.1:42252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:42648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:43408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:43472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:48000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:48924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:49048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:49062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7919, token usage: 0.12, #running-req: 1013, #queue-req: 84, 
[2025-11-05 14:19:42] INFO:     127.0.0.1:41314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:41726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:42494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:43158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:43604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:43710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:44096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:46118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:46902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:47022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:48578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:49378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8727, token usage: 0.12, #running-req: 1012, #queue-req: 72, 
[2025-11-05 14:19:42] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:42840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:44476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:46282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:46798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:46952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:47300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:48378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:48484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6692, token usage: 0.13, #running-req: 1015, #queue-req: 63, 
[2025-11-05 14:19:42] INFO:     127.0.0.1:42430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:43070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:45036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:45700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:46056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:46828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:47184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:49044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8209, token usage: 0.13, #running-req: 1013, #queue-req: 52, 
[2025-11-05 14:19:42] INFO:     127.0.0.1:42118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:42338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:43594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:43806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:43908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:44308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:46668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:48030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:48284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:48898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:49210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:50018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:42] INFO:     127.0.0.1:50064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43 TP0] Prefill batch, #new-seq: 18, #new-token: 18, #cached-token: 13042, token usage: 0.13, #running-req: 1006, #queue-req: 34, 
[2025-11-05 14:19:43] INFO:     127.0.0.1:42038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:42872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:43446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:44176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:44444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:45360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:45442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:49004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5780, token usage: 0.13, #running-req: 1016, #queue-req: 26, 
[2025-11-05 14:19:43] INFO:     127.0.0.1:41502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:42954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:43184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:47370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:48438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5865, token usage: 0.13, #running-req: 1016, #queue-req: 18, 
[2025-11-05 14:19:43 TP0] Decode batch, #running-req: 1016, #token: 121814, token usage: 0.13, cuda graph: False, gen throughput (token/s): 5520.97, #queue-req: 18, 
[2025-11-05 14:19:43] INFO:     127.0.0.1:41930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:44584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:44600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:44610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:47680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:48360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8066, token usage: 0.13, #running-req: 1013, #queue-req: 7, 
[2025-11-05 14:19:43] INFO:     127.0.0.1:41330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:42698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:45650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:47036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:47868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:47914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:48942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:49532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5038, token usage: 0.13, #running-req: 1014, #queue-req: 0, 
[2025-11-05 14:19:43] INFO:     127.0.0.1:41552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:42544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:42762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:43336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:45014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:45598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:48280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:43] INFO:     127.0.0.1:50090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:42618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:42818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:43556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:45428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:45666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:47054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:47776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP1] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP0] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP5] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP4] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP7] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP2] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP6] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP3] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44] INFO:     127.0.0.1:41580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:42374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:43018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:44040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:44518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:45474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:48926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49874 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP4] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP5] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP3] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP7] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP1] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP0] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP2] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP6] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44] INFO:     127.0.0.1:42018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:42580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:44214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:45930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:45950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:46180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:46248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:46268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:48098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:48346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:48488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:41340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:41468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:41530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:41960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:42404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:43350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:45052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:45778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:46304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:48780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49576 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP4] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP5] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP1] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP0] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP2] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP3] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP7] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP6] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44] INFO:     127.0.0.1:41514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:41670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:42426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:43064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:43298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:43536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:44192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:46046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:46368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49190 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP5] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP4] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP6] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP2] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP0] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP7] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP3] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP1] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44] INFO:     127.0.0.1:41296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:42078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:42564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:44326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:44616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:45826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:46476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:47324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:48084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:48670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:48792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:42174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:43876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:45218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:46794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:47846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP4] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP5] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP0] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP2] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP6] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP1] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP7] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP3] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44] INFO:     127.0.0.1:41678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:42330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:43678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:43758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:44486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:45068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:47430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:47624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:48604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:50096 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP4] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP5] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP0] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP6] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP2] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP1] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP3] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44 TP7] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:44] INFO:     127.0.0.1:41446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:41744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:41844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:42348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:42968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:44624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:44740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:46242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:47516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:48630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:44] INFO:     127.0.0.1:49260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:41176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:42296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:43658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:44958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:46784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:47104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP5] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP4] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP1] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP7] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP3] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP0] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP2] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP6] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45] INFO:     127.0.0.1:41800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:43986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:44542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:44944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:45900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:46524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP4] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP5] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP1] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP7] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP3] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP0] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP2] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP6] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45] INFO:     127.0.0.1:41206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:41896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:42188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:42220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:42452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:43584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP5] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP4] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP1] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP7] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP3] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP0] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP2] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP6] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45] INFO:     127.0.0.1:41388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:41656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:41712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:42440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:42752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:43386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:43512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:44034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:44100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:44342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:44730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:46090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:46616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:47566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:50054 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP5] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP4] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP1] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP3] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP7] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP0] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP6] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP2] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45] INFO:     127.0.0.1:41246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:41606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:49510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:49796 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP5] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP4] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP1] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP7] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP3] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP0] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP2] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP6] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45] INFO:     127.0.0.1:42704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:47160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:47790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:49132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:49480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:50272 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP4] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP5] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP1] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP7] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP3] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP0] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP2] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP6] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45] INFO:     127.0.0.1:41640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:43230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:43514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:44504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:44670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:44922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:45000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:45122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:47700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:49338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:41162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:41788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:42354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:43544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:45396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:45920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48862 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP1] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP3] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP5] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP4] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP7] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP0] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP2] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP6] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45] INFO:     127.0.0.1:42680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:46326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:46704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:47838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP1] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP4] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP5] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP7] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP3] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP0] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP6] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45 TP2] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:45] INFO:     127.0.0.1:42146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:42236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:44252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:44632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:45282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:45912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:47278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:48108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:45] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:41170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:42106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:44240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:44362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:46552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:46660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:48638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:48836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:50338 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP1] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP5] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP3] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP4] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP7] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP0] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP6] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP2] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46] INFO:     127.0.0.1:41834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:41866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:41878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:44574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:44934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:46358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:47204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:43586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:46992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:47172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:47262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:47962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP1] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP5] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP7] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP4] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP3] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP0] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP6] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP2] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46] INFO:     127.0.0.1:41320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:46356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:46646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP1] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP3] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP5] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP4] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP7] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP0] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP6] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP2] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46] INFO:     127.0.0.1:41808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:43460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:43884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:46138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:51380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:42532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:47132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:47252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:47456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:48210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49850 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP4] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP5] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP1] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP7] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP3] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP0] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP6] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP2] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46] INFO:     127.0.0.1:41434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:41774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:43922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:48610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:50190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:41220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:42090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:42550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:43084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:43096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:44296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:44978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:46870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:48190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:48500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:41756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:41786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:42134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:43108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:43994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:44962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:45678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49488 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP1] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP3] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP5] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP4] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP7] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP0] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP6] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46 TP2] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:46] INFO:     127.0.0.1:41410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:44456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:44664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:44816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:44854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:44936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:47766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:47824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:47950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:48808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:49108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:50960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:51094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:46] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:41564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:44846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:47514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:48594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:48932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:49646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:49900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:49922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:43390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:44764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:45128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:45542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:46062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:46292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:46612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:48590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:48704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:50998 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP1] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP4] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP5] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP7] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP3] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP0] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP6] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP2] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47] INFO:     127.0.0.1:41300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:43346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:43862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:44074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:44832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:46024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:46430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:47728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:48196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:51346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:52586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:42466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:42710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:42716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:42790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:43830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:45324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:47878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:49544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:49706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:51682 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP1] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP4] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP5] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP7] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP3] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP0] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP6] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP2] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47] INFO:     127.0.0.1:41664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:43478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:44202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:45520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:45640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:46526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:46620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:47630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:48732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:49284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:49420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:49642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP1] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP5] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP3] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP4] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP7] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP0] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP6] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP2] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47] INFO:     127.0.0.1:42468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:42474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:46720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:47188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:50104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:51092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:42012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:42026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:42804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:43430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:43698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:46474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:46580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:48616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:48666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP1] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP4] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP5] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP3] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP7] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP0] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP6] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP2] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47] INFO:     127.0.0.1:43974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:47872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:50464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:51518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP1] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP5] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP4] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP7] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP3] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP0] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP6] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP2] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP0] Decode batch, #running-req: 613, #token: 92575, token usage: 0.10, cuda graph: False, gen throughput (token/s): 7708.04, #queue-req: 0, 
[2025-11-05 14:19:47] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:45202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:45224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:45252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:45482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:45582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:46596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:50916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:51948 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP1] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP4] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP3] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP5] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP7] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP0] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP6] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP2] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47] INFO:     127.0.0.1:41498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:43210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:43566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:44166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:47130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:47] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP1] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP5] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP4] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP3] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP7] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP0] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP2] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:47 TP6] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48] INFO:     127.0.0.1:42160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:43426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:43694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:47082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:47502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:48248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:49662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP1] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP4] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP5] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP3] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP7] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP0] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP6] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP2] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48] INFO:     127.0.0.1:41208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:44152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:45216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:45384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:47742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:48170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP1] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP5] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP4] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP7] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP3] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP0] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP6] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP2] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48] INFO:     127.0.0.1:41592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:41882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:43200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:44136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:45606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:48620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:52628 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP1] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP4] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP5] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP7] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP3] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP0] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP6] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP2] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48] INFO:     127.0.0.1:41280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:41470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:44374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:47268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:47480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:47506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:48888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP1] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP5] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP4] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP3] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP7] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP0] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP2] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP6] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48] INFO:     127.0.0.1:42668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:43122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:44024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:44322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:45348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:46190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:47086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP1] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP3] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP5] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP4] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP7] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP0] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP2] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP6] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:43686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:43944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:44522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:44986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:47540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:48124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:52076 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP1] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP4] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP3] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP5] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP7] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP0] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP6] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP2] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48] INFO:     127.0.0.1:47442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:47594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:48212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:48698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51646 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP1] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP4] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP5] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP7] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP3] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP0] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP6] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48 TP2] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:19:48] INFO:     127.0.0.1:42832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:43496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:44890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:45298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:45890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:46128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:46774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:47340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:41194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:43846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:44770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:45376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:41842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:42866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:42984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:43416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:43700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:48514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:52184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:44392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:46514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:49354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:41534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:45276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:45872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:47254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:47916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:48784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:49694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:50612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:48] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:42256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:43218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:44500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:45982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:48106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:48458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:48558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:51732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:43816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:43894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:44806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:48156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:48646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:41146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:42938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:43216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:43928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:46168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:47050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:48144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:48740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:49876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:41188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:50852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:41624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:44238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:45696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:45956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:46170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:46648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:47006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:49666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:49924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:42634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:44428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:45092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:51556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:41946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:43728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:44718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:44824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:46070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:47854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:42860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:43128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:44570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:46446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:50298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:42194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:48502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:50002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:42732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:46010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:46850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:47806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:45712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:46458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:51792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:47426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:48158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:48392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:48876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:51254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:41536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:43048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:44902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:47974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:50450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:51740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:52548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:42902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:43154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:48328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:49168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:49] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:45394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:48544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:46400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:47934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:47740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:49550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:48032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:49600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:44222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:42204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:43626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:46080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:42794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:47382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:43266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:43366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:44648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:44882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:46808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:47074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:47464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:47576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:47926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50 TP0] Decode batch, #running-req: 288, #token: 51861, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6073.75, #queue-req: 0, 
[2025-11-05 14:19:50] INFO:     127.0.0.1:42782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:43046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:44438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:45144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:46844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:42926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:44348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:44954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:43490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:42706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:43252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:43590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:43638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:41500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:48406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:51704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:50] INFO:     127.0.0.1:52780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:42282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:45310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:44686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:46380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:48362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:41196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:49774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:43082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:50564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:41970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:44008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:46968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:48338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:43168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:45070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:45412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:47716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:41288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:43282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:49040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:46350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:47718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:45998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:46712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:50902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:41232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:41728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:46482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:48174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:50648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:42000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:43332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:48564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:51] INFO:     127.0.0.1:51822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:43144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:47146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:52664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:42310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:48922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:42606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:52020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:44026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:44128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:43166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:42326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:42410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:52486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:52750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:41992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:47682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:47894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:43712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:48944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:52878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52 TP0] Decode batch, #running-req: 111, #token: 26234, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3838.84, #queue-req: 0, 
[2025-11-05 14:19:52] INFO:     127.0.0.1:42654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:46392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:41492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:42388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:44284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:42344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:46152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:49396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:42266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:42632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:43422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:41850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:43774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:52] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:45184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:45850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:51612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:50978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:51214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:52248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:52110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:41770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:48426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:43888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:51536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:52380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:42516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:52274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:48296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:52834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:52974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:46454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:51108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:47584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:45836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:49370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:53] INFO:     127.0.0.1:44050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54 TP0] Decode batch, #running-req: 40, #token: 11655, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1841.44, #queue-req: 0, 
[2025-11-05 14:19:54] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:41986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:44792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:50432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:42746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:48530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:45508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:47672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:51484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:44830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:50912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:41374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:52816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:44780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:52204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:51152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:52328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:54] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:55] INFO:     127.0.0.1:51994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:55] INFO:     127.0.0.1:51952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:55 TP0] Decode batch, #running-req: 15, #token: 5427, token usage: 0.01, cuda graph: True, gen throughput (token/s): 840.85, #queue-req: 0, 
[2025-11-05 14:19:55] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:55] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:55] INFO:     127.0.0.1:51622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:55] INFO:     127.0.0.1:50834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:55] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:55] INFO:     127.0.0.1:48136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:55] INFO:     127.0.0.1:49980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:55] INFO:     127.0.0.1:49982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:55] INFO:     127.0.0.1:43000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:56 TP0] Decode batch, #running-req: 5, #token: 2412, token usage: 0.00, cuda graph: True, gen throughput (token/s): 381.88, #queue-req: 0, 
[2025-11-05 14:19:56] INFO:     127.0.0.1:48436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:56] INFO:     127.0.0.1:51204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:56] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:56] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:19:56] INFO:     127.0.0.1:49330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:09] INFO:     127.0.0.1:40948 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-05 14:20:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:20:09] INFO:     127.0.0.1:40954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:20:09 TP0] Prefill batch, #new-seq: 37, #new-token: 37, #cached-token: 26917, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:20:09 TP0] Prefill batch, #new-seq: 46, #new-token: 46, #cached-token: 33452, token usage: 0.01, #running-req: 38, #queue-req: 0, 
[2025-11-05 14:20:09 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 35190, token usage: 0.01, #running-req: 84, #queue-req: 0, 
[2025-11-05 14:20:09 TP0] Prefill batch, #new-seq: 55, #new-token: 55, #cached-token: 40057, token usage: 0.01, #running-req: 132, #queue-req: 0, 
[2025-11-05 14:20:09 TP0] Prefill batch, #new-seq: 54, #new-token: 54, #cached-token: 39111, token usage: 0.02, #running-req: 187, #queue-req: 0, 
[2025-11-05 14:20:10 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 43521, token usage: 0.02, #running-req: 241, #queue-req: 0, 
[2025-11-05 14:20:10 TP0] Prefill batch, #new-seq: 53, #new-token: 53, #cached-token: 38709, token usage: 0.02, #running-req: 301, #queue-req: 0, 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP4] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP5] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP6] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP7] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP3] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP1] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP2] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP0] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44396, token usage: 0.03, #running-req: 354, #queue-req: 0, 
[2025-11-05 14:20:10 TP0] Prefill batch, #new-seq: 58, #new-token: 58, #cached-token: 42388, token usage: 0.03, #running-req: 415, #queue-req: 0, 
[2025-11-05 14:20:10 TP0] Prefill batch, #new-seq: 66, #new-token: 66, #cached-token: 47764, token usage: 0.03, #running-req: 473, #queue-req: 0, 
[2025-11-05 14:20:10 TP0] Prefill batch, #new-seq: 20, #new-token: 20, #cached-token: 14512, token usage: 0.03, #running-req: 539, #queue-req: 0, 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP4] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP7] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP6] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP5] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP1] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP3] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP2] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP0] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP4] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP5] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP7] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP1] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP3] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP6] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP0] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:10 TP2] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:11 TP0] Prefill batch, #new-seq: 39, #new-token: 39, #cached-token: 28416, token usage: 0.04, #running-req: 559, #queue-req: 0, 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:11 TP0] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:11 TP1] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:11 TP4] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:11 TP5] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:11 TP3] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:11 TP6] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:11 TP7] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:11 TP2] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:11 TP0] Prefill batch, #new-seq: 49, #new-token: 49, #cached-token: 35604, token usage: 0.04, #running-req: 598, #queue-req: 0, 
[2025-11-05 14:20:11 TP0] Prefill batch, #new-seq: 56, #new-token: 56, #cached-token: 40888, token usage: 0.04, #running-req: 647, #queue-req: 0, 
[2025-11-05 14:20:11 TP0] Prefill batch, #new-seq: 55, #new-token: 55, #cached-token: 40000, token usage: 0.05, #running-req: 703, #queue-req: 0, 
[2025-11-05 14:20:11 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44554, token usage: 0.05, #running-req: 758, #queue-req: 0, 
[2025-11-05 14:20:11 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 43592, token usage: 0.06, #running-req: 819, #queue-req: 0, 
[2025-11-05 14:20:11 TP0] Prefill batch, #new-seq: 67, #new-token: 67, #cached-token: 48871, token usage: 0.06, #running-req: 879, #queue-req: 0, 
[2025-11-05 14:20:11 TP0] Prefill batch, #new-seq: 65, #new-token: 65, #cached-token: 47750, token usage: 0.06, #running-req: 946, #queue-req: 0, 
[2025-11-05 14:20:11 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9504, token usage: 0.06, #running-req: 1011, #queue-req: 61, 
[2025-11-05 14:20:14 TP0] Decode batch, #running-req: 1024, #token: 82347, token usage: 0.08, cuda graph: False, gen throughput (token/s): 1090.74, #queue-req: 295, 
[2025-11-05 14:20:14] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-05 14:20:15] INFO:     127.0.0.1:49612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:15] INFO:     127.0.0.1:43874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-05 14:20:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-11-05 14:20:16] INFO:     127.0.0.1:41014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:41432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:42496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:45954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2243, token usage: 0.11, #running-req: 1021, #queue-req: 289, 
[2025-11-05 14:20:16] INFO:     127.0.0.1:42472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:42956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:43766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3764, token usage: 0.11, #running-req: 1019, #queue-req: 284, 
[2025-11-05 14:20:16] INFO:     127.0.0.1:41376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:43854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:45314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2169, token usage: 0.11, #running-req: 1021, #queue-req: 281, 
[2025-11-05 14:20:16] INFO:     127.0.0.1:41334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:41556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:42294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:42418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:45454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:45544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:45552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:45934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:46714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6523, token usage: 0.11, #running-req: 1015, #queue-req: 272, 
[2025-11-05 14:20:16] INFO:     127.0.0.1:42100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:46014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:16] INFO:     127.0.0.1:46730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2166, token usage: 0.11, #running-req: 1021, #queue-req: 269, 
[2025-11-05 14:20:17] INFO:     127.0.0.1:42010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:43120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:44138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:44742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:48868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4345, token usage: 0.11, #running-req: 1018, #queue-req: 263, 
[2025-11-05 14:20:17] INFO:     127.0.0.1:43736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:48552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:49794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2871, token usage: 0.11, #running-req: 1020, #queue-req: 259, 
[2025-11-05 14:20:17] INFO:     127.0.0.1:41272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:41470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:44388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:44580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:45322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:45468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:45714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:49058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:49698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7262, token usage: 0.11, #running-req: 1014, #queue-req: 249, 
[2025-11-05 14:20:17] INFO:     127.0.0.1:40972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:42728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:43404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:43466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:43666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:46100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:47008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:49270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5934, token usage: 0.11, #running-req: 1016, #queue-req: 241, 
[2025-11-05 14:20:17] INFO:     127.0.0.1:42178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:17] INFO:     127.0.0.1:45084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:48352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:48654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4367, token usage: 0.11, #running-req: 1018, #queue-req: 235, 
[2025-11-05 14:20:18] INFO:     127.0.0.1:42572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:43962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:44904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:45376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:45410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:45696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:47304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:48426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:48732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:50154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8095, token usage: 0.11, #running-req: 1013, #queue-req: 224, 
[2025-11-05 14:20:18] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:44246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:45154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2894, token usage: 0.12, #running-req: 1020, #queue-req: 220, 
[2025-11-05 14:20:18] INFO:     127.0.0.1:42436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:42454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:43216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:44032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:46614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:49208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4356, token usage: 0.12, #running-req: 1018, #queue-req: 214, 
[2025-11-05 14:20:18] INFO:     127.0.0.1:41502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:42274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:44238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:46490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:47254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:47892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:48922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5018, token usage: 0.12, #running-req: 1017, #queue-req: 207, 
[2025-11-05 14:20:18] INFO:     127.0.0.1:41870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:42234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:45490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:45760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:46208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:49024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:18] INFO:     127.0.0.1:49164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5043, token usage: 0.12, #running-req: 1017, #queue-req: 200, 
[2025-11-05 14:20:19] INFO:     127.0.0.1:40956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:41844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:42714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:42754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:43006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:43220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:44296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:48030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7282, token usage: 0.12, #running-req: 1014, #queue-req: 190, 
[2025-11-05 14:20:19] INFO:     127.0.0.1:41528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:42232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:42902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:43088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:43598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:46806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:47022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:47176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:47404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6568, token usage: 0.12, #running-req: 1015, #queue-req: 181, 
[2025-11-05 14:20:19] INFO:     127.0.0.1:41882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:43362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:43912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:44252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:45582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:45730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:46902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:49342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:49846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7279, token usage: 0.12, #running-req: 1014, #queue-req: 171, 
[2025-11-05 14:20:19] INFO:     127.0.0.1:42380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:43750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:44312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:46874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:47532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4260, token usage: 0.12, #running-req: 1018, #queue-req: 165, 
[2025-11-05 14:20:19] INFO:     127.0.0.1:42852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:45598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:45988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:19] INFO:     127.0.0.1:46976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2872, token usage: 0.12, #running-req: 1020, #queue-req: 161, 
[2025-11-05 14:20:20 TP0] Decode batch, #running-req: 1020, #token: 116405, token usage: 0.12, cuda graph: False, gen throughput (token/s): 6655.53, #queue-req: 161, 
[2025-11-05 14:20:20] INFO:     127.0.0.1:41588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:42732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:45674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:47114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:48204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:48370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:48854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5800, token usage: 0.12, #running-req: 1016, #queue-req: 153, 
[2025-11-05 14:20:20] INFO:     127.0.0.1:41144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:41282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:41584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:42518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:44462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:45756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:45844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:46544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:49348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8881, token usage: 0.12, #running-req: 1012, #queue-req: 141, 
[2025-11-05 14:20:20] INFO:     127.0.0.1:42266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:42654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:43394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:45254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:46696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:49048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:49304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6489, token usage: 0.12, #running-req: 1015, #queue-req: 132, 
[2025-11-05 14:20:20] INFO:     127.0.0.1:42958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:46554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:46640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:47918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:48126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:49580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5839, token usage: 0.12, #running-req: 1016, #queue-req: 124, 
[2025-11-05 14:20:20] INFO:     127.0.0.1:41522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:41804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:42550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:42974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:43954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:43984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:44264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:45018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:46118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:47686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:47958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:48010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:48078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:48690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:49114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:20] INFO:     127.0.0.1:49238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11858, token usage: 0.12, #running-req: 1008, #queue-req: 108, 
[2025-11-05 14:20:21] INFO:     127.0.0.1:41344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:41454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:41714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:42600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:42694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:42710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:42920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:43182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:43936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:44128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:44724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:45570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:46416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:47042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:47272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:49136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:49146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:49600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21 TP0] Prefill batch, #new-seq: 20, #new-token: 20, #cached-token: 14607, token usage: 0.12, #running-req: 1004, #queue-req: 88, 
[2025-11-05 14:20:21] INFO:     127.0.0.1:41922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:43132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:43436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:43810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:48962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3583, token usage: 0.12, #running-req: 1019, #queue-req: 83, 
[2025-11-05 14:20:21] INFO:     127.0.0.1:41196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:41980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:43188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:43496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:44486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:46048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:47196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:48324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:49394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7994, token usage: 0.12, #running-req: 1013, #queue-req: 72, 
[2025-11-05 14:20:21] INFO:     127.0.0.1:42148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:42838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:43512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:43632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:44322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:45870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:46522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:49500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6692, token usage: 0.13, #running-req: 1015, #queue-req: 63, 
[2025-11-05 14:20:21] INFO:     127.0.0.1:42054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:42074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:42880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:44876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:45778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:46804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:47966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:21] INFO:     127.0.0.1:49126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5937, token usage: 0.13, #running-req: 1016, #queue-req: 55, 
[2025-11-05 14:20:22] INFO:     127.0.0.1:42124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:43820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:44716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:45424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:46024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:46040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:46598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:46702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:47938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:48248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:48884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:48950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:49268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:49892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11680, token usage: 0.13, #running-req: 1008, #queue-req: 39, 
[2025-11-05 14:20:22] INFO:     127.0.0.1:41688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:41706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:42060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:42286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:43870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:44054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:44432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:44926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:45422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:46128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:46624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:46960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:48458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:49052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:49924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22 TP0] Prefill batch, #new-seq: 17, #new-token: 17, #cached-token: 12359, token usage: 0.13, #running-req: 1007, #queue-req: 22, 
[2025-11-05 14:20:22] INFO:     127.0.0.1:41762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:41896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:42458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:43672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:45610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:46366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:46460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:47374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:48120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:49644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8794, token usage: 0.13, #running-req: 1012, #queue-req: 10, 
[2025-11-05 14:20:22] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:44518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:47182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:47358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:47544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:47834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:47944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:48306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:50064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:50124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7230, token usage: 0.13, #running-req: 1014, #queue-req: 0, 
[2025-11-05 14:20:22] INFO:     127.0.0.1:41228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:42352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:44160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:44172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:45942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:47162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:48998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:49528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:22] INFO:     127.0.0.1:50120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:41254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:43038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:45506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:45688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:47098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:47146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:43048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:43270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:43330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:45768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:46822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:48840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:41246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:41860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:43616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:45202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:45744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:49322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:49836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:41568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:44086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:45340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:46830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:47062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:47988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:48460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:49784 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP7] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP4] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP3] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP5] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP1] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP0] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP6] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP2] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23] INFO:     127.0.0.1:41422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:41488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:41772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:43558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:44874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:47722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:49566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:43610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:45558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:45902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:46042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:47592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:47724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:48974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:49182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:49250 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP5] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP7] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP3] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP4] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP1] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP0] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP2] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP6] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23] INFO:     127.0.0.1:41964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:43184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:43378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:44502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:46070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:46578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:47980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:49788 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP4] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP3] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP7] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP5] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP1] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP2] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP0] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP6] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23] INFO:     127.0.0.1:42036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:44222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:45048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:45656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:49218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:49230 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP5] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP1] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP4] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP7] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP3] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP0] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP2] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP6] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23] INFO:     127.0.0.1:41396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:42976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:44890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:46158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:47488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:47792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:48442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:48578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:49474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:49486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:23] INFO:     127.0.0.1:49782 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP4] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP3] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP7] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP5] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP1] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP0] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP2] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:23 TP6] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24] INFO:     127.0.0.1:41540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:42786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:43746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:45394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:48640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49314 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP5] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP1] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP7] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP4] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP3] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP0] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP2] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP6] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24] INFO:     127.0.0.1:41036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:41390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:41724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:45632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:48454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:50258 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP3] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP5] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP7] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP1] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP4] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP0] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP2] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP6] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24] INFO:     127.0.0.1:43624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:43842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:45890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:46000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:48172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49246 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP1] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP4] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP5] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP3] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP7] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP0] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP6] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP2] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24] INFO:     127.0.0.1:41064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:41610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:41652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:42126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:46562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:46878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:48558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:48670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:48746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:41486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:41664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:42520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:43028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:43972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:45392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:46428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:48270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP1] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP3] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP5] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP7] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP4] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP0] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP2] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP6] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24] INFO:     127.0.0.1:42152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:42254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:42334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:43284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP1] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP4] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP7] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP3] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP5] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP0] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP2] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP6] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24] INFO:     127.0.0.1:41786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:43224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:43898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:43990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:46022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:46032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:46378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP4] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP3] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP7] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP1] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP5] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP0] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP2] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP6] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24] INFO:     127.0.0.1:41600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:42474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:43116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:45060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:45660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:46400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49514 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP1] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP4] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP3] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP5] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP7] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP0] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP2] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP6] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24] INFO:     127.0.0.1:41002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:41516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:42656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:43546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:45216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:46802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:48874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49864 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP4] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP1] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP7] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP3] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP5] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP0] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP6] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP2] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24] INFO:     127.0.0.1:44446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:44994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:47706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:24] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP1] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP3] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP4] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP7] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP5] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP0] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP2] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:24 TP6] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25] INFO:     127.0.0.1:42144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:48002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:49454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:49624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:50838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:41030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:42224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:42830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:46086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:47940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:47978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:48856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP4] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP1] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP7] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP5] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP3] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP0] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP6] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP2] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25] INFO:     127.0.0.1:41658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:46240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:46254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:41360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:46340 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP1] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP4] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP3] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP7] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP5] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP0] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP2] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP6] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25] INFO:     127.0.0.1:41234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:41834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:42944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:43952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:46144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:46662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:48024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:48508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:49626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:49742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:50046 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP1] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP3] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP4] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP7] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP5] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP0] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP2] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP6] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25] INFO:     127.0.0.1:41758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:42320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:47480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:48568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:50772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25 TP0] Decode batch, #running-req: 754, #token: 105257, token usage: 0.11, cuda graph: False, gen throughput (token/s): 6792.53, #queue-req: 0, 
[2025-11-05 14:20:25] INFO:     127.0.0.1:42582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:46688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:47504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:48124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:51332 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP4] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP1] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP7] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP5] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP3] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP0] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP2] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP6] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP1] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP4] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP5] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP7] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP3] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP0] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP2] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP6] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25] INFO:     127.0.0.1:41490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:41678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:43892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:46356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:47664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:47932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:41092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:41130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:41732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:41818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:42354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:42644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:45850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:48614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP1] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP4] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP3] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP5] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP7] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP0] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP6] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25 TP2] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:25] INFO:     127.0.0.1:41160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:42988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:43210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:43884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:44842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:49232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:25] INFO:     127.0.0.1:50704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:44526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:45648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:45794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:46330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:47624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:47846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:48862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP1] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP4] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP3] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP5] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP7] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP0] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP6] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP2] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26] INFO:     127.0.0.1:42086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:43164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:43240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:45706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:47876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:49428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:49630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:50108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:50476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:51250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:41140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:44708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:46934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:47502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:49332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:41212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:42626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:43462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:45586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:47330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:47586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:47742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:50402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:52448 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP1] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP4] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP3] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP7] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP5] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP0] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP2] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP6] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26] INFO:     127.0.0.1:42350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:42422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:45572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:46376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:48726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:49732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:51442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:41640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:41910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:43198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:43692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:44072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:44710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:46192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:46672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:51024 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP1] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP4] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP3] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP5] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP7] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP0] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP2] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP6] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26] INFO:     127.0.0.1:41738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:43780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:44210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:46476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP1] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP3] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP4] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP7] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP5] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP0] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP2] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP6] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26] INFO:     127.0.0.1:41700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:41996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:42032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:42388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:43104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:45858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:47192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:47738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:48106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:48598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:49036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:50240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:50274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:41318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:43698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:46510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:46990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:51416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:43600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:44832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:45054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:51602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:26] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP1] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP4] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP3] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP7] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP5] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP0] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP2] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:26 TP6] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27] INFO:     127.0.0.1:41440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:42258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:43308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:43584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:47952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:47982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:48086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:49662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:50452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:50906 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP1] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP4] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP5] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP3] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP7] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP0] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP6] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP2] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27] INFO:     127.0.0.1:42108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:42378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:42934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:44918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:46370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:46856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP1] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP4] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP7] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP5] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP3] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP0] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP2] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP6] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27] INFO:     127.0.0.1:41108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:42448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:44168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:46386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:47016 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP1] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP7] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP4] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP3] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP5] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP0] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP6] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP2] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27] INFO:     127.0.0.1:42488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:42638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:43582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:44578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:46862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:50470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:41170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:41792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:42586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:47610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:48412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:48560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:50318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:52476 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP1] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP4] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP7] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP3] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP5] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP0] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP2] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP6] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27] INFO:     127.0.0.1:41308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:41820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:41848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:46446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:46530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:47758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:51830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:52730 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP1] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP4] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP3] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP7] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP5] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP0] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP2] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP6] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27] INFO:     127.0.0.1:41702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:44048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:46556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:47278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:49708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:51064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:51120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:51376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:48192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:49280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:50530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP1] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP4] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP3] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP7] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP5] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP0] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP2] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP6] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27] INFO:     127.0.0.1:43724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:47240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:48796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:49254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP1] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP3] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP4] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP7] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP5] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP0] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP2] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27 TP6] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 14:20:27] INFO:     127.0.0.1:42118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:42190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:42682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:43574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:43622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:43680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:43722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:44632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:44654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:47448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:51014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:52110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:43300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:43664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:43802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:45068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:47038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:47138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:47288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:48498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:48786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:48814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:27] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:41080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:42916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:44006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:45338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:45554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:47316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:48828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:49374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:41032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:43080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:43452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:44330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:44698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:46284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:44540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:42778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:44420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:45964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:48762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:50176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:50326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:41938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:44202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:45284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:47604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:41298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:45032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:45326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:46844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:48678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:45826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:47076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:47560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:44254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:44734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:46430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:46788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:47820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:44588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:45976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:49078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:42678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:43066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:43474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:44126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:47432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:49176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:50488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:41890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:48492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:41742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:42414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:42748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:43648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:46184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:42744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:46242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:46852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:49306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:48898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28] INFO:     127.0.0.1:52020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:28 TP0] Decode batch, #running-req: 368, #token: 63875, token usage: 0.07, cuda graph: True, gen throughput (token/s): 6554.22, #queue-req: 0, 
[2025-11-05 14:20:29] INFO:     127.0.0.1:42870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:47896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:48340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:49350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:49614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:45132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:47232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:48284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:41180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:41566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:46272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:47222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:48712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:42672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:44752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:45186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:47674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:48682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:46424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:46716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:49668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:41536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:43246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:44022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:48906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:41952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:45298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:49422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:42168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:44280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:44668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:46302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:45000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:45086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:41258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:45808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:42756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:42858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:43176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:43564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:49582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:42534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:47260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:47438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:44176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:44954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:46650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:43268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:50980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:42344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:43262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:44564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:44740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:29] INFO:     127.0.0.1:52664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:42502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:48364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:42308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:43020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:46732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:49442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:50966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:45928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:46170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:48294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:48308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:49790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:44900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:50198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:43888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:44100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:50916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:41970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:45210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:41438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:50508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:44148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:46946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:41456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:43062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:43728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:47582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:50804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:43568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:45828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:50686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:41050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:52066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:43340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:44674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:46294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:43370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:30] INFO:     127.0.0.1:46520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31 TP0] Decode batch, #running-req: 157, #token: 34471, token usage: 0.04, cuda graph: True, gen throughput (token/s): 4590.61, #queue-req: 0, 
[2025-11-05 14:20:31] INFO:     127.0.0.1:50650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:42898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:48936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:43350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:42020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:43894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:43994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:47860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:44618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:42456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:46384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:49008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:41824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:49204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:41118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:41878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:45502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:42140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:43836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:45270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:31] INFO:     127.0.0.1:52014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:42048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:50292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:43486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:44644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:51684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:46926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:45480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:51162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:42272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:45014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:50608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:51926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:48156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:50368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:51524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:46772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:44808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:42096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:42938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:52184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:52284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:52268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:52822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:42630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:46266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:47422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32 TP0] Decode batch, #running-req: 55, #token: 15010, token usage: 0.02, cuda graph: True, gen throughput (token/s): 2429.78, #queue-req: 0, 
[2025-11-05 14:20:32] INFO:     127.0.0.1:48098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:52124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:52354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:51050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:41410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:45918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:51836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:52576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:52782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:32] INFO:     127.0.0.1:51598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:50788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:48500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:50386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:47540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:44688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:49856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:41624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:52688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:49956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:50568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:42522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:43140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:52338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:51386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33] INFO:     127.0.0.1:51454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:33 TP0] Decode batch, #running-req: 19, #token: 6439, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1133.90, #queue-req: 0, 
[2025-11-05 14:20:33] INFO:     127.0.0.1:51110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34] INFO:     127.0.0.1:43926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34] INFO:     127.0.0.1:52404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34] INFO:     127.0.0.1:48050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34] INFO:     127.0.0.1:50618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34] INFO:     127.0.0.1:51904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34] INFO:     127.0.0.1:48408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34] INFO:     127.0.0.1:51436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34] INFO:     127.0.0.1:50042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:34 TP0] Decode batch, #running-req: 4, #token: 2052, token usage: 0.00, cuda graph: True, gen throughput (token/s): 449.31, #queue-req: 0, 
[2025-11-05 14:20:35] INFO:     127.0.0.1:48380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:35] INFO:     127.0.0.1:51158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:35] INFO:     127.0.0.1:51576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:35 TP0] Decode batch, #running-req: 1, #token: 1092, token usage: 0.00, cuda graph: True, gen throughput (token/s): 111.02, #queue-req: 0, 
[2025-11-05 14:20:36] INFO:     127.0.0.1:42800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:47] INFO:     127.0.0.1:37010 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-05 14:20:53] INFO:     127.0.0.1:37022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:53 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:20:54 TP0] Decode batch, #running-req: 1, #token: 3215, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2.15, #queue-req: 0, 
[2025-11-05 14:20:55] INFO:     127.0.0.1:37538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:20:55] INFO:     127.0.0.1:37552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 6, 
[2025-11-05 14:20:55] INFO:     127.0.0.1:37652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.02, #running-req: 6, #queue-req: 19, 
[2025-11-05 14:20:55] INFO:     127.0.0.1:37776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:55] INFO:     127.0.0.1:37808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:37988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56] INFO:     127.0.0.1:38730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:20:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-11-05 14:20:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-11-05 14:20:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-11-05 14:20:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.09, #running-req: 26, #queue-req: 97, 
[2025-11-05 14:21:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-11-05 14:21:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-11-05 14:21:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-11-05 14:21:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-11-05 14:21:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-11-05 14:21:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-11-05 14:21:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.20, #running-req: 61, #queue-req: 62, 
[2025-11-05 14:21:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.22, #running-req: 66, #queue-req: 57, 
[2025-11-05 14:21:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.23, #running-req: 71, #queue-req: 52, 
[2025-11-05 14:21:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.25, #running-req: 76, #queue-req: 47, 
[2025-11-05 14:21:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.27, #running-req: 81, #queue-req: 42, 
[2025-11-05 14:21:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.28, #running-req: 86, #queue-req: 37, 
[2025-11-05 14:21:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.30, #running-req: 91, #queue-req: 32, 
[2025-11-05 14:21:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.32, #running-req: 96, #queue-req: 27, 
[2025-11-05 14:21:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.33, #running-req: 101, #queue-req: 22, 
[2025-11-05 14:21:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.35, #running-req: 106, #queue-req: 17, 
[2025-11-05 14:21:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.37, #running-req: 111, #queue-req: 12, 
[2025-11-05 14:21:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.38, #running-req: 116, #queue-req: 7, 
[2025-11-05 14:21:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.40, #running-req: 121, #queue-req: 2, 
[2025-11-05 14:21:15 TP0] Prefill batch, #new-seq: 2, #new-token: 6394, #cached-token: 8, token usage: 0.41, #running-req: 126, #queue-req: 0, 
[2025-11-05 14:21:17 TP0] Decode batch, #running-req: 128, #token: 412191, token usage: 0.42, cuda graph: True, gen throughput (token/s): 116.61, #queue-req: 0, 
[2025-11-05 14:21:19 TP0] Decode batch, #running-req: 128, #token: 417311, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2540.63, #queue-req: 0, 
[2025-11-05 14:21:21 TP0] Decode batch, #running-req: 128, #token: 422431, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2515.07, #queue-req: 0, 
[2025-11-05 14:21:23 TP0] Decode batch, #running-req: 128, #token: 427551, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2501.48, #queue-req: 0, 
[2025-11-05 14:21:25 TP0] Decode batch, #running-req: 128, #token: 432671, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2491.00, #queue-req: 0, 
[2025-11-05 14:21:27 TP0] Decode batch, #running-req: 128, #token: 437791, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2482.91, #queue-req: 0, 
[2025-11-05 14:21:29 TP0] Decode batch, #running-req: 128, #token: 442911, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2475.73, #queue-req: 0, 
[2025-11-05 14:21:31 TP0] Decode batch, #running-req: 128, #token: 448031, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2470.05, #queue-req: 0, 
[2025-11-05 14:21:33 TP0] Decode batch, #running-req: 128, #token: 453151, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2462.56, #queue-req: 0, 
[2025-11-05 14:21:36 TP0] Decode batch, #running-req: 128, #token: 458271, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2455.20, #queue-req: 0, 
[2025-11-05 14:21:38 TP0] Decode batch, #running-req: 128, #token: 463391, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2450.52, #queue-req: 0, 
[2025-11-05 14:21:40 TP0] Decode batch, #running-req: 128, #token: 468511, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2449.16, #queue-req: 0, 
[2025-11-05 14:21:42 TP0] Decode batch, #running-req: 128, #token: 473631, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2444.35, #queue-req: 0, 
[2025-11-05 14:21:44 TP0] Decode batch, #running-req: 128, #token: 478751, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2435.19, #queue-req: 0, 
[2025-11-05 14:21:46 TP0] Decode batch, #running-req: 128, #token: 483871, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2430.32, #queue-req: 0, 
[2025-11-05 14:21:48 TP0] Decode batch, #running-req: 128, #token: 488991, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2423.68, #queue-req: 0, 
[2025-11-05 14:21:50 TP0] Decode batch, #running-req: 128, #token: 494111, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2418.77, #queue-req: 0, 
[2025-11-05 14:21:52 TP0] Decode batch, #running-req: 128, #token: 499231, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2414.37, #queue-req: 0, 
[2025-11-05 14:21:55 TP0] Decode batch, #running-req: 128, #token: 504351, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2412.72, #queue-req: 0, 
[2025-11-05 14:21:57 TP0] Decode batch, #running-req: 128, #token: 509471, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2407.73, #queue-req: 0, 
[2025-11-05 14:21:58] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58 TP0] Prefill batch, #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:21:58] INFO:     127.0.0.1:59792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 7, 
[2025-11-05 14:21:58] INFO:     127.0.0.1:59892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:59996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 6, #queue-req: 38, 
[2025-11-05 14:21:58] INFO:     127.0.0.1:60186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:58] INFO:     127.0.0.1:60816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:59] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:59] INFO:     127.0.0.1:60834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:59] INFO:     127.0.0.1:60842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:59] INFO:     127.0.0.1:60846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:59] INFO:     127.0.0.1:60852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:59] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:21:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-11-05 14:22:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-11-05 14:22:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-11-05 14:22:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 26, #queue-req: 97, 
[2025-11-05 14:22:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-11-05 14:22:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-11-05 14:22:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-11-05 14:22:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-11-05 14:22:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-11-05 14:22:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-11-05 14:22:07 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.20, #running-req: 61, #queue-req: 61, 
[2025-11-05 14:22:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.22, #running-req: 67, #queue-req: 56, 
[2025-11-05 14:22:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15977, #cached-token: 28, token usage: 0.23, #running-req: 72, #queue-req: 51, 
[2025-11-05 14:22:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.25, #running-req: 77, #queue-req: 46, 
[2025-11-05 14:22:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.27, #running-req: 82, #queue-req: 41, 
[2025-11-05 14:22:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.28, #running-req: 87, #queue-req: 36, 
[2025-11-05 14:22:12 TP0] Prefill batch, #new-seq: 6, #new-token: 15994, #cached-token: 3212, token usage: 0.30, #running-req: 92, #queue-req: 30, 
[2025-11-05 14:22:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.32, #running-req: 98, #queue-req: 25, 
[2025-11-05 14:22:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.34, #running-req: 103, #queue-req: 20, 
[2025-11-05 14:22:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.35, #running-req: 108, #queue-req: 15, 
[2025-11-05 14:22:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.37, #running-req: 113, #queue-req: 10, 
[2025-11-05 14:22:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.39, #running-req: 118, #queue-req: 5, 
[2025-11-05 14:22:17 TP0] Prefill batch, #new-seq: 5, #new-token: 12792, #cached-token: 3213, token usage: 0.40, #running-req: 123, #queue-req: 0, 
[2025-11-05 14:22:19 TP0] Decode batch, #running-req: 128, #token: 405789, token usage: 0.42, cuda graph: True, gen throughput (token/s): 224.69, #queue-req: 0, 
[2025-11-05 14:22:21 TP0] Decode batch, #running-req: 128, #token: 410909, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2517.00, #queue-req: 0, 
[2025-11-05 14:22:24 TP0] Decode batch, #running-req: 128, #token: 416029, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2497.94, #queue-req: 0, 
[2025-11-05 14:22:26 TP0] Decode batch, #running-req: 128, #token: 421149, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2487.35, #queue-req: 0, 
[2025-11-05 14:22:28 TP0] Decode batch, #running-req: 128, #token: 426269, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2479.95, #queue-req: 0, 
[2025-11-05 14:22:30 TP0] Decode batch, #running-req: 128, #token: 431389, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2472.21, #queue-req: 0, 
[2025-11-05 14:22:32 TP0] Decode batch, #running-req: 128, #token: 436509, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2464.76, #queue-req: 0, 
[2025-11-05 14:22:34 TP0] Decode batch, #running-req: 128, #token: 441629, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2459.62, #queue-req: 0, 
[2025-11-05 14:22:36 TP0] Decode batch, #running-req: 128, #token: 446749, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2455.27, #queue-req: 0, 
[2025-11-05 14:22:38 TP0] Decode batch, #running-req: 128, #token: 451869, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2450.00, #queue-req: 0, 
[2025-11-05 14:22:40 TP0] Decode batch, #running-req: 128, #token: 456989, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2446.40, #queue-req: 0, 
[2025-11-05 14:22:42 TP0] Decode batch, #running-req: 128, #token: 462109, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2443.71, #queue-req: 0, 
[2025-11-05 14:22:44 TP0] Decode batch, #running-req: 128, #token: 467229, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2438.87, #queue-req: 0, 
[2025-11-05 14:22:46 TP0] Decode batch, #running-req: 128, #token: 472349, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2433.27, #queue-req: 0, 
[2025-11-05 14:22:49 TP0] Decode batch, #running-req: 128, #token: 477469, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2429.46, #queue-req: 0, 
[2025-11-05 14:22:51 TP0] Decode batch, #running-req: 128, #token: 482589, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2422.29, #queue-req: 0, 
[2025-11-05 14:22:53 TP0] Decode batch, #running-req: 128, #token: 487709, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2419.33, #queue-req: 0, 
[2025-11-05 14:22:55 TP0] Decode batch, #running-req: 128, #token: 492829, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2416.15, #queue-req: 0, 
[2025-11-05 14:22:57 TP0] Decode batch, #running-req: 128, #token: 497949, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2412.93, #queue-req: 0, 
[2025-11-05 14:22:59 TP0] Decode batch, #running-req: 128, #token: 503069, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2408.64, #queue-req: 0, 
[2025-11-05 14:23:00] INFO:     127.0.0.1:36954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:00 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:23:01] INFO:     127.0.0.1:36956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:36964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:36978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:23:01] INFO:     127.0.0.1:36982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:36990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.01, #running-req: 2, #queue-req: 15, 
[2025-11-05 14:23:01] INFO:     127.0.0.1:37150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.02, #running-req: 7, #queue-req: 43, 
[2025-11-05 14:23:01] INFO:     127.0.0.1:37436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:37998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:38006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:38016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:38018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:38022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:38024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:38034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:38036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:38046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:38054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:38072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:01] INFO:     127.0.0.1:38088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:23:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.04, #running-req: 12, #queue-req: 111, 
[2025-11-05 14:23:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.06, #running-req: 17, #queue-req: 106, 
[2025-11-05 14:23:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.07, #running-req: 22, #queue-req: 101, 
[2025-11-05 14:23:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.09, #running-req: 27, #queue-req: 96, 
[2025-11-05 14:23:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.11, #running-req: 32, #queue-req: 91, 
[2025-11-05 14:23:06 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.13, #running-req: 37, #queue-req: 85, 
[2025-11-05 14:23:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.14, #running-req: 43, #queue-req: 80, 
[2025-11-05 14:23:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.16, #running-req: 48, #queue-req: 75, 
[2025-11-05 14:23:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15978, #cached-token: 27, token usage: 0.17, #running-req: 53, #queue-req: 70, 
[2025-11-05 14:23:09 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.19, #running-req: 58, #queue-req: 64, 
[2025-11-05 14:23:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.21, #running-req: 64, #queue-req: 59, 
[2025-11-05 14:23:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.23, #running-req: 69, #queue-req: 54, 
[2025-11-05 14:23:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.24, #running-req: 74, #queue-req: 49, 
[2025-11-05 14:23:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.26, #running-req: 79, #queue-req: 44, 
[2025-11-05 14:23:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.28, #running-req: 84, #queue-req: 39, 
[2025-11-05 14:23:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.29, #running-req: 89, #queue-req: 34, 
[2025-11-05 14:23:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.31, #running-req: 94, #queue-req: 29, 
[2025-11-05 14:23:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.33, #running-req: 99, #queue-req: 24, 
[2025-11-05 14:23:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.34, #running-req: 104, #queue-req: 19, 
[2025-11-05 14:23:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.36, #running-req: 109, #queue-req: 14, 
[2025-11-05 14:23:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.38, #running-req: 114, #queue-req: 9, 
[2025-11-05 14:23:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.39, #running-req: 119, #queue-req: 4, 
[2025-11-05 14:23:20 TP0] Prefill batch, #new-seq: 4, #new-token: 12795, #cached-token: 9, token usage: 0.41, #running-req: 124, #queue-req: 0, 
[2025-11-05 14:23:22 TP0] Decode batch, #running-req: 128, #token: 411299, token usage: 0.42, cuda graph: True, gen throughput (token/s): 187.15, #queue-req: 0, 
[2025-11-05 14:23:24 TP0] Decode batch, #running-req: 128, #token: 416419, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2526.34, #queue-req: 0, 
[2025-11-05 14:23:26 TP0] Decode batch, #running-req: 128, #token: 421539, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2499.45, #queue-req: 0, 
[2025-11-05 14:23:28 TP0] Decode batch, #running-req: 128, #token: 426659, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2486.93, #queue-req: 0, 
[2025-11-05 14:23:30 TP0] Decode batch, #running-req: 128, #token: 431779, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2479.38, #queue-req: 0, 
[2025-11-05 14:23:32 TP0] Decode batch, #running-req: 128, #token: 436899, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2472.58, #queue-req: 0, 
[2025-11-05 14:23:34 TP0] Decode batch, #running-req: 128, #token: 442019, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2466.39, #queue-req: 0, 
[2025-11-05 14:23:36 TP0] Decode batch, #running-req: 128, #token: 447139, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2461.69, #queue-req: 0, 
[2025-11-05 14:23:38 TP0] Decode batch, #running-req: 128, #token: 452259, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2457.81, #queue-req: 0, 
[2025-11-05 14:23:40 TP0] Decode batch, #running-req: 128, #token: 457379, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2450.63, #queue-req: 0, 
[2025-11-05 14:23:42 TP0] Decode batch, #running-req: 128, #token: 462499, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2447.90, #queue-req: 0, 
[2025-11-05 14:23:45 TP0] Decode batch, #running-req: 128, #token: 467619, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2446.43, #queue-req: 0, 
[2025-11-05 14:23:47 TP0] Decode batch, #running-req: 128, #token: 472739, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2443.07, #queue-req: 0, 
[2025-11-05 14:23:49 TP0] Decode batch, #running-req: 128, #token: 477859, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2438.64, #queue-req: 0, 
[2025-11-05 14:23:51 TP0] Decode batch, #running-req: 128, #token: 482979, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2432.55, #queue-req: 0, 
[2025-11-05 14:23:53 TP0] Decode batch, #running-req: 128, #token: 488099, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2427.77, #queue-req: 0, 
[2025-11-05 14:23:55 TP0] Decode batch, #running-req: 128, #token: 493219, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2423.15, #queue-req: 0, 
[2025-11-05 14:23:57 TP0] Decode batch, #running-req: 128, #token: 498339, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2418.37, #queue-req: 0, 
[2025-11-05 14:23:59 TP0] Decode batch, #running-req: 128, #token: 503459, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2415.70, #queue-req: 0, 
[2025-11-05 14:24:01 TP0] Decode batch, #running-req: 128, #token: 508579, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2410.94, #queue-req: 0, 
[2025-11-05 14:24:02] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:02 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.52, #running-req: 127, #queue-req: 0, 
[2025-11-05 14:24:03] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:24:03] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.01, #running-req: 2, #queue-req: 11, 
[2025-11-05 14:24:03] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 7, #queue-req: 39, 
[2025-11-05 14:24:03] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:03] INFO:     127.0.0.1:55966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:55998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:24:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 12, #queue-req: 99, 
[2025-11-05 14:24:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.06, #running-req: 17, #queue-req: 94, 
[2025-11-05 14:24:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 22, #queue-req: 89, 
[2025-11-05 14:24:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 27, #queue-req: 84, 
[2025-11-05 14:24:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.11, #running-req: 32, #queue-req: 79, 
[2025-11-05 14:24:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.12, #running-req: 37, #queue-req: 74, 
[2025-11-05 14:24:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.14, #running-req: 42, #queue-req: 69, 
[2025-11-05 14:24:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.15, #running-req: 47, #queue-req: 64, 
[2025-11-05 14:24:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.17, #running-req: 52, #queue-req: 59, 
[2025-11-05 14:24:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.19, #running-req: 57, #queue-req: 54, 
[2025-11-05 14:24:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.20, #running-req: 62, #queue-req: 49, 
[2025-11-05 14:24:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.22, #running-req: 67, #queue-req: 44, 
[2025-11-05 14:24:14 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.24, #running-req: 72, #queue-req: 38, 
[2025-11-05 14:24:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.26, #running-req: 78, #queue-req: 33, 
[2025-11-05 14:24:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.27, #running-req: 83, #queue-req: 28, 
[2025-11-05 14:24:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.29, #running-req: 88, #queue-req: 23, 
[2025-11-05 14:24:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.31, #running-req: 93, #queue-req: 18, 
[2025-11-05 14:24:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.32, #running-req: 98, #queue-req: 13, 
[2025-11-05 14:24:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.34, #running-req: 103, #queue-req: 8, 
[2025-11-05 14:24:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.36, #running-req: 108, #queue-req: 3, 
[2025-11-05 14:24:21 TP0] Prefill batch, #new-seq: 3, #new-token: 9590, #cached-token: 13, token usage: 0.37, #running-req: 113, #queue-req: 0, 
[2025-11-05 14:24:22 TP0] Decode batch, #running-req: 116, #token: 372284, token usage: 0.38, cuda graph: True, gen throughput (token/s): 214.44, #queue-req: 0, 
[2025-11-05 14:24:24 TP0] Decode batch, #running-req: 116, #token: 376924, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2346.24, #queue-req: 0, 
[2025-11-05 14:24:26 TP0] Decode batch, #running-req: 116, #token: 381564, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2326.51, #queue-req: 0, 
[2025-11-05 14:24:28 TP0] Decode batch, #running-req: 116, #token: 386204, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2315.43, #queue-req: 0, 
[2025-11-05 14:24:30 TP0] Decode batch, #running-req: 116, #token: 390844, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2306.25, #queue-req: 0, 
[2025-11-05 14:24:32 TP0] Decode batch, #running-req: 116, #token: 395484, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2298.32, #queue-req: 0, 
[2025-11-05 14:24:34 TP0] Decode batch, #running-req: 116, #token: 400124, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2294.90, #queue-req: 0, 
[2025-11-05 14:24:36 TP0] Decode batch, #running-req: 116, #token: 404764, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2288.67, #queue-req: 0, 
[2025-11-05 14:24:38 TP0] Decode batch, #running-req: 116, #token: 409404, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2283.84, #queue-req: 0, 
[2025-11-05 14:24:40 TP0] Decode batch, #running-req: 116, #token: 414044, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2281.22, #queue-req: 0, 
[2025-11-05 14:24:43 TP0] Decode batch, #running-req: 116, #token: 418684, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2277.86, #queue-req: 0, 
[2025-11-05 14:24:45 TP0] Decode batch, #running-req: 116, #token: 423324, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2276.91, #queue-req: 0, 
[2025-11-05 14:24:47 TP0] Decode batch, #running-req: 116, #token: 427964, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2273.98, #queue-req: 0, 
[2025-11-05 14:24:49 TP0] Decode batch, #running-req: 116, #token: 432604, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2270.38, #queue-req: 0, 
[2025-11-05 14:24:51 TP0] Decode batch, #running-req: 116, #token: 437244, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2265.66, #queue-req: 0, 
[2025-11-05 14:24:53 TP0] Decode batch, #running-req: 116, #token: 441884, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2259.69, #queue-req: 0, 
[2025-11-05 14:24:55 TP0] Decode batch, #running-req: 116, #token: 446524, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2254.75, #queue-req: 0, 
[2025-11-05 14:24:57 TP0] Decode batch, #running-req: 116, #token: 451164, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2252.67, #queue-req: 0, 
[2025-11-05 14:24:59 TP0] Decode batch, #running-req: 116, #token: 455804, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2247.57, #queue-req: 0, 
[2025-11-05 14:25:01 TP0] Decode batch, #running-req: 116, #token: 460444, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2246.87, #queue-req: 0, 
[2025-11-05 14:25:03] INFO:     127.0.0.1:60648 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-05 14:25:04] INFO:     127.0.0.1:60654 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-05 14:25:20] INFO:     127.0.0.1:59660 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-05 14:25:26] INFO:     127.0.0.1:59060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:26 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:25:27 TP0] Decode batch, #running-req: 1, #token: 3212, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.44, #queue-req: 0, 
[2025-11-05 14:25:28] INFO:     127.0.0.1:59068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:25:28] INFO:     127.0.0.1:59092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 7, 
[2025-11-05 14:25:28] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:28] INFO:     127.0.0.1:59238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 20, 
[2025-11-05 14:25:29] INFO:     127.0.0.1:59348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29] INFO:     127.0.0.1:59590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:25:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-05 14:25:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-05 14:25:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-05 14:25:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-05 14:25:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-05 14:25:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-11-05 14:25:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-11-05 14:25:35 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-11-05 14:25:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-11-05 14:25:37 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.19, #running-req: 56, #queue-req: 2, 
[2025-11-05 14:25:37 TP0] Prefill batch, #new-seq: 2, #new-token: 6398, #cached-token: 4, token usage: 0.20, #running-req: 62, #queue-req: 0, 
[2025-11-05 14:25:39 TP0] Decode batch, #running-req: 64, #token: 205928, token usage: 0.21, cuda graph: True, gen throughput (token/s): 94.63, #queue-req: 0, 
[2025-11-05 14:25:41 TP0] Decode batch, #running-req: 64, #token: 208488, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1540.57, #queue-req: 0, 
[2025-11-05 14:25:43 TP0] Decode batch, #running-req: 64, #token: 211048, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1521.04, #queue-req: 0, 
[2025-11-05 14:25:44 TP0] Decode batch, #running-req: 64, #token: 213608, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1513.08, #queue-req: 0, 
[2025-11-05 14:25:46 TP0] Decode batch, #running-req: 64, #token: 216168, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1505.96, #queue-req: 0, 
[2025-11-05 14:25:48 TP0] Decode batch, #running-req: 64, #token: 218728, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1503.76, #queue-req: 0, 
[2025-11-05 14:25:49 TP0] Decode batch, #running-req: 64, #token: 221288, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1497.35, #queue-req: 0, 
[2025-11-05 14:25:51 TP0] Decode batch, #running-req: 64, #token: 223848, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1494.41, #queue-req: 0, 
[2025-11-05 14:25:53 TP0] Decode batch, #running-req: 64, #token: 226408, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1492.63, #queue-req: 0, 
[2025-11-05 14:25:55 TP0] Decode batch, #running-req: 64, #token: 228968, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1486.22, #queue-req: 0, 
[2025-11-05 14:25:56 TP0] Decode batch, #running-req: 64, #token: 231528, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1487.63, #queue-req: 0, 
[2025-11-05 14:25:58 TP0] Decode batch, #running-req: 64, #token: 234088, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1487.97, #queue-req: 0, 
[2025-11-05 14:26:00 TP0] Decode batch, #running-req: 64, #token: 236648, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1488.37, #queue-req: 0, 
[2025-11-05 14:26:01 TP0] Decode batch, #running-req: 64, #token: 239208, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1486.12, #queue-req: 0, 
[2025-11-05 14:26:03 TP0] Decode batch, #running-req: 64, #token: 241768, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1487.00, #queue-req: 0, 
[2025-11-05 14:26:05 TP0] Decode batch, #running-req: 64, #token: 244328, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1482.84, #queue-req: 0, 
[2025-11-05 14:26:07 TP0] Decode batch, #running-req: 64, #token: 246888, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1482.09, #queue-req: 0, 
[2025-11-05 14:26:08 TP0] Decode batch, #running-req: 64, #token: 249448, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1477.99, #queue-req: 0, 
[2025-11-05 14:26:10 TP0] Decode batch, #running-req: 64, #token: 252008, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1477.82, #queue-req: 0, 
[2025-11-05 14:26:12 TP0] Decode batch, #running-req: 64, #token: 254568, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1474.97, #queue-req: 0, 
[2025-11-05 14:26:13] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13 TP0] Prefill batch, #new-seq: 1, #new-token: 3195, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:26:13] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-05 14:26:13] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 41, 
[2025-11-05 14:26:13] INFO:     127.0.0.1:53456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:13] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-05 14:26:15 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.06, #running-req: 16, #queue-req: 42, 
[2025-11-05 14:26:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 22, #queue-req: 37, 
[2025-11-05 14:26:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.09, #running-req: 27, #queue-req: 32, 
[2025-11-05 14:26:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.11, #running-req: 32, #queue-req: 27, 
[2025-11-05 14:26:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.12, #running-req: 37, #queue-req: 22, 
[2025-11-05 14:26:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.14, #running-req: 42, #queue-req: 17, 
[2025-11-05 14:26:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 47, #queue-req: 12, 
[2025-11-05 14:26:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.17, #running-req: 52, #queue-req: 7, 
[2025-11-05 14:26:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.19, #running-req: 57, #queue-req: 2, 
[2025-11-05 14:26:22 TP0] Prefill batch, #new-seq: 2, #new-token: 6393, #cached-token: 9, token usage: 0.20, #running-req: 62, #queue-req: 0, 
[2025-11-05 14:26:24 TP0] Decode batch, #running-req: 64, #token: 205911, token usage: 0.21, cuda graph: True, gen throughput (token/s): 210.79, #queue-req: 0, 
[2025-11-05 14:26:26 TP0] Decode batch, #running-req: 64, #token: 208471, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1536.30, #queue-req: 0, 
[2025-11-05 14:26:27 TP0] Decode batch, #running-req: 64, #token: 211031, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1522.00, #queue-req: 0, 
[2025-11-05 14:26:29 TP0] Decode batch, #running-req: 64, #token: 213591, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1514.97, #queue-req: 0, 
[2025-11-05 14:26:31 TP0] Decode batch, #running-req: 64, #token: 216151, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1511.01, #queue-req: 0, 
[2025-11-05 14:26:32 TP0] Decode batch, #running-req: 64, #token: 218711, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1505.31, #queue-req: 0, 
[2025-11-05 14:26:34 TP0] Decode batch, #running-req: 64, #token: 221271, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1499.24, #queue-req: 0, 
[2025-11-05 14:26:36 TP0] Decode batch, #running-req: 64, #token: 223831, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1496.02, #queue-req: 0, 
[2025-11-05 14:26:38 TP0] Decode batch, #running-req: 64, #token: 226391, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1495.41, #queue-req: 0, 
[2025-11-05 14:26:39 TP0] Decode batch, #running-req: 64, #token: 228951, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1492.88, #queue-req: 0, 
[2025-11-05 14:26:41 TP0] Decode batch, #running-req: 64, #token: 231511, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1490.21, #queue-req: 0, 
[2025-11-05 14:26:43 TP0] Decode batch, #running-req: 64, #token: 234071, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1491.37, #queue-req: 0, 
[2025-11-05 14:26:44 TP0] Decode batch, #running-req: 64, #token: 236631, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1491.84, #queue-req: 0, 
[2025-11-05 14:26:46 TP0] Decode batch, #running-req: 64, #token: 239191, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1489.74, #queue-req: 0, 
[2025-11-05 14:26:48 TP0] Decode batch, #running-req: 64, #token: 241751, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1488.51, #queue-req: 0, 
[2025-11-05 14:26:50 TP0] Decode batch, #running-req: 64, #token: 244311, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1481.75, #queue-req: 0, 
[2025-11-05 14:26:51 TP0] Decode batch, #running-req: 64, #token: 246871, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1480.48, #queue-req: 0, 
[2025-11-05 14:26:53 TP0] Decode batch, #running-req: 64, #token: 249431, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1477.99, #queue-req: 0, 
[2025-11-05 14:26:55 TP0] Decode batch, #running-req: 64, #token: 251991, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1478.97, #queue-req: 0, 
[2025-11-05 14:26:56 TP0] Decode batch, #running-req: 64, #token: 254551, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1476.87, #queue-req: 0, 
[2025-11-05 14:26:57] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:57 TP0] Prefill batch, #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:26:57] INFO:     127.0.0.1:57576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:57] INFO:     127.0.0.1:57592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:57] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:57] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-11-05 14:26:58] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 39, 
[2025-11-05 14:26:58] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:58] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:26:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-05 14:26:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-05 14:27:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-05 14:27:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-05 14:27:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-05 14:27:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-11-05 14:27:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-11-05 14:27:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-11-05 14:27:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-11-05 14:27:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-11-05 14:27:07 TP0] Prefill batch, #new-seq: 3, #new-token: 9592, #cached-token: 11, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-11-05 14:27:09 TP0] Decode batch, #running-req: 64, #token: 205925, token usage: 0.21, cuda graph: True, gen throughput (token/s): 209.01, #queue-req: 0, 
[2025-11-05 14:27:10 TP0] Decode batch, #running-req: 64, #token: 208485, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1533.42, #queue-req: 0, 
[2025-11-05 14:27:12 TP0] Decode batch, #running-req: 64, #token: 211045, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1516.86, #queue-req: 0, 
[2025-11-05 14:27:14 TP0] Decode batch, #running-req: 64, #token: 213605, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1506.59, #queue-req: 0, 
[2025-11-05 14:27:16 TP0] Decode batch, #running-req: 64, #token: 216165, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1499.84, #queue-req: 0, 
[2025-11-05 14:27:17 TP0] Decode batch, #running-req: 64, #token: 218725, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1494.18, #queue-req: 0, 
[2025-11-05 14:27:19 TP0] Decode batch, #running-req: 64, #token: 221285, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1489.90, #queue-req: 0, 
[2025-11-05 14:27:21 TP0] Decode batch, #running-req: 64, #token: 223845, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1485.18, #queue-req: 0, 
[2025-11-05 14:27:22 TP0] Decode batch, #running-req: 64, #token: 226405, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1482.27, #queue-req: 0, 
[2025-11-05 14:27:24 TP0] Decode batch, #running-req: 64, #token: 228965, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1482.75, #queue-req: 0, 
[2025-11-05 14:27:26 TP0] Decode batch, #running-req: 64, #token: 231525, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1480.69, #queue-req: 0, 
[2025-11-05 14:27:28 TP0] Decode batch, #running-req: 64, #token: 234085, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1485.44, #queue-req: 0, 
[2025-11-05 14:27:29 TP0] Decode batch, #running-req: 64, #token: 236645, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1486.70, #queue-req: 0, 
[2025-11-05 14:27:31 TP0] Decode batch, #running-req: 64, #token: 239205, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1484.37, #queue-req: 0, 
[2025-11-05 14:27:33 TP0] Decode batch, #running-req: 64, #token: 241765, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1485.02, #queue-req: 0, 
[2025-11-05 14:27:34 TP0] Decode batch, #running-req: 64, #token: 244325, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1482.16, #queue-req: 0, 
[2025-11-05 14:27:36 TP0] Decode batch, #running-req: 64, #token: 246885, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1478.67, #queue-req: 0, 
[2025-11-05 14:27:38 TP0] Decode batch, #running-req: 64, #token: 249445, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1472.56, #queue-req: 0, 
[2025-11-05 14:27:40 TP0] Decode batch, #running-req: 64, #token: 252005, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1472.27, #queue-req: 0, 
[2025-11-05 14:27:41 TP0] Decode batch, #running-req: 64, #token: 254565, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1473.65, #queue-req: 0, 
[2025-11-05 14:27:42] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:27:42] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:42 TP0] Prefill batch, #new-seq: 6, #new-token: 15987, #cached-token: 3219, token usage: 0.01, #running-req: 1, #queue-req: 8, 
[2025-11-05 14:27:42] INFO:     127.0.0.1:60206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.02, #running-req: 7, #queue-req: 37, 
[2025-11-05 14:27:43] INFO:     127.0.0.1:60488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:43] INFO:     127.0.0.1:60600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:27:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.04, #running-req: 12, #queue-req: 47, 
[2025-11-05 14:27:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.06, #running-req: 17, #queue-req: 42, 
[2025-11-05 14:27:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 22, #queue-req: 37, 
[2025-11-05 14:27:46 TP0] Prefill batch, #new-seq: 7, #new-token: 15993, #cached-token: 6414, token usage: 0.10, #running-req: 27, #queue-req: 30, 
[2025-11-05 14:27:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.11, #running-req: 34, #queue-req: 25, 
[2025-11-05 14:27:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.13, #running-req: 39, #queue-req: 20, 
[2025-11-05 14:27:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.14, #running-req: 44, #queue-req: 15, 
[2025-11-05 14:27:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.16, #running-req: 49, #queue-req: 10, 
[2025-11-05 14:27:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.18, #running-req: 54, #queue-req: 5, 
[2025-11-05 14:27:51 TP0] Prefill batch, #new-seq: 5, #new-token: 12792, #cached-token: 3213, token usage: 0.19, #running-req: 59, #queue-req: 0, 
[2025-11-05 14:27:53 TP0] Decode batch, #running-req: 64, #token: 202714, token usage: 0.21, cuda graph: True, gen throughput (token/s): 220.42, #queue-req: 0, 
[2025-11-05 14:27:55 TP0] Decode batch, #running-req: 64, #token: 205274, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1523.38, #queue-req: 0, 
[2025-11-05 14:27:56 TP0] Decode batch, #running-req: 64, #token: 207834, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1508.05, #queue-req: 0, 
[2025-11-05 14:27:58 TP0] Decode batch, #running-req: 64, #token: 210394, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1504.38, #queue-req: 0, 
[2025-11-05 14:28:00 TP0] Decode batch, #running-req: 64, #token: 212954, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1503.91, #queue-req: 0, 
[2025-11-05 14:28:02 TP0] Decode batch, #running-req: 64, #token: 215514, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1503.37, #queue-req: 0, 
[2025-11-05 14:28:03 TP0] Decode batch, #running-req: 64, #token: 218074, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1502.91, #queue-req: 0, 
[2025-11-05 14:28:05 TP0] Decode batch, #running-req: 64, #token: 220634, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1499.18, #queue-req: 0, 
[2025-11-05 14:28:07 TP0] Decode batch, #running-req: 64, #token: 223194, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1496.45, #queue-req: 0, 
[2025-11-05 14:28:08 TP0] Decode batch, #running-req: 64, #token: 225754, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1495.33, #queue-req: 0, 
[2025-11-05 14:28:10 TP0] Decode batch, #running-req: 64, #token: 228314, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1491.51, #queue-req: 0, 
[2025-11-05 14:28:12 TP0] Decode batch, #running-req: 64, #token: 230874, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1492.51, #queue-req: 0, 
[2025-11-05 14:28:13 TP0] Decode batch, #running-req: 64, #token: 233434, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1492.56, #queue-req: 0, 
[2025-11-05 14:28:15 TP0] Decode batch, #running-req: 64, #token: 235994, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1491.79, #queue-req: 0, 
[2025-11-05 14:28:17 TP0] Decode batch, #running-req: 64, #token: 238554, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1489.96, #queue-req: 0, 
[2025-11-05 14:28:19 TP0] Decode batch, #running-req: 64, #token: 241114, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1490.28, #queue-req: 0, 
[2025-11-05 14:28:20 TP0] Decode batch, #running-req: 64, #token: 243674, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1487.80, #queue-req: 0, 
[2025-11-05 14:28:22 TP0] Decode batch, #running-req: 64, #token: 246234, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1486.58, #queue-req: 0, 
[2025-11-05 14:28:24 TP0] Decode batch, #running-req: 64, #token: 248794, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1482.09, #queue-req: 0, 
[2025-11-05 14:28:26 TP0] Decode batch, #running-req: 64, #token: 251354, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1486.05, #queue-req: 0, 
[2025-11-05 14:28:26] INFO:     127.0.0.1:39414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:26 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:28:27] INFO:     127.0.0.1:39420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-11-05 14:28:27] INFO:     127.0.0.1:39574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.02, #running-req: 6, #queue-req: 35, 
[2025-11-05 14:28:27] INFO:     127.0.0.1:39878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:39990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:40010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:40024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:40030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:40046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:27] INFO:     127.0.0.1:40058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:28:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-05 14:28:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-05 14:28:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-05 14:28:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-05 14:28:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-05 14:28:32 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.12, #running-req: 36, #queue-req: 22, 
[2025-11-05 14:28:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.14, #running-req: 42, #queue-req: 17, 
[2025-11-05 14:28:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.15, #running-req: 47, #queue-req: 12, 
[2025-11-05 14:28:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.17, #running-req: 52, #queue-req: 7, 
[2025-11-05 14:28:35 TP0] Prefill batch, #new-seq: 6, #new-token: 15985, #cached-token: 3221, token usage: 0.19, #running-req: 57, #queue-req: 1, 
[2025-11-05 14:28:36 TP0] Prefill batch, #new-seq: 1, #new-token: 3193, #cached-token: 8, token usage: 0.21, #running-req: 63, #queue-req: 0, 
[2025-11-05 14:28:37 TP0] Decode batch, #running-req: 64, #token: 205914, token usage: 0.21, cuda graph: True, gen throughput (token/s): 214.46, #queue-req: 0, 
[2025-11-05 14:28:39 TP0] Decode batch, #running-req: 64, #token: 208474, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1530.63, #queue-req: 0, 
[2025-11-05 14:28:41 TP0] Decode batch, #running-req: 64, #token: 211034, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1519.80, #queue-req: 0, 
[2025-11-05 14:28:43 TP0] Decode batch, #running-req: 64, #token: 213594, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1516.44, #queue-req: 0, 
[2025-11-05 14:28:44 TP0] Decode batch, #running-req: 64, #token: 216154, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1511.05, #queue-req: 0, 
[2025-11-05 14:28:46 TP0] Decode batch, #running-req: 64, #token: 218714, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1503.91, #queue-req: 0, 
[2025-11-05 14:28:48 TP0] Decode batch, #running-req: 64, #token: 221274, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1501.42, #queue-req: 0, 
[2025-11-05 14:28:49 TP0] Decode batch, #running-req: 64, #token: 223834, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1501.21, #queue-req: 0, 
[2025-11-05 14:28:51 TP0] Decode batch, #running-req: 64, #token: 226394, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1497.65, #queue-req: 0, 
[2025-11-05 14:28:53 TP0] Decode batch, #running-req: 64, #token: 228954, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1495.59, #queue-req: 0, 
[2025-11-05 14:28:54 TP0] Decode batch, #running-req: 64, #token: 231514, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1496.53, #queue-req: 0, 
[2025-11-05 14:28:56 TP0] Decode batch, #running-req: 64, #token: 234074, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1496.34, #queue-req: 0, 
[2025-11-05 14:28:58 TP0] Decode batch, #running-req: 64, #token: 236634, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1499.57, #queue-req: 0, 
[2025-11-05 14:29:00 TP0] Decode batch, #running-req: 64, #token: 239194, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1497.62, #queue-req: 0, 
[2025-11-05 14:29:01 TP0] Decode batch, #running-req: 64, #token: 241754, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1497.01, #queue-req: 0, 
[2025-11-05 14:29:03 TP0] Decode batch, #running-req: 64, #token: 244314, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1494.74, #queue-req: 0, 
[2025-11-05 14:29:05 TP0] Decode batch, #running-req: 64, #token: 246874, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1491.81, #queue-req: 0, 
[2025-11-05 14:29:06 TP0] Decode batch, #running-req: 64, #token: 249434, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1492.30, #queue-req: 0, 
[2025-11-05 14:29:08 TP0] Decode batch, #running-req: 64, #token: 251994, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1493.89, #queue-req: 0, 
[2025-11-05 14:29:10 TP0] Decode batch, #running-req: 64, #token: 254554, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1489.47, #queue-req: 0, 
[2025-11-05 14:29:11] INFO:     127.0.0.1:33224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:29:11] INFO:     127.0.0.1:33232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-11-05 14:29:11] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 37, 
[2025-11-05 14:29:11] INFO:     127.0.0.1:33630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:11] INFO:     127.0.0.1:33750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-05 14:29:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-05 14:29:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-05 14:29:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-05 14:29:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-05 14:29:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-11-05 14:29:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-11-05 14:29:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-11-05 14:29:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-11-05 14:29:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-11-05 14:29:20 TP0] Prefill batch, #new-seq: 3, #new-token: 9597, #cached-token: 6, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-11-05 14:29:22 TP0] Decode batch, #running-req: 64, #token: 205928, token usage: 0.21, cuda graph: True, gen throughput (token/s): 209.29, #queue-req: 0, 
[2025-11-05 14:29:24 TP0] Decode batch, #running-req: 64, #token: 208488, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1538.64, #queue-req: 0, 
[2025-11-05 14:29:25 TP0] Decode batch, #running-req: 64, #token: 211048, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1517.02, #queue-req: 0, 
[2025-11-05 14:29:27 TP0] Decode batch, #running-req: 64, #token: 213608, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1510.38, #queue-req: 0, 
[2025-11-05 14:29:29 TP0] Decode batch, #running-req: 64, #token: 216168, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1505.25, #queue-req: 0, 
[2025-11-05 14:29:31 TP0] Decode batch, #running-req: 64, #token: 218728, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1501.64, #queue-req: 0, 
[2025-11-05 14:29:32 TP0] Decode batch, #running-req: 64, #token: 221288, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1502.98, #queue-req: 0, 
[2025-11-05 14:29:34 TP0] Decode batch, #running-req: 64, #token: 223848, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1500.62, #queue-req: 0, 
[2025-11-05 14:29:36 TP0] Decode batch, #running-req: 64, #token: 226408, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1501.59, #queue-req: 0, 
[2025-11-05 14:29:37 TP0] Decode batch, #running-req: 64, #token: 228968, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1496.43, #queue-req: 0, 
[2025-11-05 14:29:39 TP0] Decode batch, #running-req: 64, #token: 231528, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1495.49, #queue-req: 0, 
[2025-11-05 14:29:41 TP0] Decode batch, #running-req: 64, #token: 234088, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1500.69, #queue-req: 0, 
[2025-11-05 14:29:42 TP0] Decode batch, #running-req: 64, #token: 236648, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1502.88, #queue-req: 0, 
[2025-11-05 14:29:44 TP0] Decode batch, #running-req: 64, #token: 239208, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1503.58, #queue-req: 0, 
[2025-11-05 14:29:46 TP0] Decode batch, #running-req: 64, #token: 241768, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1506.44, #queue-req: 0, 
[2025-11-05 14:29:48 TP0] Decode batch, #running-req: 64, #token: 244328, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1501.16, #queue-req: 0, 
[2025-11-05 14:29:49 TP0] Decode batch, #running-req: 64, #token: 246888, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1497.07, #queue-req: 0, 
[2025-11-05 14:29:51 TP0] Decode batch, #running-req: 64, #token: 249448, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1491.93, #queue-req: 0, 
[2025-11-05 14:29:53 TP0] Decode batch, #running-req: 64, #token: 252008, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1493.69, #queue-req: 0, 
[2025-11-05 14:29:54 TP0] Decode batch, #running-req: 64, #token: 254568, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1491.37, #queue-req: 0, 
[2025-11-05 14:29:55] INFO:     127.0.0.1:52022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:55 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:29:55] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:55] INFO:     127.0.0.1:52046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:55] INFO:     127.0.0.1:52052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:55] INFO:     127.0.0.1:52062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:55] INFO:     127.0.0.1:52072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:55] INFO:     127.0.0.1:52078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:55] INFO:     127.0.0.1:52094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:55] INFO:     127.0.0.1:52110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:55] INFO:     127.0.0.1:52114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:55] INFO:     127.0.0.1:52124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:55] INFO:     127.0.0.1:52136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-11-05 14:29:56] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 38, 
[2025-11-05 14:29:56] INFO:     127.0.0.1:52486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:56] INFO:     127.0.0.1:52584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:29:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-05 14:29:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-05 14:29:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-05 14:29:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-05 14:30:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-05 14:30:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-11-05 14:30:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-11-05 14:30:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-11-05 14:30:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-11-05 14:30:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-11-05 14:30:05 TP0] Prefill batch, #new-seq: 3, #new-token: 9593, #cached-token: 10, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-11-05 14:30:07 TP0] Decode batch, #running-req: 64, #token: 205918, token usage: 0.21, cuda graph: True, gen throughput (token/s): 208.71, #queue-req: 0, 
[2025-11-05 14:30:08 TP0] Decode batch, #running-req: 64, #token: 208478, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1533.30, #queue-req: 0, 
[2025-11-05 14:30:10 TP0] Decode batch, #running-req: 64, #token: 211038, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1519.39, #queue-req: 0, 
[2025-11-05 14:30:12 TP0] Decode batch, #running-req: 64, #token: 213598, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1512.07, #queue-req: 0, 
[2025-11-05 14:30:13 TP0] Decode batch, #running-req: 64, #token: 216158, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1505.81, #queue-req: 0, 
[2025-11-05 14:30:15 TP0] Decode batch, #running-req: 64, #token: 218718, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1502.23, #queue-req: 0, 
[2025-11-05 14:30:17 TP0] Decode batch, #running-req: 64, #token: 221278, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1498.65, #queue-req: 0, 
[2025-11-05 14:30:19 TP0] Decode batch, #running-req: 64, #token: 223838, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1497.57, #queue-req: 0, 
[2025-11-05 14:30:20 TP0] Decode batch, #running-req: 64, #token: 226398, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1494.95, #queue-req: 0, 
[2025-11-05 14:30:22 TP0] Decode batch, #running-req: 64, #token: 228958, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1495.90, #queue-req: 0, 
[2025-11-05 14:30:24 TP0] Decode batch, #running-req: 64, #token: 231518, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1494.94, #queue-req: 0, 
[2025-11-05 14:30:25 TP0] Decode batch, #running-req: 64, #token: 234078, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1496.57, #queue-req: 0, 
[2025-11-05 14:30:27 TP0] Decode batch, #running-req: 64, #token: 236638, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1500.85, #queue-req: 0, 
[2025-11-05 14:30:29 TP0] Decode batch, #running-req: 64, #token: 239198, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1497.67, #queue-req: 0, 
[2025-11-05 14:30:31 TP0] Decode batch, #running-req: 64, #token: 241758, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1494.90, #queue-req: 0, 
[2025-11-05 14:30:32 TP0] Decode batch, #running-req: 64, #token: 244318, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1491.74, #queue-req: 0, 
[2025-11-05 14:30:34 TP0] Decode batch, #running-req: 64, #token: 246878, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1490.00, #queue-req: 0, 
[2025-11-05 14:30:36 TP0] Decode batch, #running-req: 64, #token: 249438, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1488.76, #queue-req: 0, 
[2025-11-05 14:30:37 TP0] Decode batch, #running-req: 64, #token: 251998, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1486.49, #queue-req: 0, 
[2025-11-05 14:30:39 TP0] Decode batch, #running-req: 64, #token: 254558, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1487.76, #queue-req: 0, 
[2025-11-05 14:30:40] INFO:     127.0.0.1:38348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:30:40] INFO:     127.0.0.1:38364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-11-05 14:30:40] INFO:     127.0.0.1:38476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 6, #queue-req: 36, 
[2025-11-05 14:30:40] INFO:     127.0.0.1:38748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:40] INFO:     127.0.0.1:38762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:30:41 TP0] Prefill batch, #new-seq: 6, #new-token: 15988, #cached-token: 3218, token usage: 0.04, #running-req: 11, #queue-req: 35, 
[2025-11-05 14:30:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.06, #running-req: 17, #queue-req: 30, 
[2025-11-05 14:30:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 22, #queue-req: 25, 
[2025-11-05 14:30:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.09, #running-req: 27, #queue-req: 20, 
[2025-11-05 14:30:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.11, #running-req: 32, #queue-req: 15, 
[2025-11-05 14:30:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 37, #queue-req: 10, 
[2025-11-05 14:30:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.14, #running-req: 42, #queue-req: 5, 
[2025-11-05 14:30:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.15, #running-req: 47, #queue-req: 0, 
[2025-11-05 14:30:49 TP0] Decode batch, #running-req: 52, #token: 167310, token usage: 0.17, cuda graph: True, gen throughput (token/s): 233.33, #queue-req: 0, 
[2025-11-05 14:30:51 TP0] Decode batch, #running-req: 52, #token: 169390, token usage: 0.17, cuda graph: True, gen throughput (token/s): 1292.33, #queue-req: 0, 
[2025-11-05 14:30:52 TP0] Decode batch, #running-req: 52, #token: 171470, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1282.80, #queue-req: 0, 
[2025-11-05 14:30:54 TP0] Decode batch, #running-req: 52, #token: 173550, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1278.34, #queue-req: 0, 
[2025-11-05 14:30:56 TP0] Decode batch, #running-req: 52, #token: 175630, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1278.49, #queue-req: 0, 
[2025-11-05 14:30:57 TP0] Decode batch, #running-req: 52, #token: 177710, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1270.50, #queue-req: 0, 
[2025-11-05 14:30:59 TP0] Decode batch, #running-req: 52, #token: 179790, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1268.14, #queue-req: 0, 
[2025-11-05 14:31:01 TP0] Decode batch, #running-req: 52, #token: 181870, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1266.40, #queue-req: 0, 
[2025-11-05 14:31:02 TP0] Decode batch, #running-req: 52, #token: 183950, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1264.13, #queue-req: 0, 
[2025-11-05 14:31:04 TP0] Decode batch, #running-req: 52, #token: 186030, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1262.90, #queue-req: 0, 
[2025-11-05 14:31:06 TP0] Decode batch, #running-req: 52, #token: 188110, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1262.24, #queue-req: 0, 
[2025-11-05 14:31:07 TP0] Decode batch, #running-req: 52, #token: 190190, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1262.96, #queue-req: 0, 
[2025-11-05 14:31:09 TP0] Decode batch, #running-req: 52, #token: 192270, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1263.21, #queue-req: 0, 
[2025-11-05 14:31:11 TP0] Decode batch, #running-req: 52, #token: 194350, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1259.59, #queue-req: 0, 
[2025-11-05 14:31:12 TP0] Decode batch, #running-req: 52, #token: 196430, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1257.43, #queue-req: 0, 
[2025-11-05 14:31:14 TP0] Decode batch, #running-req: 52, #token: 198510, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1255.14, #queue-req: 0, 
[2025-11-05 14:31:15 TP0] Decode batch, #running-req: 52, #token: 200590, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1254.64, #queue-req: 0, 
[2025-11-05 14:31:17 TP0] Decode batch, #running-req: 52, #token: 202670, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1251.59, #queue-req: 0, 
[2025-11-05 14:31:19 TP0] Decode batch, #running-req: 52, #token: 204750, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1251.50, #queue-req: 0, 
[2025-11-05 14:31:20 TP0] Decode batch, #running-req: 52, #token: 206830, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1253.56, #queue-req: 0, 
[2025-11-05 14:31:21] INFO:     127.0.0.1:36134 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-05 14:31:22] INFO:     127.0.0.1:36146 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-05 14:31:39] INFO:     127.0.0.1:43082 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-05 14:31:45] INFO:     127.0.0.1:40114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:45 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:31:45 TP0] Decode batch, #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 46.85, #queue-req: 0, 
[2025-11-05 14:31:47] INFO:     127.0.0.1:40122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:31:47] INFO:     127.0.0.1:40136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47] INFO:     127.0.0.1:40246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:31:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-11-05 14:31:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-05 14:31:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-05 14:31:50 TP0] Decode batch, #running-req: 16, #token: 51618, token usage: 0.05, cuda graph: True, gen throughput (token/s): 96.10, #queue-req: 0, 
[2025-11-05 14:31:51 TP0] Decode batch, #running-req: 16, #token: 52258, token usage: 0.05, cuda graph: True, gen throughput (token/s): 619.66, #queue-req: 0, 
[2025-11-05 14:31:52 TP0] Decode batch, #running-req: 16, #token: 52898, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.64, #queue-req: 0, 
[2025-11-05 14:31:53 TP0] Decode batch, #running-req: 16, #token: 53538, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.75, #queue-req: 0, 
[2025-11-05 14:31:54 TP0] Decode batch, #running-req: 16, #token: 54178, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.50, #queue-req: 0, 
[2025-11-05 14:31:55 TP0] Decode batch, #running-req: 16, #token: 54818, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.08, #queue-req: 0, 
[2025-11-05 14:31:56 TP0] Decode batch, #running-req: 16, #token: 55458, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.19, #queue-req: 0, 
[2025-11-05 14:31:57 TP0] Decode batch, #running-req: 16, #token: 56098, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.69, #queue-req: 0, 
[2025-11-05 14:31:58 TP0] Decode batch, #running-req: 16, #token: 56738, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.55, #queue-req: 0, 
[2025-11-05 14:31:59 TP0] Decode batch, #running-req: 16, #token: 57378, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.78, #queue-req: 0, 
[2025-11-05 14:32:00 TP0] Decode batch, #running-req: 16, #token: 58018, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.49, #queue-req: 0, 
[2025-11-05 14:32:01 TP0] Decode batch, #running-req: 16, #token: 58658, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.78, #queue-req: 0, 
[2025-11-05 14:32:02 TP0] Decode batch, #running-req: 16, #token: 59298, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.56, #queue-req: 0, 
[2025-11-05 14:32:03 TP0] Decode batch, #running-req: 16, #token: 59938, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.47, #queue-req: 0, 
[2025-11-05 14:32:04 TP0] Decode batch, #running-req: 16, #token: 60578, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.78, #queue-req: 0, 
[2025-11-05 14:32:05 TP0] Decode batch, #running-req: 16, #token: 61218, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.58, #queue-req: 0, 
[2025-11-05 14:32:06 TP0] Decode batch, #running-req: 16, #token: 61858, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.83, #queue-req: 0, 
[2025-11-05 14:32:08 TP0] Decode batch, #running-req: 16, #token: 62498, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.47, #queue-req: 0, 
[2025-11-05 14:32:09 TP0] Decode batch, #running-req: 16, #token: 63138, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.44, #queue-req: 0, 
[2025-11-05 14:32:10 TP0] Decode batch, #running-req: 16, #token: 63778, token usage: 0.07, cuda graph: True, gen throughput (token/s): 608.71, #queue-req: 0, 
[2025-11-05 14:32:10] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:32:10] INFO:     127.0.0.1:55416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-05 14:32:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-05 14:32:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-05 14:32:13 TP0] Decode batch, #running-req: 16, #token: 51612, token usage: 0.05, cuda graph: True, gen throughput (token/s): 174.07, #queue-req: 0, 
[2025-11-05 14:32:14 TP0] Decode batch, #running-req: 16, #token: 52252, token usage: 0.05, cuda graph: True, gen throughput (token/s): 617.17, #queue-req: 0, 
[2025-11-05 14:32:15 TP0] Decode batch, #running-req: 16, #token: 52892, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.43, #queue-req: 0, 
[2025-11-05 14:32:16 TP0] Decode batch, #running-req: 16, #token: 53532, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.96, #queue-req: 0, 
[2025-11-05 14:32:17 TP0] Decode batch, #running-req: 16, #token: 54172, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.46, #queue-req: 0, 
[2025-11-05 14:32:19 TP0] Decode batch, #running-req: 16, #token: 54812, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.85, #queue-req: 0, 
[2025-11-05 14:32:20 TP0] Decode batch, #running-req: 16, #token: 55452, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.57, #queue-req: 0, 
[2025-11-05 14:32:21 TP0] Decode batch, #running-req: 16, #token: 56092, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.17, #queue-req: 0, 
[2025-11-05 14:32:22 TP0] Decode batch, #running-req: 16, #token: 56732, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.72, #queue-req: 0, 
[2025-11-05 14:32:23 TP0] Decode batch, #running-req: 16, #token: 57372, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.27, #queue-req: 0, 
[2025-11-05 14:32:24 TP0] Decode batch, #running-req: 16, #token: 58012, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.97, #queue-req: 0, 
[2025-11-05 14:32:25 TP0] Decode batch, #running-req: 16, #token: 58652, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.43, #queue-req: 0, 
[2025-11-05 14:32:26 TP0] Decode batch, #running-req: 16, #token: 59292, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.32, #queue-req: 0, 
[2025-11-05 14:32:27 TP0] Decode batch, #running-req: 16, #token: 59932, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.08, #queue-req: 0, 
[2025-11-05 14:32:28 TP0] Decode batch, #running-req: 16, #token: 60572, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.58, #queue-req: 0, 
[2025-11-05 14:32:29 TP0] Decode batch, #running-req: 16, #token: 61212, token usage: 0.06, cuda graph: True, gen throughput (token/s): 603.54, #queue-req: 0, 
[2025-11-05 14:32:30 TP0] Decode batch, #running-req: 16, #token: 61852, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.67, #queue-req: 0, 
[2025-11-05 14:32:31 TP0] Decode batch, #running-req: 16, #token: 62492, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.89, #queue-req: 0, 
[2025-11-05 14:32:32 TP0] Decode batch, #running-req: 16, #token: 63132, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.11, #queue-req: 0, 
[2025-11-05 14:32:33 TP0] Decode batch, #running-req: 16, #token: 63772, token usage: 0.07, cuda graph: True, gen throughput (token/s): 605.80, #queue-req: 0, 
[2025-11-05 14:32:34] INFO:     127.0.0.1:50978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:50992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:32:34] INFO:     127.0.0.1:50998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:51016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:51024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:51036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:51098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:51108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34] INFO:     127.0.0.1:51138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-05 14:32:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-05 14:32:35 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-05 14:32:37 TP0] Decode batch, #running-req: 16, #token: 51614, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.99, #queue-req: 0, 
[2025-11-05 14:32:38 TP0] Decode batch, #running-req: 16, #token: 52254, token usage: 0.05, cuda graph: True, gen throughput (token/s): 626.20, #queue-req: 0, 
[2025-11-05 14:32:39 TP0] Decode batch, #running-req: 16, #token: 52894, token usage: 0.05, cuda graph: True, gen throughput (token/s): 621.36, #queue-req: 0, 
[2025-11-05 14:32:40 TP0] Decode batch, #running-req: 16, #token: 53534, token usage: 0.06, cuda graph: True, gen throughput (token/s): 618.71, #queue-req: 0, 
[2025-11-05 14:32:41 TP0] Decode batch, #running-req: 16, #token: 54174, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.42, #queue-req: 0, 
[2025-11-05 14:32:42 TP0] Decode batch, #running-req: 16, #token: 54814, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.75, #queue-req: 0, 
[2025-11-05 14:32:43 TP0] Decode batch, #running-req: 16, #token: 55454, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.38, #queue-req: 0, 
[2025-11-05 14:32:44 TP0] Decode batch, #running-req: 16, #token: 56094, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.06, #queue-req: 0, 
[2025-11-05 14:32:45 TP0] Decode batch, #running-req: 16, #token: 56734, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.52, #queue-req: 0, 
[2025-11-05 14:32:46 TP0] Decode batch, #running-req: 16, #token: 57374, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.49, #queue-req: 0, 
[2025-11-05 14:32:47 TP0] Decode batch, #running-req: 16, #token: 58014, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.01, #queue-req: 0, 
[2025-11-05 14:32:48 TP0] Decode batch, #running-req: 16, #token: 58654, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.40, #queue-req: 0, 
[2025-11-05 14:32:50 TP0] Decode batch, #running-req: 16, #token: 59294, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.92, #queue-req: 0, 
[2025-11-05 14:32:51 TP0] Decode batch, #running-req: 16, #token: 59934, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.47, #queue-req: 0, 
[2025-11-05 14:32:52 TP0] Decode batch, #running-req: 16, #token: 60574, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.78, #queue-req: 0, 
[2025-11-05 14:32:53 TP0] Decode batch, #running-req: 16, #token: 61214, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.44, #queue-req: 0, 
[2025-11-05 14:32:54 TP0] Decode batch, #running-req: 16, #token: 61854, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.14, #queue-req: 0, 
[2025-11-05 14:32:55 TP0] Decode batch, #running-req: 16, #token: 62494, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.55, #queue-req: 0, 
[2025-11-05 14:32:56 TP0] Decode batch, #running-req: 16, #token: 63134, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.70, #queue-req: 0, 
[2025-11-05 14:32:57 TP0] Decode batch, #running-req: 16, #token: 63774, token usage: 0.07, cuda graph: True, gen throughput (token/s): 605.28, #queue-req: 0, 
[2025-11-05 14:32:57] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:32:57] INFO:     127.0.0.1:36040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57] INFO:     127.0.0.1:36176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:32:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-05 14:32:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-05 14:32:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-05 14:33:01 TP0] Decode batch, #running-req: 16, #token: 51617, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.39, #queue-req: 0, 
[2025-11-05 14:33:02 TP0] Decode batch, #running-req: 16, #token: 52257, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.84, #queue-req: 0, 
[2025-11-05 14:33:03 TP0] Decode batch, #running-req: 16, #token: 52897, token usage: 0.05, cuda graph: True, gen throughput (token/s): 613.62, #queue-req: 0, 
[2025-11-05 14:33:04 TP0] Decode batch, #running-req: 16, #token: 53537, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.41, #queue-req: 0, 
[2025-11-05 14:33:05 TP0] Decode batch, #running-req: 16, #token: 54177, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.48, #queue-req: 0, 
[2025-11-05 14:33:06 TP0] Decode batch, #running-req: 16, #token: 54817, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.96, #queue-req: 0, 
[2025-11-05 14:33:07 TP0] Decode batch, #running-req: 16, #token: 55457, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.12, #queue-req: 0, 
[2025-11-05 14:33:08 TP0] Decode batch, #running-req: 16, #token: 56097, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.57, #queue-req: 0, 
[2025-11-05 14:33:09 TP0] Decode batch, #running-req: 16, #token: 56737, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.25, #queue-req: 0, 
[2025-11-05 14:33:10 TP0] Decode batch, #running-req: 16, #token: 57377, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.19, #queue-req: 0, 
[2025-11-05 14:33:11 TP0] Decode batch, #running-req: 16, #token: 58017, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.10, #queue-req: 0, 
[2025-11-05 14:33:12 TP0] Decode batch, #running-req: 16, #token: 58657, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.10, #queue-req: 0, 
[2025-11-05 14:33:13 TP0] Decode batch, #running-req: 16, #token: 59297, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.19, #queue-req: 0, 
[2025-11-05 14:33:14 TP0] Decode batch, #running-req: 16, #token: 59937, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.61, #queue-req: 0, 
[2025-11-05 14:33:15 TP0] Decode batch, #running-req: 16, #token: 60577, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.88, #queue-req: 0, 
[2025-11-05 14:33:16 TP0] Decode batch, #running-req: 16, #token: 61217, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.39, #queue-req: 0, 
[2025-11-05 14:33:17 TP0] Decode batch, #running-req: 16, #token: 61857, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.59, #queue-req: 0, 
[2025-11-05 14:33:18 TP0] Decode batch, #running-req: 16, #token: 62497, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.16, #queue-req: 0, 
[2025-11-05 14:33:19 TP0] Decode batch, #running-req: 16, #token: 63137, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.79, #queue-req: 0, 
[2025-11-05 14:33:21 TP0] Decode batch, #running-req: 16, #token: 63777, token usage: 0.07, cuda graph: True, gen throughput (token/s): 612.89, #queue-req: 0, 
[2025-11-05 14:33:21] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21 TP0] Prefill batch, #new-seq: 1, #new-token: 3195, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:33:21] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21] INFO:     127.0.0.1:58504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-05 14:33:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-05 14:33:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-05 14:33:24 TP0] Decode batch, #running-req: 16, #token: 51612, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.80, #queue-req: 0, 
[2025-11-05 14:33:25 TP0] Decode batch, #running-req: 16, #token: 52252, token usage: 0.05, cuda graph: True, gen throughput (token/s): 621.84, #queue-req: 0, 
[2025-11-05 14:33:26 TP0] Decode batch, #running-req: 16, #token: 52892, token usage: 0.05, cuda graph: True, gen throughput (token/s): 619.34, #queue-req: 0, 
[2025-11-05 14:33:27 TP0] Decode batch, #running-req: 16, #token: 53532, token usage: 0.06, cuda graph: True, gen throughput (token/s): 618.80, #queue-req: 0, 
[2025-11-05 14:33:28 TP0] Decode batch, #running-req: 16, #token: 54172, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.29, #queue-req: 0, 
[2025-11-05 14:33:29 TP0] Decode batch, #running-req: 16, #token: 54812, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.89, #queue-req: 0, 
[2025-11-05 14:33:30 TP0] Decode batch, #running-req: 16, #token: 55452, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.45, #queue-req: 0, 
[2025-11-05 14:33:31 TP0] Decode batch, #running-req: 16, #token: 56092, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.80, #queue-req: 0, 
[2025-11-05 14:33:33 TP0] Decode batch, #running-req: 16, #token: 56732, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.85, #queue-req: 0, 
[2025-11-05 14:33:34 TP0] Decode batch, #running-req: 16, #token: 57372, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.49, #queue-req: 0, 
[2025-11-05 14:33:35 TP0] Decode batch, #running-req: 16, #token: 58012, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.10, #queue-req: 0, 
[2025-11-05 14:33:36 TP0] Decode batch, #running-req: 16, #token: 58652, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.31, #queue-req: 0, 
[2025-11-05 14:33:37 TP0] Decode batch, #running-req: 16, #token: 59292, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.53, #queue-req: 0, 
[2025-11-05 14:33:38 TP0] Decode batch, #running-req: 16, #token: 59932, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.91, #queue-req: 0, 
[2025-11-05 14:33:39 TP0] Decode batch, #running-req: 16, #token: 60572, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.83, #queue-req: 0, 
[2025-11-05 14:33:40 TP0] Decode batch, #running-req: 16, #token: 61212, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.31, #queue-req: 0, 
[2025-11-05 14:33:41 TP0] Decode batch, #running-req: 16, #token: 61852, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.66, #queue-req: 0, 
[2025-11-05 14:33:42 TP0] Decode batch, #running-req: 16, #token: 62492, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.48, #queue-req: 0, 
[2025-11-05 14:33:43 TP0] Decode batch, #running-req: 16, #token: 63132, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.79, #queue-req: 0, 
[2025-11-05 14:33:44 TP0] Decode batch, #running-req: 16, #token: 63772, token usage: 0.07, cuda graph: True, gen throughput (token/s): 614.66, #queue-req: 0, 
[2025-11-05 14:33:44] INFO:     127.0.0.1:51334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:33:44] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:44] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:33:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15978, #cached-token: 27, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-05 14:33:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-05 14:33:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-05 14:33:48 TP0] Decode batch, #running-req: 16, #token: 51612, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.37, #queue-req: 0, 
[2025-11-05 14:33:49 TP0] Decode batch, #running-req: 16, #token: 52252, token usage: 0.05, cuda graph: True, gen throughput (token/s): 622.16, #queue-req: 0, 
[2025-11-05 14:33:50 TP0] Decode batch, #running-req: 16, #token: 52892, token usage: 0.05, cuda graph: True, gen throughput (token/s): 618.46, #queue-req: 0, 
[2025-11-05 14:33:51 TP0] Decode batch, #running-req: 16, #token: 53532, token usage: 0.06, cuda graph: True, gen throughput (token/s): 617.08, #queue-req: 0, 
[2025-11-05 14:33:52 TP0] Decode batch, #running-req: 16, #token: 54172, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.62, #queue-req: 0, 
[2025-11-05 14:33:53 TP0] Decode batch, #running-req: 16, #token: 54812, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.16, #queue-req: 0, 
[2025-11-05 14:33:54 TP0] Decode batch, #running-req: 16, #token: 55452, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.15, #queue-req: 0, 
[2025-11-05 14:33:55 TP0] Decode batch, #running-req: 16, #token: 56092, token usage: 0.06, cuda graph: True, gen throughput (token/s): 617.01, #queue-req: 0, 
[2025-11-05 14:33:56 TP0] Decode batch, #running-req: 16, #token: 56732, token usage: 0.06, cuda graph: True, gen throughput (token/s): 619.03, #queue-req: 0, 
[2025-11-05 14:33:57 TP0] Decode batch, #running-req: 16, #token: 57372, token usage: 0.06, cuda graph: True, gen throughput (token/s): 616.81, #queue-req: 0, 
[2025-11-05 14:33:58 TP0] Decode batch, #running-req: 16, #token: 58012, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.57, #queue-req: 0, 
[2025-11-05 14:33:59 TP0] Decode batch, #running-req: 16, #token: 58652, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.01, #queue-req: 0, 
[2025-11-05 14:34:00 TP0] Decode batch, #running-req: 16, #token: 59292, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.88, #queue-req: 0, 
[2025-11-05 14:34:01 TP0] Decode batch, #running-req: 16, #token: 59932, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.92, #queue-req: 0, 
[2025-11-05 14:34:02 TP0] Decode batch, #running-req: 16, #token: 60572, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.20, #queue-req: 0, 
[2025-11-05 14:34:03 TP0] Decode batch, #running-req: 16, #token: 61212, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.59, #queue-req: 0, 
[2025-11-05 14:34:04 TP0] Decode batch, #running-req: 16, #token: 61852, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.62, #queue-req: 0, 
[2025-11-05 14:34:05 TP0] Decode batch, #running-req: 16, #token: 62492, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.37, #queue-req: 0, 
[2025-11-05 14:34:06 TP0] Decode batch, #running-req: 16, #token: 63132, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.47, #queue-req: 0, 
[2025-11-05 14:34:08 TP0] Decode batch, #running-req: 16, #token: 63772, token usage: 0.07, cuda graph: True, gen throughput (token/s): 614.67, #queue-req: 0, 
[2025-11-05 14:34:08] INFO:     127.0.0.1:58728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:34:08] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-05 14:34:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-05 14:34:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-05 14:34:11 TP0] Decode batch, #running-req: 16, #token: 51614, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.73, #queue-req: 0, 
[2025-11-05 14:34:12 TP0] Decode batch, #running-req: 16, #token: 52254, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.95, #queue-req: 0, 
[2025-11-05 14:34:13 TP0] Decode batch, #running-req: 16, #token: 52894, token usage: 0.05, cuda graph: True, gen throughput (token/s): 613.36, #queue-req: 0, 
[2025-11-05 14:34:14 TP0] Decode batch, #running-req: 16, #token: 53534, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.96, #queue-req: 0, 
[2025-11-05 14:34:15 TP0] Decode batch, #running-req: 16, #token: 54174, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.78, #queue-req: 0, 
[2025-11-05 14:34:16 TP0] Decode batch, #running-req: 16, #token: 54814, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.51, #queue-req: 0, 
[2025-11-05 14:34:18 TP0] Decode batch, #running-req: 16, #token: 55454, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.68, #queue-req: 0, 
[2025-11-05 14:34:19 TP0] Decode batch, #running-req: 16, #token: 56094, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.02, #queue-req: 0, 
[2025-11-05 14:34:20 TP0] Decode batch, #running-req: 16, #token: 56734, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.60, #queue-req: 0, 
[2025-11-05 14:34:21 TP0] Decode batch, #running-req: 16, #token: 57374, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.32, #queue-req: 0, 
[2025-11-05 14:34:22 TP0] Decode batch, #running-req: 16, #token: 58014, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.88, #queue-req: 0, 
[2025-11-05 14:34:23 TP0] Decode batch, #running-req: 16, #token: 58654, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.04, #queue-req: 0, 
[2025-11-05 14:34:24 TP0] Decode batch, #running-req: 16, #token: 59294, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.91, #queue-req: 0, 
[2025-11-05 14:34:25 TP0] Decode batch, #running-req: 16, #token: 59934, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.91, #queue-req: 0, 
[2025-11-05 14:34:26 TP0] Decode batch, #running-req: 16, #token: 60574, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.10, #queue-req: 0, 
[2025-11-05 14:34:27 TP0] Decode batch, #running-req: 16, #token: 61214, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.39, #queue-req: 0, 
[2025-11-05 14:34:28 TP0] Decode batch, #running-req: 16, #token: 61854, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.33, #queue-req: 0, 
[2025-11-05 14:34:29 TP0] Decode batch, #running-req: 16, #token: 62494, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.05, #queue-req: 0, 
[2025-11-05 14:34:30 TP0] Decode batch, #running-req: 16, #token: 63134, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.23, #queue-req: 0, 
[2025-11-05 14:34:31 TP0] Decode batch, #running-req: 16, #token: 63774, token usage: 0.07, cuda graph: True, gen throughput (token/s): 605.17, #queue-req: 0, 
[2025-11-05 14:34:32] INFO:     127.0.0.1:40530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:34:32] INFO:     127.0.0.1:40550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32] INFO:     127.0.0.1:40678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:34:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-05 14:34:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-05 14:34:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-05 14:34:35 TP0] Decode batch, #running-req: 16, #token: 51615, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.64, #queue-req: 0, 
[2025-11-05 14:34:36 TP0] Decode batch, #running-req: 16, #token: 52255, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.21, #queue-req: 0, 
[2025-11-05 14:34:37 TP0] Decode batch, #running-req: 16, #token: 52895, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.61, #queue-req: 0, 
[2025-11-05 14:34:38 TP0] Decode batch, #running-req: 16, #token: 53535, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.74, #queue-req: 0, 
[2025-11-05 14:34:39 TP0] Decode batch, #running-req: 16, #token: 54175, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.50, #queue-req: 0, 
[2025-11-05 14:34:40 TP0] Decode batch, #running-req: 16, #token: 54815, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.98, #queue-req: 0, 
[2025-11-05 14:34:41 TP0] Decode batch, #running-req: 16, #token: 55455, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.28, #queue-req: 0, 
[2025-11-05 14:34:42 TP0] Decode batch, #running-req: 16, #token: 56095, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.43, #queue-req: 0, 
[2025-11-05 14:34:43 TP0] Decode batch, #running-req: 16, #token: 56735, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.04, #queue-req: 0, 
[2025-11-05 14:34:44 TP0] Decode batch, #running-req: 16, #token: 57375, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.04, #queue-req: 0, 
[2025-11-05 14:34:45 TP0] Decode batch, #running-req: 16, #token: 58015, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.77, #queue-req: 0, 
[2025-11-05 14:34:46 TP0] Decode batch, #running-req: 16, #token: 58655, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.24, #queue-req: 0, 
[2025-11-05 14:34:48 TP0] Decode batch, #running-req: 16, #token: 59295, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.32, #queue-req: 0, 
[2025-11-05 14:34:49 TP0] Decode batch, #running-req: 16, #token: 59935, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.54, #queue-req: 0, 
[2025-11-05 14:34:50 TP0] Decode batch, #running-req: 16, #token: 60575, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.90, #queue-req: 0, 
[2025-11-05 14:34:51 TP0] Decode batch, #running-req: 16, #token: 61215, token usage: 0.06, cuda graph: True, gen throughput (token/s): 603.53, #queue-req: 0, 
[2025-11-05 14:34:52 TP0] Decode batch, #running-req: 16, #token: 61855, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.14, #queue-req: 0, 
[2025-11-05 14:34:53 TP0] Decode batch, #running-req: 16, #token: 62495, token usage: 0.06, cuda graph: True, gen throughput (token/s): 603.44, #queue-req: 0, 
[2025-11-05 14:34:54 TP0] Decode batch, #running-req: 16, #token: 63135, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.85, #queue-req: 0, 
[2025-11-05 14:34:55 TP0] Decode batch, #running-req: 16, #token: 63775, token usage: 0.07, cuda graph: True, gen throughput (token/s): 604.27, #queue-req: 0, 
[2025-11-05 14:34:55] INFO:     127.0.0.1:57432 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-05 14:34:56] INFO:     127.0.0.1:57446 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-05 14:35:12] INFO:     127.0.0.1:52590 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-05 14:35:18] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:35:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:35:18 TP0] Decode batch, #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 10.75, #queue-req: 0, 
[2025-11-05 14:35:19] INFO:     127.0.0.1:55462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:35:19] INFO:     127.0.0.1:55468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:35:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:35:19] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:35:19] INFO:     127.0.0.1:55490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:35:19 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:35:20 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 71.39, #queue-req: 0, 
[2025-11-05 14:35:21 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-11-05 14:35:22 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-11-05 14:35:23 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-05 14:35:23 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-05 14:35:24 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-05 14:35:25 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-05 14:35:26 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-05 14:35:27 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-05 14:35:28 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-05 14:35:28 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-05 14:35:29 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:35:30 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-05 14:35:31 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-05 14:35:32 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-05 14:35:33 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-05 14:35:33 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:35:34 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:35:35 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-05 14:35:36 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-05 14:35:36] INFO:     127.0.0.1:41474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:35:36] INFO:     127.0.0.1:41488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:35:36] INFO:     127.0.0.1:41500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:35:36 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:35:36] INFO:     127.0.0.1:41504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:35:36 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:35:37 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.78, #queue-req: 0, 
[2025-11-05 14:35:38 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-11-05 14:35:39 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-05 14:35:39 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-05 14:35:40 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-05 14:35:41 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-05 14:35:42 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-05 14:35:43 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-05 14:35:43 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-05 14:35:44 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-05 14:35:45 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:35:46 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:35:47 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:35:48 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:35:48 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:35:49 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:35:50 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:35:51 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:35:52 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:35:53 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:35:53] INFO:     127.0.0.1:58232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:35:53] INFO:     127.0.0.1:58234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:35:53] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:35:53 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:35:53] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:35:53 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:35:54 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.20, #queue-req: 0, 
[2025-11-05 14:35:54 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-11-05 14:35:55 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-11-05 14:35:56 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-05 14:35:57 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-05 14:35:58 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-05 14:35:59 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-05 14:35:59 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-05 14:36:00 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-05 14:36:01 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:36:02 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:36:03 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:36:04 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:36:04 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:36:05 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:36:06 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:36:07 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:36:08 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:36:08 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:36:09 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:36:09] INFO:     127.0.0.1:36438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:09] INFO:     127.0.0.1:36448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:09] INFO:     127.0.0.1:36452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:09 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:36:09] INFO:     127.0.0.1:36468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:09 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:36:10 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.83, #queue-req: 0, 
[2025-11-05 14:36:11 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-05 14:36:12 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-05 14:36:13 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-05 14:36:14 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-05 14:36:14 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:36:15 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:36:16 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:36:17 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:36:18 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:36:19 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:36:19 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:36:20 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:36:21 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:36:22 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:36:23 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:36:24 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:36:24 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-05 14:36:25 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:36:26 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:36:26] INFO:     127.0.0.1:35378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:26] INFO:     127.0.0.1:35382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:26] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:26 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:36:26] INFO:     127.0.0.1:35392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:26 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:36:27 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.95, #queue-req: 0, 
[2025-11-05 14:36:28 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-11-05 14:36:29 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-05 14:36:29 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:36:30 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-05 14:36:31 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:36:32 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:36:33 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:36:34 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:36:34 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:36:35 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:36:36 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:36:37 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:36:38 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:36:39 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:36:39 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:36:40 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:36:41 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:36:42 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:36:43 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:36:43] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:43] INFO:     127.0.0.1:53210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:36:43] INFO:     127.0.0.1:53216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:43] INFO:     127.0.0.1:53220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:43 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:36:44 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.01, #queue-req: 0, 
[2025-11-05 14:36:44 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-05 14:36:45 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-05 14:36:46 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-05 14:36:47 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-05 14:36:48 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-05 14:36:49 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:36:49 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:36:50 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:36:51 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:36:52 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:36:53 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:36:54 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:36:54 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:36:55 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:36:56 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:36:57 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:36:58 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:36:59 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:36:59 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:36:59] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:59] INFO:     127.0.0.1:56190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:59] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:36:59 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:37:00] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:37:00 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:37:00 TP0] Decode batch, #running-req: 4, #token: 12936, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.77, #queue-req: 0, 
[2025-11-05 14:37:01 TP0] Decode batch, #running-req: 4, #token: 13096, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-05 14:37:02 TP0] Decode batch, #running-req: 4, #token: 13256, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-05 14:37:03 TP0] Decode batch, #running-req: 4, #token: 13416, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-05 14:37:04 TP0] Decode batch, #running-req: 4, #token: 13576, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-05 14:37:04 TP0] Decode batch, #running-req: 4, #token: 13736, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-05 14:37:05 TP0] Decode batch, #running-req: 4, #token: 13896, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:37:06 TP0] Decode batch, #running-req: 4, #token: 14056, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:37:07 TP0] Decode batch, #running-req: 4, #token: 14216, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:37:08 TP0] Decode batch, #running-req: 4, #token: 14376, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:37:09 TP0] Decode batch, #running-req: 4, #token: 14536, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:37:09 TP0] Decode batch, #running-req: 4, #token: 14696, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:37:10 TP0] Decode batch, #running-req: 4, #token: 14856, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:37:11 TP0] Decode batch, #running-req: 4, #token: 15016, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:37:12 TP0] Decode batch, #running-req: 4, #token: 15176, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:37:13 TP0] Decode batch, #running-req: 4, #token: 15336, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:37:14 TP0] Decode batch, #running-req: 4, #token: 15496, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:37:14 TP0] Decode batch, #running-req: 4, #token: 15656, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:37:15 TP0] Decode batch, #running-req: 4, #token: 15816, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:37:16 TP0] Decode batch, #running-req: 4, #token: 15976, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:37:16] INFO:     127.0.0.1:35742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:37:16] INFO:     127.0.0.1:35746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:37:16] INFO:     127.0.0.1:35750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:37:16 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:37:16] INFO:     127.0.0.1:35756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:37:16 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:37:17 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.72, #queue-req: 0, 
[2025-11-05 14:37:18 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-05 14:37:19 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:37:20 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:37:20 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-05 14:37:21 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:37:22 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:37:23 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:37:24 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:37:25 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:37:25 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:37:26 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:37:27 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:37:28 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:37:29 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:37:29 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:37:30 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:37:31 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:37:32 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:37:33 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:37:33] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:37:33] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:37:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:37:33] INFO:     127.0.0.1:53082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:37:33] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:37:33 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:37:34 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.04, #queue-req: 0, 
[2025-11-05 14:37:35 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-11-05 14:37:35 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-11-05 14:37:36 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-05 14:37:37 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:37:38 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-05 14:37:39 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:37:40 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:37:40 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-05 14:37:41 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:37:42 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:37:43 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:37:44 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-05 14:37:45 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-05 14:37:45 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:37:46 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:37:47 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:37:48 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:37:49 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:37:49 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:37:50] INFO:     127.0.0.1:51742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:37:50] INFO:     127.0.0.1:51746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:37:50] INFO:     127.0.0.1:51758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:37:50 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:37:50] INFO:     127.0.0.1:51772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:37:50 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:37:50 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.74, #queue-req: 0, 
[2025-11-05 14:37:51 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-05 14:37:52 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-05 14:37:53 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-05 14:37:54 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:37:55 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:37:55 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:37:56 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:37:57 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:37:58 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:37:59 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-05 14:38:00 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-05 14:38:00 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-05 14:38:01 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:38:02 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:38:03 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-05 14:38:04 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:38:05 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-05 14:38:05 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:38:06 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:38:06] INFO:     127.0.0.1:44392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:06] INFO:     127.0.0.1:44400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:06] INFO:     127.0.0.1:44408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:06 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:38:06] INFO:     127.0.0.1:44416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:06 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:38:07 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 158.66, #queue-req: 0, 
[2025-11-05 14:38:08 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:38:09 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:38:10 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:38:10 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:38:11 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:38:12 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-05 14:38:13 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:38:14 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:38:15 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-05 14:38:15 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:38:16 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:38:17 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:38:18 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:38:19 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:38:20 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:38:20 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:38:21 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:38:22 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:38:23 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:38:23] INFO:     127.0.0.1:60662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:23] INFO:     127.0.0.1:60678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:23] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:23 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:38:23] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:23 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:38:24 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.82, #queue-req: 0, 
[2025-11-05 14:38:25 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.92, #queue-req: 0, 
[2025-11-05 14:38:26 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-11-05 14:38:26 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-11-05 14:38:27 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-05 14:38:28 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-05 14:38:29 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-05 14:38:30 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:38:30 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:38:31 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:38:32 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:38:33 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:38:34 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:38:35 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:38:35 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:38:36 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:38:37 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:38:38 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:38:39 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:38:40 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:38:40] INFO:     127.0.0.1:42476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:40] INFO:     127.0.0.1:42490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:38:40] INFO:     127.0.0.1:42492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:40] INFO:     127.0.0.1:42500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:40 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:38:41 TP0] Decode batch, #running-req: 4, #token: 12937, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.90, #queue-req: 0, 
[2025-11-05 14:38:41 TP0] Decode batch, #running-req: 4, #token: 13097, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-05 14:38:42 TP0] Decode batch, #running-req: 4, #token: 13257, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-05 14:38:43 TP0] Decode batch, #running-req: 4, #token: 13417, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:38:44 TP0] Decode batch, #running-req: 4, #token: 13577, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:38:45 TP0] Decode batch, #running-req: 4, #token: 13737, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:38:46 TP0] Decode batch, #running-req: 4, #token: 13897, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:38:46 TP0] Decode batch, #running-req: 4, #token: 14057, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-05 14:38:47 TP0] Decode batch, #running-req: 4, #token: 14217, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:38:48 TP0] Decode batch, #running-req: 4, #token: 14377, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:38:49 TP0] Decode batch, #running-req: 4, #token: 14537, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:38:50 TP0] Decode batch, #running-req: 4, #token: 14697, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:38:51 TP0] Decode batch, #running-req: 4, #token: 14857, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:38:51 TP0] Decode batch, #running-req: 4, #token: 15017, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:38:52 TP0] Decode batch, #running-req: 4, #token: 15177, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-05 14:38:53 TP0] Decode batch, #running-req: 4, #token: 15337, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:38:54 TP0] Decode batch, #running-req: 4, #token: 15497, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-05 14:38:55 TP0] Decode batch, #running-req: 4, #token: 15657, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:38:55 TP0] Decode batch, #running-req: 4, #token: 15817, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:38:56 TP0] Decode batch, #running-req: 4, #token: 15977, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:38:56] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:56] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:56] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:56 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:38:56] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:38:57 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:38:57 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.69, #queue-req: 0, 
[2025-11-05 14:38:58 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:38:59 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-05 14:39:00 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:39:01 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:39:01 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-05 14:39:02 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-05 14:39:03 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-05 14:39:04 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-05 14:39:05 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-05 14:39:06 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-05 14:39:06 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-05 14:39:07 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-05 14:39:08 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-05 14:39:09 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-05 14:39:10 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-05 14:39:11 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-05 14:39:11 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-05 14:39:12 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-05 14:39:13 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-05 14:39:13] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:39:13] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:39:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:39:13] INFO:     127.0.0.1:53110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:39:13] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:39:13 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:39:14 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.83, #queue-req: 0, 
[2025-11-05 14:39:15 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-05 14:39:16 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-05 14:39:16 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:39:17 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:39:18 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:39:19 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:39:20 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:39:21 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:39:21 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:39:22 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:39:23 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:39:24 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:39:25 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:39:26 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:39:26 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:39:27 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:39:28 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:39:29 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:39:30 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-05 14:39:30] INFO:     127.0.0.1:50794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:39:30] INFO:     127.0.0.1:50800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:39:30] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:39:30 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:39:30] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:39:30 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:39:31 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.82, #queue-req: 0, 
[2025-11-05 14:39:32 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-05 14:39:32 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:39:33 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:39:34 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:39:35 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:39:36 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:39:37 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:39:37 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-05 14:39:38 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:39:39 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-05 14:39:40 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:39:41 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:39:41 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:39:42 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:39:43 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:39:44 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:39:45 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:39:46 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:39:46 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:39:47] INFO:     127.0.0.1:37518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:39:47] INFO:     127.0.0.1:37526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:39:47 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:39:47] INFO:     127.0.0.1:37542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:39:47] INFO:     127.0.0.1:37548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:39:47 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:39:47 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.85, #queue-req: 0, 
[2025-11-05 14:39:48 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:39:49 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:39:50 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:39:51 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:39:52 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:39:52 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-05 14:39:53 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-05 14:39:54 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:39:55 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:39:56 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:39:57 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:39:57 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:39:58 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:39:59 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:40:00 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:40:01 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:40:01 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:40:02 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-05 14:40:03 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:40:03] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:03] INFO:     127.0.0.1:47598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:03] INFO:     127.0.0.1:47610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:40:03] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:03 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:40:04 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.99, #queue-req: 0, 
[2025-11-05 14:40:05 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-05 14:40:06 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-05 14:40:07 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:40:07 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:40:08 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:40:09 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:40:10 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:40:11 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:40:12 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:40:12 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:40:13 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:40:14 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-05 14:40:15 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-05 14:40:16 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:40:17 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:40:17 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:40:18 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:40:19 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:40:20 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:40:20] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:20] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:20] INFO:     127.0.0.1:55846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:40:20] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:20 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:40:21 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.99, #queue-req: 0, 
[2025-11-05 14:40:22 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-05 14:40:22 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:40:23 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:40:24 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:40:25 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:40:26 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-05 14:40:27 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-05 14:40:27 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-05 14:40:28 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-05 14:40:29 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-05 14:40:30 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:40:31 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:40:32 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:40:32 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:40:33 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:40:34 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:40:35 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:40:36 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:40:37 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-05 14:40:37] INFO:     127.0.0.1:45176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:37] INFO:     127.0.0.1:45186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:37] INFO:     127.0.0.1:45196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:40:37] INFO:     127.0.0.1:45198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:37 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:40:38 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.02, #queue-req: 0, 
[2025-11-05 14:40:38 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:40:39 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:40:40 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-05 14:40:41 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:40:42 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:40:42 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-05 14:40:43 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-05 14:40:44 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-05 14:40:45 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-05 14:40:46 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-05 14:40:47 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-05 14:40:47 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-05 14:40:48 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-05 14:40:49 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-05 14:40:50 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-05 14:40:51 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-05 14:40:52 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-05 14:40:52 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-05 14:40:53 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-05 14:40:53] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:53] INFO:     127.0.0.1:54550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:53] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:53 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:40:53] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:40:53 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:40:54 TP0] Decode batch, #running-req: 4, #token: 12937, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.84, #queue-req: 0, 
[2025-11-05 14:40:55 TP0] Decode batch, #running-req: 4, #token: 13097, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-05 14:40:56 TP0] Decode batch, #running-req: 4, #token: 13257, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-05 14:40:57 TP0] Decode batch, #running-req: 4, #token: 13417, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-05 14:40:58 TP0] Decode batch, #running-req: 4, #token: 13577, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:40:58 TP0] Decode batch, #running-req: 4, #token: 13737, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-05 14:40:59 TP0] Decode batch, #running-req: 4, #token: 13897, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:41:00 TP0] Decode batch, #running-req: 4, #token: 14057, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-05 14:41:01 TP0] Decode batch, #running-req: 4, #token: 14217, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:41:02 TP0] Decode batch, #running-req: 4, #token: 14377, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:41:03 TP0] Decode batch, #running-req: 4, #token: 14537, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-05 14:41:03 TP0] Decode batch, #running-req: 4, #token: 14697, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:41:04 TP0] Decode batch, #running-req: 4, #token: 14857, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:41:05 TP0] Decode batch, #running-req: 4, #token: 15017, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:41:06 TP0] Decode batch, #running-req: 4, #token: 15177, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:41:07 TP0] Decode batch, #running-req: 4, #token: 15337, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-05 14:41:07 TP0] Decode batch, #running-req: 4, #token: 15497, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:41:08 TP0] Decode batch, #running-req: 4, #token: 15657, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:41:09 TP0] Decode batch, #running-req: 4, #token: 15817, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:41:10 TP0] Decode batch, #running-req: 4, #token: 15977, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:41:10] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:41:10] INFO:     127.0.0.1:51724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:41:10] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:41:10 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:41:10] INFO:     127.0.0.1:51744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:41:10 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:41:11 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.92, #queue-req: 0, 
[2025-11-05 14:41:12 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-11-05 14:41:13 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-11-05 14:41:13 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-05 14:41:14 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-05 14:41:15 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-05 14:41:16 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-05 14:41:17 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-05 14:41:18 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-05 14:41:18 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-05 14:41:19 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-05 14:41:20 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-05 14:41:21 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-05 14:41:22 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-05 14:41:23 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-05 14:41:23 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-05 14:41:24 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-05 14:41:25 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-05 14:41:26 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-05 14:41:27 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-05 14:41:27] INFO:     127.0.0.1:33704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:41:27] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:41:27] INFO:     127.0.0.1:33716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:41:27 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:41:27] INFO:     127.0.0.1:33720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:41:27 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:41:28 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.43, #queue-req: 0, 
[2025-11-05 14:41:28 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:41:29 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-05 14:41:30 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-05 14:41:31 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-05 14:41:32 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-05 14:41:33 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:41:33 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:41:34 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-05 14:41:35 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-05 14:41:36 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-05 14:41:37 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-05 14:41:38 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-05 14:41:38 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-05 14:41:39 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-05 14:41:40 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-05 14:41:41 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-05 14:41:42 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-05 14:41:43 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-05 14:41:43 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-11-05 14:41:43] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:41:43] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:41:43] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:41:43 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:41:43] INFO:     127.0.0.1:57590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:41:44 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:41:44 TP0] Decode batch, #running-req: 4, #token: 12937, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.30, #queue-req: 0, 
[2025-11-05 14:41:45 TP0] Decode batch, #running-req: 4, #token: 13097, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-05 14:41:46 TP0] Decode batch, #running-req: 4, #token: 13257, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-05 14:41:47 TP0] Decode batch, #running-req: 4, #token: 13417, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-05 14:41:48 TP0] Decode batch, #running-req: 4, #token: 13577, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:41:48 TP0] Decode batch, #running-req: 4, #token: 13737, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:41:49 TP0] Decode batch, #running-req: 4, #token: 13897, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:41:50 TP0] Decode batch, #running-req: 4, #token: 14057, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:41:51 TP0] Decode batch, #running-req: 4, #token: 14217, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:41:52 TP0] Decode batch, #running-req: 4, #token: 14377, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:41:53 TP0] Decode batch, #running-req: 4, #token: 14537, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:41:53 TP0] Decode batch, #running-req: 4, #token: 14697, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:41:54 TP0] Decode batch, #running-req: 4, #token: 14857, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:41:55 TP0] Decode batch, #running-req: 4, #token: 15017, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:41:56 TP0] Decode batch, #running-req: 4, #token: 15177, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:41:57 TP0] Decode batch, #running-req: 4, #token: 15337, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:41:58 TP0] Decode batch, #running-req: 4, #token: 15497, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-05 14:41:58 TP0] Decode batch, #running-req: 4, #token: 15657, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-05 14:41:59 TP0] Decode batch, #running-req: 4, #token: 15817, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-05 14:42:00 TP0] Decode batch, #running-req: 4, #token: 15977, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-05 14:42:00] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:00] INFO:     127.0.0.1:57636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:00] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:00 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:42:00] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:00 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:42:01 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.46, #queue-req: 0, 
[2025-11-05 14:42:02 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:42:03 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:42:04 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:42:04 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:42:05 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:42:06 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:42:07 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:42:08 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-05 14:42:09 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-05 14:42:09 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-05 14:42:10 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-05 14:42:11 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-05 14:42:12 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-05 14:42:13 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-05 14:42:13 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-05 14:42:14 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-05 14:42:15 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-05 14:42:16 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-05 14:42:17 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-05 14:42:17] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:17] INFO:     127.0.0.1:49414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:17] INFO:     127.0.0.1:49430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:17 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:42:17] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:17 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:42:18 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.51, #queue-req: 0, 
[2025-11-05 14:42:19 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-05 14:42:19 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:42:20 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-05 14:42:21 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:42:22 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-05 14:42:23 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:42:24 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:42:24 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:42:25 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:42:26 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:42:27 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:42:28 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:42:29 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:42:29 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:42:30 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:42:31 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:42:32 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:42:33 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:42:33 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:42:34] INFO:     127.0.0.1:37764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:34] INFO:     127.0.0.1:37780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:34] INFO:     127.0.0.1:37786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:42:34] INFO:     127.0.0.1:37800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:34 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:42:34 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.06, #queue-req: 0, 
[2025-11-05 14:42:35 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-05 14:42:36 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-05 14:42:37 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:42:38 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-05 14:42:39 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:42:39 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:42:40 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:42:41 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:42:42 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:42:43 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-05 14:42:44 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-05 14:42:44 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-05 14:42:45 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-05 14:42:46 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-05 14:42:47 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-05 14:42:48 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-05 14:42:49 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-05 14:42:49 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-05 14:42:50 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-05 14:42:50] INFO:     127.0.0.1:47004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:50] INFO:     127.0.0.1:47018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:42:50] INFO:     127.0.0.1:47034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:50] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:42:50 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:42:51 TP0] Decode batch, #running-req: 4, #token: 12937, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.65, #queue-req: 0, 
[2025-11-05 14:42:52 TP0] Decode batch, #running-req: 4, #token: 13097, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:42:53 TP0] Decode batch, #running-req: 4, #token: 13257, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:42:54 TP0] Decode batch, #running-req: 4, #token: 13417, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:42:55 TP0] Decode batch, #running-req: 4, #token: 13577, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:42:55 TP0] Decode batch, #running-req: 4, #token: 13737, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-05 14:42:56 TP0] Decode batch, #running-req: 4, #token: 13897, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:42:57 TP0] Decode batch, #running-req: 4, #token: 14057, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-05 14:42:58 TP0] Decode batch, #running-req: 4, #token: 14217, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-05 14:42:59 TP0] Decode batch, #running-req: 4, #token: 14377, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:42:59 TP0] Decode batch, #running-req: 4, #token: 14537, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-05 14:43:00 TP0] Decode batch, #running-req: 4, #token: 14697, token usage: 0.02, cuda graph: True, gen throughput (token/s): 192.40, #queue-req: 0, 
[2025-11-05 14:43:01 TP0] Decode batch, #running-req: 4, #token: 14857, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-05 14:43:02 TP0] Decode batch, #running-req: 4, #token: 15017, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-05 14:43:03 TP0] Decode batch, #running-req: 4, #token: 15177, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-05 14:43:04 TP0] Decode batch, #running-req: 4, #token: 15337, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-05 14:43:04 TP0] Decode batch, #running-req: 4, #token: 15497, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-05 14:43:05 TP0] Decode batch, #running-req: 4, #token: 15657, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-05 14:43:06 TP0] Decode batch, #running-req: 4, #token: 15817, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-05 14:43:07 TP0] Decode batch, #running-req: 4, #token: 15977, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-05 14:43:07] INFO:     127.0.0.1:50206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:07] INFO:     127.0.0.1:50218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:43:07] INFO:     127.0.0.1:50222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:07] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:07 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:43:08 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.61, #queue-req: 0, 
[2025-11-05 14:43:09 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:43:10 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:43:10 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 192.50, #queue-req: 0, 
[2025-11-05 14:43:11 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:43:12 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:43:13 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-05 14:43:14 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:43:15 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:43:15 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:43:16 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-05 14:43:17 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-05 14:43:18 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:43:19 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:43:20 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-05 14:43:20 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-05 14:43:21 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 192.80, #queue-req: 0, 
[2025-11-05 14:43:22 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-05 14:43:23 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-05 14:43:24 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-05 14:43:24] INFO:     127.0.0.1:36790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:24] INFO:     127.0.0.1:36796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:24] INFO:     127.0.0.1:36808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:43:24] INFO:     127.0.0.1:36818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-05 14:43:25 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.49, #queue-req: 0, 
[2025-11-05 14:43:25 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:43:26 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:43:27 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:43:28 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-05 14:43:29 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-05 14:43:30 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:43:30 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-05 14:43:31 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:43:32 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-05 14:43:33 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-05 14:43:34 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-05 14:43:35 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-05 14:43:35 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-05 14:43:36 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-05 14:43:37 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-05 14:43:38 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-05 14:43:39 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-05 14:43:40 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:43:40 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:43:40] INFO:     127.0.0.1:33630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:40] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:43:40] INFO:     127.0.0.1:33658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:40] INFO:     127.0.0.1:33668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:41 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:43:41 TP0] Decode batch, #running-req: 4, #token: 12938, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.67, #queue-req: 0, 
[2025-11-05 14:43:42 TP0] Decode batch, #running-req: 4, #token: 13098, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-05 14:43:43 TP0] Decode batch, #running-req: 4, #token: 13258, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:43:44 TP0] Decode batch, #running-req: 4, #token: 13418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:43:45 TP0] Decode batch, #running-req: 4, #token: 13578, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:43:45 TP0] Decode batch, #running-req: 4, #token: 13738, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-05 14:43:46 TP0] Decode batch, #running-req: 4, #token: 13898, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-05 14:43:47 TP0] Decode batch, #running-req: 4, #token: 14058, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:43:48 TP0] Decode batch, #running-req: 4, #token: 14218, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-05 14:43:49 TP0] Decode batch, #running-req: 4, #token: 14378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-05 14:43:50 TP0] Decode batch, #running-req: 4, #token: 14538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:43:50 TP0] Decode batch, #running-req: 4, #token: 14698, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:43:51 TP0] Decode batch, #running-req: 4, #token: 14858, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:43:52 TP0] Decode batch, #running-req: 4, #token: 15018, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:43:53 TP0] Decode batch, #running-req: 4, #token: 15178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-05 14:43:54 TP0] Decode batch, #running-req: 4, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-05 14:43:55 TP0] Decode batch, #running-req: 4, #token: 15498, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-05 14:43:55 TP0] Decode batch, #running-req: 4, #token: 15658, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-05 14:43:56 TP0] Decode batch, #running-req: 4, #token: 15818, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-05 14:43:57 TP0] Decode batch, #running-req: 4, #token: 15978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-05 14:43:57] INFO:     127.0.0.1:44138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:57] INFO:     127.0.0.1:44146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:43:57] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:57] INFO:     127.0.0.1:44158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:43:57 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-05 14:43:58 TP0] Decode batch, #running-req: 4, #token: 12936, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.67, #queue-req: 0, 
[2025-11-05 14:43:59 TP0] Decode batch, #running-req: 4, #token: 13096, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-05 14:44:00 TP0] Decode batch, #running-req: 4, #token: 13256, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-05 14:44:01 TP0] Decode batch, #running-req: 4, #token: 13416, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-05 14:44:01 TP0] Decode batch, #running-req: 4, #token: 13576, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-05 14:44:02 TP0] Decode batch, #running-req: 4, #token: 13736, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-05 14:44:03 TP0] Decode batch, #running-req: 4, #token: 13896, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-05 14:44:04 TP0] Decode batch, #running-req: 4, #token: 14056, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-05 14:44:05 TP0] Decode batch, #running-req: 4, #token: 14216, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-05 14:44:06 TP0] Decode batch, #running-req: 4, #token: 14376, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-05 14:44:06 TP0] Decode batch, #running-req: 4, #token: 14536, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-05 14:44:07 TP0] Decode batch, #running-req: 4, #token: 14696, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:44:08 TP0] Decode batch, #running-req: 4, #token: 14856, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-05 14:44:09 TP0] Decode batch, #running-req: 4, #token: 15016, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-05 14:44:10 TP0] Decode batch, #running-req: 4, #token: 15176, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-05 14:44:10 TP0] Decode batch, #running-req: 4, #token: 15336, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:44:11 TP0] Decode batch, #running-req: 4, #token: 15496, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-05 14:44:12 TP0] Decode batch, #running-req: 4, #token: 15656, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:44:13 TP0] Decode batch, #running-req: 4, #token: 15816, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-05 14:44:14 TP0] Decode batch, #running-req: 4, #token: 15976, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-05 14:44:14] INFO:     127.0.0.1:33756 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-05 14:44:14] INFO:     127.0.0.1:58192 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-05 14:44:31] INFO:     127.0.0.1:58696 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-05 14:44:36] INFO:     127.0.0.1:38720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:44:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:44:38] INFO:     127.0.0.1:38722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:44:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:44:38 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2.37, #queue-req: 0, 
[2025-11-05 14:44:39 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:44:40 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:44:41 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:44:42 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:44:42 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:44:43 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:44:44 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:44:45 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:44:46 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:44:46 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:44:47 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:44:48 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:44:49 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:44:50 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:44:51 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:44:51 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:44:52 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:44:53 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:44:54 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:44:55] INFO:     127.0.0.1:56724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:44:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:44:55 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-05 14:44:56 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:44:56 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:44:57 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:44:58 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:44:59 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:45:00 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:45:00 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:45:01 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:45:02 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:45:03 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:45:04 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:45:04 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:45:05 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:45:06 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:45:07 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:45:08 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:45:09 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:45:09 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:45:10 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:45:11] INFO:     127.0.0.1:39400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:45:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:45:11 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-05 14:45:12 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:45:13 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:45:14 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:45:14 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:45:15 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:45:16 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:45:17 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:45:18 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:45:18 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:45:19 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:45:20 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:45:21 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:45:22 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:45:23 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:45:23 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:45:24 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:45:25 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:45:26 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:45:27 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:45:27] INFO:     127.0.0.1:34800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:45:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:45:28 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-05 14:45:28 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:45:29 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:45:30 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:45:31 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:45:32 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:45:32 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:45:33 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:45:34 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:45:35 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:45:36 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:45:37 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:45:37 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:45:38 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:45:39 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:45:40 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:45:41 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:45:41 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:45:42 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:45:43 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:45:44] INFO:     127.0.0.1:46662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:45:44 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:45:44 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 14:45:45 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:45:46 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:45:46 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:45:47 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:45:48 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:45:49 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:45:50 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:45:51 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:45:51 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:45:52 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:45:53 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:45:54 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:45:55 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:45:55 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:45:56 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:45:57 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:45:58 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:45:59 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:46:00 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:46:00] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:46:00 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:46:00 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-05 14:46:01 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:46:02 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:46:03 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:46:04 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:46:05 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:46:05 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:46:06 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:46:07 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:46:08 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:46:09 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:46:09 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:46:10 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:46:11 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:46:12 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:46:13 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:46:14 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:46:14 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:46:15 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:46:16 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:46:17] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:46:17 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:46:17 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-05 14:46:18 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:46:19 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:46:19 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:46:20 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:46:21 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:46:22 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:46:23 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:46:23 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:46:24 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:46:25 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:46:26 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:46:27 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:46:28 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:46:28 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:46:29 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:46:30 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:46:31 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:46:32 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:46:32 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:46:33] INFO:     127.0.0.1:45300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:46:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:46:33 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-11-05 14:46:34 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:46:35 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:46:36 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:46:37 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:46:37 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:46:38 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:46:39 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:46:40 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:46:41 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:46:42 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:46:42 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:46:43 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:46:44 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:46:45 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:46:46 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:46:46 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:46:47 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:46:48 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:46:49 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:46:50] INFO:     127.0.0.1:60576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:46:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:46:50 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-11-05 14:46:51 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:46:51 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:46:52 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:46:53 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:46:54 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:46:55 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:46:56 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:46:56 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:46:57 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:46:58 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:46:59 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:47:00 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:00 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:01 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:02 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:03 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:04 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:05 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:47:05 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:47:06] INFO:     127.0.0.1:44996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:47:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:47:06 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.72, #queue-req: 0, 
[2025-11-05 14:47:07 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:47:08 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:47:09 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:47:10 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:47:10 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:47:11 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:47:12 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:47:13 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:47:14 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:47:14 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:47:15 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:16 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:17 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:47:18 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:19 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:19 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:47:20 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:21 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:22 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:47:23] INFO:     127.0.0.1:57128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:47:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:47:23 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-11-05 14:47:24 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:47:24 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:47:25 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:47:26 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:47:27 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:47:28 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:47:28 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:47:29 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:47:30 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:47:31 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:47:32 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:47:33 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:47:33 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:47:34 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:47:35 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:47:36 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:47:37 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:47:37 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:47:38 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:47:39] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:47:39 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:47:39 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-05 14:47:40 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 14:47:41 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:47:42 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:47:42 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:47:43 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:47:44 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:47:45 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:47:46 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:47:47 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:47:47 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:47:48 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:47:49 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:47:50 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:47:51 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:47:51 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:47:52 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:47:53 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:47:54 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:47:55 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:47:55] INFO:     127.0.0.1:46040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:47:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:47:56 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.41, #queue-req: 0, 
[2025-11-05 14:47:56 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:57 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:47:58 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:47:59 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:00 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:00 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:01 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:02 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:03 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:04 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:05 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:48:05 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:48:06 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:48:07 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:48:08 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:48:09 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:48:09 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:48:10 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:48:11 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:48:12] INFO:     127.0.0.1:39338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:48:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:48:12 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-05 14:48:13 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:48:14 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:48:14 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:48:15 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:48:16 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:48:17 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:18 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:19 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:48:19 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:48:20 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:21 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:22 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:48:23 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:23 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:48:24 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:48:25 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:48:26 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:48:27 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:48:28 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:48:28] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:48:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:48:28 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-05 14:48:29 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:48:30 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:48:31 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:48:32 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:48:33 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:48:33 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:48:34 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:48:35 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:48:36 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:48:37 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:37 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:48:38 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:48:39 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:48:40 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:41 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:48:42 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:48:42 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:48:43 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:48:44 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:48:45] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:48:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:48:45 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-05 14:48:46 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:48:47 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:48:47 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:48:48 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:48:49 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:48:50 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:48:51 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:48:51 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:48:52 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:48:53 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:48:54 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:48:55 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:48:56 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:48:56 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:48:57 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:48:58 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:48:59 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:49:00 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:49:00 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:49:01] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:49:01 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:49:01 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-11-05 14:49:02 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:49:03 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:49:04 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:49:05 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:49:05 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:49:06 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:49:07 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:49:08 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:49:09 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:49:10 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:49:10 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:49:11 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:49:12 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:49:13 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:49:14 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:49:14 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:49:15 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:49:16 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:49:17 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:49:18] INFO:     127.0.0.1:44754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:49:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:49:18 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-05 14:49:19 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:49:19 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:49:20 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:49:21 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:49:22 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:49:23 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:49:24 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:49:24 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:49:25 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:49:26 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:49:27 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:49:28 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:49:28 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:49:29 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:49:30 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:49:31 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:49:32 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:49:33 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:49:33 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:49:34] INFO:     127.0.0.1:37166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:49:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:49:34 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-05 14:49:35 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:49:36 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:49:37 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:49:38 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:49:38 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:49:39 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:49:40 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:49:41 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:49:42 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:49:42 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:49:43 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:49:44 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:49:45 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:49:46 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:49:47 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:49:47 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:49:48 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:49:49 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:49:50 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:49:51] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:49:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:49:51 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-05 14:49:52 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 14:49:52 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 14:49:53 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:49:54 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:49:55 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:49:56 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:49:56 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:49:57 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:49:58 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:49:59 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:50:00 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:50:01 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:50:01 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:50:02 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:50:03 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:50:04 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:50:05 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:50:05 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:50:06 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:50:07] INFO:     127.0.0.1:38778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:50:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:50:07 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-05 14:50:08 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:50:09 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:50:10 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:50:10 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:50:11 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:50:12 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:50:13 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:50:14 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:50:15 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:50:15 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:50:16 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:50:17 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:50:18 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:50:19 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:50:19 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:50:20 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:50:21 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:50:22 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:50:23 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:50:23] INFO:     127.0.0.1:57006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:50:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:50:24 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-05 14:50:24 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:50:25 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:50:26 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:50:27 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:50:28 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:50:29 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:50:29 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:50:30 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:50:31 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:50:32 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:50:33 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:50:33 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:50:34 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:50:35 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:50:36 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:50:37 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:50:38 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:50:38 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:50:39 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:50:40] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:50:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:50:40 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-05 14:50:41 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:50:42 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 14:50:43 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:50:43 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:50:44 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:50:45 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:50:46 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:50:47 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:50:47 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:50:48 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:50:49 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:50:50 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:50:51 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:50:52 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:50:52 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:50:53 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:50:54 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:50:55 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:50:56 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:50:56] INFO:     127.0.0.1:45856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:50:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:50:57 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-05 14:50:57 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:50:58 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:50:59 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:51:00 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:51:01 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:51:01 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:51:02 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:51:03 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:51:04 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:51:05 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:51:06 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:51:06 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:51:07 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:51:08 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:51:09 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:51:10 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:51:10 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:51:11 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:51:12 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:51:13] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:51:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:51:13 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-05 14:51:14 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:51:15 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:51:15 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:51:16 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:51:17 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:51:18 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:51:19 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:51:20 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:51:20 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:51:21 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:51:22 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:51:23 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:51:24 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:51:24 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:51:25 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:51:26 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:51:27 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:51:28 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:51:29 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:51:29] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:51:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:51:29 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-05 14:51:30 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:51:31 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:51:32 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:51:33 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:51:34 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:51:34 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:51:35 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:51:36 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:51:37 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:51:38 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:51:38 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:51:39 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:51:40 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:51:41 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:51:42 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:51:43 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:51:43 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:51:44 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:51:45 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:51:46] INFO:     127.0.0.1:57294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:51:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:51:46 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-05 14:51:47 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:51:47 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:51:48 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:51:49 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:51:50 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:51:51 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:51:52 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:51:52 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:51:53 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:51:54 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:51:55 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:51:56 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:51:56 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:51:57 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:51:58 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:51:59 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:00 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:01 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:01 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:02] INFO:     127.0.0.1:48774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:52:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:52:02 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-05 14:52:03 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-05 14:52:04 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 14:52:05 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:52:06 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:52:06 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:52:07 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:52:08 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:52:09 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:52:10 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:52:10 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:52:11 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:52:12 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:52:13 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:52:14 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:52:15 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:52:15 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:52:16 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:17 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:18 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:52:19] INFO:     127.0.0.1:35758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:52:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:52:19 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-05 14:52:20 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:52:20 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:52:21 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:52:22 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:52:23 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:52:24 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:24 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:25 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:52:26 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:52:27 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:28 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:29 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:52:29 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:52:30 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:52:31 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:52:32 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:52:33 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:52:33 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:52:34 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:52:35] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:52:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:52:35 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-05 14:52:36 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:52:37 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:52:38 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:52:38 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:52:39 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:40 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:52:41 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:52:42 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:52:43 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:52:43 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:52:44 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:52:45 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:52:46 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:52:47 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:52:47 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:52:48 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:52:49 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:52:50 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:52:51 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:52:52] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:52:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:52:52 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 14:52:52 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:53 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:52:54 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:55 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:52:56 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:52:57 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:52:57 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:52:58 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:52:59 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:53:00 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:53:01 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:53:01 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:53:02 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:53:03 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:53:04 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:53:05 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:53:06 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:06 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:07 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:08] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:53:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:53:08 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-05 14:53:09 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:53:10 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:53:11 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:11 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:53:12 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:13 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:14 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:15 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:15 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:53:16 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:53:17 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:53:18 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:53:19 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:53:20 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:53:20 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 14:53:21 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 14:53:22 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:53:23 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 14:53:24 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 14:53:24] INFO:     127.0.0.1:52412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:53:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:53:25 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-11-05 14:53:25 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:53:26 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:53:27 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:53:28 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:53:29 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:53:29 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:53:30 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:53:31 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:53:32 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:53:33 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:53:34 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:34 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:35 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:36 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:53:37 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:53:38 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:38 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:53:39 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:53:40 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:53:41] INFO:     127.0.0.1:40802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:53:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:53:41 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-11-05 14:53:42 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:53:43 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:53:43 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:53:44 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:53:45 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:53:46 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:47 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:53:48 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:53:48 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:49 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:50 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:53:51 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:53:52 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:53:53 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 14:53:53 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:53:54 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:53:55 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:53:56 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:53:57 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 14:53:57] INFO:     127.0.0.1:58260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:53:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:53:58 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-11-05 14:53:58 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:53:59 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:54:00 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:54:01 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:54:02 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:54:02 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:54:03 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:54:04 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:54:05 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:54:06 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:54:06 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:54:07 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:54:08 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:54:09 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:54:10 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:54:11 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:54:11 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:54:12 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:54:13 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:54:14] INFO:     127.0.0.1:45996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:54:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:54:14 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-05 14:54:15 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 14:54:16 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:54:16 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 14:54:17 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:54:18 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:54:19 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:54:20 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:54:20 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:54:21 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:54:22 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:54:23 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:54:24 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:54:25 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:54:25 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:54:26 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:54:27 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:54:28 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:54:29 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:54:29 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:54:30] INFO:     127.0.0.1:38876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:54:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:54:30 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-05 14:54:31 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:54:32 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:54:33 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:54:34 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:54:34 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:54:35 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:54:36 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:54:37 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:54:38 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:54:39 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:54:39 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:54:40 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:54:41 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:54:42 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:54:43 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:54:43 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:54:44 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:54:45 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:54:46 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:54:47] INFO:     127.0.0.1:52200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:54:47 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:54:47 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-05 14:54:48 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:54:48 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:54:49 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:54:50 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:54:51 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:54:52 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:54:53 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:54:53 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:54:54 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:54:55 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:54:56 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:54:57 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:54:57 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:54:58 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:54:59 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:55:00 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:55:01 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:55:02 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:55:02 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:55:03] INFO:     127.0.0.1:45540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:55:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:55:03 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-05 14:55:04 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:55:05 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:55:06 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:55:07 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:55:07 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:55:08 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:55:09 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:55:10 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:11 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:11 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:12 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:55:13 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:14 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:55:15 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:16 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:16 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:55:17 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:18 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:19 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:55:20] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:55:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:55:20 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-05 14:55:21 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:55:21 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:55:22 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:55:23 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:55:24 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:55:25 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:55:25 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:55:26 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:55:27 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:55:28 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:55:29 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:55:30 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:55:30 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:55:31 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:55:32 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:55:33 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:55:34 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:55:34 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:55:35 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:55:36] INFO:     127.0.0.1:48674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:55:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:55:36 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-05 14:55:37 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:55:38 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:55:39 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:55:39 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:55:40 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:55:41 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:55:42 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:55:43 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:55:44 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:55:44 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:55:45 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:46 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:47 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:48 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:48 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:49 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:55:50 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:55:51 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:55:52 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:55:52] INFO:     127.0.0.1:51690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:55:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:55:53 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-05 14:55:53 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:55:54 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:55:55 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:55:56 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:55:57 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:58 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:55:58 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:55:59 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:56:00 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:56:01 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:56:02 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:56:02 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:56:03 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:56:04 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:56:05 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:56:06 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:56:07 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:56:07 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 14:56:08 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:56:09] INFO:     127.0.0.1:41486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:56:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:56:09 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-11-05 14:56:10 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:56:11 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:56:12 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:56:12 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:56:13 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:56:14 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:56:15 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:56:16 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:56:16 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:56:17 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:56:18 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:56:19 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:56:20 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:56:21 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:56:21 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:56:22 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:56:23 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:56:24 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:56:25 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:56:25] INFO:     127.0.0.1:37382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:56:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:56:26 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 14:56:26 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:56:27 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:56:28 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:56:29 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:56:30 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:56:30 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:56:31 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:56:32 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:56:33 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:56:34 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:56:35 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:56:35 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:56:36 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:56:37 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:56:38 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:56:39 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:56:39 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:56:40 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:56:41 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:56:42] INFO:     127.0.0.1:47338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:56:42 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:56:42 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.84, #queue-req: 0, 
[2025-11-05 14:56:43 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:56:44 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:56:44 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:56:45 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:56:46 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:56:47 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:56:48 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:56:49 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:56:49 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:56:50 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:56:51 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:56:52 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:56:53 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:56:53 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:56:54 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:56:55 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:56:56 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:56:57 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:56:58 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:56:58] INFO:     127.0.0.1:43924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:56:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:56:58 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-05 14:56:59 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:57:00 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:57:01 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:57:02 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:57:03 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:57:03 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:57:04 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:57:05 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:57:06 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:57:07 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:57:07 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:57:08 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:57:09 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:57:10 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:57:11 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:57:12 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:57:12 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:57:13 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:57:14 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:57:15] INFO:     127.0.0.1:34020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:57:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:57:15 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-05 14:57:16 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-05 14:57:17 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 14:57:17 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 14:57:18 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:57:19 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:57:20 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:57:21 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:57:21 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 14:57:22 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:57:23 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:57:24 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 14:57:25 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:57:26 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:57:26 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:57:27 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:57:28 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 14:57:29 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:57:30 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:57:30 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:57:31] INFO:     127.0.0.1:54874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:57:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:57:31 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.91, #queue-req: 0, 
[2025-11-05 14:57:32 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:57:33 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 14:57:34 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:57:35 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:57:35 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:57:36 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:57:37 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:57:38 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:57:39 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:57:40 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:57:40 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:57:41 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:57:42 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:57:43 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:57:44 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:57:44 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:57:45 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:57:46 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:57:47 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:57:48] INFO:     127.0.0.1:53414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:57:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:57:48 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 14:57:49 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:57:49 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:57:50 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:57:51 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:57:52 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:57:53 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:57:54 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:57:54 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:57:55 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:57:56 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:57:57 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:57:58 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:57:58 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:57:59 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:00 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:01 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:02 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:58:03 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:58:03 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:58:04] INFO:     127.0.0.1:35396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:58:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:58:04 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-05 14:58:05 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:58:06 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:07 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:08 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:58:08 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:58:09 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:10 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:11 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:12 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:12 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:13 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:14 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:58:15 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:58:16 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:58:17 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:58:17 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:58:18 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:58:19 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 14:58:20 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:58:21] INFO:     127.0.0.1:36320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:58:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:58:21 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.81, #queue-req: 0, 
[2025-11-05 14:58:22 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 14:58:22 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:58:23 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:58:24 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:25 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:58:26 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:26 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:27 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:28 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:58:29 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:30 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:31 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:31 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:58:32 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:33 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:58:34 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:58:35 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:58:35 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:58:36 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:58:37] INFO:     127.0.0.1:36548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:58:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:58:37 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 14:58:38 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:58:39 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 14:58:40 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:58:40 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:41 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 14:58:42 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:58:43 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:44 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:45 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:45 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:46 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:58:47 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:58:48 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:58:49 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:49 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:58:50 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:58:51 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:58:52 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:58:53 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:58:53] INFO:     127.0.0.1:52168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:58:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:58:54 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 14:58:54 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:55 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:58:56 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:57 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:58:58 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:58:59 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:58:59 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:59:00 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 14:59:01 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 14:59:02 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 14:59:03 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:59:03 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:59:04 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:59:05 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:59:06 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:59:07 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 14:59:08 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:59:08 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:59:09 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:59:10] INFO:     127.0.0.1:49498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:59:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:59:10 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-05 14:59:11 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:59:12 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:59:13 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:59:13 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:59:14 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:59:15 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 14:59:16 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:59:17 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 14:59:17 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 14:59:18 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:59:19 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:59:20 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 14:59:21 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 14:59:22 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 14:59:22 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 14:59:23 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 14:59:24 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 14:59:25 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:59:26 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 14:59:26] INFO:     127.0.0.1:36348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:59:26 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:59:27 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-11-05 14:59:27 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.01, #queue-req: 0, 
[2025-11-05 14:59:28 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.01, #queue-req: 0, 
[2025-11-05 14:59:29 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.01, #queue-req: 0, 
[2025-11-05 14:59:30 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.02, #queue-req: 0, 
[2025-11-05 14:59:31 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.02, #queue-req: 0, 
[2025-11-05 14:59:31 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.01, #queue-req: 0, 
[2025-11-05 14:59:32 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-05 14:59:33 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.03, #queue-req: 0, 
[2025-11-05 14:59:34 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-05 14:59:35 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.01, #queue-req: 0, 
[2025-11-05 14:59:36 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-05 14:59:36 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-05 14:59:37 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 14:59:38 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-05 14:59:39 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 14:59:40 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:59:40 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 14:59:41 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 14:59:42 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 14:59:43] INFO:     127.0.0.1:35094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:59:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:59:43 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.41, #queue-req: 0, 
[2025-11-05 14:59:44 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:59:45 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:59:45 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 14:59:46 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 14:59:47 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 14:59:48 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 14:59:49 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 14:59:50 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 14:59:50 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 14:59:51 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 14:59:52 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 14:59:53 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 14:59:54 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 14:59:54 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 14:59:55 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 14:59:56 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 14:59:57 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 14:59:58 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-05 14:59:59 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 14:59:59] INFO:     127.0.0.1:36502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 14:59:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 14:59:59 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-11-05 15:00:00 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:00:01 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:00:02 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:00:03 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:00:04 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:00:04 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:00:05 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:00:06 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:00:07 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:00:08 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:00:08 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:00:09 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:00:10 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:00:11 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:00:12 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:00:13 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:00:13 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:00:14 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:00:15 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:00:16] INFO:     127.0.0.1:52756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:00:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:00:16 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-11-05 15:00:17 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:00:18 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:00:18 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:00:19 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:00:20 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:00:21 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:00:22 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:00:22 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:00:23 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:00:24 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:00:25 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:00:26 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:00:27 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:00:27 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:00:28 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:00:29 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:00:30 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:00:31 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:00:31 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:00:32] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:00:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:00:32 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-05 15:00:33 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:00:34 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:00:35 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:00:36 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:00:36 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:00:37 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:00:38 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:00:39 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:00:40 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:00:41 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:00:41 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:00:42 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:00:43 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:00:44 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:00:45 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:00:45 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:00:46 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:00:47 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:00:48 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:00:49] INFO:     127.0.0.1:46460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:00:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:00:49 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-11-05 15:00:50 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:00:50 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:00:51 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:00:52 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:00:53 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:00:54 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:00:55 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:00:55 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:00:56 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:00:57 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:00:58 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:00:59 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:00:59 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:01:00 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:01:01 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:01:02 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:01:03 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:01:04 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:01:04 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:01:05] INFO:     127.0.0.1:39822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:01:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:01:05 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-11-05 15:01:06 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 15:01:07 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:01:08 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:01:09 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:01:09 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:01:10 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:01:11 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:01:12 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:01:13 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:01:13 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:01:14 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:01:15 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:01:16 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:01:17 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:01:18 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:01:18 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:01:19 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:01:20 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:01:21 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:01:22] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:01:22 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:01:22 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-05 15:01:23 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:01:23 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:01:24 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:01:25 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:01:26 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:01:27 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:01:27 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:01:28 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:01:29 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:01:30 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:01:31 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:01:32 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:01:32 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:01:33 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:01:34 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:01:35 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:01:36 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:01:36 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:01:37 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:01:38] INFO:     127.0.0.1:42308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:01:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:01:38 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-05 15:01:39 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:01:40 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:01:41 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:01:41 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:01:42 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:01:43 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:01:44 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:01:45 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:01:46 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:01:46 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:01:47 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:01:48 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:01:49 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:01:50 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:01:50 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:01:51 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:01:52 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:01:53 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:01:54 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:01:55] INFO:     127.0.0.1:53772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:01:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:01:55 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.81, #queue-req: 0, 
[2025-11-05 15:01:55 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:01:56 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:01:57 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:01:58 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:01:59 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:02:00 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:02:00 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:02:01 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:02:02 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:02:03 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:02:04 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:02:04 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:02:05 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:02:06 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:02:07 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:02:08 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:02:09 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:02:09 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:02:10 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:02:11] INFO:     127.0.0.1:48886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:02:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:02:11 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-11-05 15:02:12 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:02:13 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:02:14 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:02:14 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:02:15 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:02:16 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:17 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:18 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:02:19 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:19 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:20 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:02:21 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:02:22 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:02:23 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:02:23 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:02:24 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:02:25 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:02:26 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:02:27 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:02:27] INFO:     127.0.0.1:45722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:02:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:02:28 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 15:02:28 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:02:29 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:02:30 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:02:31 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:02:32 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:02:33 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:33 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:34 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:35 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:36 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:02:37 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:02:37 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:02:38 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:02:39 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:02:40 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:02:41 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:02:42 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:02:42 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:02:43 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:02:44] INFO:     127.0.0.1:52872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:02:44 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:02:44 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-05 15:02:45 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:02:46 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:02:46 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:02:47 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:48 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:02:49 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:50 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:02:51 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:02:51 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:52 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:02:53 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:54 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:55 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:02:55 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:02:56 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:02:57 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:02:58 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:02:59 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:03:00 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:03:00] INFO:     127.0.0.1:35438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:03:00 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:03:00 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-05 15:03:01 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:03:02 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:03:03 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:03:04 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:03:05 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:03:05 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:03:06 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:03:07 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:03:08 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:03:09 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:03:10 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:03:10 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:03:11 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:03:12 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:03:13 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:03:14 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:03:14 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:03:15 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:03:16 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:03:17] INFO:     127.0.0.1:60164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:03:17 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:03:17 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-05 15:03:18 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:03:19 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:03:19 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:03:20 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:03:21 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:03:22 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:03:23 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:03:24 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:03:24 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:03:25 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:03:26 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:03:27 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:03:28 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:03:28 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:03:29 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:03:30 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:03:31 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:03:32 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:03:33 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:03:33] INFO:     127.0.0.1:35942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:03:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:03:33 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-05 15:03:34 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:03:35 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:03:36 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:03:37 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:03:38 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:03:38 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:03:39 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:03:40 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:03:41 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:03:42 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:03:42 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:03:43 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:03:44 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:03:45 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:03:46 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:03:47 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:03:47 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:03:48 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:03:49 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:03:50] INFO:     127.0.0.1:51064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:03:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:03:50 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-05 15:03:51 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:03:52 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:03:52 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:03:53 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:03:54 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:03:55 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:03:56 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:03:56 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:03:57 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:03:58 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:03:59 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:04:00 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:04:01 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:01 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:02 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:04:03 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:04:04 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:04:05 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:05 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:04:06] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:04:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:04:06 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.77, #queue-req: 0, 
[2025-11-05 15:04:07 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:04:08 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:04:09 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:04:10 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:04:10 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:04:11 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:04:12 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:04:13 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:04:14 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:04:15 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:04:15 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:04:16 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:04:17 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:04:18 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:04:19 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:04:19 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:04:20 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:21 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:22 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:23] INFO:     127.0.0.1:46606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:04:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:04:23 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 15:04:24 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:04:24 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:04:25 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:04:26 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:04:27 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:04:28 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:04:29 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:29 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:04:30 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:04:31 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:04:32 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:04:33 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:04:33 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:04:34 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:04:35 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:36 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:04:37 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:04:38 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:04:38 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:04:39] INFO:     127.0.0.1:35598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:04:39 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:04:39 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-05 15:04:40 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:04:41 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:04:42 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:04:43 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:04:43 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:04:44 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:04:45 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:46 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:04:47 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:47 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:04:48 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:04:49 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:04:50 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:51 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:04:52 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:52 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:53 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:04:54 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:04:55 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:04:56] INFO:     127.0.0.1:42674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:04:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:04:56 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-05 15:04:57 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:04:57 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:04:58 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:04:59 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:05:00 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:05:01 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:05:01 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:05:02 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:05:03 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:05:04 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:05:05 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:05:06 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:05:06 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:05:07 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:05:08 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:05:09 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:05:10 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:05:10 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:05:11 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:05:12] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:05:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:05:12 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-05 15:05:13 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:05:14 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:05:15 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:05:15 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:05:16 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:05:17 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:05:18 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:05:19 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:05:20 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:05:20 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:05:21 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:05:22 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:05:23 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:05:24 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:05:24 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:05:25 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:05:26 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:05:27 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:05:28 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:05:28] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:05:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:05:29 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 15:05:29 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:05:30 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:05:31 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:05:32 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:05:33 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:05:34 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:05:34 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:05:35 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:05:36 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:05:37 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:05:38 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:05:38 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:05:39 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:05:40 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:05:41 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:05:42 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:05:43 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:05:43 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:05:44 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:05:45] INFO:     127.0.0.1:41748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:05:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:05:45 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-11-05 15:05:46 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:05:47 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:05:48 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:05:48 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:05:49 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:05:50 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:05:51 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:05:52 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:05:52 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:05:53 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:05:54 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:05:55 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:05:56 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:05:57 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:05:57 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:05:58 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:05:59 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:06:00 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:06:01 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:06:01] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:06:01 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:06:02 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.83, #queue-req: 0, 
[2025-11-05 15:06:02 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:06:03 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:06:04 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:06:05 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:06:06 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:06:06 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:06:07 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:06:08 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:06:09 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:06:10 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:06:11 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:06:11 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:06:12 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:06:13 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:06:14 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:06:15 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:06:16 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:06:16 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:06:17 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:06:18] INFO:     127.0.0.1:43076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:06:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:06:18 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-11-05 15:06:19 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:06:20 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:06:20 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:06:21 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:06:22 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:06:23 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:06:24 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:06:25 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:06:25 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:06:26 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:06:27 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:06:28 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:06:29 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:06:30 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:06:30 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:06:31 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:06:32 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:06:33 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:06:34 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:06:34] INFO:     127.0.0.1:53406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:06:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:06:35 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 15:06:35 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:06:36 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:06:37 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:06:38 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:06:39 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:06:39 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:06:40 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:06:41 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:06:42 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:06:43 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:06:44 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:06:44 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:06:45 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:06:46 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:06:47 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:06:48 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:06:48 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:06:49 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:06:50 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:06:51] INFO:     127.0.0.1:36418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:06:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:06:51 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-05 15:06:52 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:06:53 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:06:53 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:06:54 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:06:55 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:06:56 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:06:57 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:06:58 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:06:58 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:06:59 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:07:00 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:07:01 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:07:02 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:07:02 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:07:03 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:07:04 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:07:05 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:07:06 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:07:07 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:07:07] INFO:     127.0.0.1:44442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:07:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:07:07 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-05 15:07:08 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:07:09 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:07:10 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:07:11 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:07:12 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:07:12 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:07:13 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:07:14 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:07:15 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:07:16 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:07:16 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:07:17 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:07:18 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:07:19 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:07:20 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:07:21 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:07:21 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:07:22 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:07:23 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:07:24] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:07:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:07:24 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.89, #queue-req: 0, 
[2025-11-05 15:07:25 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:07:26 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:07:26 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:07:27 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:07:28 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:07:29 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:07:30 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:07:30 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:07:31 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:07:32 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:07:33 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:07:34 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:07:35 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:07:35 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:07:36 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:07:37 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:07:38 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:07:39 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:07:39 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:07:40] INFO:     127.0.0.1:40974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:07:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:07:40 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.76, #queue-req: 0, 
[2025-11-05 15:07:41 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-05 15:07:42 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-05 15:07:43 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 15:07:44 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 15:07:44 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 15:07:45 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:07:46 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 15:07:47 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 15:07:48 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:07:49 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 15:07:49 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:07:50 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 15:07:51 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:07:52 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 15:07:53 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:07:53 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:07:54 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:07:55 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:07:56 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:07:57] INFO:     127.0.0.1:44622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:07:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:07:57 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-05 15:07:58 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:07:58 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:07:59 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:08:00 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:08:01 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:02 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:02 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:08:03 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:08:04 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:05 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:08:06 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:07 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:08:07 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:08:08 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:08:09 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:08:10 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:08:11 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:08:11 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:08:12 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:08:13] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:08:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:08:13 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-05 15:08:14 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 15:08:15 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 15:08:16 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 15:08:16 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 15:08:17 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:08:18 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:08:19 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 15:08:20 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 15:08:21 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:08:21 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:08:22 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:08:23 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:08:24 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:08:25 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:08:25 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:08:26 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:08:27 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:08:28 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:08:29 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:08:30] INFO:     127.0.0.1:46344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:08:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:08:30 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.85, #queue-req: 0, 
[2025-11-05 15:08:30 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:08:31 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:08:32 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:08:33 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:34 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:08:35 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:35 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:08:36 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:37 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:08:38 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:39 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:08:39 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:08:40 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:08:41 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:08:42 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:08:43 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:08:44 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:08:44 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:08:45 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:08:46] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:08:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:08:46 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 15:08:47 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:08:48 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:08:49 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:08:49 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:08:50 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:51 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:08:52 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:53 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:08:53 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:54 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:55 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:08:56 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:57 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:08:58 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:08:58 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:08:59 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:09:00 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:09:01 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:09:02 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:09:02] INFO:     127.0.0.1:54584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:09:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:09:03 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 15:09:03 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:09:04 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:09:05 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:09:06 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:09:07 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:09:07 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:09:08 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:09:09 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:09:10 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:09:11 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:09:12 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:09:12 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:09:13 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:09:14 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:09:15 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:09:16 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:09:16 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:09:17 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:09:18 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:09:19] INFO:     127.0.0.1:51074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:09:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:09:19 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-05 15:09:20 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:09:21 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:09:21 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:09:22 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:09:23 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:09:24 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:09:25 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:09:26 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:09:26 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:09:27 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:09:28 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:09:29 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:09:30 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:09:30 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:09:31 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:09:32 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:09:33 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:09:34 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:09:35 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:09:35] INFO:     127.0.0.1:52376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:09:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:09:35 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-11-05 15:09:36 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:09:37 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:09:38 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:09:39 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:09:40 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:09:40 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:09:41 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:09:42 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:09:43 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:09:44 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:09:44 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:09:45 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:09:46 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:09:47 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:09:48 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:09:49 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-05 15:09:49 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:09:50 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:09:51 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:09:52] INFO:     127.0.0.1:41776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:09:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:09:52 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-11-05 15:09:53 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:09:54 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:09:54 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:09:55 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:09:56 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:09:57 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:09:58 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:09:58 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:09:59 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:10:00 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:10:01 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:10:02 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:10:03 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:10:03 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:10:04 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:10:05 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:10:06 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:10:07 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:10:07 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:10:08] INFO:     127.0.0.1:37188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:10:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:10:08 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-11-05 15:10:09 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:10:10 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:10:11 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:10:12 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:10:12 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:10:13 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:10:14 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:10:15 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:10:16 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:10:17 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:10:17 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:10:18 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:10:19 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:10:20 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:10:21 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:10:21 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:10:22 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:10:23 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:10:24 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:10:25] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:10:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:10:25 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.15, #queue-req: 0, 
[2025-11-05 15:10:26 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:10:26 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:10:27 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:10:28 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:10:29 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:10:30 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:10:31 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:10:31 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:10:32 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:10:33 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:10:34 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:10:35 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:10:35 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:10:36 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:10:37 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:10:38 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:10:39 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:10:40 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:10:40 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:10:41] INFO:     127.0.0.1:51508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:10:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:10:41 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.76, #queue-req: 0, 
[2025-11-05 15:10:42 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:10:43 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:10:44 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:10:45 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:10:45 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:10:46 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:10:47 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:10:48 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:10:49 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:10:50 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:10:50 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:10:51 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:10:52 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:10:53 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:10:54 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:10:54 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:10:55 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:10:56 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:10:57 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:10:58] INFO:     127.0.0.1:38326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:10:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:10:58 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-11-05 15:10:59 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:10:59 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:11:00 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:11:01 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:11:02 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:11:03 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:11:04 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:11:04 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:11:05 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:11:06 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:11:07 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:11:08 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:11:08 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:11:09 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:11:10 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:11:11 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:11:12 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:11:13 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:11:13 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:11:14] INFO:     127.0.0.1:48346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:11:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:11:14 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-11-05 15:11:15 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:11:16 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:11:17 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:11:18 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:11:18 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:11:19 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:11:20 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:11:21 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-05 15:11:22 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:11:22 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-05 15:11:23 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:11:24 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-05 15:11:25 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-05 15:11:26 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-05 15:11:27 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-05 15:11:27 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-05 15:11:28 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-05 15:11:29 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-05 15:11:30 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-05 15:11:31] INFO:     127.0.0.1:46416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:11:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:11:31 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-11-05 15:11:32 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:11:32 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:11:33 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:11:34 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:11:35 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:11:36 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:11:36 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:11:37 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:11:38 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:11:39 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:11:40 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:11:41 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:11:41 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:11:42 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:11:43 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:11:44 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:11:45 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:11:45 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:11:46 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:11:47] INFO:     127.0.0.1:48366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:11:47 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:11:47 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-11-05 15:11:48 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-05 15:11:49 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 15:11:50 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 15:11:50 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-05 15:11:51 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 15:11:52 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 15:11:53 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-05 15:11:54 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 15:11:55 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 15:11:55 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:11:56 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:11:57 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:11:58 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 15:11:59 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:11:59 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:12:00 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:12:01 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:12:02 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:12:03 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:12:03] INFO:     127.0.0.1:46712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:12:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:12:04 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-05 15:12:04 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:12:05 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-05 15:12:06 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:12:07 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:12:08 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:12:09 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:12:09 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:12:10 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:12:11 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:12:12 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:12:13 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:12:13 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:12:14 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:12:15 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:12:16 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:12:17 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:12:18 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:12:18 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:12:19 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:12:20] INFO:     127.0.0.1:43572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:12:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:12:20 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-05 15:12:21 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:12:22 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:12:23 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-05 15:12:23 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-05 15:12:24 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:12:25 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:12:26 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:12:27 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:12:27 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:12:28 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:12:29 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:12:30 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:12:31 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:12:32 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:12:32 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:12:33 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:12:34 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:12:35 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:12:36 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:12:36] INFO:     127.0.0.1:38486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:12:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:12:37 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-05 15:12:37 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:12:38 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:12:39 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:12:40 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:12:41 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:12:41 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:12:42 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:12:43 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:12:44 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:12:45 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:12:46 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:12:46 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:12:47 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:12:48 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:12:49 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:12:50 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:12:50 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:12:51 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:12:52 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:12:53] INFO:     127.0.0.1:36134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:12:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:12:53 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-11-05 15:12:54 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:12:55 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:12:55 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:12:56 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:12:57 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:12:58 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:12:59 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:13:00 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:13:00 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:13:01 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:13:02 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:13:03 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:13:04 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:13:04 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:13:05 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:13:06 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:13:07 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:13:08 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:13:09 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:13:09] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:13:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:13:09 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.81, #queue-req: 0, 
[2025-11-05 15:13:10 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:13:11 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:13:12 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:13:13 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:13:14 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:13:14 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:13:15 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:13:16 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:13:17 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:13:18 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:13:18 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:13:19 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:13:20 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:13:21 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:13:22 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:13:23 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:13:23 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:13:24 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:13:25 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:13:26] INFO:     127.0.0.1:39330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:13:26 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:13:26 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-11-05 15:13:27 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:13:28 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:13:28 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:13:29 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:13:30 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:13:31 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:13:32 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:13:32 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:13:33 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:13:34 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:13:35 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:13:36 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:13:37 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:13:37 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:13:38 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:13:39 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:13:40 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:13:41 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-05 15:13:41 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:13:42] INFO:     127.0.0.1:32864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:13:42 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:13:42 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.74, #queue-req: 0, 
[2025-11-05 15:13:43 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:13:44 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:13:45 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:13:46 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:13:47 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:13:47 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:13:48 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:13:49 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:13:50 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:13:51 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:13:51 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:13:52 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:13:53 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:13:54 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:13:55 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:13:56 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:13:56 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:13:57 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:13:58 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:13:59] INFO:     127.0.0.1:39630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:13:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:13:59 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-11-05 15:14:00 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:14:01 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:14:01 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:14:02 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:14:03 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:14:04 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:14:05 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:14:05 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:14:06 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:14:07 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:14:08 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:14:09 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:14:10 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:14:10 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:14:11 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:14:12 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:14:13 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:14:14 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:14:14 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:14:15] INFO:     127.0.0.1:35344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:14:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:14:15 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-11-05 15:14:16 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:14:17 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:14:18 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:14:19 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:14:19 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:14:20 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:14:21 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:14:22 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:14:23 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:14:24 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:14:24 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:14:25 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:14:26 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:14:27 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:14:28 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:14:28 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:14:29 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:14:30 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:14:31 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:14:32] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:14:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:14:32 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-11-05 15:14:33 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:14:33 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:14:34 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:14:35 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:14:36 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:14:37 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:14:38 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:14:38 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:14:39 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:14:40 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:14:41 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:14:42 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:14:42 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:14:43 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:14:44 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:14:45 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:14:46 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:14:47 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:14:47 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:14:48] INFO:     127.0.0.1:42394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:14:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:14:48 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-11-05 15:14:49 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:14:50 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:14:51 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:14:52 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:14:52 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:14:53 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:14:54 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:14:55 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:14:56 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:14:56 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:14:57 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:14:58 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:14:59 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:15:00 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:15:01 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:15:01 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:15:02 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:15:03 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:15:04 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:15:05] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:15:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:15:05 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-05 15:15:06 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:15:06 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:15:07 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:15:08 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:15:09 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:15:10 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:15:10 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:15:11 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:15:12 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:15:13 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:15:14 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:15:15 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:15:15 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:15:16 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:15:17 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:15:18 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:15:19 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:15:19 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:15:20 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:15:21] INFO:     127.0.0.1:33446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:15:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:15:21 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-05 15:15:22 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:15:23 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:15:24 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:15:24 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:15:25 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:15:26 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:15:27 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:15:28 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:15:29 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:15:29 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:15:30 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:15:31 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:15:32 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:15:33 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:15:33 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:15:34 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:15:35 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:15:36 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:15:37 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:15:38] INFO:     127.0.0.1:48782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:15:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:15:38 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-11-05 15:15:38 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:15:39 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:15:40 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:15:41 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:15:42 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:15:43 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:15:43 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:15:44 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:15:45 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:15:46 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:15:47 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:15:47 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:15:48 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:15:49 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:15:50 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:15:51 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:15:52 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:15:52 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:15:53 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:15:54] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:15:54 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:15:54 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-11-05 15:15:55 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:15:56 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:15:57 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:15:57 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:15:58 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:15:59 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:16:00 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:16:01 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:16:01 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:16:02 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:16:03 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:16:04 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:16:05 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:16:06 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:16:06 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:16:07 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:16:08 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:16:09 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:16:10 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:16:10] INFO:     127.0.0.1:43126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:16:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:16:11 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-05 15:16:11 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:16:12 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:16:13 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:16:14 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:16:15 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:16:15 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:16:16 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:16:17 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:16:18 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:16:19 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:16:20 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:16:20 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:16:21 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:16:22 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:16:23 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:16:24 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:16:25 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:16:25 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:16:26 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:16:27] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:16:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:16:27 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-11-05 15:16:28 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:16:29 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:16:30 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:16:30 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:16:31 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:16:32 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:16:33 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:16:34 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:16:34 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:16:35 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:16:36 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:16:37 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:16:38 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:16:39 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:16:39 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:16:40 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:16:41 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:16:42 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:16:43 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:16:43] INFO:     127.0.0.1:45940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:16:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:16:44 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-05 15:16:44 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:16:45 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:16:46 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:16:47 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:16:48 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:16:48 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:16:49 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:16:50 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:16:51 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:16:52 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:16:53 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:16:53 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:16:54 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:16:55 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:16:56 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:16:57 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-05 15:16:57 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:16:58 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-05 15:16:59 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:17:00] INFO:     127.0.0.1:52130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:17:00 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:17:00 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-11-05 15:17:01 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:17:02 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:17:02 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:17:03 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:17:04 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:17:05 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:17:06 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:17:07 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:17:07 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:17:08 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:17:09 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:17:10 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:17:11 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:17:11 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:17:12 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:17:13 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:17:14 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:17:15 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:17:16 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:17:16] INFO:     127.0.0.1:37830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:17:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:17:16 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-11-05 15:17:17 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:17:18 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:17:19 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:17:20 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:17:21 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:17:21 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:17:22 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:17:23 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:17:24 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:17:25 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:17:25 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:17:26 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:17:27 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:17:28 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:17:29 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:17:30 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:17:30 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:17:31 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:17:32 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:17:33] INFO:     127.0.0.1:35558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:17:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:17:33 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.74, #queue-req: 0, 
[2025-11-05 15:17:34 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:17:35 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:17:35 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:17:36 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:17:37 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:17:38 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:17:39 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:17:39 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:17:40 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:17:41 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:17:42 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:17:43 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:17:44 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:17:44 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:17:45 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:17:46 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:17:47 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:17:48 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:17:48 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:17:49] INFO:     127.0.0.1:40626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:17:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:17:49 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-11-05 15:17:50 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:17:51 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:17:52 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:17:53 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:17:53 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:17:54 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:17:55 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-05 15:17:56 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:17:57 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:17:58 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:17:58 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:17:59 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:18:00 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:18:01 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:18:02 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-05 15:18:02 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:18:03 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:18:04 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-05 15:18:05 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-05 15:18:06] INFO:     127.0.0.1:53362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:18:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:18:06 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.70, #queue-req: 0, 
[2025-11-05 15:18:07 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:18:08 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:18:08 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:18:09 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:18:10 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:18:11 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:18:12 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:18:12 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:18:13 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:18:14 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:18:15 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:18:16 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:18:17 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:18:17 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:18:18 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:18:19 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:18:20 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:18:21 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:18:21 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:18:22] INFO:     127.0.0.1:49532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:18:22 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:18:22 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 15:18:23 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:18:24 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:18:25 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:18:26 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:18:26 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:18:27 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:18:28 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:18:29 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:18:30 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:18:31 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:18:31 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:18:32 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:18:33 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:18:34 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:18:35 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:18:35 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:18:36 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:18:37 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:18:38 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:18:39] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:18:39 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:18:39 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-11-05 15:18:40 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:18:40 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:18:41 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:18:42 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:18:43 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:18:44 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:18:45 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:18:45 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:18:46 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:18:47 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:18:48 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:18:49 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:18:49 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:18:50 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:18:51 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:18:52 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:18:53 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:18:54 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:18:54 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:18:55] INFO:     127.0.0.1:41542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:18:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:18:55 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-05 15:18:56 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-05 15:18:57 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:18:58 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:18:59 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:18:59 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:19:00 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:19:01 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:19:02 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:19:03 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:19:03 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:19:04 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:19:05 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:19:06 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:19:07 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:19:08 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:19:08 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:19:09 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:19:10 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:19:11 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:19:12] INFO:     127.0.0.1:35114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:19:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:19:12 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-11-05 15:19:13 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:19:13 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:19:14 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:19:15 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:19:16 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:19:17 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:19:17 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:19:18 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:19:19 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:19:20 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:19:21 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:19:22 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:19:22 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:19:23 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:19:24 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:19:25 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-05 15:19:26 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:19:26 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-05 15:19:27 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-05 15:19:28] INFO:     127.0.0.1:33672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 15:19:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 15:19:28 TP0] Decode batch, #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-11-05 15:19:29 TP0] Decode batch, #running-req: 1, #token: 3244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:19:30 TP0] Decode batch, #running-req: 1, #token: 3284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:19:31 TP0] Decode batch, #running-req: 1, #token: 3324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-05 15:19:31 TP0] Decode batch, #running-req: 1, #token: 3364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-05 15:19:32 TP0] Decode batch, #running-req: 1, #token: 3404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:19:33 TP0] Decode batch, #running-req: 1, #token: 3444, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-05 15:19:34 TP0] Decode batch, #running-req: 1, #token: 3484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:19:35 TP0] Decode batch, #running-req: 1, #token: 3524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:19:36 TP0] Decode batch, #running-req: 1, #token: 3564, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:19:36 TP0] Decode batch, #running-req: 1, #token: 3604, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-05 15:19:37 TP0] Decode batch, #running-req: 1, #token: 3644, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:19:38 TP0] Decode batch, #running-req: 1, #token: 3684, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:19:39 TP0] Decode batch, #running-req: 1, #token: 3724, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:19:40 TP0] Decode batch, #running-req: 1, #token: 3764, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:19:40 TP0] Decode batch, #running-req: 1, #token: 3804, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-05 15:19:41 TP0] Decode batch, #running-req: 1, #token: 3844, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:19:42 TP0] Decode batch, #running-req: 1, #token: 3884, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-05 15:19:43 TP0] Decode batch, #running-req: 1, #token: 3924, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-05 15:19:44 TP0] Decode batch, #running-req: 1, #token: 3964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-05 15:19:44] INFO:     127.0.0.1:58434 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-05 15:19:45] INFO:     127.0.0.1:58436 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-05 15:19:52] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-11-05 15:19:55] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
